/.curlrc:
-----------------------

--retry-max-time 60


-----------------------

result*
e2e/assets/installed/

# Building
build/
target/

# Nix-related directories.
.cargo-home/

# IDEs
.idea/
.vscode/
*~

# Mac Temporary Files
.DS_Store
**/.DS_Store

# Node Modules
**/node_modules/


-----------------------

/.github/CODEOWNERS:
-----------------------

# The whole SDK team owns the whole repo for now.
* @dfinity/dx

# The infra team can still approve Nix PRs.
**/*.nix @dfinity-lab/infra
nix/** @dfinity-lab/infra

# Allows sources.json to be updated without full approval (only CI).
nix/sources.json @ghost


-----------------------

/.github/ISSUE_TEMPLATE/bug_report.md:
-----------------------

---
name: Bug Report
about: Create a bug report for dfx.
labels: needs-triage
---
<!--
Thank you for filing a bug report! ðŸ› Please provide a short summary of the bug,
along with any information you feel relevant to replicating the bug.
-->

I tried the following:

1. <step 1>
2. <step 2>

I expected to see this happen: *explanation*

Instead, this happened: *explanation*

### Meta

`dfx --version`:
```
<version>
```

<!--
If possible, please add log output using the --verbose flag.
This will make debugging a lot easier.
-->
<details><summary>Further log output</summary>
<p>

```
<log output>
```

</p>
</details>

-----------------------

/.github/ISSUE_TEMPLATE/config.yml:
-----------------------

blank_issues_enabled: true
contact_links:
  - name: Question about dfx use
    url: https://forum.dfinity.org/
    about: Please ask and answer questions about how to use dfx in the developer forum.
  - name: Feature Request
    url: https://github.com/dfinity/sdk/discussions
    about: Please create and upvote feature requests in Discussions

-----------------------

/.github/PULL_REQUEST_TEMPLATE.md:
-----------------------

# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. 

Fixes # (issue)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration.

# Checklist:

- [ ] The title of this PR complies with [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/).
- [ ] I have edited the CHANGELOG accordingly.
- [ ] I have made corresponding changes to the documentation.


-----------------------

/.github/workflows/audit.yml:
-----------------------

name: Audit

on:
  push:
    branches:
      - master
  pull_request:
  schedule:
    # * is a special character in YAML so you have to quote this string
    - cron: "0 14 * * *"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10

jobs:
  test:
    name: audit:required
    runs-on: ubuntu-latest
    permissions:
      contents: read
      issues: write

    steps:
      - uses: actions/checkout@v4
      - uses: lwshang/audit@cargo_install_locked


-----------------------

/.github/workflows/bitcoin-canister-update.yml:
-----------------------

name: Check Bitcoin Canister Release Update

on:
  workflow_dispatch:
  schedule:
    - cron: "0 0 * * *" # Runs at UTC midnight every day

jobs:
  check-update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout dfx repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Fetch Bitcoin Canister latest release tag
        env:
          GH_TOKEN: "${{ secrets.NIV_UPDATER_TOKEN }}"
        run: |
          LATEST_TAG=$(gh release view --repo dfinity/bitcoin-canister --json tagName -q .tagName)
          echo "Latest tag is $LATEST_TAG"
          echo "LATEST_TAG=$LATEST_TAG" >> $GITHUB_ENV

      - name: Check if the latest release tag has been updated
        run: |
          URL_ENCODED_CURRENT_TAG=$(jq -r '.["ic-btc-canister"].version' nix/sources.json)
          CURRENT_TAG=$(python -c "import sys, urllib.parse as ul; print(ul.unquote_plus(sys.argv[1]))" "$URL_ENCODED_CURRENT_TAG")
          echo "Current tag is $CURRENT_TAG"
          if [[ "$CURRENT_TAG" == "$LATEST_TAG" ]]; then
            echo "No update is required."
            exit 1
          else
            echo "An update is required."
          fi

      - name: install Nix
        uses: cachix/install-nix-action@v21
        with:
          nix_path: nixpkgs=channel:nixos-unstable

      - name: install niv (dependency manager for Nix projects)
        run: nix-env -i niv -f '<nixpkgs>'

      - name: install packages from nix/sources.json
        run: niv update

      - name: update sources
        run: |
          URL_ENCODED_LATEST_TAG=$(echo -n "$LATEST_TAG" | python -c 'import sys, urllib.parse; print(urllib.parse.quote(sys.stdin.read().strip(), safe=""))')
          niv update ic-btc-canister -a version=$URL_ENCODED_LATEST_TAG
          ./scripts/write-dfx-asset-sources.sh

      - name: Update dfx to use the latest Bitcoin Canister version
        env:
          GH_TOKEN: "${{ secrets.NIV_UPDATER_TOKEN }}"
        run: |
          git config user.name github-actions
          git config user.email github-actions@github.com
          git checkout -b bot/update-bitcoin-canister/$LATEST_TAG
          git add .
          git commit -m "Update Bitcoin Canister to $LATEST_TAG"
          git push --set-upstream origin bot/update-bitcoin-canister/$LATEST_TAG
          PR_TITLE="chore: Update Bitcoin Canister Version to $LATEST_TAG"
          PR_BODY="This PR updates the Bitcoin Canister version to the latest tag: $LATEST_TAG"
          gh pr create --title "$PR_TITLE" --body "$PR_BODY" --base master --head $(git branch --show-current)


-----------------------

/.github/workflows/broadcast-frontend-hash.yml:
-----------------------

name: Broadcast Frontend Hash
on:
  workflow_dispatch:
    inputs:
      dfx_version:
        description: 'Release version of dfx'
        default: "latest"
  # release:
  #   # https://docs.github.com/en/webhooks-and-events/webhooks/webhook-events-and-payloads?actionType=released#release
  #   # "A release was published, or a pre-release was changed to a release."
  #   types: [released]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PLAYGROUND_REPO: ${{ github.repository_owner }}/motoko-playground
  FILE: service/wasm-utils/whitelisted_wasms.txt

jobs:
  update-frontend-hash:
    runs-on: ubuntu-latest
    steps:
      - name: Install sponge
        run: sudo apt-get install --yes moreutils

      - name: Checkout dfinity/sdk repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # workaround to fetch all tags: https://github.com/actions/checkout/issues/701
          path: sdk

      - name: Get new hash
        working-directory: ./sdk
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          git checkout ${{ inputs.dfx_version }}
          echo "NEW_HASH=$(shasum -a 256 src/distributed/assetstorage.wasm.gz | cut -f1 -d" ")" >> $GITHUB_ENV

      - name: Checkout dfinity/motoko-playground repo
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.NIV_UPDATER_TOKEN }}
          repository: ${{ env.PLAYGROUND_REPO }}
          path: motoko-playground

      - name: Modify Rust file
        working-directory: ./motoko-playground
        run: |
          # Below won't work on MacOS. If you want to debug it on MacOS, run:
          # > sudo docker run -ti --rm ubuntu /bin/bash
          # ubuntu> apt-get update && apt-get install --yes moreutils --fix-missing
          head -n -1 ${{ env.FILE }} | sponge ${{ env.FILE }}
          echo '    "${{ env.NEW_HASH }}", // dfx ${{ inputs.dfx_version }} frontend canister' >> ${{ env.FILE }}
          echo ']' >> ${{ env.FILE }}

      - name: Commit files, push changes, and create PR
        env:
          GH_TOKEN: ${{ secrets.NIV_UPDATER_TOKEN }}
        working-directory: ./motoko-playground
        run: |
          git config author.email "${{ github.event.sender.id }}+${{ github.event.sender.login }}@users.noreply.github.com"
          git config author.name "${{ github.event.sender.login }}"
          git config committer.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config committer.name "GitHub Actions Bot"
          git config user.email "${{ github.event.sender.id }}+${{ github.event.sender.login }}@users.noreply.github.com"
          git config user.name "${{ github.event.sender.login }}"
          git checkout -b broadcast-frontend-hash-${{ inputs.dfx_version }}
          git add ${{ env.FILE }}
          git status
          git commit -m "Update hash for dfx version ${{ inputs.dfx_version }}"
          git push origin broadcast-frontend-hash-${{ inputs.dfx_version }}
          echo "- new hash: ${{ env.NEW_HASH }}" >> pr.md
          echo "- release: https://github.com/${{ github.repository_owner }}/sdk/releases/tag/${{ inputs.dfx_version }}" >> pr.md
          gh pr create --title "chore: Whitelist frontend canister hash from dfx version ${{ inputs.dfx_version }}" \
                       --body-file pr.md \
                       --base main \
                       --head broadcast-frontend-hash-${{ inputs.dfx_version }} \
                       --repo ${{ env.PLAYGROUND_REPO }} \
                       --draft


-----------------------

/.github/workflows/build-frontend-canister-dummy.yml:
-----------------------

name: Check frontend canister build (skipped)

on:
  push:
    branches:
      - master
    paths-ignore:
      - src/distributed/assetstorage.wasm.gz
      - src/canisters/frontend/ic-certified-assets/**
      - src/canisters/frontend/ic-frontend-canister/**
  pull_request:
    paths-ignore:
      - src/distributed/assetstorage.wasm.gz
      - src/canisters/frontend/ic-certified-assets/**
      - src/canisters/frontend/ic-frontend-canister/**

jobs:
  frontend:
    runs-on: ubuntu-latest
    name: frontend-canister-up-to-date:required
    steps:
      - name: skip
        run: |
          echo skipped


-----------------------

/.github/workflows/build-frontend-canister.yml:
-----------------------

name: Check frontend canister build

on:
  push:
    branches:
      - master
    paths:
      - src/distributed/assetstorage.wasm.gz
      - src/canisters/frontend/ic-certified-assets/**
      - src/canisters/frontend/ic-frontend-canister/**
  pull_request:
    paths:
      - src/distributed/assetstorage.wasm.gz
      - src/canisters/frontend/ic-certified-assets/**
      - src/canisters/frontend/ic-frontend-canister/**

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  frontend:
    runs-on: ubuntu-latest
    name: frontend-canister-up-to-date:required
    steps:
      - name: Check out the repo
        uses: actions/checkout@v4
      - name: Build frontend canister
        run: |
          ./scripts/update-frontend-canister.sh --release-build
      - name: Artifact
        uses: actions/upload-artifact@v4
        with:
          name: assetstorage
          path: ${{ github.workspace }}/src/distributed/assetstorage.wasm.gz
      - name: Compare
        run: |
          if [ "$(git diff src/distributed/assetstorage.wasm.gz)" != "" ]
          then
            echo "src/distributed/assetstorage.wasm.gz needs to be updated; run ./scripts/update-frontend-canister.sh --release-build"
            exit 1
          fi


-----------------------

/.github/workflows/consistent-sources.yml:
-----------------------

name: Check that nix/sources.json and scripts/dfx-asset-sources.sh are consistent
on:
  pull_request:
  push:
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: cachix/install-nix-action@v18
        with:
          nix_path: nixpkgs=channel:nixos-21.05-small
      - name: Install dependencies
        run: sudo apt-get install --yes jq
      - name: Check
        run: |
          ./scripts/write-dfx-asset-sources.sh
          if [ -z "$(git status --porcelain)" ]
          then
            exit 0
          else
            git diff
            echo "scripts/dfx-asset-sources.sh is out of date.  Please run:"
            echo "    scripts/write-dfx-asset-sources.sh"
            exit 1
          fi


-----------------------

/.github/workflows/conventional-commits.yml:
-----------------------

name: PR title format
on:
  pull_request:
    types:
      - opened
      - reopened
      - edited
      - synchronize

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  check:
    name: conventional-pr-title:required
    runs-on: ubuntu-latest
    steps:
        # Conventional commit patterns:
        #   verb: description
        #   verb!: description of breaking change
        #   verb(scope): Description of change to $scope
        #   verb(scope)!: Description of breaking change to $scope
        # verb: feat, fix, ...
        # scope: refers to the part of code being changed.  E.g. " (accounts)" or " (accounts,canisters)"
        # !: Indicates that the PR contains a breaking change.
      - env:
          TITLE: ${{ github.event.pull_request.title }}
        run: |
          echo "PR title: $TITLE"
          if [[ "$TITLE" =~ ^(feat|fix|chore|build|ci|docs|style|refactor|perf|test)(\([-a-zA-Z0-9,]+\))?\!?\: ]]; then
              echo pass
          else
              echo "PR title does not match conventions"
              exit 1
          fi


-----------------------

/.github/workflows/deny.yml:
-----------------------

name: License Check

on:
  push:
    branches:
      - master
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  cargo-deny:
    name: license-check:required
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: EmbarkStudios/cargo-deny-action@v1
        with:
          command: check bans licenses sources # skip advisories, which are handled by audit.yml


-----------------------

/.github/workflows/e2e.yml:
-----------------------

name: e2e
on:
  push:
    branches:
      - master
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  changes:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: read
    outputs:
      sources: ${{ steps.filter.outputs.sources }}
    steps:
      - uses: actions/checkout@v4
        if: github.event_name == 'push'
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            sources:
              - .github/workflows/e2e.yml
              - e2e/**
              - scripts/workflows/e2e-matrix.py
              - scripts/workflows/provision-darwin.sh
              - scripts/workflows/provision-linux.sh
              - scripts/test-uis.py
              - src/canisters/frontend/**
              - src/dfx/**
              - src/dfx-core/**
              - src/distributed/**
              - src/lib/**
              - Cargo.lock
              - Cargo.toml
              - rust-toolchain.toml
  build_dfx:
    if: needs.changes.outputs.sources == 'true'
    needs: changes
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        # We build a dynamic-linked linux binary because otherwise HSM support fails with:
        #   Error: IO: Dynamic loading not supported
        os: [macos-12, ubuntu-20.04, ubuntu-22.04, windows-2022]
        include:
          - os: macos-12
            target: x86_64-apple-darwin
            binary_path: target/x86_64-apple-darwin/release/dfx
          - os: ubuntu-20.04
            target: x86_64-unknown-linux-gnu
            binary_path: target/x86_64-unknown-linux-gnu/release/dfx
          - os: ubuntu-22.04
            target: x86_64-unknown-linux-gnu
            binary_path: target/x86_64-unknown-linux-gnu/release/dfx
          - os: windows-2022
            target: x86_64-pc-windows-msvc
            binary_path: target\x86_64-pc-windows-msvc\release\dfx.exe
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Setup environment variables
        run: |
          echo "RUSTFLAGS=--remap-path-prefix=${GITHUB_WORKSPACE}=/builds/dfinity" >> $GITHUB_ENV
      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ matrix.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}-${{ hashFiles('rust-toolchain.toml') }}-1
      - name: Build
        run: |
          cargo build --target ${{ matrix.target }} --locked --release
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dfx-${{ matrix.os }}-rs-${{ hashFiles('rust-toolchain.toml') }}
          path: ${{ matrix.binary_path }}

  list_tests:
    if: needs.changes.outputs.sources == 'true'
    needs: changes
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
      - id: set-matrix
        run: echo "matrix=$(scripts/workflows/e2e-matrix.py)" >> $GITHUB_OUTPUT

  smoke:
    runs-on: ${{ matrix.os }}
    if: needs.changes.outputs.sources == 'true'
    needs: [changes, build_dfx]
    strategy:
      fail-fast: false
      matrix:
        os: [macos-12, ubuntu-20.04, ubuntu-22.04]
    steps:
      - uses: actions/checkout@v4
      - name: Download dfx binary
        uses: actions/download-artifact@v4
        with:
          name: dfx-${{ matrix.os }}-rs-${{ hashFiles('rust-toolchain.toml') }}
          path: /usr/local/bin
      - name: Setup dfx binary
        run: chmod +x /usr/local/bin/dfx
      - name: start and deploy
        run: |
          pwd
          time dfx cache install
          time dfx new smoke
          cd smoke
          time dfx start --background
          time dfx deploy
          time dfx canister call smoke_backend greet '("fire")'
          time curl --fail http://localhost:"$(dfx info webserver-port)"/sample-asset.txt?canisterId=$(dfx canister id smoke_frontend)
          time dfx stop

  test:
    runs-on: ${{ matrix.os }}
    if: needs.changes.outputs.sources == 'true'
    needs: [changes, build_dfx, list_tests]
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.list_tests.outputs.matrix)}}
    env:
      E2E_TEST: tests-${{ matrix.test }}.bash
    steps:
      - uses: actions/checkout@v4
      - name: Download dfx binary
        uses: actions/download-artifact@v4
        with:
          name: dfx-${{ matrix.os }}-rs-${{ hashFiles('rust-toolchain.toml') }}
          path: /usr/local/bin
      - name: Setup dfx binary
        run: chmod +x /usr/local/bin/dfx
      - name: Provision Darwin
        if: contains(matrix.os, 'macos')
        run: bash scripts/workflows/provision-darwin.sh || bash scripts/workflows/provision-darwin.sh
      - name: Provision Linux
        if: contains(matrix.os, 'ubuntu')
        run: bash scripts/workflows/provision-linux.sh || bash scripts/workflows/provision-linux.sh
      - name: Prepare environment
        run: |
          echo "archive=$(pwd)/e2e/archive" >> "$GITHUB_ENV"
          echo "assets=$(pwd)/e2e/assets" >> "$GITHUB_ENV"
          echo "utils=$(pwd)/e2e/utils" >> "$GITHUB_ENV"
          if [ "${{ matrix.backend == 'pocketic' || '' }}" ]; then
            echo "USE_POCKETIC=1" >> "$GITHUB_ENV"
          fi
          export
      - name: Download bats-support as a git submodule
        run: git submodule update --init --recursive
      - name: Cache mops files
        uses: actions/cache@v4
        with:
          path: |
            e2e/assets/playground_backend/.mops
          key: playground-backend-mops-${{ hashFiles('e2e/assets/playground_backend/mops.toml') }}
      - name: Run e2e test
        run: timeout 2400 bats "e2e/$E2E_TEST"

  ui_test:
    runs-on: ${{ matrix.os }}
    if: needs.changes.outputs.sources == 'true'
    needs: [changes, build_dfx]
    strategy:
      fail-fast: false
      matrix:
        os: [macos-12, ubuntu-20.04, ubuntu-22.04]
    steps:
      - name: Checking out repo
        uses: actions/checkout@v4
      - name: Setting up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.9"
      - name: Installing playwright
        run: |
          pip install playwright==1.40.0
          playwright install
          playwright install-deps
      - name: Download dfx binary
        uses: actions/download-artifact@v4
        with:
          name: dfx-${{ matrix.os }}-rs-${{ hashFiles('rust-toolchain.toml') }}
          path: /usr/local/bin
      - name: Setup dfx binary
        run: chmod +x /usr/local/bin/dfx
      - name: Deploy default dfx project
        run: |
          dfx new e2e_project
          cd e2e_project
          dfx start --background --clean
          dfx deploy 2>&1 | tee deploy.log
          echo FRONTEND_URL=$(grep "_frontend:" deploy.log | grep -Eo "(http|https)://[a-zA-Z0-9./?=_&%:-]*") >> $GITHUB_ENV
          echo CANDID_URL=$(grep "_backend:" deploy.log | grep -Eo "(http|https)://[a-zA-Z0-9./?=_&%:-]*") >> $GITHUB_ENV
      - name: Running the Python script
        run: |
          python scripts/test-uis.py \
            --frontend_url "$FRONTEND_URL" \
            --candid_url "$CANDID_URL" \
            --browser chromium firefox webkit
          export FRONTEND_URL_LOCALHOST="${FRONTEND_URL/127\.0\.0\.1/localhost}"
          export CANDID_URL_LOCALHOST="${CANDID_URL/127\.0\.0\.1/localhost}"
          python scripts/test-uis.py \
            --frontend_url "$FRONTEND_URL_LOCALHOST" \
            --candid_url "$CANDID_URL_LOCALHOST" \
            --browser chromium firefox webkit

  aggregate:
    name: e2e:required
    if: always() && needs.changes.outputs.sources == 'true'
    needs: [changes, test, smoke, ui_test]
    runs-on: ubuntu-latest
    steps:
      - name: check smoke test result
        if: ${{ needs.smoke.result != 'success' }}
        run: exit 1
      - name: check UI test result
        if: ${{ needs.ui_test.result != 'success' }}
        run: exit 1
      - name: check e2e test result
        if: ${{ needs.test.result != 'success' }}
        run: exit 1


-----------------------

/.github/workflows/fmt.yml:
-----------------------

name: Format

on: [pull_request]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  changes:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: read
    outputs:
      sources: ${{ steps.filter.outputs.sources }}
    steps:
      - uses: actions/checkout@v4
        if: github.event_name == 'push'
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            sources:
              - .github/workflows/fmt.yml
              - src/**
              - Cargo.lock
              - Cargo.toml
              - rust-toolchain.toml

  test:
    name: fmt
    if: needs.changes.outputs.sources == 'true'
    needs: changes
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}-1

      - name: Run Cargo Fmt
        run: cargo fmt --all -- --check
        env:
          RUST_BACKTRACE: 1

  aggregate:
    name: fmt:required
    if: always() && needs.changes.outputs.sources == 'true'
    needs: [changes, test]
    runs-on: ubuntu-latest
    steps:
      - name: check step result directly
        if: ${{ needs.test.result != 'success' }}
        run: exit 1


-----------------------

/.github/workflows/lint.yml:
-----------------------

name: Lint

on: [pull_request]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  changes:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: read
    outputs:
      sources: ${{ steps.filter.outputs.sources }}
    steps:
      - uses: actions/checkout@v4
        if: github.event_name == 'push'
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            sources:
              - .github/workflows/lint.yml
              - src/**
              - Cargo.lock
              - Cargo.toml
              - rust-toolchain.toml

  test:
    name: lint
    if: needs.changes.outputs.sources == 'true'
    needs: changes
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, macos-12, windows-latest ]

    steps:
      - uses: actions/checkout@v4

      - uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Run Lint
        run: cargo clippy --verbose --tests --benches --workspace -- -D warnings
        env:
          RUST_BACKTRACE: 1

  aggregate:
    name: lint:required
    if: always() && needs.changes.outputs.sources == 'true'
    needs: [changes, test]
    runs-on: ubuntu-latest
    steps:
      - name: check step result directly
        if: ${{ needs.test.result != 'success' }}
        run: exit 1


-----------------------

/.github/workflows/prepare-dfx-assets.yml:
-----------------------

name: Check dfx asset preparation
# The cargo build steps in other workflows often benefit from caching of the results of this process,
# so this workflow runs it separately.

on:
  pull_request:
  push:
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  prepare:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, macos-12 ]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
            !target/*/build/dfx-*/out/dfx-assets
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
      - name: Run build script
        run: |
          cargo check

  aggregate:
    name: prepare-dfx-assets:required
    if: ${{ always() }}
    needs: prepare
    runs-on: ubuntu-latest
    steps:
      - name: check step result directly
        if: ${{ needs.prepare.result != 'success' }}
        run: exit 1


-----------------------

/.github/workflows/publish-manifest.yml:
-----------------------

name: Publish manifest

on:
  push:
    branches:
      - master
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  publish-manifest:
    name: install-script-shellcheck:required
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install shfmt
        run: go install mvdan.cc/sh/v3/cmd/shfmt@latest
      - name: Generate
        run: |
          shellcheck --shell=sh public/install-dfxvm.sh --exclude SC2154,SC2034,SC3003,SC3014,SC3043
          ~/go/bin/shfmt -d -p -i 4 -ci -bn -s public/install-dfxvm.sh
          sed -i "s/@revision@/${GITHUB_SHA}/" public/install-dfxvm.sh
          mkdir _out
          cp public/install-dfxvm.sh _out/install.sh
          cp public/manifest.json _out/manifest.json
      - name: Upload Artifacts
        if: github.event_name == 'push'
        uses: JamesIves/github-pages-deploy-action@releases/v3
        with:
          single_commit: yes
          branch: public-manifest
          folder: _out/


-----------------------

/.github/workflows/publish.yml:
-----------------------

name: Publish

# We have to use gtar on macOS because apple's tar is literally broken.
# Yes, I know how stupid that sounds. But it's true:
# https://github.com/actions/virtual-environments/issues/2619

on:
  push:
    tags:
      - '[0-9]+.[0-9]+.[0-9]+'
      - '[0-9]+.[0-9]+.[0-9]+-[A-Za-z]+.[0-9]+'
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  build_dfx:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        # We build a dynamic-linked linux binary because otherwise HSM support fails with:
        #   Error: IO: Dynamic loading not supported
        target: [ x86_64-apple-darwin, x86_64-unknown-linux-gnu ]
        include:
          - os: macos-12
            target: x86_64-apple-darwin
            binary_path: target/x86_64-apple-darwin/release
            name: x86_64-darwin
            tar: gtar
          - os: ubuntu-20.04
            target: x86_64-unknown-linux-gnu
            binary_path: target/x86_64-unknown-linux-gnu/release
            name: x86_64-linux
            tar: tar
    steps:
      - uses: actions/checkout@v4

      - name: Setup environment variables
        run: |
          echo "RUSTFLAGS=--remap-path-prefix=${GITHUB_WORKSPACE}=/builds/dfinity" >> $GITHUB_ENV

      # GITHUB_REF_NAME will be something link 2353/merge for branch builds, which isn't great as a dfx version
      - name: Set dfx version (tag builds only)
        if: github.ref_type == 'tag'
        run: |
          echo "DFX_VERSION=$GITHUB_REF_NAME" >> $GITHUB_ENV
          echo "TARBALL_1_FILENAME=dfx-$GITHUB_REF_NAME-${{ matrix.name }}.tar.gz" >> $GITHUB_ENV
          echo "SHA256_1_FILENAME=dfx-$GITHUB_REF_NAME-${{ matrix.name }}.tar.gz.sha256" >> $GITHUB_ENV
          echo "TARBALL_2_FILENAME=dfx-${{ matrix.target }}.tar.gz" >> $GITHUB_ENV
          echo "SHA256_2_FILENAME=dfx-${{ matrix.target }}.tar.gz.sha256" >> $GITHUB_ENV

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}-${{ hashFiles('rust-toolchain.toml') }}-publish-1

      - name: Build
        run: |
          cargo clean --target ${{ matrix.target }} --release
          cargo build --target ${{ matrix.target }} --locked --release

      - name: Check dynamically-linked libraries (macos)
        run: |
          ACTUAL="$(otool -L ${{ matrix.binary_path }}/dfx | awk 'NR > 1{ print $1 }' | grep -v /System/Library/Frameworks | sort | awk -v d=" " '{s=(NR==1?s:s d)$0}END{printf "%s",s}')"
          EXPECTED="/usr/lib/libSystem.B.dylib /usr/lib/libc++.1.dylib /usr/lib/libiconv.2.dylib"
          echo "Dynamically-linked libraries:"
          echo "  Actual:   $ACTUAL"
          echo "  Expected: $EXPECTED"
          if [ "$ACTUAL" != "$EXPECTED" ]; then
              exit 1
          fi
        if: contains(matrix.os, 'macos')

      - name: Check dynamically-linked libraries (ubuntu)
        run: |
          ACTUAL="$(ldd ${{ matrix.binary_path }}/dfx | awk '{ print $1 }' | sort | awk -v d=" " '{s=(NR==1?s:s d)$0}END{printf "%s",s}')"
          EXPECTED="/lib64/ld-linux-x86-64.so.2 libc.so.6 libdl.so.2 libgcc_s.so.1 libm.so.6 libpthread.so.0 libstdc++.so.6 linux-vdso.so.1"
          echo "Dynamically-linked libraries:"
          echo "  Actual:   $ACTUAL"
          echo "  Expected: $EXPECTED"
          if [ "$ACTUAL" != "$EXPECTED" ]; then
              exit 1
          fi
        if: contains(matrix.os, 'ubuntu')

      - name: Strip binaries
        run: |
          cd ${{ matrix.binary_path }}
          sudo chown -R $(whoami) .
          strip dfx
        if: contains(matrix.os, 'ubuntu')

      - name: Create tarball of binaries and sha256 of tarball
        if: github.ref_type == 'tag'
        run: |
          mkdir dfx-${{ matrix.target }}
          cp ${{ matrix.binary_path }}/dfx dfx-${{ matrix.target }}
          cp LICENSE dfx-${{ matrix.target }}
          ${{ matrix.tar }} -zc -f ${{ env.TARBALL_2_FILENAME }} dfx-${{ matrix.target }}
          shasum -a 256 ${{ env.TARBALL_2_FILENAME }} > ${{ env.SHA256_2_FILENAME }}
          shasum -c ${{ env.SHA256_2_FILENAME }}

          ${{ matrix.tar }} -zcC ${{ matrix.binary_path }} -f ${{ env.TARBALL_1_FILENAME }} dfx
          shasum -a 256 ${{ env.TARBALL_1_FILENAME }} > $SHA256_1_FILENAME
          shasum -c $SHA256_1_FILENAME

      - name: Upload Artifacts
        if: github.ref_type == 'tag'
        uses: actions/upload-artifact@v4
        with:
          name: dfx-artifacts-${{ hashFiles('rust-toolchain.toml') }}-${{ matrix.name }}
          path: |
            ${{ env.TARBALL_1_FILENAME }}
            ${{ env.SHA256_1_FILENAME }}
            ${{ env.TARBALL_2_FILENAME }}
            ${{ env.SHA256_2_FILENAME }}

  aggregate:
    name: publishable:required
    if: ${{ always() }}
    needs: [build_dfx]
    runs-on: ubuntu-latest
    steps:
      - name: check build result
        if: ${{ needs.build_dfx.result != 'success' }}
        run: exit 1

  publish:
    runs-on: ubuntu-latest
    if: github.ref_type == 'tag'
    needs: build_dfx
    strategy:
      fail-fast: false
      matrix:
        name: [ 'x86_64-darwin', 'x86_64-linux' ]
    steps:
      - uses: actions/checkout@v4

      - name: Setup environment variables
        run: echo "VERSION=$GITHUB_REF_NAME" >> $GITHUB_ENV

      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          name: dfx-artifacts-${{ hashFiles('rust-toolchain.toml') }}-${{ matrix.name }}

      - name: Upload tarball and sha256
        uses: svenstaro/upload-release-action@v2
        with:
          repo_token: ${{ secrets.GITHUB_TOKEN }}
          file: dfx-*.tar.*
          file_glob: true
          tag: ${{ env.VERSION }}
          prerelease: true
          make_latest: false


-----------------------

/.github/workflows/shellcheck.yml:
-----------------------

name: Check shell scripts
on:
  pull_request:
    paths:
      - 'e2e/**'
      - '.github/**'
      - 'scripts/release.sh'
      - 'scripts/prepare-dfx-assets.sh'
      - 'scripts/update-frontend-canister.sh'
  push:
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  shellcheck:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Check e2e scripts
        run: shellcheck e2e/**/*.*sh
      - name: Check scripts/
        run: shellcheck scripts/*.sh



-----------------------

/.github/workflows/unit.yml:
-----------------------

name: Check cargo (unit) test
on:
  pull_request:
  push:
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  changes:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: read
    outputs:
      sources: ${{ steps.filter.outputs.sources }}
    steps:
      - uses: actions/checkout@v4
        if: github.event_name == 'push'
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            sources:
              - .github/workflows/unit.yml
              - src/**
              - Cargo.lock
              - Cargo.toml
              - rust-toolchain.toml

  test:
    if: needs.changes.outputs.sources == 'true'
    needs: changes
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, macos-12 ]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
      - name: Check cargo test
        run: cargo test --workspace --all-features --no-fail-fast

  aggregate:
    name: unit:required
    if: always() && needs.changes.outputs.sources == 'true'
    needs: [changes, test]
    runs-on: ubuntu-latest
    steps:
      - name: check unit test result
        if: ${{ needs.test.result != 'success' }}
        run: exit 1


-----------------------

/.github/workflows/update-docs.yml:
-----------------------

name: Ensure JSON schema docs are up-to-date
on: pull_request

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  check:
    name: json-schema-docs-up-to-date:required
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check cargo build
        run: cargo build
      - name: Show download worked
        run: cargo run -- --version
      - name: Check for changes
        run: |
          cargo run -- schema --outfile docs/dfx-json-schema.json
          cargo run -- schema --for networks --outfile docs/networks-json-schema.json
          cargo run -- schema --for dfx-metadata --outfile docs/dfx-metadata-schema.json
          cargo run -- schema --for extension-manifest --outfile docs/extension-manifest-schema.json
          cargo run -- schema --for extension-dependencies --outfile docs/extension-dependencies-schema.json
          cargo run -- schema --for extension-catalog --outfile docs/extension-catalog-schema.json

          echo "JSON Schema changes:"
          if git diff --exit-code ; then
            echo "(None)"
          else
            echo
            echo "There are code changes in this PR that the JSON schema should reflect,"
            echo "but the JSON schema docs have not been updated."
            echo
            echo "Run the above commands locally to update the JSON schema docs."
            exit 1
          fi


-----------------------

/.github/workflows/update-motoko.yml:
-----------------------

name: "chore: Update Motoko"
on: 
  workflow_dispatch:
    inputs:
      motokoVersion:
        description: 'Motoko version'
        default: "latest"
      sdkBranch:
        description: 'Open PR against this sdk branch'
        default: "master"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  GH_API_RELEASES_LATEST: "https://api.github.com/repos/dfinity/motoko/releases/latest"
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  update-motoko:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        ref: ${{ github.event.inputs.sdkBranch }}

    - name: determine Motoko version
      run: |
        if [ '${{ github.event.inputs.motokoVersion }}' = 'latest' ]; then
          echo "MOTOKO_VERSION=$(curl -s "${{ env.GH_API_RELEASES_LATEST }}" | jq  -r '.tag_name')" >> $GITHUB_ENV
        else 
          echo "MOTOKO_VERSION=${{ github.event.inputs.motokoVersion }}" >> $GITHUB_ENV
        fi
        grep -s "MOTOKO_VERSION" $GITHUB_ENV
   
    - name: install Nix
      uses: cachix/install-nix-action@v21
      with:
        nix_path: nixpkgs=channel:nixos-unstable

    - name: install niv (dependency manager for Nix projects)
      run: nix-env -i niv -f '<nixpkgs>'

    - name: install packages from nix/sources.json
      run: niv update

    - name: update Motoko
      run: |
        echo "updating Motoko"
        scripts/update-motoko.sh ${{ env.MOTOKO_VERSION }} 

    - name: setup git config, then create new branch and push new commit to it
      run: |
        git config author.email "${{ github.event.sender.id }}+${{ github.event.sender.login }}@users.noreply.github.com"
        git config author.name "${{ github.event.sender.login }}"
        git config committer.email "41898282+github-actions[bot]@users.noreply.github.com"
        git config committer.name "GitHub Actions Bot"
        git config user.email "${{ github.event.sender.id }}+${{ github.event.sender.login }}@users.noreply.github.com"
        git config user.name "${{ github.event.sender.login }}"
        git checkout -b chore-update-motoko-${{ env.MOTOKO_VERSION }} 
        git add .
        git commit -m "chore: update Motoko version to ${{ env.MOTOKO_VERSION }}"
        git push origin chore-update-motoko-${{ env.MOTOKO_VERSION }} 
      
    - name: create Pull Request, with CHANGELOG.md entry suggestion 
      uses: actions/github-script@v6
      with:
        github-token: ${{ secrets.NIV_UPDATER_TOKEN }} # act on behalf of https://github.com/dfinity-bot 
        script: |
          const { repo, owner } = context.repo;

          const pr_create_result = await github.rest.pulls.create({
            title: `chore: update Motoko version to ${{ env.MOTOKO_VERSION }}`,
            owner,
            repo,
            head: 'chore-update-motoko-${{ env.MOTOKO_VERSION }}',
            base: '${{ github.event.inputs.sdkBranch }}',
            body: [
              `## Suggested [CHANGELOG.md](https://github.com/${owner}/${repo}/edit/chore-update-motoko-${{ env.MOTOKO_VERSION }}/CHANGELOG.md) changes`,
              '```',
              '## Dependencies',
              '',
              '### Motoko',
              '',
              'Updated Motoko to [${{ env.MOTOKO_VERSION }}](https://github.com/dfinity/motoko/releases/tag/${{ env.MOTOKO_VERSION }})'
            ].join('\n')
          });
          github.rest.issues.addLabels({
            owner,
            repo,
            issue_number: pr_create_result.data.number,
            labels: ['chore']
          });
          core.startGroup('new PR JSON object');
          core.info(JSON.stringify(pr_create_result, null, 2));
          core.endGroup();


-----------------------

/.github/workflows/update-replica-version.yml:
-----------------------

name: "chore: Update replica"
on: 
  workflow_dispatch:
    inputs:
      replicaVersionLatestOrCustom:
        description: 'use latest replica version, or provide custom revision'
        type: choice
        default: 'latest'
        options:
        - latest
        - custom
      customReplicaVersion:
        description: 'dfinity/ic commit SHA - get the latest Elect Replica Version from https://dashboard.internetcomputer.org/releases'     
        default: "required if custom"
      sdkBranch:
        description: 'Open PR against this sdk branch'
        default: "master"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  IC_RELEASES_API: "https://ic-api.internetcomputer.org/api/v3/subnet-replica-versions?limit=50&offset=0"
  # When getting Rust dependencies, retry on network error:
  CARGO_NET_RETRY: 10
  # Use the local .curlrc
  CURL_HOME: .

jobs:
  update-replica:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        ref: ${{ github.event.inputs.sdkBranch }}

    - name: determine replica commit sha
      run: |
        if [ '${{ github.event.inputs.replicaVersionLatestOrCustom }}' = 'latest' ]; then
          echo "REPLICA_VERSION=$(curl -s "${{ env.IC_RELEASES_API }}" | jq  -r '.data[0].replica_version_id')" >> $GITHUB_ENV
        else 
          echo "REPLICA_VERSION=${{ github.event.inputs.customReplicaVersion }}" >> $GITHUB_ENV
        fi
        grep -s "REPLICA_VERSION" $GITHUB_ENV
   
    - name: install Nix
      uses: cachix/install-nix-action@v21
      with:
        nix_path: nixpkgs=channel:nixos-unstable

    - name: install niv (dependency manager for Nix projects)
      run: nix-env -i niv -f '<nixpkgs>'

    - name: install packages from nix/sources.json
      run: niv update

    - name: update replica
      run: |
        echo "updating the replica"
        scripts/update-replica.sh ${{ env.REPLICA_VERSION }} 

    - name: setup git config, then create new branch and push new commit to it
      run: |
        git config author.email "${{ github.event.sender.id }}+${{ github.event.sender.login }}@users.noreply.github.com"
        git config author.name "${{ github.event.sender.login }}"
        git config committer.email "41898282+github-actions[bot]@users.noreply.github.com"
        git config committer.name "GitHub Actions Bot"
        git config user.email "${{ github.event.sender.id }}+${{ github.event.sender.login }}@users.noreply.github.com"
        git config user.name "${{ github.event.sender.login }}"
        git checkout -b chore-update-replica-${{ env.REPLICA_VERSION }}-${{ github.event.inputs.sdkBranch }}
        git add .
        git commit -m "chore: update replica version to ${{ env.REPLICA_VERSION }}"
        git push origin chore-update-replica-${{ env.REPLICA_VERSION }}-${{ github.event.inputs.sdkBranch }}
      
    - name: create Pull Request, with CHANGELOG.md entry suggestion 
      uses: actions/github-script@v6
      with:
        github-token: ${{ secrets.NIV_UPDATER_TOKEN }} # act on behalf of https://github.com/dfinity-bot 
        script: |
          const { repo, owner } = context.repo;

          let latest_dfx_release = await github.rest.repos.getLatestRelease({ owner, repo });
          core.startGroup('latest dfx release');
          core.info(JSON.stringify(latest_dfx_release, null, 2));
          core.endGroup();

          const re = /replica version used: ([a-f0-9]+)/g;
          let latest_release_replica_version;
          try {
            latest_release_replica_version = re.exec(latest_dfx_release.data.body)[1];
            core.info(`latest_release_replica_version = ${latest_release_replica_version}`);
          } catch {
            latest_release_replica_version = "";
            core.warning("the phrase \"replica version used: <SHA>\" has not been found in latest GitHub Release");
          }

          let elected_replicas = await github.request("GET ${{ env.IC_RELEASES_API }}");
          core.startGroup('elected_replicas fetched from ic-api.internetcomputer.org');
          core.info(JSON.stringify(elected_replicas, null, 2));
          core.endGroup();

          let idx_start = elected_replicas.data.data.findIndex(el => el.replica_version_id === "${{ env.REPLICA_VERSION }}");
          let idx_end = elected_replicas.data.data.findIndex(el => el.replica_version_id === latest_release_replica_version);
          core.info(`idx_start:idx_end is ${idx_start}:${idx_end}`);
          let new_proposals_since_last_release = elected_replicas.data.data.slice(idx_start, idx_end);
          core.startGroup('new proposals since last release');
          core.info(JSON.stringify(new_proposals_since_last_release, null, 2));
          core.endGroup();

          const new_replica_sha__short = "${{ env.REPLICA_VERSION }}".substring(0, 8);
          const pr_create_result = await github.rest.pulls.create({
            title: `chore: update replica version to ${new_replica_sha__short}`,
            owner,
            repo,
            head: 'chore-update-replica-${{ env.REPLICA_VERSION }}-${{ github.event.inputs.sdkBranch }}',
            base: '${{ github.event.inputs.sdkBranch }}',
            body: [
              `## Suggested [CHANGELOG.md](https://github.com/${owner}/${repo}/edit/chore-update-replica-${{ env.REPLICA_VERSION }}-${{ github.event.inputs.sdkBranch }}/CHANGELOG.md) changes`,
              '```',
              '## Dependencies',
              '',
              '### Replica',
              '',
              'Updated replica to elected commit ${{ env.REPLICA_VERSION }}.',
              'This incorporates the following executed proposals:',
              '',
              new_proposals_since_last_release.map(el => `- [${el.proposal_id}](https://dashboard.internetcomputer.org/proposal/${el.proposal_id})`).join('\n'),
              '```',
              '## Previous replica version',
              `\`${latest_release_replica_version}\``
            ].join('\n')
          });
          github.rest.issues.addLabels({
            owner,
            repo,
            issue_number: pr_create_result.data.number,
            labels: ['chore']
          });
          core.startGroup('new PR JSON object');
          core.info(JSON.stringify(pr_create_result, null, 2));
          core.endGroup();


-----------------------

/.gitignore:
-----------------------

result*
e2e/assets/installed/
e2e/tests-dfx/.bats/

# Building
build/
target/

# Nix-related directories.
.cargo-home/

# IDEs
.idea/
.vscode/
*~

# Mac Temporary Files
.DS_Store
**/.DS_Store

# Node Modules
**/node_modules/

# Environment Variables
.env

# Backup files
*.bak

# DB repo for cargo-audit
advisory-db/

# mops
**/.mops/


-----------------------

/.gitmodules:
-----------------------

[submodule "e2e/utils/bats-support"]
	path = e2e/utils/bats-support
	url = https://github.com/bats-core/bats-support.git


-----------------------

/.npmrc:
-----------------------

fetch-retries=10


-----------------------

/CHANGELOG.md:
-----------------------

/CONTRIBUTING.md:
-----------------------

# Contributing to DFX

## Guidelines
We welcome your contributions to the Internet Computer SDK! Please note the following guidelines that will help
ensure your work is incorporated with the least friction possible

1. PRs with a large footprint should start as a discussion on GitHub first so that the approach can be fleshed out and agreed upon with members of the SDK team. [Start a discussion](https://github.com/dfinity/sdk/discussions/new?category=feature-requests).
2. PRs should generally be small and testable and include unit and/or e2e tests.
3. PRs should be void of any TODO comments, dead code, or remnants of failed experiments, etc. We will be happy to help get your PR into a state that would meet our standards during the review process. 

## Developing `dfx`

Use `cargo` with the relevant [Rust toolchain](../rust-toolchain.toml) like any other Rust project.

``` bash
sdk $ cargo build
sdk $ cargo test
```

Then use `cargo run` or the method of your choice to run `dfx`. Dfx will display the version `dfx 0.9.1-63-gd7020bbb`,
even if you are compiling from the latest master.

``` bash
sdk $ alias dfx=$(pwd)/target/debug/dfx
sdk $ dfx --version
dfx 0.9.1-63-gd7020bbb
```

## Developing `ic-certified-assets`

When you change `ic-certified-assets` code (or `ic-frontend-canister`), you must update the canister binary stored in `src/distributed`.
This is done via `./scripts/update-frontend-canister.sh --release-build`. You need to have Docker (buildx >=v0.8) installed and running.

Then update `CHANGELOG.md` to include the newest sha2 hash (*not* git commit hash) and a link to your PR.
If there is already an entry in the 'Unreleased' section, change it; if not, add a new one. See the example in [#2699](https://github.com/dfinity/sdk/pull/2699/commits/c191ce5ac529de4499c50a0d2bc70ac6a3cb3afc). This step can be automated by running `./scripts/update-frontend-canister.sh --changelog`.

## End-to-End Tests

#### Setup

1. Install `bats`, `jq` and `sponge`. See the CI provisioning scripts for examples (search for the line with the comment `# Install Bats + moreutils.`):
    - [Linux](./scripts/workflows/provision-linux.sh)
    - [Darwin](./scripts/workflows/provision-darwin.sh)
1. Download `bats-support` (which is included as this repository' submodule) 
    ```bash
    git submodule update --init --recursive
    ```
1. Build dfx and add its target directory to your path:
    ``` bash
    sdk $ cargo build
    sdk $ export PATH="$(pwd)/target/debug:$PATH"
    ```
1. Set up the environment variables required by the e2e tests:
    ``` bash
    export archive="$(pwd)/e2e/archive"
    export utils="$(pwd)/e2e/utils"
    export assets="$(pwd)/e2e/assets"
    ```

#### Running End-to-End Tests

``` bash
sdk $ bats e2e/tests-dfx/*.bash
sdk $ bats e2e/tests-replica/*.bash
```

#### Running End-to-End Tests Against Reference IC

This runs the end-to-end tests against the
[PocketIC emulator](https://github.com/dfinity/pocketic).

``` bash
sdk $ USE_POCKETIC=1 bats e2e/tests-dfx/*.bash
sdk $ USE_POCKETIC=1 bats e2e/tests-replica/*.bash
```

## Conventional Commits

We use a squash & merge PR strategy, which means that each PR will result in exactly
one commit in `master`. When releasing, we are using the history to know which commits
and what messages make into the release notes (and what needs to be documented).

That means we enforce conventional commits to help us distinguish those commits. When
creating a PR, we have a special check that validate the PR title and will fail if it
doesn't follow the conventional commit standard (see
https://www.conventionalcommits.org/).

What that means is your PR title should start with one of the following prefix:

- `feat:`. Your PR implement a new feature and should be documented. If version numbers
  were following semver, this would mean that we need to put the PR in the next minor.
- `fix:`. Your PR fixes an issue. There should be a link to the issue being fixed.
  In SemVer, this would be merged in both minor and patch branches.
- `refactor:`, `chore:`, `build:`, `docs:`, `test:` does not affect the release notes
  and will be ignored.
- `release:`. Your PR is for tagging a release and should be ignored, but will be
  a break point for the log history when doing release notes.

## Dependencies

### Updating the Replica

#### Using GitHub Action
- Head over to [the GitHub Action](https://github.com/dfinity/sdk/actions/workflows/update-replica-version.yml).
- Click "Run workflow" button, choose appropriate options (you're probably fine with using defaults), and click "Run workflow" (the green one). 
- Depending on the selected options, the workflow will run anything between 3 to 35 minutes. After that time, a new PR will be created.
- The PR contains the content that needs to be pasted into CHANGELOG.md, as well as the link for editing the CHANGELOG.md directly on the branch of that PR.
- After making changes to the CHANGELOG.md file, PR is ready for review.

#### Locally
To update the replica to a given $SHA from the dfinity repo, execute the following:
``` bash
# Requires niv to run. To install niv, run nix-env -iA nixpkgs.niv
./scripts/update-replica.sh $SHA
```

### Updating Motoko

To update Motoko to a given $VERSION from the motoko and motoko-base repos, run the [the GitHub Action](https://github.com/dfinity/sdk/actions/workflows/update-motoko.yml).


You can also execute the following locally:
``` bash
# Requires niv to run. To install niv, run nix-env -iA nixpkgs.niv
./scripts/update-motoko.sh $VERSION
```

### Licenses

[Latest licenses of all dependencies of dfx (build for x86_64-linux)](https://hydra.oregon.dfinity.build/latest/dfinity-ci-build/sdk/licenses.dfx.x86_64-linux/licenses.dfinity-sdk-dfx.html).


-----------------------

/Cargo.lock:
-----------------------

# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 3

[[package]]
name = "abnf"
version = "0.12.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "33741baa462d86e43fdec5e8ffca7c6ac82847ad06cbfb382c1bdbf527de9e6b"
dependencies = [
 "abnf-core",
 "nom",
]

[[package]]
name = "abnf-core"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c44e09c43ae1c368fb91a03a566472d0087c26cf7e1b9e8e289c14ede681dd7d"
dependencies = [
 "nom",
]

[[package]]
name = "abnf_to_pest"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "939d59666dd9a7964a3a5312b9d24c9c107630752ee64f2dd5038189a23fe331"
dependencies = [
 "abnf",
 "indexmap 1.9.3",
 "itertools 0.10.5",
 "pretty 0.11.3",
]

[[package]]
name = "actix"
version = "0.13.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fb72882332b6d6282f428b77ba0358cb2687e61a6f6df6a6d3871e8a177c2d4f"
dependencies = [
 "actix-macros",
 "actix-rt",
 "actix_derive",
 "bitflags 2.5.0",
 "bytes",
 "crossbeam-channel",
 "futures-core",
 "futures-sink",
 "futures-task",
 "futures-util",
 "log",
 "once_cell",
 "parking_lot 0.12.2",
 "pin-project-lite",
 "smallvec",
 "tokio",
 "tokio-util",
]

[[package]]
name = "actix-macros"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e01ed3140b2f8d422c68afa1ed2e85d996ea619c988ac834d255db32138655cb"
dependencies = [
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "actix-rt"
version = "2.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "28f32d40287d3f402ae0028a9d54bef51af15c8769492826a69d28f81893151d"
dependencies = [
 "futures-core",
 "tokio",
]

[[package]]
name = "actix_derive"
version = "0.6.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7c7db3d5a9718568e4cf4a537cfd7070e6e6ff7481510d0237fb529ac850f6d3"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "addr2line"
version = "0.21.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8a30b2e23b9e17a9f90641c7ab1549cd9b44f296d3ccbf309d2863cfe398a0cb"
dependencies = [
 "gimli 0.28.1",
]

[[package]]
name = "adler"
version = "1.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f26201604c87b1e01bd3d98f8d5d9a8fcbb815e8cedb41ffccbeb4bf593a35fe"

[[package]]
name = "adler32"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "aae1277d39aeec15cb388266ecc24b11c80469deae6067e17a1a7aa9e5c1f234"

[[package]]
name = "aead"
version = "0.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d122413f284cf2d62fb1b7db97e02edb8cda96d769b16e443a4f6195e35662b0"
dependencies = [
 "crypto-common",
 "generic-array",
]

[[package]]
name = "aes"
version = "0.7.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9e8b47f52ea9bae42228d07ec09eb676433d7c4ed1ebdf0f1d1c29ed446f1ab8"
dependencies = [
 "cfg-if",
 "cipher 0.3.0",
 "cpufeatures",
 "opaque-debug",
]

[[package]]
name = "aes"
version = "0.8.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b169f7a6d4742236a0a00c541b845991d0ac43e546831af1249753ab4c3aa3a0"
dependencies = [
 "cfg-if",
 "cipher 0.4.4",
 "cpufeatures",
]

[[package]]
name = "aes-gcm"
version = "0.10.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "831010a0f742e1209b3bcea8fab6a8e149051ba6099432c8cb2cc117dec3ead1"
dependencies = [
 "aead",
 "aes 0.8.4",
 "cipher 0.4.4",
 "ctr",
 "ghash",
 "subtle",
]

[[package]]
name = "ahash"
version = "0.7.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "891477e0c6a8957309ee5c45a6368af3ae14bb510732d2684ffa19af310920f9"
dependencies = [
 "getrandom",
 "once_cell",
 "version_check",
]

[[package]]
name = "ahash"
version = "0.8.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e89da841a80418a9b391ebaea17f5c112ffaaa96f621d2c285b5174da76b9011"
dependencies = [
 "cfg-if",
 "once_cell",
 "version_check",
 "zerocopy",
]

[[package]]
name = "aho-corasick"
version = "1.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
dependencies = [
 "memchr",
]

[[package]]
name = "alloc-no-stdlib"
version = "2.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cc7bb162ec39d46ab1ca8c77bf72e890535becd1751bb45f64c597edb4c8c6b3"

[[package]]
name = "alloc-stdlib"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "94fb8275041c72129eb51b7d0322c29b8387a0386127718b096429201a5d6ece"
dependencies = [
 "alloc-no-stdlib",
]

[[package]]
name = "allocator-api2"
version = "0.2.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c6cb57a04249c6480766f7f7cef5467412af1490f8d1e243141daddada3264f"

[[package]]
name = "android-tzdata"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e999941b234f3131b00bc13c22d06e8c5ff726d1b6318ac7eb276997bbb4fef0"

[[package]]
name = "android_system_properties"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "819e7219dbd41043ac279b19830f2efc897156490d7fd6ea916720117ee66311"
dependencies = [
 "libc",
]

[[package]]
name = "annotate-snippets"
version = "0.9.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ccaf7e9dfbb6ab22c82e473cd1a8a7bd313c19a5b7e40970f3d89ef5a5c9e81e"
dependencies = [
 "unicode-width",
]

[[package]]
name = "anstream"
version = "0.6.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "418c75fa768af9c03be99d17643f93f79bbba589895012a80e3452a19ddda15b"
dependencies = [
 "anstyle",
 "anstyle-parse",
 "anstyle-query",
 "anstyle-wincon",
 "colorchoice",
 "is_terminal_polyfill",
 "utf8parse",
]

[[package]]
name = "anstyle"
version = "1.0.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "038dfcf04a5feb68e9c60b21c9625a54c2c0616e79b72b0fd87075a056ae1d1b"

[[package]]
name = "anstyle-parse"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c03a11a9034d92058ceb6ee011ce58af4a9bf61491aa7e1e59ecd24bd40d22d4"
dependencies = [
 "utf8parse",
]

[[package]]
name = "anstyle-query"
version = "1.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a64c907d4e79225ac72e2a354c9ce84d50ebb4586dee56c82b3ee73004f537f5"
dependencies = [
 "windows-sys 0.52.0",
]

[[package]]
name = "anstyle-wincon"
version = "3.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "61a38449feb7068f52bb06c12759005cf459ee52bb4adc1d5a7c4322d716fb19"
dependencies = [
 "anstyle",
 "windows-sys 0.52.0",
]

[[package]]
name = "anyhow"
version = "1.0.83"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "25bdb32cbbdce2b519a9cd7df3a678443100e265d5e25ca763b7572a5104f5f3"

[[package]]
name = "apply-patch"
version = "0.1.0"
dependencies = [
 "patch",
 "thiserror",
]

[[package]]
name = "arbitrary"
version = "1.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7d5a26814d8dcb93b0e5a0ff3c6d80a8843bafb21b39e8e18a6f05471870e110"

[[package]]
name = "argon2"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "db4ce4441f99dbd377ca8a8f57b698c44d0d6e712d8329b5040da5a64aa1ce73"
dependencies = [
 "base64ct",
 "blake2",
 "password-hash",
]

[[package]]
name = "arrayvec"
version = "0.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "23b62fc65de8e4e7f52534fb52b0f3ed04746ae267519eef2a83941e8085068b"

[[package]]
name = "arrayvec"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "96d30a06541fbafbc7f82ed10c06164cfbd2c401138f6addd8404629c4b16711"

[[package]]
name = "ascii-canvas"
version = "3.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8824ecca2e851cec16968d54a01dd372ef8f95b244fb84b84e70128be347c3c6"
dependencies = [
 "term",
]

[[package]]
name = "assert-json-diff"
version = "2.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "47e4f2b81832e72834d7518d8487a0396a28cc408186a2e8854c0f98011faf12"
dependencies = [
 "serde",
 "serde_json",
]

[[package]]
name = "assert_matches"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9b34d609dfbaf33d6889b2b7106d3ca345eacad44200913df5ba02bfd31d2ba9"

[[package]]
name = "async-io"
version = "1.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0fc5b45d93ef0529756f812ca52e44c221b35341892d3dcc34132ac02f3dd2af"
dependencies = [
 "async-lock 2.8.0",
 "autocfg",
 "cfg-if",
 "concurrent-queue",
 "futures-lite",
 "log",
 "parking",
 "polling",
 "rustix 0.37.27",
 "slab",
 "socket2 0.4.10",
 "waker-fn",
]

[[package]]
name = "async-lock"
version = "2.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "287272293e9d8c41773cec55e365490fe034813a2f172f502d6ddcf75b2f582b"
dependencies = [
 "event-listener 2.5.3",
]

[[package]]
name = "async-lock"
version = "3.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d034b430882f8381900d3fe6f0aaa3ad94f2cb4ac519b429692a1bc2dda4ae7b"
dependencies = [
 "event-listener 4.0.3",
 "event-listener-strategy",
 "pin-project-lite",
]

[[package]]
name = "async-trait"
version = "0.1.80"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c6fa2087f2753a7da8cc1c0dbfcf89579dd57458e36769de5ac750b4671737ca"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "autocfg"
version = "1.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0c4b4d0bd25bd0b74681c0ad21497610ce1b7c91b1022cd21c80c6fbdd9476b0"

[[package]]
name = "backoff"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b62ddb9cb1ec0a098ad4bbf9344d0713fa193ae1a80af55febcff2627b6a00c1"
dependencies = [
 "futures-core",
 "getrandom",
 "instant",
 "pin-project-lite",
 "rand",
 "tokio",
]

[[package]]
name = "backtrace"
version = "0.3.71"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "26b05800d2e817c8b3b4b54abd461726265fa9789ae34330622f2db9ee696f9d"
dependencies = [
 "addr2line",
 "cc",
 "cfg-if",
 "libc",
 "miniz_oxide",
 "object",
 "rustc-demangle",
]

[[package]]
name = "base16ct"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "349a06037c7bf932dd7e7d1f653678b2038b9ad46a74102f1fc7bd7872678cce"

[[package]]
name = "base16ct"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4c7f02d4ea65f2c1853089ffd8d2787bdbc63de2f0d29dedbcf8ccdfa0ccd4cf"

[[package]]
name = "base32"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "23ce669cd6c8588f79e15cf450314f9638f967fc5770ff1c7c1deb0925ea7cfa"

[[package]]
name = "base64"
version = "0.13.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9e1b586273c5702936fe7b7d6896644d8be71e6314cfe09d3167c95f712589e8"

[[package]]
name = "base64"
version = "0.21.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9d297deb1925b89f2ccc13d7635fa0714f12c87adce1c75356b39ca9b7178567"

[[package]]
name = "base64"
version = "0.22.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72b3254f16251a8381aa12e40e3c4d2f0199f8c6508fbecb9d91f575e0fbb8c6"

[[package]]
name = "base64ct"
version = "1.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8c3c1a368f70d6cf7302d78f8f7093da241fb8e8807c05cc9e51a125895a6d5b"

[[package]]
name = "beef"
version = "0.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3a8241f3ebb85c056b509d4327ad0358fbbba6ffb340bf388f26350aeda225b1"

[[package]]
name = "bincode"
version = "1.3.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b1f45e9417d87227c7a56d22e471c6206462cba514c7590c09aff4cf6d1ddcad"
dependencies = [
 "serde",
]

[[package]]
name = "binread"
version = "2.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "16598dfc8e6578e9b597d9910ba2e73618385dc9f4b1d43dd92c349d6be6418f"
dependencies = [
 "binread_derive",
 "lazy_static",
 "rustversion",
]

[[package]]
name = "binread_derive"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1d9672209df1714ee804b1f4d4f68c8eb2a90b1f7a07acf472f88ce198ef1fed"
dependencies = [
 "either",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "bip32"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b30ed1d6f8437a487a266c8293aeb95b61a23261273e3e02912cdb8b68bf798b"
dependencies = [
 "bs58",
 "hmac 0.12.1",
 "k256 0.11.6",
 "once_cell",
 "pbkdf2",
 "rand_core",
 "ripemd",
 "sha2 0.10.8",
 "subtle",
 "zeroize",
]

[[package]]
name = "bit-set"
version = "0.5.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0700ddab506f33b20a03b13996eccd309a48e5ff77d0d95926aa0210fb4e95f1"
dependencies = [
 "bit-vec",
]

[[package]]
name = "bit-vec"
version = "0.6.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "349f9b6a179ed607305526ca489b34ad0a41aed5f7980fa90eb03160b69598fb"

[[package]]
name = "bitflags"
version = "1.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"

[[package]]
name = "bitflags"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cf4b9d6a944f767f8e5e0db018570623c85f3d925ac718db4e06d0187adb21c1"

[[package]]
name = "bitvec"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1bc2832c24239b0141d5674bb9174f9d68a8b5b3f2753311927c172ca46f7e9c"
dependencies = [
 "funty",
 "radium",
 "tap",
 "wyz",
]

[[package]]
name = "blake2"
version = "0.10.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "46502ad458c9a52b69d4d4d32775c788b7a1b85e8bc9d482d92250fc0e3f8efe"
dependencies = [
 "digest 0.10.7",
]

[[package]]
name = "block-buffer"
version = "0.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4152116fd6e9dadb291ae18fc1ec3575ed6d84c29642d97890f4b4a3417297e4"
dependencies = [
 "generic-array",
]

[[package]]
name = "block-buffer"
version = "0.10.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3078c7629b62d3f0439517fa394996acacc5cbc91c5a20d8c658e77abd503a71"
dependencies = [
 "generic-array",
]

[[package]]
name = "block-modes"
version = "0.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2cb03d1bed155d89dce0f845b7899b18a9a163e148fd004e1c28421a783e2d8e"
dependencies = [
 "block-padding",
 "cipher 0.3.0",
]

[[package]]
name = "block-padding"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8d696c370c750c948ada61c69a0ee2cbbb9c50b1019ddb86d9317157a99c2cae"

[[package]]
name = "borsh"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dbe5b10e214954177fb1dc9fbd20a1a2608fe99e6c832033bdc7cea287a20d77"
dependencies = [
 "borsh-derive",
 "cfg_aliases",
]

[[package]]
name = "borsh-derive"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d7a8646f94ab393e43e8b35a2558b1624bed28b97ee09c5d15456e3c9463f46d"
dependencies = [
 "once_cell",
 "proc-macro-crate 3.1.0",
 "proc-macro2",
 "quote",
 "syn 2.0.61",
 "syn_derive",
]

[[package]]
name = "brotli"
version = "6.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "74f7971dbd9326d58187408ab83117d8ac1bb9c17b085fdacd1cf2f598719b6b"
dependencies = [
 "alloc-no-stdlib",
 "alloc-stdlib",
 "brotli-decompressor",
]

[[package]]
name = "brotli-decompressor"
version = "4.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9a45bd2e4095a8b518033b128020dd4a55aab1c0a381ba4404a472630f4bc362"
dependencies = [
 "alloc-no-stdlib",
 "alloc-stdlib",
]

[[package]]
name = "bs58"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "771fe0050b883fcc3ea2359b1a96bcfbc090b7116eae7c3c512c7a083fdf23d3"
dependencies = [
 "sha2 0.9.9",
]

[[package]]
name = "bstr"
version = "1.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "05efc5cfd9110c8416e471df0e96702d58690178e206e61b7173706673c93706"
dependencies = [
 "memchr",
 "serde",
]

[[package]]
name = "bumpalo"
version = "3.16.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "79296716171880943b8470b5f8d03aa55eb2e645a4874bdbb28adb49162e012c"

[[package]]
name = "byte-unit"
version = "4.0.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "da78b32057b8fdfc352504708feeba7216dcd65a2c9ab02978cbd288d1279b6c"
dependencies = [
 "serde",
 "utf8-width",
]

[[package]]
name = "bytecheck"
version = "0.6.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "23cdc57ce23ac53c931e88a43d06d070a6fd142f2617be5855eb75efc9beb1c2"
dependencies = [
 "bytecheck_derive",
 "ptr_meta",
 "simdutf8",
]

[[package]]
name = "bytecheck_derive"
version = "0.6.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3db406d29fbcd95542e92559bed4d8ad92636d1ca8b3b72ede10b4bcc010e659"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "bytecount"
version = "0.6.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5ce89b21cab1437276d2650d57e971f9d548a2d9037cc231abdc0562b97498ce"

[[package]]
name = "byteorder"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1fd0f2584146f6f2ef48085050886acf353beff7305ebd1ae69500e27c67f64b"

[[package]]
name = "bytes"
version = "1.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "514de17de45fdb8dc022b1a7975556c53c86f9f0aa5f534b98977b171857c2c9"

[[package]]
name = "cached"
version = "0.41.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec6d20b3d24b6c74e2c5331d2d3d8d1976a9883c7da179aa851afa4c90d62e36"
dependencies = [
 "hashbrown 0.12.3",
 "instant",
 "once_cell",
 "thiserror",
]

[[package]]
name = "cached"
version = "0.47.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "69b0116662497bc24e4b177c90eaf8870e39e2714c3fcfa296327a93f593fc21"
dependencies = [
 "ahash 0.8.11",
 "cached_proc_macro",
 "cached_proc_macro_types",
 "hashbrown 0.14.5",
 "instant",
 "once_cell",
 "thiserror",
]

[[package]]
name = "cached"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a8466736fe5dbcaf8b8ee24f9bbefe43c884dc3e9ff7178da70f55bffca1133c"
dependencies = [
 "ahash 0.8.11",
 "hashbrown 0.14.5",
 "instant",
 "once_cell",
 "thiserror",
]

[[package]]
name = "cached_proc_macro"
version = "0.18.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c878c71c2821aa2058722038a59a67583a4240524687c6028571c9b395ded61f"
dependencies = [
 "darling 0.14.4",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "cached_proc_macro_types"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ade8366b8bd5ba243f0a58f036cc0ca8a2f069cff1a2351ef1cac6b083e16fc0"

[[package]]
name = "camino"
version = "1.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c59e92b5a388f549b863a7bea62612c09f24c8393560709a54558a9abdfb3b9c"
dependencies = [
 "serde",
]

[[package]]
name = "candid"
version = "0.10.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dd5902d37352dffd8bd9177a2daa6444ce3cd0279c91763fb0171c053aa04335"
dependencies = [
 "anyhow",
 "binread",
 "byteorder",
 "candid_derive",
 "hex",
 "ic_principal",
 "leb128",
 "num-bigint 0.4.5",
 "num-traits",
 "paste",
 "pretty 0.12.3",
 "serde",
 "serde_bytes",
 "stacker",
 "thiserror",
]

[[package]]
name = "candid_derive"
version = "0.6.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3de398570c386726e7a59d9887b68763c481477f9a043fb998a2e09d428df1a9"
dependencies = [
 "lazy_static",
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "candid_parser"
version = "0.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "48a3da76f989cd350b7342c64c6c6008341bb6186f6832ef04e56dc50ba0fd76"
dependencies = [
 "anyhow",
 "arbitrary",
 "candid",
 "codespan-reporting",
 "console",
 "convert_case 0.6.0",
 "ctrlc",
 "dialoguer",
 "fake",
 "hex",
 "lalrpop",
 "lalrpop-util",
 "logos",
 "num-bigint 0.4.5",
 "num-traits",
 "pretty 0.12.3",
 "rand",
 "serde",
 "serde_dhall",
 "thiserror",
]

[[package]]
name = "cargo-platform"
version = "0.1.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24b1f0365a6c6bb4020cd05806fd0d33c44d38046b8bd7f0e40814b9763cabfc"
dependencies = [
 "serde",
]

[[package]]
name = "cargo_metadata"
version = "0.18.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2d886547e41f740c616ae73108f6eb70afe6d940c7bc697cb30f13daec073037"
dependencies = [
 "camino",
 "cargo-platform",
 "semver",
 "serde",
 "serde_json",
 "thiserror",
]

[[package]]
name = "cc"
version = "1.0.97"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "099a5357d84c4c61eb35fc8eafa9a79a902c2f76911e5747ced4e032edd8d9b4"
dependencies = [
 "jobserver",
 "libc",
 "once_cell",
]

[[package]]
name = "cfg-if"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"

[[package]]
name = "cfg_aliases"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fd16c4719339c4530435d38e511904438d07cce7950afa3718a84ac36c10e89e"

[[package]]
name = "chrono"
version = "0.4.38"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a21f936df1771bf62b77f047b726c4625ff2e8aa607c01ec06e5a05bd8463401"
dependencies = [
 "android-tzdata",
 "iana-time-zone",
 "js-sys",
 "num-traits",
 "serde",
 "wasm-bindgen",
 "windows-targets 0.52.6",
]

[[package]]
name = "ci_info"
version = "0.14.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "840dbb7bdd1f2c4d434d6b08420ef204e0bfad0ab31a07a80a1248d24cc6e38b"
dependencies = [
 "envmnt",
]

[[package]]
name = "cipher"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7ee52072ec15386f770805afd189a01c8841be8696bed250fa2f13c4c0d6dfb7"
dependencies = [
 "generic-array",
]

[[package]]
name = "cipher"
version = "0.4.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "773f3b9af64447d2ce9850330c473515014aa235e6a783b02db81ff39e4a3dad"
dependencies = [
 "crypto-common",
 "inout",
]

[[package]]
name = "clap"
version = "4.5.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "90bc066a67923782aa8515dbaea16946c5bcc5addbd668bb80af688e53e548a0"
dependencies = [
 "clap_builder",
 "clap_derive",
]

[[package]]
name = "clap_builder"
version = "4.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ae129e2e766ae0ec03484e609954119f123cc1fe650337e155d03b022f24f7b4"
dependencies = [
 "anstream",
 "anstyle",
 "clap_lex",
 "strsim 0.11.1",
 "terminal_size",
]

[[package]]
name = "clap_complete"
version = "4.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dd79504325bf38b10165b02e89b4347300f855f273c4cb30c4a3209e6583275e"
dependencies = [
 "clap",
]

[[package]]
name = "clap_derive"
version = "4.5.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "528131438037fd55894f62d6e9f068b8f45ac57ffa77517819645d10aed04f64"
dependencies = [
 "heck 0.5.0",
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "clap_lex"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "98cc8fbded0c607b7ba9dd60cd98df59af97e84d24e49c8557331cfc26d301ce"

[[package]]
name = "cmake"
version = "0.1.50"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a31c789563b815f77f4250caee12365734369f942439b7defd71e18a48197130"
dependencies = [
 "cc",
]

[[package]]
name = "codespan-reporting"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3538270d33cc669650c4b093848450d380def10c331d38c768e34cac80576e6e"
dependencies = [
 "termcolor",
 "unicode-width",
]

[[package]]
name = "colorchoice"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0b6a852b24ab71dffc585bcb46eaf7959d175cb865a7152e35b348d1b2960422"

[[package]]
name = "colored"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cbf2150cce219b664a8a70df7a1f933836724b503f8a413af9365b4dcc4d90b8"
dependencies = [
 "lazy_static",
 "windows-sys 0.48.0",
]

[[package]]
name = "comparable"
version = "0.5.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eb513ee8037bf08c5270ecefa48da249f4c58e57a71ccfce0a5b0877d2a20eb2"
dependencies = [
 "comparable_derive",
 "comparable_helper",
 "pretty_assertions",
 "serde",
]

[[package]]
name = "comparable_derive"
version = "0.5.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a54b9c40054eb8999c5d1d36fdc90e4e5f7ff0d1d9621706f360b3cbc8beb828"
dependencies = [
 "convert_case 0.4.0",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "comparable_helper"
version = "0.5.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fb5437e327e861081c91270becff184859f706e3e50f5301a9d4dc8eb50752c3"
dependencies = [
 "convert_case 0.6.0",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "concurrent-queue"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4ca0197aee26d1ae37445ee532fefce43251d24cc7c166799f4d46817f1d3973"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "console"
version = "0.15.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0e1f83fc076bd6dd27517eacdf25fef6c4dfe5f1d7448bafaaf3a26f13b5e4eb"
dependencies = [
 "encode_unicode",
 "lazy_static",
 "libc",
 "unicode-width",
 "windows-sys 0.52.0",
]

[[package]]
name = "console_error_panic_hook"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a06aeb73f470f66dcdbf7223caeebb85984942f22f1adb2a088cf9668146bbbc"
dependencies = [
 "cfg-if",
 "wasm-bindgen",
]

[[package]]
name = "const-oid"
version = "0.9.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c2459377285ad874054d797f3ccebf984978aa39129f6eafde5cdc8315b612f8"

[[package]]
name = "convert_case"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6245d59a3e82a7fc217c5828a6692dbc6dfb63a0c8c90495621f7b9d79704a0e"

[[package]]
name = "convert_case"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec182b0ca2f35d8fc196cf3404988fd8b8c739a4d270ff118a398feb0cbec1ca"
dependencies = [
 "unicode-segmentation",
]

[[package]]
name = "core-foundation"
version = "0.9.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "91e195e091a93c46f7102ec7818a2aa394e1e1771c3ab4825963fa03e45afb8f"
dependencies = [
 "core-foundation-sys",
 "libc",
]

[[package]]
name = "core-foundation-sys"
version = "0.8.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "06ea2b9bc92be3c2baa9334a323ebca2d6f074ff852cd1d7b11064035cd3868f"

[[package]]
name = "core2"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b49ba7ef1ad6107f8824dbe97de947cbaac53c44e7f9756a1fba0d37c1eec505"
dependencies = [
 "memchr",
]

[[package]]
name = "cpufeatures"
version = "0.2.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "53fe5e26ff1b7aef8bca9c6080520cfb8d9333c7568e1829cef191a9723e5504"
dependencies = [
 "libc",
]

[[package]]
name = "crc32fast"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b3855a8a784b474f333699ef2bbca9db2c4a1f6d9088a90a2d25b1eb53111eaa"
dependencies = [
 "cfg-if",
]

[[package]]
name = "crossbeam"
version = "0.8.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1137cd7e7fc0fb5d3c5a8678be38ec56e819125d8d7907411fe24ccb943faca8"
dependencies = [
 "crossbeam-channel",
 "crossbeam-deque",
 "crossbeam-epoch",
 "crossbeam-queue",
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-channel"
version = "0.5.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ab3db02a9c5b5121e1e42fbdb1aeb65f5e02624cc58c43f2884c6ccac0b82f95"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-deque"
version = "0.8.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "613f8cc01fe9cf1a3eb3d7f488fd2fa8388403e97039e2f73692932e291a770d"
dependencies = [
 "crossbeam-epoch",
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-epoch"
version = "0.9.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b82ac4a3c2ca9c3460964f020e1402edd5753411d7737aa39c3714ad1b5420e"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-queue"
version = "0.3.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "df0346b5d5e76ac2fe4e327c5fd1118d6be7c51dfb18f9b7922923f287471e35"
dependencies = [
 "crossbeam-utils",
]

[[package]]
name = "crossbeam-utils"
version = "0.8.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "248e3bacc7dc6baa3b21e405ee045c3047101a49145e7e9eca583ab4c2ca5345"

[[package]]
name = "crunchy"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7a81dae078cea95a014a339291cec439d2f232ebe854a9d672b796c6afafa9b7"

[[package]]
name = "crypto-bigint"
version = "0.4.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ef2b4b23cddf68b89b8f8069890e8c270d54e2d5fe1b143820234805e4cb17ef"
dependencies = [
 "generic-array",
 "rand_core",
 "subtle",
 "zeroize",
]

[[package]]
name = "crypto-bigint"
version = "0.5.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0dc92fb57ca44df6db8059111ab3af99a63d5d0f8375d9972e319a379c6bab76"
dependencies = [
 "generic-array",
 "rand_core",
 "subtle",
 "zeroize",
]

[[package]]
name = "crypto-common"
version = "0.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1bfb12502f3fc46cca1bb51ac28df9d618d813cdc3d2f25b9fe775a34af26bb3"
dependencies = [
 "generic-array",
 "rand_core",
 "typenum",
]

[[package]]
name = "crypto-mac"
version = "0.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "25fab6889090c8133f3deb8f73ba3c65a7f456f66436fc012a1b1e272b1e103e"
dependencies = [
 "generic-array",
 "subtle",
]

[[package]]
name = "ctr"
version = "0.9.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0369ee1ad671834580515889b80f2ea915f23b8be8d0daa4bbaf2ac5c7590835"
dependencies = [
 "cipher 0.4.4",
]

[[package]]
name = "ctrlc"
version = "3.4.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "672465ae37dc1bc6380a6547a8883d5dd397b0f1faaad4f265726cc7042a5345"
dependencies = [
 "nix 0.28.0",
 "windows-sys 0.52.0",
]

[[package]]
name = "curve25519-dalek-ng"
version = "4.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1c359b7249347e46fb28804470d071c921156ad62b3eef5d34e2ba867533dec8"
dependencies = [
 "byteorder",
 "digest 0.9.0",
 "rand_core",
 "subtle-ng",
 "zeroize",
]

[[package]]
name = "cvt"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d2ae9bf77fbf2d39ef573205d554d87e86c12f1994e9ea335b0651b9b278bcf1"
dependencies = [
 "cfg-if",
]

[[package]]
name = "cxx"
version = "1.0.122"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bb497fad022245b29c2a0351df572e2d67c1046bcef2260ebc022aec81efea82"
dependencies = [
 "cc",
 "cxxbridge-flags",
 "cxxbridge-macro",
 "link-cplusplus",
]

[[package]]
name = "cxx-build"
version = "1.0.122"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9327c7f9fbd6329a200a5d4aa6f674c60ab256525ff0084b52a889d4e4c60cee"
dependencies = [
 "cc",
 "codespan-reporting",
 "once_cell",
 "proc-macro2",
 "quote",
 "scratch",
 "syn 2.0.61",
]

[[package]]
name = "cxxbridge-flags"
version = "1.0.122"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "688c799a4a846f1c0acb9f36bb9c6272d9b3d9457f3633c7753c6057270df13c"

[[package]]
name = "cxxbridge-macro"
version = "1.0.122"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "928bc249a7e3cd554fd2e8e08a426e9670c50bbfc9a621653cfa9accc9641783"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "darling"
version = "0.13.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a01d95850c592940db9b8194bc39f4bc0e89dee5c4265e4b1807c34a9aba453c"
dependencies = [
 "darling_core 0.13.4",
 "darling_macro 0.13.4",
]

[[package]]
name = "darling"
version = "0.14.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7b750cb3417fd1b327431a470f388520309479ab0bf5e323505daf0290cd3850"
dependencies = [
 "darling_core 0.14.4",
 "darling_macro 0.14.4",
]

[[package]]
name = "darling_core"
version = "0.13.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "859d65a907b6852c9361e3185c862aae7fafd2887876799fa55f5f99dc40d610"
dependencies = [
 "fnv",
 "ident_case",
 "proc-macro2",
 "quote",
 "strsim 0.10.0",
 "syn 1.0.109",
]

[[package]]
name = "darling_core"
version = "0.14.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "109c1ca6e6b7f82cc233a97004ea8ed7ca123a9af07a8230878fcfda9b158bf0"
dependencies = [
 "fnv",
 "ident_case",
 "proc-macro2",
 "quote",
 "strsim 0.10.0",
 "syn 1.0.109",
]

[[package]]
name = "darling_macro"
version = "0.13.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9c972679f83bdf9c42bd905396b6c3588a843a17f0f16dfcfa3e2c5d57441835"
dependencies = [
 "darling_core 0.13.4",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "darling_macro"
version = "0.14.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a4aab4dbc9f7611d8b55048a3a16d2d010c2c8334e46304b40ac1cc14bf3b48e"
dependencies = [
 "darling_core 0.14.4",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "dary_heap"
version = "0.3.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7762d17f1241643615821a8455a0b2c3e803784b058693d990b11f2dce25a0ca"

[[package]]
name = "data-encoding"
version = "2.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e8566979429cf69b49a5c740c60791108e86440e8be149bbea4fe54d2c32d6e2"

[[package]]
name = "delay"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8546bb2c80129c9c85cd2b44e160b917f34a8b7ce9f3af675502cf9960e33d62"

[[package]]
name = "der"
version = "0.6.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f1a467a65c5e759bce6e65eaf91cc29f466cdc57cb65777bd646872a8a1fd4de"
dependencies = [
 "const-oid",
 "pem-rfc7468 0.6.0",
 "zeroize",
]

[[package]]
name = "der"
version = "0.7.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f55bf8e7b65898637379c1b74eb1551107c8294ed26d855ceb9fd1a09cfc9bc0"
dependencies = [
 "const-oid",
 "pem-rfc7468 0.7.0",
 "zeroize",
]

[[package]]
name = "deranged"
version = "0.3.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b42b6fa04a440b495c8b04d0e71b707c585f83cb9cb28cf8cd0d976c315e31b4"
dependencies = [
 "powerfmt",
 "serde",
]

[[package]]
name = "derivative"
version = "2.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fcc3dd5e9e9c0b295d6e1e4d811fb6f157d5ffd784b8d202fc62eac8035a770b"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "derive_more"
version = "0.99.8-alpha.0"
source = "git+https://github.com/dfinity-lab/derive_more?rev=9f1b894e6fde640da4e9ea71a8fc0e4dd98d01da#9f1b894e6fde640da4e9ea71a8fc0e4dd98d01da"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "deunicode"
version = "1.4.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "322ef0094744e63628e6f0eb2295517f79276a5b342a4c2ff3042566ca181d4e"

[[package]]
name = "dfx"
version = "0.24.0"
dependencies = [
 "actix",
 "aes-gcm",
 "anstyle",
 "anyhow",
 "apply-patch",
 "argon2",
 "backoff",
 "base64 0.13.1",
 "byte-unit",
 "bytes",
 "candid",
 "candid_parser",
 "cargo_metadata",
 "ci_info",
 "clap",
 "clap_complete",
 "console",
 "crc32fast",
 "crossbeam",
 "ctrlc",
 "dfx-core",
 "dialoguer",
 "directories-next",
 "env_logger",
 "flate2",
 "fn-error-context",
 "futures",
 "futures-util",
 "handlebars",
 "hex",
 "humantime",
 "hyper-rustls 0.24.2",
 "ic-agent",
 "ic-asset",
 "ic-cdk",
 "ic-identity-hsm",
 "ic-utils 0.38.0",
 "ic-wasm",
 "icrc-ledger-types",
 "idl2json",
 "indicatif",
 "itertools 0.10.5",
 "json-patch",
 "junction",
 "keyring",
 "lazy_static",
 "mime",
 "mime_guess",
 "mockito",
 "num-traits",
 "os_str_bytes",
 "patch",
 "pem 1.1.1",
 "petgraph",
 "pocket-ic",
 "proptest",
 "rand",
 "regex",
 "reqwest",
 "ring 0.16.20",
 "rust_decimal",
 "rustls-webpki 0.101.7",
 "schemars",
 "sec1 0.3.0",
 "semver",
 "serde",
 "serde_bytes",
 "serde_cbor",
 "serde_json",
 "sha2 0.10.8",
 "shell-words",
 "slog",
 "slog-async",
 "slog-term",
 "socket2 0.5.7",
 "supports-color",
 "sysinfo",
 "tar",
 "tempfile",
 "term",
 "thiserror",
 "time",
 "tokio",
 "toml 0.7.8",
 "url",
 "walkdir",
 "walrus",
 "which",
]

[[package]]
name = "dfx-core"
version = "0.1.0"
dependencies = [
 "aes-gcm",
 "argon2",
 "backoff",
 "bip32",
 "byte-unit",
 "bytes",
 "candid",
 "clap",
 "dialoguer",
 "directories-next",
 "dunce",
 "flate2",
 "futures",
 "handlebars",
 "hex",
 "humantime-serde",
 "ic-agent",
 "ic-identity-hsm",
 "ic-utils 0.38.0",
 "itertools 0.10.5",
 "k256 0.11.6",
 "keyring",
 "lazy_static",
 "proptest",
 "reqwest",
 "ring 0.16.20",
 "schemars",
 "sec1 0.3.0",
 "semver",
 "serde",
 "serde_json",
 "sha2 0.10.8",
 "slog",
 "tar",
 "tempfile",
 "thiserror",
 "time",
 "tiny-bip39",
 "tokio",
 "url",
]

[[package]]
name = "dhall"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9093ee48621ca9db16cd4948c7acf24a8ecc9af41cc9e226e39ea719df06d8b5"
dependencies = [
 "abnf_to_pest",
 "annotate-snippets",
 "elsa",
 "hex",
 "home",
 "itertools 0.9.0",
 "lazy_static",
 "once_cell",
 "percent-encoding",
 "pest",
 "pest_consume",
 "pest_generator",
 "quote",
 "serde",
 "serde_cbor",
 "sha2 0.9.9",
 "url",
]

[[package]]
name = "dhall_proc_macros"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "df7c81d16870879ef530b07cef32bc6088f98937ab4168106cc8e382a05146bf"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "dialoguer"
version = "0.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "658bce805d770f407bc62102fca7c2c64ceef2fbcb2b8bd19d2765ce093980de"
dependencies = [
 "console",
 "fuzzy-matcher",
 "shell-words",
 "tempfile",
 "thiserror",
 "zeroize",
]

[[package]]
name = "diff"
version = "0.1.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56254986775e3233ffa9c4d7d3faaf6d36a2c09d30b20687e9f88bc8bafc16c8"

[[package]]
name = "digest"
version = "0.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d3dd60d1080a57a05ab032377049e0591415d2b31afd7028356dbf3cc6dcb066"
dependencies = [
 "generic-array",
]

[[package]]
name = "digest"
version = "0.10.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9ed9a281f7bc9b7576e61468ba615a66a5c8cfdff42420a70aa82701a3b1e292"
dependencies = [
 "block-buffer 0.10.4",
 "const-oid",
 "crypto-common",
 "subtle",
]

[[package]]
name = "directories-next"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "339ee130d97a610ea5a5872d2bbb130fdf68884ff09d3028b81bec8a1ac23bbc"
dependencies = [
 "cfg-if",
 "dirs-sys-next",
]

[[package]]
name = "dirs-next"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b98cf8ebf19c3d1b223e151f99a4f9f0690dca41414773390fc824184ac833e1"
dependencies = [
 "cfg-if",
 "dirs-sys-next",
]

[[package]]
name = "dirs-sys-next"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4ebda144c4fe02d1f7ea1a7d9641b6fc6b580adcfa024ae48797ecdeb6825b4d"
dependencies = [
 "libc",
 "redox_users",
 "winapi",
]

[[package]]
name = "doc-comment"
version = "0.3.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fea41bba32d969b513997752735605054bc0dfa92b4c56bf1189f2e174be7a10"

[[package]]
name = "dunce"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56ce8c6da7551ec6c462cbaf3bfbc75131ebbfa1c944aeaa9dab51ca1c5f0c3b"

[[package]]
name = "dyn-clone"
version = "1.0.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0d6ef0072f8a535281e4876be788938b528e9a1d43900b82c2569af7da799125"

[[package]]
name = "ecdsa"
version = "0.14.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "413301934810f597c1d19ca71c8710e99a3f1ba28a0d2ebc01551a2daeea3c5c"
dependencies = [
 "der 0.6.1",
 "elliptic-curve 0.12.3",
 "rfc6979 0.3.1",
 "signature 1.6.4",
]

[[package]]
name = "ecdsa"
version = "0.16.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ee27f32b5c5292967d2d4a9d7f1e0b0aed2c15daded5a60300e4abb9d8020bca"
dependencies = [
 "der 0.7.9",
 "digest 0.10.7",
 "elliptic-curve 0.13.8",
 "rfc6979 0.4.0",
 "signature 2.2.0",
 "spki 0.7.3",
]

[[package]]
name = "ed25519-consensus"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3c8465edc8ee7436ffea81d21a019b16676ee3db267aa8d5a8d729581ecf998b"
dependencies = [
 "curve25519-dalek-ng",
 "hex",
 "rand_core",
 "serde",
 "sha2 0.9.9",
 "thiserror",
 "zeroize",
]

[[package]]
name = "either"
version = "1.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a47c1c47d2f5964e29c61246e81db715514cd532db6b5116a25ea3c03d6780a2"

[[package]]
name = "elliptic-curve"
version = "0.12.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e7bb888ab5300a19b8e5bceef25ac745ad065f3c9f7efc6de1b91958110891d3"
dependencies = [
 "base16ct 0.1.1",
 "crypto-bigint 0.4.9",
 "der 0.6.1",
 "digest 0.10.7",
 "ff 0.12.1",
 "generic-array",
 "group 0.12.1",
 "pem-rfc7468 0.6.0",
 "pkcs8 0.9.0",
 "rand_core",
 "sec1 0.3.0",
 "subtle",
 "zeroize",
]

[[package]]
name = "elliptic-curve"
version = "0.13.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b5e6043086bf7973472e0c7dff2142ea0b680d30e18d9cc40f267efbf222bd47"
dependencies = [
 "base16ct 0.2.0",
 "crypto-bigint 0.5.5",
 "digest 0.10.7",
 "ff 0.13.0",
 "generic-array",
 "group 0.13.0",
 "pem-rfc7468 0.7.0",
 "pkcs8 0.10.2",
 "rand_core",
 "sec1 0.7.3",
 "subtle",
 "zeroize",
]

[[package]]
name = "elsa"
version = "1.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d98e71ae4df57d214182a2e5cb90230c0192c6ddfcaa05c36453d46a54713e10"
dependencies = [
 "stable_deref_trait",
]

[[package]]
name = "ena"
version = "0.14.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3d248bdd43ce613d87415282f69b9bb99d947d290b10962dd6c56233312c2ad5"
dependencies = [
 "log",
]

[[package]]
name = "encode_unicode"
version = "0.3.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a357d28ed41a50f9c765dbfe56cbc04a64e53e5fc58ba79fbc34c10ef3df831f"

[[package]]
name = "enumflags2"
version = "0.6.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "83c8d82922337cd23a15f88b70d8e4ef5f11da38dd7cdb55e84dd5de99695da0"
dependencies = [
 "enumflags2_derive",
 "serde",
]

[[package]]
name = "enumflags2_derive"
version = "0.6.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "946ee94e3dbf58fdd324f9ce245c7b238d46a66f00e86a020b71996349e46cce"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "env_logger"
version = "0.10.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4cd405aab171cb85d6735e5c8d9db038c17d3ca007a4d2c25f337935c3d90580"
dependencies = [
 "humantime",
 "is-terminal",
 "log",
 "regex",
 "termcolor",
]

[[package]]
name = "envmnt"
version = "0.10.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d73999a2b8871e74c8b8bc23759ee9f3d85011b24fafc91a4b3b5c8cc8185501"
dependencies = [
 "fsio",
 "indexmap 1.9.3",
]

[[package]]
name = "equivalent"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5443807d6dff69373d433ab9ef5378ad8df50ca6298caf15de6e52e24aaf54d5"

[[package]]
name = "erased-serde"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c138974f9d5e7fe373eb04df7cae98833802ae4b11c24ac7039a21d5af4b26c"
dependencies = [
 "serde",
]

[[package]]
name = "errno"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "534c5cf6194dfab3db3242765c03bbe257cf92f22b38f6bc0c58d59108a820ba"
dependencies = [
 "libc",
 "windows-sys 0.52.0",
]

[[package]]
name = "event-listener"
version = "2.5.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0206175f82b8d6bf6652ff7d71a1e27fd2e4efde587fd368662814d6ec1d9ce0"

[[package]]
name = "event-listener"
version = "4.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "67b215c49b2b248c855fb73579eb1f4f26c38ffdc12973e20e07b91d78d5646e"
dependencies = [
 "concurrent-queue",
 "parking",
 "pin-project-lite",
]

[[package]]
name = "event-listener-strategy"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "958e4d70b6d5e81971bebec42271ec641e7ff4e170a6fa605f2b8a8b65cb97d3"
dependencies = [
 "event-listener 4.0.3",
 "pin-project-lite",
]

[[package]]
name = "fake"
version = "2.9.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1c25829bde82205da46e1823b2259db6273379f626fc211f126f65654a2669be"
dependencies = [
 "deunicode",
 "rand",
]

[[package]]
name = "fallible-iterator"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4443176a9f2c162692bd3d352d745ef9413eec5782a80d8fd6f8a1ac692a07f7"

[[package]]
name = "fastrand"
version = "1.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e51093e27b0797c359783294ca4f0a911c270184cb10f85783b118614a1501be"
dependencies = [
 "instant",
]

[[package]]
name = "fastrand"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9fc0510504f03c51ada170672ac806f1f105a88aa97a5281117e1ddc3368e51a"

[[package]]
name = "ff"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d013fc25338cc558c5c2cfbad646908fb23591e2404481826742b651c9af7160"
dependencies = [
 "rand_core",
 "subtle",
]

[[package]]
name = "ff"
version = "0.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ded41244b729663b1e574f1b4fb731469f69f79c17667b5d776b16cda0479449"
dependencies = [
 "rand_core",
 "subtle",
]

[[package]]
name = "filetime"
version = "0.2.23"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1ee447700ac8aa0b2f2bd7bc4462ad686ba06baa6727ac149a2d6277f0d240fd"
dependencies = [
 "cfg-if",
 "libc",
 "redox_syscall 0.4.1",
 "windows-sys 0.52.0",
]

[[package]]
name = "fixedbitset"
version = "0.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0ce7134b9999ecaf8bcd65542e436736ef32ddca1b3e06094cb6ec5755203b80"

[[package]]
name = "flate2"
version = "1.0.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5f54427cfd1c7829e2a139fcefea601bf088ebca651d2bf53ebc600eac295dae"
dependencies = [
 "crc32fast",
 "libz-ng-sys",
 "miniz_oxide",
]

[[package]]
name = "fn-error-context"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2cd66269887534af4b0c3e3337404591daa8dc8b9b2b3db71f9523beb4bafb41"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "fnv"
version = "1.0.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3f9eec918d3f24069decb9af1554cad7c880e2da24a9afd88aca000531ab82c1"

[[package]]
name = "form_urlencoded"
version = "1.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e13624c2627564efccf4934284bdd98cbaa14e79b0b5a141218e507b3a823456"
dependencies = [
 "percent-encoding",
]

[[package]]
name = "fsio"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dad0ce30be0cc441b325c5d705c8b613a0ca0d92b6a8953d41bd236dc09a36d0"
dependencies = [
 "dunce",
]

[[package]]
name = "funty"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6d5a32815ae3f33302d95fdcb2ce17862f8c65363dcfd29360480ba1001fc9c"

[[package]]
name = "futures"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "645c6916888f6cb6350d2550b80fb63e734897a8498abe35cfb732b6487804b0"
dependencies = [
 "futures-channel",
 "futures-core",
 "futures-executor",
 "futures-io",
 "futures-sink",
 "futures-task",
 "futures-util",
]

[[package]]
name = "futures-channel"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eac8f7d7865dcb88bd4373ab671c8cf4508703796caa2b1985a9ca867b3fcb78"
dependencies = [
 "futures-core",
 "futures-sink",
]

[[package]]
name = "futures-core"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dfc6580bb841c5a68e9ef15c77ccc837b40a7504914d52e47b8b0e9bbda25a1d"

[[package]]
name = "futures-executor"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a576fc72ae164fca6b9db127eaa9a9dda0d61316034f33a0a0d4eda41f02b01d"
dependencies = [
 "futures-core",
 "futures-task",
 "futures-util",
]

[[package]]
name = "futures-intrusive"
version = "0.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a604f7a68fbf8103337523b1fadc8ade7361ee3f112f7c680ad179651616aed5"
dependencies = [
 "futures-core",
 "lock_api",
 "parking_lot 0.11.2",
]

[[package]]
name = "futures-io"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a44623e20b9681a318efdd71c299b6b222ed6f231972bfe2f224ebad6311f0c1"

[[package]]
name = "futures-lite"
version = "1.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "49a9d51ce47660b1e808d3c990b4709f2f415d928835a17dfd16991515c46bce"
dependencies = [
 "fastrand 1.9.0",
 "futures-core",
 "futures-io",
 "memchr",
 "parking",
 "pin-project-lite",
 "waker-fn",
]

[[package]]
name = "futures-macro"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87750cf4b7a4c0625b1529e4c543c2182106e4dedc60a2a6455e00d212c489ac"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "futures-sink"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9fb8e00e87438d937621c1c6269e53f536c14d3fbd6a042bb24879e57d474fb5"

[[package]]
name = "futures-task"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "38d84fa142264698cdce1a9f9172cf383a0c82de1bddcf3092901442c4097004"

[[package]]
name = "futures-util"
version = "0.3.30"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3d6401deb83407ab3da39eba7e33987a73c3df0c82b4bb5813ee871c19c41d48"
dependencies = [
 "futures-channel",
 "futures-core",
 "futures-io",
 "futures-macro",
 "futures-sink",
 "futures-task",
 "memchr",
 "pin-project-lite",
 "pin-utils",
 "slab",
]

[[package]]
name = "fuzzy-matcher"
version = "0.3.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "54614a3312934d066701a80f20f15fa3b56d67ac7722b39eea5b4c9dd1d66c94"
dependencies = [
 "thread_local",
]

[[package]]
name = "generic-array"
version = "0.14.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "85649ca51fd72272d7821adaf274ad91c288277713d9c18820d8499a7ff69e9a"
dependencies = [
 "typenum",
 "version_check",
 "zeroize",
]

[[package]]
name = "getrandom"
version = "0.2.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c4567c8db10ae91089c99af84c68c38da3ec2f087c3f82960bcdbf3656b6f4d7"
dependencies = [
 "cfg-if",
 "js-sys",
 "libc",
 "wasi",
 "wasm-bindgen",
]

[[package]]
name = "ghash"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f0d8a4362ccb29cb0b265253fb0a2728f592895ee6854fd9bc13f2ffda266ff1"
dependencies = [
 "opaque-debug",
 "polyval",
]

[[package]]
name = "gimli"
version = "0.26.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "22030e2c5a68ec659fde1e949a745124b48e6fa8b045b7ed5bd1fe4ccc5c4e5d"
dependencies = [
 "fallible-iterator",
 "indexmap 1.9.3",
 "stable_deref_trait",
]

[[package]]
name = "gimli"
version = "0.28.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4271d37baee1b8c7e4b708028c57d816cf9d2434acb33a549475f78c181f6253"

[[package]]
name = "globset"
version = "0.4.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "57da3b9b5b85bd66f31093f8c408b90a74431672542466497dcbdfdc02034be1"
dependencies = [
 "aho-corasick",
 "bstr",
 "log",
 "regex-automata 0.4.6",
 "regex-syntax 0.8.3",
]

[[package]]
name = "group"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5dfbfb3a6cfbd390d5c9564ab283a0349b9b9fcd46a706c1eb10e0db70bfbac7"
dependencies = [
 "ff 0.12.1",
 "rand_core",
 "subtle",
]

[[package]]
name = "group"
version = "0.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f0f9ef7462f7c099f518d754361858f86d8a07af53ba9af0fe635bbccb151a63"
dependencies = [
 "ff 0.13.0",
 "rand_core",
 "subtle",
]

[[package]]
name = "h2"
version = "0.3.26"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "81fe527a889e1532da5c525686d96d4c2e74cdd345badf8dfef9f6b39dd5f5e8"
dependencies = [
 "bytes",
 "fnv",
 "futures-core",
 "futures-sink",
 "futures-util",
 "http 0.2.12",
 "indexmap 2.2.6",
 "slab",
 "tokio",
 "tokio-util",
 "tracing",
]

[[package]]
name = "h2"
version = "0.4.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "816ec7294445779408f36fe57bc5b7fc1cf59664059096c65f905c1c61f58069"
dependencies = [
 "bytes",
 "fnv",
 "futures-core",
 "futures-sink",
 "futures-util",
 "http 1.1.0",
 "indexmap 2.2.6",
 "slab",
 "tokio",
 "tokio-util",
 "tracing",
]

[[package]]
name = "half"
version = "1.8.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1b43ede17f21864e81be2fa654110bf1e793774238d86ef8555c37e6519c0403"

[[package]]
name = "handlebars"
version = "4.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "faa67bab9ff362228eb3d00bd024a4965d8231bbb7921167f0cfa66c6626b225"
dependencies = [
 "log",
 "pest",
 "pest_derive",
 "serde",
 "serde_json",
 "thiserror",
]

[[package]]
name = "hashbrown"
version = "0.12.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8a9ee70c43aaf417c914396645a0fa852624801b24ebb7ae78fe8272889ac888"
dependencies = [
 "ahash 0.7.8",
]

[[package]]
name = "hashbrown"
version = "0.14.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e5274423e17b7c9fc20b6e7e208532f9b19825d82dfd615708b70edd83df41f1"
dependencies = [
 "ahash 0.8.11",
 "allocator-api2",
 "serde",
]

[[package]]
name = "heck"
version = "0.3.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6d621efb26863f0e9924c6ac577e8275e5e6b77455db64ffa6c65c904e9e132c"
dependencies = [
 "unicode-segmentation",
]

[[package]]
name = "heck"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "95505c38b4572b2d910cecb0281560f54b440a19336cbbcb27bf6ce6adc6f5a8"

[[package]]
name = "heck"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2304e00983f87ffb38b55b444b5e3b60a884b5d30c0fca7d82fe33449bbe55ea"

[[package]]
name = "hermit-abi"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d231dfb89cfffdbc30e7fc41579ed6066ad03abda9e567ccafae602b97ec5024"

[[package]]
name = "hex"
version = "0.4.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7f24254aa9a54b5c858eaee2f5bccdb46aaf0e486a595ed5fd8f86ba55232a70"
dependencies = [
 "serde",
]

[[package]]
name = "hkdf"
version = "0.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "01706d578d5c281058480e673ae4086a9f4710d8df1ad80a5b03e39ece5f886b"
dependencies = [
 "digest 0.9.0",
 "hmac 0.11.0",
]

[[package]]
name = "hmac"
version = "0.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2a2a2320eb7ec0ebe8da8f744d7812d9fc4cb4d09344ac01898dbcb6a20ae69b"
dependencies = [
 "crypto-mac",
 "digest 0.9.0",
]

[[package]]
name = "hmac"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c49c37c09c17a53d937dfbb742eb3a961d65a994e6bcdcf37e7399d0cc8ab5e"
dependencies = [
 "digest 0.10.7",
]

[[package]]
name = "home"
version = "0.5.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e3d1354bf6b7235cb4a0576c2619fd4ed18183f689b12b006a0ee7329eeff9a5"
dependencies = [
 "windows-sys 0.52.0",
]

[[package]]
name = "http"
version = "0.2.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "601cbb57e577e2f5ef5be8e7b83f0f63994f25aa94d673e54a92d5c516d101f1"
dependencies = [
 "bytes",
 "fnv",
 "itoa",
]

[[package]]
name = "http"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "21b9ddb458710bc376481b842f5da65cdf31522de232c1ca8146abce2a358258"
dependencies = [
 "bytes",
 "fnv",
 "itoa",
]

[[package]]
name = "http-body"
version = "0.4.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7ceab25649e9960c0311ea418d17bee82c0dcec1bd053b5f9a66e265a693bed2"
dependencies = [
 "bytes",
 "http 0.2.12",
 "pin-project-lite",
]

[[package]]
name = "http-body"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1cac85db508abc24a2e48553ba12a996e87244a0395ce011e62b37158745d643"
dependencies = [
 "bytes",
 "http 1.1.0",
]

[[package]]
name = "http-body-util"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0475f8b2ac86659c21b64320d5d653f9efe42acd2a4e560073ec61a155a34f1d"
dependencies = [
 "bytes",
 "futures-core",
 "http 1.1.0",
 "http-body 1.0.0",
 "pin-project-lite",
]

[[package]]
name = "httparse"
version = "1.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d897f394bad6a705d5f4104762e116a75639e470d80901eed05a860a95cb1904"

[[package]]
name = "httpdate"
version = "1.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "df3b46402a9d5adb4c86a0cf463f42e19994e3ee891101b1841f30a545cb49a9"

[[package]]
name = "humantime"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9a3a5bfb195931eeb336b2a7b4d761daec841b97f947d34394601737a7bba5e4"

[[package]]
name = "humantime-serde"
version = "1.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "57a3db5ea5923d99402c94e9feb261dc5ee9b4efa158b0315f788cf549cc200c"
dependencies = [
 "humantime",
 "serde",
]

[[package]]
name = "hyper"
version = "0.14.28"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bf96e135eb83a2a8ddf766e426a841d8ddd7449d5f00d34ea02b41d2f19eef80"
dependencies = [
 "bytes",
 "futures-channel",
 "futures-core",
 "futures-util",
 "h2 0.3.26",
 "http 0.2.12",
 "http-body 0.4.6",
 "httparse",
 "httpdate",
 "itoa",
 "pin-project-lite",
 "tokio",
 "tower-service",
 "tracing",
 "want",
]

[[package]]
name = "hyper"
version = "1.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fe575dd17d0862a9a33781c8c4696a55c320909004a67a00fb286ba8b1bc496d"
dependencies = [
 "bytes",
 "futures-channel",
 "futures-util",
 "h2 0.4.4",
 "http 1.1.0",
 "http-body 1.0.0",
 "httparse",
 "itoa",
 "pin-project-lite",
 "smallvec",
 "tokio",
 "want",
]

[[package]]
name = "hyper-rustls"
version = "0.24.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec3efd23720e2049821a693cbc7e65ea87c72f1c58ff2f9522ff332b1491e590"
dependencies = [
 "futures-util",
 "http 0.2.12",
 "hyper 0.14.28",
 "rustls 0.21.12",
 "tokio",
 "tokio-rustls 0.24.1",
 "webpki-roots 0.25.4",
]

[[package]]
name = "hyper-rustls"
version = "0.27.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5ee4be2c948921a1a5320b629c4193916ed787a7f7f293fd3f7f5a6c9de74155"
dependencies = [
 "futures-util",
 "http 1.1.0",
 "hyper 1.3.1",
 "hyper-util",
 "rustls 0.23.7",
 "rustls-native-certs",
 "rustls-pki-types",
 "tokio",
 "tokio-rustls 0.26.0",
 "tower-service",
 "webpki-roots 0.26.1",
]

[[package]]
name = "hyper-util"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ca38ef113da30126bbff9cd1705f9273e15d45498615d138b0c20279ac7a76aa"
dependencies = [
 "bytes",
 "futures-channel",
 "futures-util",
 "http 1.1.0",
 "http-body 1.0.0",
 "hyper 1.3.1",
 "pin-project-lite",
 "socket2 0.5.7",
 "tokio",
 "tower",
 "tower-service",
 "tracing",
]

[[package]]
name = "iana-time-zone"
version = "0.1.60"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e7ffbb5a1b541ea2561f8c41c087286cc091e21e556a4f09a8f6cbf17b69b141"
dependencies = [
 "android_system_properties",
 "core-foundation-sys",
 "iana-time-zone-haiku",
 "js-sys",
 "wasm-bindgen",
 "windows-core",
]

[[package]]
name = "iana-time-zone-haiku"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f31827a206f56af32e590ba56d5d2d085f558508192593743f16b2306495269f"
dependencies = [
 "cc",
]

[[package]]
name = "ic-agent"
version = "0.38.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "def8c34690d68caa452486af1fff4782d2f5a9d4c5b4e0e1afca648c616fa380"
dependencies = [
 "async-lock 3.3.0",
 "backoff",
 "cached 0.52.0",
 "candid",
 "ed25519-consensus",
 "futures-util",
 "hex",
 "http 1.1.0",
 "http-body 1.0.0",
 "ic-certification 2.5.0",
 "ic-transport-types",
 "ic-verify-bls-signature",
 "k256 0.13.3",
 "leb128",
 "p256",
 "pem 3.0.4",
 "pkcs8 0.10.2",
 "rand",
 "rangemap",
 "reqwest",
 "ring 0.17.8",
 "rustls-webpki 0.102.3",
 "sec1 0.7.3",
 "serde",
 "serde_bytes",
 "serde_cbor",
 "serde_repr",
 "sha2 0.10.8",
 "simple_asn1",
 "thiserror",
 "time",
 "tokio",
 "url",
]

[[package]]
name = "ic-asset"
version = "0.21.0"
dependencies = [
 "backoff",
 "brotli",
 "candid",
 "derivative",
 "dfx-core",
 "flate2",
 "futures",
 "futures-intrusive",
 "globset",
 "hex",
 "ic-agent",
 "ic-utils 0.38.0",
 "itertools 0.10.5",
 "json5",
 "mime",
 "mime_guess",
 "mockito",
 "proptest",
 "serde",
 "serde_bytes",
 "serde_json",
 "sha2 0.10.8",
 "slog",
 "tempfile",
 "thiserror",
 "tokio",
 "walkdir",
]

[[package]]
name = "ic-base-types"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "base32",
 "byte-unit",
 "bytes",
 "candid",
 "comparable",
 "crc32fast",
 "ic-crypto-sha2",
 "ic-protobuf",
 "ic-stable-structures",
 "phantom_newtype",
 "prost",
 "serde",
 "strum 0.25.0",
 "strum_macros 0.25.3",
]

[[package]]
name = "ic-btc-interface"
version = "0.1.0"
source = "git+https://github.com/dfinity/bitcoin-canister?rev=9b239d1d67253eb14a35be6061e3967d5ec9db9d#9b239d1d67253eb14a35be6061e3967d5ec9db9d"
dependencies = [
 "candid",
 "serde",
 "serde_bytes",
]

[[package]]
name = "ic-btc-types-internal"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "candid",
 "ic-btc-interface",
 "ic-error-types",
 "ic-protobuf",
 "serde",
 "serde_bytes",
]

[[package]]
name = "ic-cbor"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "07bc917070b8fc4bd88e3199746372e44d507f54c93a9b191787e1caefca1eba"
dependencies = [
 "candid",
 "ic-certification 2.5.0",
 "leb128",
 "nom",
 "thiserror",
]

[[package]]
name = "ic-cdk"
version = "0.13.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3b1da6a25b045f9da3c9459c0cb2b0700ac368ee16382975a17185a23b9c18ab"
dependencies = [
 "candid",
 "ic-cdk-macros",
 "ic0",
 "serde",
 "serde_bytes",
]

[[package]]
name = "ic-cdk-macros"
version = "0.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a45800053d80a6df839a71aaea5797e723188c0b992618208ca3b941350c7355"
dependencies = [
 "candid",
 "proc-macro2",
 "quote",
 "serde",
 "serde_tokenstream",
 "syn 1.0.109",
]

[[package]]
name = "ic-certificate-verification"
version = "2.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "769142849e241e6cf7f5611f9b04983e958729495ea67d2de95e5d9a9c687d9b"
dependencies = [
 "cached 0.47.0",
 "candid",
 "ic-cbor",
 "ic-certification 2.5.0",
 "lazy_static",
 "leb128",
 "miracl_core_bls12381",
 "nom",
 "parking_lot 0.12.2",
 "sha2 0.10.8",
 "thiserror",
]

[[package]]
name = "ic-certification"
version = "2.3.0"
source = "git+https://github.com/dfinity/response-verification.git?rev=a65009624b61736df6d2dc17756bdbd02a84f599#a65009624b61736df6d2dc17756bdbd02a84f599"
dependencies = [
 "hex",
 "serde",
 "serde_bytes",
 "sha2 0.10.8",
]

[[package]]
name = "ic-certification"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "20052ce9255fbe2de7041a4f6996fddd095ba1f31ae83b6c0ccdee5be6e7bbcf"
dependencies = [
 "hex",
 "serde",
 "serde_bytes",
 "sha2 0.10.8",
]

[[package]]
name = "ic-certification-testing"
version = "2.3.0"
source = "git+https://github.com/dfinity/response-verification.git?rev=a65009624b61736df6d2dc17756bdbd02a84f599#a65009624b61736df6d2dc17756bdbd02a84f599"
dependencies = [
 "console_error_panic_hook",
 "getrandom",
 "ic-crypto-internal-seed",
 "ic-crypto-internal-threshold-sig-bls12381",
 "ic-crypto-internal-types",
 "ic-crypto-tree-hash",
 "ic-types",
 "js-sys",
 "leb128",
 "log",
 "rand",
 "serde",
 "serde-wasm-bindgen",
 "serde_cbor",
 "thiserror",
 "wasm-bindgen",
 "wasm-bindgen-console-logger",
]

[[package]]
name = "ic-certified-assets"
version = "0.2.5"
dependencies = [
 "anyhow",
 "base64 0.13.1",
 "candid",
 "candid_parser",
 "hex",
 "ic-cdk",
 "ic-certification 2.5.0",
 "ic-certification-testing",
 "ic-crypto-tree-hash",
 "ic-http-certification",
 "ic-representation-independent-hash",
 "ic-response-verification",
 "ic-response-verification-test-utils",
 "itertools 0.10.5",
 "num-traits",
 "percent-encoding",
 "serde",
 "serde_bytes",
 "serde_cbor",
 "sha2 0.10.8",
]

[[package]]
name = "ic-constants"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"

[[package]]
name = "ic-crypto-getrandom-for-wasm"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "getrandom",
]

[[package]]
name = "ic-crypto-internal-bls12-381-type"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "hex",
 "ic-crypto-getrandom-for-wasm",
 "ic_bls12_381 0.8.0",
 "itertools 0.12.1",
 "lazy_static",
 "pairing 0.22.0",
 "paste",
 "rand",
 "rand_chacha",
 "sha2 0.9.9",
 "subtle",
 "zeroize",
]

[[package]]
name = "ic-crypto-internal-seed"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "hex",
 "ic-crypto-sha2",
 "ic-types",
 "rand",
 "rand_chacha",
 "serde",
 "zeroize",
]

[[package]]
name = "ic-crypto-internal-sha2"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "sha2 0.10.8",
]

[[package]]
name = "ic-crypto-internal-threshold-sig-bls12381"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "base64 0.13.1",
 "cached 0.41.0",
 "hex",
 "ic-crypto-internal-bls12-381-type",
 "ic-crypto-internal-seed",
 "ic-crypto-internal-threshold-sig-bls12381-der",
 "ic-crypto-internal-types",
 "ic-crypto-secrets-containers",
 "ic-crypto-sha2",
 "ic-types",
 "lazy_static",
 "parking_lot 0.12.2",
 "rand",
 "rand_chacha",
 "serde",
 "serde_bytes",
 "serde_cbor",
 "strum_macros 0.25.3",
 "subtle",
 "zeroize",
]

[[package]]
name = "ic-crypto-internal-threshold-sig-bls12381-der"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "simple_asn1",
]

[[package]]
name = "ic-crypto-internal-types"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "arrayvec 0.7.4",
 "hex",
 "ic-protobuf",
 "phantom_newtype",
 "serde",
 "serde_cbor",
 "strum 0.25.0",
 "strum_macros 0.25.3",
 "thiserror",
 "zeroize",
]

[[package]]
name = "ic-crypto-secrets-containers"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "serde",
 "zeroize",
]

[[package]]
name = "ic-crypto-sha2"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "ic-crypto-internal-sha2",
]

[[package]]
name = "ic-crypto-tree-hash"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "assert_matches",
 "ic-crypto-internal-types",
 "ic-crypto-sha2",
 "ic-protobuf",
 "serde",
 "serde_bytes",
 "thiserror",
]

[[package]]
name = "ic-error-types"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "ic-utils 0.9.0",
 "serde",
 "strum 0.25.0",
 "strum_macros 0.25.3",
]

[[package]]
name = "ic-frontend-canister"
version = "0.2.5"
dependencies = [
 "candid",
 "ic-cdk",
 "ic-certified-assets",
]

[[package]]
name = "ic-http-certification"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7ddb96501529c2380e087fa9f4552fd0d416f5784bb1e48142d746e9b3d6ae13"
dependencies = [
 "candid",
 "http 0.2.12",
 "ic-certification 2.5.0",
 "ic-representation-independent-hash",
 "serde",
 "thiserror",
 "urlencoding",
]

[[package]]
name = "ic-ic00-types"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "candid",
 "ic-base-types",
 "ic-btc-interface",
 "ic-btc-types-internal",
 "ic-error-types",
 "ic-protobuf",
 "num-traits",
 "serde",
 "serde_bytes",
 "serde_cbor",
 "strum 0.25.0",
 "strum_macros 0.25.3",
]

[[package]]
name = "ic-identity-hsm"
version = "0.38.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fe9975ba49e50ccf10d8268b168b9a35da165904f56725a333e8294d7e5f1d92"
dependencies = [
 "hex",
 "ic-agent",
 "pkcs11",
 "sha2 0.10.8",
 "simple_asn1",
 "thiserror",
]

[[package]]
name = "ic-protobuf"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "bincode",
 "candid",
 "erased-serde",
 "maplit",
 "prost",
 "serde",
 "serde_json",
 "slog",
]

[[package]]
name = "ic-representation-independent-hash"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b4d9c969c80e9b445255341da79772680f503ef856b95b3ddf162b41d096df1f"
dependencies = [
 "leb128",
 "sha2 0.10.8",
]

[[package]]
name = "ic-response-verification"
version = "2.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "69ce81210d3747a4e334a86851c57725a5ec4d2d0c40b05b5dc4f9114c19c78a"
dependencies = [
 "base64 0.21.7",
 "candid",
 "flate2",
 "hex",
 "http 0.2.12",
 "ic-cbor",
 "ic-certificate-verification",
 "ic-certification 2.5.0",
 "ic-http-certification",
 "ic-representation-independent-hash",
 "leb128",
 "log",
 "nom",
 "sha2 0.10.8",
 "thiserror",
 "urlencoding",
]

[[package]]
name = "ic-response-verification-test-utils"
version = "2.3.0"
source = "git+https://github.com/dfinity/response-verification.git?rev=a65009624b61736df6d2dc17756bdbd02a84f599#a65009624b61736df6d2dc17756bdbd02a84f599"
dependencies = [
 "base64 0.21.7",
 "flate2",
 "hex",
 "ic-certification 2.3.0",
 "ic-certification-testing",
 "ic-types",
 "leb128",
 "serde",
 "serde_cbor",
 "sha2 0.10.8",
]

[[package]]
name = "ic-stable-structures"
version = "0.5.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "95dce29e3ceb0e6da3e78b305d95365530f2efd2146ca18590c0ef3aa6038568"

[[package]]
name = "ic-sys"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "hex",
 "ic-crypto-sha2",
 "lazy_static",
 "libc",
 "nix 0.24.3",
 "phantom_newtype",
 "tokio",
 "wsl",
]

[[package]]
name = "ic-transport-types"
version = "0.38.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc1834f2f91324b7263e0a05b95d385fd109b51efec991f053112d8cd2503d6e"
dependencies = [
 "candid",
 "hex",
 "ic-certification 2.5.0",
 "leb128",
 "serde",
 "serde_bytes",
 "serde_repr",
 "sha2 0.10.8",
 "thiserror",
]

[[package]]
name = "ic-types"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "base64 0.13.1",
 "bincode",
 "candid",
 "chrono",
 "derive_more",
 "hex",
 "ic-base-types",
 "ic-btc-types-internal",
 "ic-constants",
 "ic-crypto-internal-types",
 "ic-crypto-sha2",
 "ic-crypto-tree-hash",
 "ic-error-types",
 "ic-ic00-types",
 "ic-protobuf",
 "ic-utils 0.9.0",
 "maplit",
 "once_cell",
 "phantom_newtype",
 "prost",
 "serde",
 "serde_bytes",
 "serde_cbor",
 "serde_json",
 "serde_with",
 "strum 0.25.0",
 "strum_macros 0.25.3",
 "thiserror",
 "thousands",
]

[[package]]
name = "ic-utils"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "cvt",
 "hex",
 "ic-sys",
 "libc",
 "nix 0.24.3",
 "prost",
 "rand",
 "scoped_threadpool",
 "serde",
 "thiserror",
]

[[package]]
name = "ic-utils"
version = "0.38.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "75135bc8838398f2b4a512b900ee1d58fbf34bdedb74aedc0ae772e785535814"
dependencies = [
 "async-trait",
 "candid",
 "futures-util",
 "ic-agent",
 "once_cell",
 "semver",
 "serde",
 "serde_bytes",
 "sha2 0.10.8",
 "strum 0.26.3",
 "strum_macros 0.26.4",
 "thiserror",
 "time",
 "tokio",
]

[[package]]
name = "ic-verify-bls-signature"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d420b25c0091059f6c3c23a21427a81915e6e0aca3b79e0d403ed767f286a3b9"
dependencies = [
 "hex",
 "ic_bls12_381 0.10.0",
 "lazy_static",
 "pairing 0.23.0",
 "rand",
 "sha2 0.10.8",
]

[[package]]
name = "ic-wasm"
version = "0.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9742ed6640abfc3bd1c8ca82de222e907275d1e851249b13fca7588f13759fc7"
dependencies = [
 "anyhow",
 "candid",
 "clap",
 "libflate 2.1.0",
 "rustc-demangle",
 "serde",
 "serde_json",
 "tempfile",
 "thiserror",
 "walrus",
 "wasm-opt",
]

[[package]]
name = "ic0"
version = "0.21.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a54b5297861c651551676e8c43df805dad175cc33bc97dbd992edbbb85dcbcdf"

[[package]]
name = "ic_bls12_381"
version = "0.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c682cb199cd8fcb582a6023325d571a6464edda26c8063fe04b6f6082a1a363c"
dependencies = [
 "digest 0.9.0",
 "ff 0.12.1",
 "group 0.12.1",
 "pairing 0.22.0",
 "rand_core",
 "subtle",
 "zeroize",
]

[[package]]
name = "ic_bls12_381"
version = "0.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "22c65787944f32af084dffd0c68c1e544237b76e215654ddea8cd9f527dd8b69"
dependencies = [
 "digest 0.10.7",
 "ff 0.13.0",
 "group 0.13.0",
 "pairing 0.23.0",
 "rand_core",
 "subtle",
]

[[package]]
name = "ic_principal"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1762deb6f7c8d8c2bdee4b6c5a47b60195b74e9b5280faa5ba29692f8e17429c"
dependencies = [
 "arbitrary",
 "crc32fast",
 "data-encoding",
 "serde",
 "sha2 0.10.8",
 "thiserror",
]

[[package]]
name = "icrc-ledger-types"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "804c892bf95652101660a25cea10f059f73eb8973f6b04e0349758fda1190447"
dependencies = [
 "base32",
 "candid",
 "crc32fast",
 "hex",
 "num-bigint 0.4.5",
 "num-traits",
 "serde",
 "serde_bytes",
 "sha2 0.10.8",
]

[[package]]
name = "icx-asset"
version = "0.21.0"
dependencies = [
 "anstyle",
 "anyhow",
 "candid",
 "clap",
 "delay",
 "humantime",
 "ic-agent",
 "ic-asset",
 "ic-utils 0.38.0",
 "libflate 1.4.0",
 "num-traits",
 "pem 1.1.1",
 "serde",
 "serde_bytes",
 "serde_json",
 "slog",
 "slog-async",
 "slog-term",
 "thiserror",
 "time",
 "tokio",
 "walkdir",
]

[[package]]
name = "id-arena"
version = "2.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "25a2bc672d1148e28034f176e01fffebb08b35768468cc954630da77a1449005"

[[package]]
name = "ident_case"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b9e0384b61958566e926dc50660321d12159025e767c18e043daf26b70104c39"

[[package]]
name = "idl2json"
version = "0.10.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "96c98794e1d7ff2814db503a7fb3aed059b8c816cbaaaa7cf7420ab39edbca41"
dependencies = [
 "candid",
 "candid_parser",
 "serde_json",
 "sha2 0.10.8",
]

[[package]]
name = "idna"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "634d9b1461af396cad843f47fdba5597a4f9e6ddd4bfb6ff5d85028c25cb12f6"
dependencies = [
 "unicode-bidi",
 "unicode-normalization",
]

[[package]]
name = "indexmap"
version = "1.9.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bd070e393353796e801d209ad339e89596eb4c8d430d18ede6a1cced8fafbd99"
dependencies = [
 "autocfg",
 "hashbrown 0.12.3",
]

[[package]]
name = "indexmap"
version = "2.2.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "168fb715dda47215e360912c096649d23d58bf392ac62f73919e831745e40f26"
dependencies = [
 "equivalent",
 "hashbrown 0.14.5",
 "serde",
]

[[package]]
name = "indicatif"
version = "0.16.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2d207dc617c7a380ab07ff572a6e52fa202a2a8f355860ac9c38e23f8196be1b"
dependencies = [
 "console",
 "lazy_static",
 "number_prefix",
 "regex",
]

[[package]]
name = "inout"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a0c10553d664a4d0bcff9f4215d0aac67a639cc68ef660840afe309b807bc9f5"
dependencies = [
 "generic-array",
]

[[package]]
name = "instant"
version = "0.1.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7a5bbe824c507c5da5956355e86a746d82e0e1464f65d862cc5e71da70e94b2c"
dependencies = [
 "cfg-if",
]

[[package]]
name = "io-lifetimes"
version = "1.0.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eae7b9aee968036d54dce06cebaefd919e4472e753296daccd6d344e3e2df0c2"
dependencies = [
 "hermit-abi",
 "libc",
 "windows-sys 0.48.0",
]

[[package]]
name = "ipnet"
version = "2.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8f518f335dce6725a761382244631d86cf0ccb2863413590b31338feb467f9c3"

[[package]]
name = "is-terminal"
version = "0.4.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f23ff5ef2b80d608d61efee834934d862cd92461afc0560dedf493e4c033738b"
dependencies = [
 "hermit-abi",
 "libc",
 "windows-sys 0.52.0",
]

[[package]]
name = "is_ci"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7655c9839580ee829dfacba1d1278c2b7883e50a277ff7541299489d6bdfdc45"

[[package]]
name = "is_terminal_polyfill"
version = "1.70.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f8478577c03552c21db0e2724ffb8986a5ce7af88107e6be5d2ee6e158c12800"

[[package]]
name = "itertools"
version = "0.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "284f18f85651fe11e8a991b2adb42cb078325c996ed026d994719efcfca1d54b"
dependencies = [
 "either",
]

[[package]]
name = "itertools"
version = "0.10.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b0fd2260e829bddf4cb6ea802289de2f86d6a7a690192fbe91b3f46e0f2c8473"
dependencies = [
 "either",
]

[[package]]
name = "itertools"
version = "0.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b1c173a5686ce8bfa551b3563d0c2170bf24ca44da99c7ca4bfdab5418c3fe57"
dependencies = [
 "either",
]

[[package]]
name = "itertools"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ba291022dbbd398a455acf126c1e341954079855bc60dfdda641363bd6922569"
dependencies = [
 "either",
]

[[package]]
name = "itoa"
version = "1.0.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "49f1f14873335454500d59611f1cf4a4b0f786f9ac11f4312a78e4cf2566695b"

[[package]]
name = "jobserver"
version = "0.1.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d2b099aaa34a9751c5bf0878add70444e1ed2dd73f347be99003d4577277de6e"
dependencies = [
 "libc",
]

[[package]]
name = "js-sys"
version = "0.3.69"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "29c15563dc2726973df627357ce0c9ddddbea194836909d655df6a75d2cf296d"
dependencies = [
 "wasm-bindgen",
]

[[package]]
name = "json-patch"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec9ad60d674508f3ca8f380a928cfe7b096bc729c4e2dbfe3852bc45da3ab30b"
dependencies = [
 "serde",
 "serde_json",
 "thiserror",
]

[[package]]
name = "json5"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "96b0db21af676c1ce64250b5f40f3ce2cf27e4e47cb91ed91eb6fe9350b430c1"
dependencies = [
 "pest",
 "pest_derive",
 "serde",
]

[[package]]
name = "junction"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1c9c415a9b7b1e86cd5738f39d34c9e78c765da7fb1756dbd7d31b3b0d2e7afa"
dependencies = [
 "scopeguard",
 "windows-sys 0.52.0",
]

[[package]]
name = "k256"
version = "0.11.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72c1e0b51e7ec0a97369623508396067a486bd0cbed95a2659a4b863d28cfc8b"
dependencies = [
 "cfg-if",
 "ecdsa 0.14.8",
 "elliptic-curve 0.12.3",
 "sha2 0.10.8",
 "sha3",
]

[[package]]
name = "k256"
version = "0.13.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "956ff9b67e26e1a6a866cb758f12c6f8746208489e3e4a4b5580802f2f0a587b"
dependencies = [
 "cfg-if",
 "ecdsa 0.16.9",
 "elliptic-curve 0.13.8",
 "once_cell",
 "sha2 0.10.8",
 "signature 2.2.0",
]

[[package]]
name = "keccak"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ecc2af9a1119c51f12a14607e783cb977bde58bc069ff0c3da1095e635d70654"
dependencies = [
 "cpufeatures",
]

[[package]]
name = "keyring"
version = "1.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ba264b266563c1363dcce004776cbf198d7422a4262f77f4ca285bf26ae30955"
dependencies = [
 "byteorder",
 "secret-service",
 "security-framework",
 "winapi",
]

[[package]]
name = "lalrpop"
version = "0.20.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "55cb077ad656299f160924eb2912aa147d7339ea7d69e1b5517326fdcec3c1ca"
dependencies = [
 "ascii-canvas",
 "bit-set",
 "ena",
 "itertools 0.11.0",
 "lalrpop-util",
 "petgraph",
 "pico-args",
 "regex",
 "regex-syntax 0.8.3",
 "string_cache",
 "term",
 "tiny-keccak",
 "unicode-xid",
 "walkdir",
]

[[package]]
name = "lalrpop-util"
version = "0.20.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "507460a910eb7b32ee961886ff48539633b788a36b65692b95f225b844c82553"
dependencies = [
 "regex-automata 0.4.6",
]

[[package]]
name = "lazy_static"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646"

[[package]]
name = "leb128"
version = "0.2.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "884e2677b40cc8c339eaefcb701c32ef1fd2493d71118dc0ca4b6a736c93bd67"

[[package]]
name = "libc"
version = "0.2.154"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ae743338b92ff9146ce83992f766a31066a91a8c84a45e0e9f21e7cf6de6d346"

[[package]]
name = "libflate"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5ff4ae71b685bbad2f2f391fe74f6b7659a34871c08b210fdc039e43bee07d18"
dependencies = [
 "adler32",
 "crc32fast",
 "libflate_lz77 1.2.0",
]

[[package]]
name = "libflate"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "45d9dfdc14ea4ef0900c1cddbc8dcd553fbaacd8a4a282cf4018ae9dd04fb21e"
dependencies = [
 "adler32",
 "core2",
 "crc32fast",
 "dary_heap",
 "libflate_lz77 2.1.0",
]

[[package]]
name = "libflate_lz77"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a52d3a8bfc85f250440e4424db7d857e241a3aebbbe301f3eb606ab15c39acbf"
dependencies = [
 "rle-decode-fast",
]

[[package]]
name = "libflate_lz77"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6e0d73b369f386f1c44abd9c570d5318f55ccde816ff4b562fa452e5182863d"
dependencies = [
 "core2",
 "hashbrown 0.14.5",
 "rle-decode-fast",
]

[[package]]
name = "libloading"
version = "0.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f2b111a074963af1d37a139918ac6d49ad1d0d5e47f72fd55388619691a7d753"
dependencies = [
 "cc",
 "winapi",
]

[[package]]
name = "libm"
version = "0.2.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4ec2a862134d2a7d32d7983ddcdd1c4923530833c9f2ea1a44fc5fa473989058"

[[package]]
name = "libredox"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c0ff37bd590ca25063e35af745c343cb7a0271906fb7b37e4813e8f79f00268d"
dependencies = [
 "bitflags 2.5.0",
 "libc",
]

[[package]]
name = "libz-ng-sys"
version = "1.1.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c6409efc61b12687963e602df8ecf70e8ddacf95bc6576bcf16e3ac6328083c5"
dependencies = [
 "cmake",
 "libc",
]

[[package]]
name = "link-cplusplus"
version = "1.0.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9d240c6f7e1ba3a28b0249f774e6a9dd0175054b52dfbb61b16eb8505c3785c9"
dependencies = [
 "cc",
]

[[package]]
name = "linux-raw-sys"
version = "0.3.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ef53942eb7bf7ff43a617b3e2c1c4a5ecf5944a7c1bc12d7ee39bbb15e5c1519"

[[package]]
name = "linux-raw-sys"
version = "0.4.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "01cda141df6706de531b6c46c3a33ecca755538219bd484262fa09410c13539c"

[[package]]
name = "lock_api"
version = "0.4.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "07af8b9cdd281b7915f413fa73f29ebd5d55d0d3f0155584dade1ff18cea1b17"
dependencies = [
 "autocfg",
 "scopeguard",
]

[[package]]
name = "log"
version = "0.4.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "90ed8c1e510134f979dbc4f070f87d4313098b704861a105fe34231c70a3901c"

[[package]]
name = "logos"
version = "0.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c000ca4d908ff18ac99b93a062cb8958d331c3220719c52e77cb19cc6ac5d2c1"
dependencies = [
 "logos-derive",
]

[[package]]
name = "logos-codegen"
version = "0.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc487311295e0002e452025d6b580b77bb17286de87b57138f3b5db711cded68"
dependencies = [
 "beef",
 "fnv",
 "proc-macro2",
 "quote",
 "regex-syntax 0.6.29",
 "syn 2.0.61",
]

[[package]]
name = "logos-derive"
version = "0.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dbfc0d229f1f42d790440136d941afd806bc9e949e2bcb8faa813b0f00d1267e"
dependencies = [
 "logos-codegen",
]

[[package]]
name = "maplit"
version = "1.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3e2e65a1a2e43cfcb47a895c4c8b10d1f4a61097f9f254f183aee60cad9c651d"

[[package]]
name = "matchers"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8263075bb86c5a1b1427b5ae862e8889656f126e9f77c484496e8b47cf5c5558"
dependencies = [
 "regex-automata 0.1.10",
]

[[package]]
name = "memchr"
version = "2.7.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c8640c5d730cb13ebd907d8d04b52f55ac9a2eec55b440c8892f40d56c76c1d"

[[package]]
name = "memoffset"
version = "0.6.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5aa361d4faea93603064a027415f07bd8e1d5c88c9fbf68bf56a285428fd79ce"
dependencies = [
 "autocfg",
]

[[package]]
name = "mime"
version = "0.3.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6877bb514081ee2a7ff5ef9de3281f14a4dd4bceac4c09388074a6b5df8a139a"

[[package]]
name = "mime_guess"
version = "2.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4192263c238a5f0d0c6bfd21f336a313a4ce1c450542449ca191bb657b4642ef"
dependencies = [
 "mime",
 "unicase",
]

[[package]]
name = "minimal-lexical"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "68354c5c6bd36d73ff3feceb05efa59b6acb7626617f4962be322a825e61f79a"

[[package]]
name = "miniz_oxide"
version = "0.7.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9d811f3e15f28568be3407c8e7fdb6514c1cda3cb30683f15b6a1a1dc4ea14a7"
dependencies = [
 "adler",
]

[[package]]
name = "mio"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4569e456d394deccd22ce1c1913e6ea0e54519f577285001215d33557431afe4"
dependencies = [
 "hermit-abi",
 "libc",
 "wasi",
 "windows-sys 0.52.0",
]

[[package]]
name = "miracl_core_bls12381"
version = "4.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d07cbe42e2a8dd41df582fb8e00fc24d920b5561cc301fcb6d14e2e0434b500f"

[[package]]
name = "mockito"
version = "0.31.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "80f9fece9bd97ab74339fe19f4bcaf52b76dcc18e5364c7977c1838f76b38de9"
dependencies = [
 "assert-json-diff",
 "colored",
 "httparse",
 "lazy_static",
 "log",
 "rand",
 "regex",
 "serde_json",
 "serde_urlencoded",
 "similar",
]

[[package]]
name = "nb-connect"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b1bb540dc6ef51cfe1916ec038ce7a620daf3a111e2502d745197cd53d6bca15"
dependencies = [
 "libc",
 "socket2 0.4.10",
]

[[package]]
name = "new_debug_unreachable"
version = "1.0.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "650eef8c711430f1a879fdd01d4745a7deea475becfb90269c06775983bbf086"

[[package]]
name = "nix"
version = "0.22.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e4916f159ed8e5de0082076562152a76b7a1f64a01fd9d1e0fea002c37624faf"
dependencies = [
 "bitflags 1.3.2",
 "cc",
 "cfg-if",
 "libc",
 "memoffset",
]

[[package]]
name = "nix"
version = "0.24.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fa52e972a9a719cecb6864fb88568781eb706bac2cd1d4f04a648542dbf78069"
dependencies = [
 "bitflags 1.3.2",
 "cfg-if",
 "libc",
 "memoffset",
]

[[package]]
name = "nix"
version = "0.28.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ab2156c4fce2f8df6c499cc1c763e4394b7482525bf2a9701c9d79d215f519e4"
dependencies = [
 "bitflags 2.5.0",
 "cfg-if",
 "cfg_aliases",
 "libc",
]

[[package]]
name = "nom"
version = "7.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d273983c5a657a70a3e8f2a01329822f3b8c8172b73826411a55751e404a0a4a"
dependencies = [
 "memchr",
 "minimal-lexical",
]

[[package]]
name = "nom_locate"
version = "4.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1e3c83c053b0713da60c5b8de47fe8e494fe3ece5267b2f23090a07a053ba8f3"
dependencies = [
 "bytecount",
 "memchr",
 "nom",
]

[[package]]
name = "ntapi"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e8a3895c6391c39d7fe7ebc444a87eb2991b2a0bc718fdabd071eec617fc68e4"
dependencies = [
 "winapi",
]

[[package]]
name = "nu-ansi-term"
version = "0.46.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "77a8165726e8236064dbb45459242600304b42a5ea24ee2948e18e023bf7ba84"
dependencies = [
 "overload",
 "winapi",
]

[[package]]
name = "num"
version = "0.4.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "35bd024e8b2ff75562e5f34e7f4905839deb4b22955ef5e73d2fea1b9813cb23"
dependencies = [
 "num-bigint 0.4.5",
 "num-complex",
 "num-integer",
 "num-iter",
 "num-rational",
 "num-traits",
]

[[package]]
name = "num-bigint"
version = "0.2.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "090c7f9998ee0ff65aa5b723e4009f7b217707f1fb5ea551329cc4d6231fb304"
dependencies = [
 "autocfg",
 "num-integer",
 "num-traits",
]

[[package]]
name = "num-bigint"
version = "0.4.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c165a9ab64cf766f73521c0dd2cfdff64f488b8f0b3e621face3462d3db536d7"
dependencies = [
 "num-integer",
 "num-traits",
 "serde",
]

[[package]]
name = "num-complex"
version = "0.4.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "73f88a1307638156682bada9d7604135552957b7818057dcef22705b4d509495"
dependencies = [
 "num-traits",
]

[[package]]
name = "num-conv"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "51d515d32fb182ee37cda2ccdcb92950d6a3c2893aa280e540671c2cd0f3b1d9"

[[package]]
name = "num-integer"
version = "0.1.46"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7969661fd2958a5cb096e56c8e1ad0444ac2bbcd0061bd28660485a44879858f"
dependencies = [
 "num-traits",
]

[[package]]
name = "num-iter"
version = "0.1.45"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1429034a0490724d0075ebb2bc9e875d6503c3cf69e235a8941aa757d83ef5bf"
dependencies = [
 "autocfg",
 "num-integer",
 "num-traits",
]

[[package]]
name = "num-rational"
version = "0.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f83d14da390562dca69fc84082e73e548e1ad308d24accdedd2720017cb37824"
dependencies = [
 "num-bigint 0.4.5",
 "num-integer",
 "num-traits",
]

[[package]]
name = "num-traits"
version = "0.2.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "071dfc062690e90b734c0b2273ce72ad0ffa95f0c74596bc250dcfd960262841"
dependencies = [
 "autocfg",
 "libm",
]

[[package]]
name = "num_threads"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c7398b9c8b70908f6371f47ed36737907c87c52af34c268fed0bf0ceb92ead9"
dependencies = [
 "libc",
]

[[package]]
name = "number_prefix"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "830b246a0e5f20af87141b25c173cd1b609bd7779a4617d6ec582abaf90870f3"

[[package]]
name = "object"
version = "0.32.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a6a622008b6e321afc04970976f62ee297fdbaa6f95318ca343e3eebb9648441"
dependencies = [
 "memchr",
]

[[package]]
name = "once_cell"
version = "1.19.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3fdb12b2476b595f9358c5161aa467c2438859caa136dec86c26fdd2efe17b92"

[[package]]
name = "opaque-debug"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c08d65885ee38876c4f86fa503fb49d7b507c2b62552df7c70b2fce627e06381"

[[package]]
name = "openssl-probe"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ff011a302c396a5197692431fc1948019154afc178baf7d8e37367442a4601cf"

[[package]]
name = "os_str_bytes"
version = "6.6.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2355d85b9a3786f481747ced0e0ff2ba35213a1f9bd406ed906554d7af805a1"
dependencies = [
 "memchr",
]

[[package]]
name = "overload"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b15813163c1d831bf4a13c3610c05c0d03b39feb07f7e09fa234dac9b15aaf39"

[[package]]
name = "p256"
version = "0.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c9863ad85fa8f4460f9c48cb909d38a0d689dba1f6f6988a5e3e0d31071bcd4b"
dependencies = [
 "ecdsa 0.16.9",
 "elliptic-curve 0.13.8",
 "primeorder",
 "sha2 0.10.8",
]

[[package]]
name = "pairing"
version = "0.22.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "135590d8bdba2b31346f9cd1fb2a912329f5135e832a4f422942eb6ead8b6b3b"
dependencies = [
 "group 0.12.1",
]

[[package]]
name = "pairing"
version = "0.23.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "81fec4625e73cf41ef4bb6846cafa6d44736525f442ba45e407c4a000a13996f"
dependencies = [
 "group 0.13.0",
]

[[package]]
name = "parking"
version = "2.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bb813b8af86854136c6922af0598d719255ecb2179515e6e7730d468f05c9cae"

[[package]]
name = "parking_lot"
version = "0.11.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7d17b78036a60663b797adeaee46f5c9dfebb86948d1255007a1d6be0271ff99"
dependencies = [
 "instant",
 "lock_api",
 "parking_lot_core 0.8.6",
]

[[package]]
name = "parking_lot"
version = "0.12.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7e4af0ca4f6caed20e900d564c242b8e5d4903fdacf31d3daf527b66fe6f42fb"
dependencies = [
 "lock_api",
 "parking_lot_core 0.9.10",
]

[[package]]
name = "parking_lot_core"
version = "0.8.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "60a2cfe6f0ad2bfc16aefa463b497d5c7a5ecd44a23efa72aa342d90177356dc"
dependencies = [
 "cfg-if",
 "instant",
 "libc",
 "redox_syscall 0.2.16",
 "smallvec",
 "winapi",
]

[[package]]
name = "parking_lot_core"
version = "0.9.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1e401f977ab385c9e4e3ab30627d6f26d00e2c73eef317493c4ec6d468726cf8"
dependencies = [
 "cfg-if",
 "libc",
 "redox_syscall 0.5.1",
 "smallvec",
 "windows-targets 0.52.6",
]

[[package]]
name = "password-hash"
version = "0.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7676374caaee8a325c9e7a2ae557f216c5563a171d6997b0ef8a65af35147700"
dependencies = [
 "base64ct",
 "rand_core",
 "subtle",
]

[[package]]
name = "paste"
version = "1.0.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "57c0d7b74b563b49d38dae00a0c37d4d6de9b432382b2892f0574ddcae73fd0a"

[[package]]
name = "patch"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "15c07fdcdd8b05bdcf2a25bc195b6c34cbd52762ada9dba88bf81e7686d14e7a"
dependencies = [
 "chrono",
 "nom",
 "nom_locate",
]

[[package]]
name = "pbkdf2"
version = "0.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "83a0692ec44e4cf1ef28ca317f14f8f07da2d95ec3fa01f86e4467b725e60917"
dependencies = [
 "digest 0.10.7",
]

[[package]]
name = "pem"
version = "1.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a8835c273a76a90455d7344889b0964598e3316e2a79ede8e36f16bdcf2228b8"
dependencies = [
 "base64 0.13.1",
]

[[package]]
name = "pem"
version = "3.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e459365e590736a54c3fa561947c84837534b8e9af6fc5bf781307e82658fae"
dependencies = [
 "base64 0.22.1",
 "serde",
]

[[package]]
name = "pem-rfc7468"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24d159833a9105500e0398934e205e0773f0b27529557134ecfc51c27646adac"
dependencies = [
 "base64ct",
]

[[package]]
name = "pem-rfc7468"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "88b39c9bfcfc231068454382784bb460aae594343fb030d46e9f50a645418412"
dependencies = [
 "base64ct",
]

[[package]]
name = "percent-encoding"
version = "2.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e3148f5046208a5d56bcfc03053e3ca6334e51da8dfb19b6cdc8b306fae3283e"

[[package]]
name = "pest"
version = "2.7.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "560131c633294438da9f7c4b08189194b20946c8274c6b9e38881a7874dc8ee8"
dependencies = [
 "memchr",
 "thiserror",
 "ucd-trie",
]

[[package]]
name = "pest_consume"
version = "1.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "79447402d15d18e7142e14c72f2e63fa3d155be1bc5b70b3ccbb610ac55f536b"
dependencies = [
 "pest",
 "pest_consume_macros",
 "pest_derive",
]

[[package]]
name = "pest_consume_macros"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9d8630a7a899cb344ec1c16ba0a6b24240029af34bdc0a21f84e411d7f793f29"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "pest_derive"
version = "2.7.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "26293c9193fbca7b1a3bf9b79dc1e388e927e6cacaa78b4a3ab705a1d3d41459"
dependencies = [
 "pest",
 "pest_generator",
]

[[package]]
name = "pest_generator"
version = "2.7.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3ec22af7d3fb470a85dd2ca96b7c577a1eb4ef6f1683a9fe9a8c16e136c04687"
dependencies = [
 "pest",
 "pest_meta",
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "pest_meta"
version = "2.7.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d7a240022f37c361ec1878d646fc5b7d7c4d28d5946e1a80ad5a7a4f4ca0bdcd"
dependencies = [
 "once_cell",
 "pest",
 "sha2 0.10.8",
]

[[package]]
name = "petgraph"
version = "0.6.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b4c5cc86750666a3ed20bdaf5ca2a0344f9c67674cae0515bec2da16fbaa47db"
dependencies = [
 "fixedbitset",
 "indexmap 2.2.6",
]

[[package]]
name = "phantom_newtype"
version = "0.9.0"
source = "git+https://github.com/dfinity/ic.git?rev=1290256484f59c3d950c5e9a098e97383b248ad6#1290256484f59c3d950c5e9a098e97383b248ad6"
dependencies = [
 "candid",
 "serde",
 "slog",
]

[[package]]
name = "phf_shared"
version = "0.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b6796ad771acdc0123d2a88dc428b5e38ef24456743ddb1744ed628f9815c096"
dependencies = [
 "siphasher",
]

[[package]]
name = "pico-args"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5be167a7af36ee22fe3115051bc51f6e6c7054c9348e28deb4f49bd6f705a315"

[[package]]
name = "pin-project"
version = "1.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b6bf43b791c5b9e34c3d182969b4abb522f9343702850a2e57f460d00d09b4b3"
dependencies = [
 "pin-project-internal",
]

[[package]]
name = "pin-project-internal"
version = "1.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2f38a4412a78282e09a2cf38d195ea5420d15ba0602cb375210efbc877243965"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "pin-project-lite"
version = "0.2.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bda66fc9667c18cb2758a2ac84d1167245054bcf85d5d1aaa6923f45801bdd02"

[[package]]
name = "pin-utils"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"

[[package]]
name = "pkcs11"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3aca6d67e4c8613bfe455599d0233d00735f85df2001f6bfd9bb7ac0496b10af"
dependencies = [
 "libloading",
 "num-bigint 0.2.6",
]

[[package]]
name = "pkcs8"
version = "0.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9eca2c590a5f85da82668fa685c09ce2888b9430e83299debf1f34b65fd4a4ba"
dependencies = [
 "der 0.6.1",
 "spki 0.6.0",
]

[[package]]
name = "pkcs8"
version = "0.10.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f950b2377845cebe5cf8b5165cb3cc1a5e0fa5cfa3e1f7f55707d8fd82e0a7b7"
dependencies = [
 "der 0.7.9",
 "spki 0.7.3",
]

[[package]]
name = "pocket-ic"
version = "4.0.0"
source = "git+https://github.com/dfinity/ic?rev=179973553248415fc85679d853b48b0e0ec231c6#179973553248415fc85679d853b48b0e0ec231c6"
dependencies = [
 "base64 0.13.1",
 "candid",
 "hex",
 "ic-cdk",
 "reqwest",
 "schemars",
 "serde",
 "serde_bytes",
 "serde_json",
 "sha2 0.10.8",
 "slog",
 "tokio",
 "tracing",
 "tracing-appender",
 "tracing-subscriber",
]

[[package]]
name = "polling"
version = "2.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4b2d323e8ca7996b3e23126511a523f7e62924d93ecd5ae73b333815b0eb3dce"
dependencies = [
 "autocfg",
 "bitflags 1.3.2",
 "cfg-if",
 "concurrent-queue",
 "libc",
 "log",
 "pin-project-lite",
 "windows-sys 0.48.0",
]

[[package]]
name = "polyval"
version = "0.6.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9d1fe60d06143b2430aa532c94cfe9e29783047f06c0d7fd359a9a51b729fa25"
dependencies = [
 "cfg-if",
 "cpufeatures",
 "opaque-debug",
 "universal-hash",
]

[[package]]
name = "powerfmt"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "439ee305def115ba05938db6eb1644ff94165c5ab5e9420d1c1bcedbba909391"

[[package]]
name = "ppv-lite86"
version = "0.2.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b40af805b3121feab8a3c29f04d8ad262fa8e0561883e7653e024ae4479e6de"

[[package]]
name = "precomputed-hash"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "925383efa346730478fb4838dbe9137d2a47675ad789c546d150a6e1dd4ab31c"

[[package]]
name = "pretty"
version = "0.11.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "83f3aa1e3ca87d3b124db7461265ac176b40c277f37e503eaa29c9c75c037846"
dependencies = [
 "arrayvec 0.5.2",
 "log",
 "typed-arena",
 "unicode-segmentation",
]

[[package]]
name = "pretty"
version = "0.12.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b55c4d17d994b637e2f4daf6e5dc5d660d209d5642377d675d7a1c3ab69fa579"
dependencies = [
 "arrayvec 0.5.2",
 "typed-arena",
 "unicode-width",
]

[[package]]
name = "pretty_assertions"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "af7cee1a6c8a5b9208b3cb1061f10c0cb689087b3d8ce85fb9d2dd7a29b6ba66"
dependencies = [
 "diff",
 "yansi",
]

[[package]]
name = "primeorder"
version = "0.13.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "353e1ca18966c16d9deb1c69278edbc5f194139612772bd9537af60ac231e1e6"
dependencies = [
 "elliptic-curve 0.13.8",
]

[[package]]
name = "proc-macro-crate"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1d6ea3c4595b96363c13943497db34af4460fb474a95c43f4446ad341b8c9785"
dependencies = [
 "toml 0.5.11",
]

[[package]]
name = "proc-macro-crate"
version = "1.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7f4c021e1093a56626774e81216a4ce732a735e5bad4868a03f3ed65ca0c3919"
dependencies = [
 "once_cell",
 "toml_edit 0.19.15",
]

[[package]]
name = "proc-macro-crate"
version = "3.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6d37c51ca738a55da99dc0c4a34860fd675453b8b36209178c2249bb13651284"
dependencies = [
 "toml_edit 0.21.1",
]

[[package]]
name = "proc-macro-error"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "da25490ff9892aab3fcf7c36f08cfb902dd3e71ca0f9f9517bea02a73a5ce38c"
dependencies = [
 "proc-macro-error-attr",
 "proc-macro2",
 "quote",
 "version_check",
]

[[package]]
name = "proc-macro-error-attr"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a1be40180e52ecc98ad80b184934baf3d0d29f979574e439af5a55274b35f869"
dependencies = [
 "proc-macro2",
 "quote",
 "version_check",
]

[[package]]
name = "proc-macro2"
version = "1.0.82"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8ad3d49ab951a01fbaafe34f2ec74122942fe18a3f9814c3268f1bb72042131b"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "proptest"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "31b476131c3c86cb68032fdc5cb6d5a1045e3e42d96b69fa599fd77701e1f5bf"
dependencies = [
 "bit-set",
 "bit-vec",
 "bitflags 2.5.0",
 "lazy_static",
 "num-traits",
 "rand",
 "rand_chacha",
 "rand_xorshift",
 "regex-syntax 0.8.3",
 "rusty-fork",
 "tempfile",
 "unarray",
]

[[package]]
name = "prost"
version = "0.12.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d0f5d036824e4761737860779c906171497f6d55681139d8312388f8fe398922"
dependencies = [
 "bytes",
 "prost-derive",
]

[[package]]
name = "prost-derive"
version = "0.12.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9554e3ab233f0a932403704f1a1d08c30d5ccd931adfdfa1e8b5a19b52c1d55a"
dependencies = [
 "anyhow",
 "itertools 0.12.1",
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "psm"
version = "0.1.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5787f7cda34e3033a72192c018bc5883100330f362ef279a8cbccfce8bb4e874"
dependencies = [
 "cc",
]

[[package]]
name = "ptr_meta"
version = "0.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0738ccf7ea06b608c10564b31debd4f5bc5e197fc8bfe088f68ae5ce81e7a4f1"
dependencies = [
 "ptr_meta_derive",
]

[[package]]
name = "ptr_meta_derive"
version = "0.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "16b845dbfca988fa33db069c0e230574d15a3088f147a87b64c7589eb662c9ac"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "quick-error"
version = "1.2.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a1d01941d82fa2ab50be1e79e6714289dd7cde78eba4c074bc5a4374f650dfe0"

[[package]]
name = "quinn"
version = "0.11.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b22d8e7369034b9a7132bc2008cac12f2013c8132b45e0554e6e20e2617f2156"
dependencies = [
 "bytes",
 "pin-project-lite",
 "quinn-proto",
 "quinn-udp",
 "rustc-hash 2.0.0",
 "rustls 0.23.7",
 "socket2 0.5.7",
 "thiserror",
 "tokio",
 "tracing",
]

[[package]]
name = "quinn-proto"
version = "0.11.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fadfaed2cd7f389d0161bb73eeb07b7b78f8691047a6f3e73caaeae55310a4a6"
dependencies = [
 "bytes",
 "rand",
 "ring 0.17.8",
 "rustc-hash 2.0.0",
 "rustls 0.23.7",
 "slab",
 "thiserror",
 "tinyvec",
 "tracing",
]

[[package]]
name = "quinn-udp"
version = "0.5.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8bffec3605b73c6f1754535084a85229fa8a30f86014e6c81aeec4abb68b0285"
dependencies = [
 "libc",
 "once_cell",
 "socket2 0.5.7",
 "tracing",
 "windows-sys 0.52.0",
]

[[package]]
name = "quote"
version = "1.0.36"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0fa76aaf39101c457836aec0ce2316dbdc3ab723cdda1c6bd4e6ad4208acaca7"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "radium"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc33ff2d4973d518d823d61aa239014831e521c75da58e3df4840d3f47749d09"

[[package]]
name = "rand"
version = "0.8.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "34af8d1a0e25924bc5b7c43c079c942339d8f0a8b57c39049bef581b46327404"
dependencies = [
 "libc",
 "rand_chacha",
 "rand_core",
]

[[package]]
name = "rand_chacha"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6c10a63a0fa32252be49d21e7709d4d4baf8d231c2dbce1eaa8141b9b127d88"
dependencies = [
 "ppv-lite86",
 "rand_core",
]

[[package]]
name = "rand_core"
version = "0.6.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec0be4795e2f6a28069bec0b5ff3e2ac9bafc99e6a9a7dc3547996c5c816922c"
dependencies = [
 "getrandom",
]

[[package]]
name = "rand_xorshift"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d25bf25ec5ae4a3f1b92f929810509a2f53d7dca2f50b794ff57e3face536c8f"
dependencies = [
 "rand_core",
]

[[package]]
name = "rangemap"
version = "1.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f60fcc7d6849342eff22c4350c8b9a989ee8ceabc4b481253e8946b9fe83d684"

[[package]]
name = "rayon"
version = "1.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b418a60154510ca1a002a752ca9714984e21e4241e804d32555251faf8b78ffa"
dependencies = [
 "either",
 "rayon-core",
]

[[package]]
name = "rayon-core"
version = "1.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1465873a3dfdaa8ae7cb14b4383657caab0b3e8a0aa9ae8e04b044854c8dfce2"
dependencies = [
 "crossbeam-deque",
 "crossbeam-utils",
]

[[package]]
name = "redox_syscall"
version = "0.2.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fb5a58c1855b4b6819d59012155603f0b22ad30cad752600aadfcb695265519a"
dependencies = [
 "bitflags 1.3.2",
]

[[package]]
name = "redox_syscall"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4722d768eff46b75989dd134e5c353f0d6296e5aaa3132e776cbdb56be7731aa"
dependencies = [
 "bitflags 1.3.2",
]

[[package]]
name = "redox_syscall"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "469052894dcb553421e483e4209ee581a45100d31b4018de03e5a7ad86374a7e"
dependencies = [
 "bitflags 2.5.0",
]

[[package]]
name = "redox_users"
version = "0.4.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bd283d9651eeda4b2a83a43c1c91b266c40fd76ecd39a50a8c630ae69dc72891"
dependencies = [
 "getrandom",
 "libredox",
 "thiserror",
]

[[package]]
name = "regex"
version = "1.10.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c117dbdfde9c8308975b6a18d71f3f385c89461f7b3fb054288ecf2a2058ba4c"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-automata 0.4.6",
 "regex-syntax 0.8.3",
]

[[package]]
name = "regex-automata"
version = "0.1.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c230d73fb8d8c1b9c0b3135c5142a8acee3a0558fb8db5cf1cb65f8d7862132"
dependencies = [
 "regex-syntax 0.6.29",
]

[[package]]
name = "regex-automata"
version = "0.4.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "86b83b8b9847f9bf95ef68afb0b8e6cdb80f498442f5179a29fad448fcc1eaea"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-syntax 0.8.3",
]

[[package]]
name = "regex-syntax"
version = "0.6.29"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f162c6dd7b008981e4d40210aca20b4bd0f9b60ca9271061b07f78537722f2e1"

[[package]]
name = "regex-syntax"
version = "0.8.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "adad44e29e4c806119491a7f06f03de4d1af22c3a680dd47f1e6e179439d1f56"

[[package]]
name = "rend"
version = "0.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "71fe3824f5629716b1589be05dacd749f6aa084c87e00e016714a8cdfccc997c"
dependencies = [
 "bytecheck",
]

[[package]]
name = "reqwest"
version = "0.12.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f8f4955649ef5c38cc7f9e8aa41761d48fb9677197daea9984dc54f56aad5e63"
dependencies = [
 "base64 0.22.1",
 "bytes",
 "futures-channel",
 "futures-core",
 "futures-util",
 "h2 0.4.4",
 "http 1.1.0",
 "http-body 1.0.0",
 "http-body-util",
 "hyper 1.3.1",
 "hyper-rustls 0.27.2",
 "hyper-util",
 "ipnet",
 "js-sys",
 "log",
 "mime",
 "mime_guess",
 "once_cell",
 "percent-encoding",
 "pin-project-lite",
 "quinn",
 "rustls 0.23.7",
 "rustls-native-certs",
 "rustls-pemfile",
 "rustls-pki-types",
 "serde",
 "serde_json",
 "serde_urlencoded",
 "sync_wrapper",
 "tokio",
 "tokio-rustls 0.26.0",
 "tokio-socks",
 "tokio-util",
 "tower-service",
 "url",
 "wasm-bindgen",
 "wasm-bindgen-futures",
 "wasm-streams",
 "web-sys",
 "webpki-roots 0.26.1",
 "windows-registry",
]

[[package]]
name = "rfc6979"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7743f17af12fa0b03b803ba12cd6a8d9483a587e89c69445e3909655c0b9fabb"
dependencies = [
 "crypto-bigint 0.4.9",
 "hmac 0.12.1",
 "zeroize",
]

[[package]]
name = "rfc6979"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f8dd2a808d456c4a54e300a23e9f5a67e122c3024119acbfd73e3bf664491cb2"
dependencies = [
 "hmac 0.12.1",
 "subtle",
]

[[package]]
name = "ring"
version = "0.16.20"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3053cf52e236a3ed746dfc745aa9cacf1b791d846bdaf412f60a8d7d6e17c8fc"
dependencies = [
 "cc",
 "libc",
 "once_cell",
 "spin 0.5.2",
 "untrusted 0.7.1",
 "web-sys",
 "winapi",
]

[[package]]
name = "ring"
version = "0.17.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c17fa4cb658e3583423e915b9f3acc01cceaee1860e33d59ebae66adc3a2dc0d"
dependencies = [
 "cc",
 "cfg-if",
 "getrandom",
 "libc",
 "spin 0.9.8",
 "untrusted 0.9.0",
 "windows-sys 0.52.0",
]

[[package]]
name = "ripemd"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bd124222d17ad93a644ed9d011a40f4fb64aa54275c08cc216524a9ea82fb09f"
dependencies = [
 "digest 0.10.7",
]

[[package]]
name = "rkyv"
version = "0.7.44"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5cba464629b3394fc4dbc6f940ff8f5b4ff5c7aef40f29166fd4ad12acbc99c0"
dependencies = [
 "bitvec",
 "bytecheck",
 "bytes",
 "hashbrown 0.12.3",
 "ptr_meta",
 "rend",
 "rkyv_derive",
 "seahash",
 "tinyvec",
 "uuid",
]

[[package]]
name = "rkyv_derive"
version = "0.7.44"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a7dddfff8de25e6f62b9d64e6e432bf1c6736c57d20323e15ee10435fbda7c65"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "rle-decode-fast"
version = "1.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3582f63211428f83597b51b2ddb88e2a91a9d52d12831f9d08f5e624e8977422"

[[package]]
name = "rust_decimal"
version = "1.35.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1790d1c4c0ca81211399e0e0af16333276f375209e71a37b67698a373db5b47a"
dependencies = [
 "arrayvec 0.7.4",
 "borsh",
 "bytes",
 "num-traits",
 "rand",
 "rkyv",
 "serde",
 "serde_json",
]

[[package]]
name = "rustc-demangle"
version = "0.1.24"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "719b953e2095829ee67db738b3bfa9fa368c94900df327b3f07fe6e794d2fe1f"

[[package]]
name = "rustc-hash"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "08d43f7aa6b08d49f382cde6a7982047c3426db949b1424bc4b7ec9ae12c6ce2"

[[package]]
name = "rustc-hash"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "583034fd73374156e66797ed8e5b0d5690409c9226b22d87cb7f19821c05d152"

[[package]]
name = "rustix"
version = "0.37.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fea8ca367a3a01fe35e6943c400addf443c0f57670e6ec51196f71a4b8762dd2"
dependencies = [
 "bitflags 1.3.2",
 "errno",
 "io-lifetimes",
 "libc",
 "linux-raw-sys 0.3.8",
 "windows-sys 0.48.0",
]

[[package]]
name = "rustix"
version = "0.38.34"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "70dc5ec042f7a43c4a73241207cecc9873a06d45debb38b329f8541d85c2730f"
dependencies = [
 "bitflags 2.5.0",
 "errno",
 "libc",
 "linux-raw-sys 0.4.13",
 "windows-sys 0.52.0",
]

[[package]]
name = "rustls"
version = "0.21.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3f56a14d1f48b391359b22f731fd4bd7e43c97f3c50eee276f3aa09c94784d3e"
dependencies = [
 "ring 0.17.8",
 "rustls-webpki 0.101.7",
 "sct",
]

[[package]]
name = "rustls"
version = "0.23.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ebbbdb961df0ad3f2652da8f3fdc4b36122f568f968f45ad3316f26c025c677b"
dependencies = [
 "once_cell",
 "ring 0.17.8",
 "rustls-pki-types",
 "rustls-webpki 0.102.3",
 "subtle",
 "zeroize",
]

[[package]]
name = "rustls-native-certs"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8f1fb85efa936c42c6d5fc28d2629bb51e4b2f4b8a5211e297d599cc5a093792"
dependencies = [
 "openssl-probe",
 "rustls-pemfile",
 "rustls-pki-types",
 "schannel",
 "security-framework",
]

[[package]]
name = "rustls-pemfile"
version = "2.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "29993a25686778eb88d4189742cd713c9bce943bc54251a33509dc63cbacf73d"
dependencies = [
 "base64 0.22.1",
 "rustls-pki-types",
]

[[package]]
name = "rustls-pki-types"
version = "1.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "976295e77ce332211c0d24d92c0e83e50f5c5f046d11082cea19f3df13a3562d"

[[package]]
name = "rustls-webpki"
version = "0.101.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8b6275d1ee7a1cd780b64aca7726599a1dbc893b1e64144529e55c3c2f745765"
dependencies = [
 "ring 0.17.8",
 "untrusted 0.9.0",
]

[[package]]
name = "rustls-webpki"
version = "0.102.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f3bce581c0dd41bce533ce695a1437fa16a7ab5ac3ccfa99fe1a620a7885eabf"
dependencies = [
 "ring 0.17.8",
 "rustls-pki-types",
 "untrusted 0.9.0",
]

[[package]]
name = "rustversion"
version = "1.0.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "092474d1a01ea8278f69e6a358998405fae5b8b963ddaeb2b0b04a128bf1dfb0"

[[package]]
name = "rusty-fork"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cb3dcc6e454c328bb824492db107ab7c0ae8fcffe4ad210136ef014458c1bc4f"
dependencies = [
 "fnv",
 "quick-error",
 "tempfile",
 "wait-timeout",
]

[[package]]
name = "ryu"
version = "1.0.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f3cb5ba0dc43242ce17de99c180e96db90b235b8a9fdc9543c96d2209116bd9f"

[[package]]
name = "same-file"
version = "1.0.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "93fc1dc3aaa9bfed95e02e6eadabb4baf7e3078b0bd1b4d7b6b0b68378900502"
dependencies = [
 "winapi-util",
]

[[package]]
name = "schannel"
version = "0.1.23"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fbc91545643bcf3a0bbb6569265615222618bdf33ce4ffbbd13c4bbd4c093534"
dependencies = [
 "windows-sys 0.52.0",
]

[[package]]
name = "schemars"
version = "0.8.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fc6e7ed6919cb46507fb01ff1654309219f62b4d603822501b0b80d42f6f21ef"
dependencies = [
 "dyn-clone",
 "schemars_derive",
 "serde",
 "serde_json",
]

[[package]]
name = "schemars_derive"
version = "0.8.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "185f2b7aa7e02d418e453790dde16890256bbd2bcd04b7dc5348811052b53f49"
dependencies = [
 "proc-macro2",
 "quote",
 "serde_derive_internals",
 "syn 2.0.61",
]

[[package]]
name = "scoped-tls"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e1cf6437eb19a8f4a6cc0f7dca544973b0b78843adbfeb3683d1a94a0024a294"

[[package]]
name = "scoped_threadpool"
version = "0.1.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1d51f5df5af43ab3f1360b429fa5e0152ac5ce8c0bd6485cae490332e96846a8"

[[package]]
name = "scopeguard"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "94143f37725109f92c262ed2cf5e59bce7498c01bcc1502d7b9afe439a4e9f49"

[[package]]
name = "scratch"
version = "1.0.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a3cf7c11c38cb994f3d40e8a8cde3bbd1f72a435e4c49e85d6553d8312306152"

[[package]]
name = "sct"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "da046153aa2352493d6cb7da4b6e5c0c057d8a1d0a9aa8560baffdd945acd414"
dependencies = [
 "ring 0.17.8",
 "untrusted 0.9.0",
]

[[package]]
name = "seahash"
version = "4.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1c107b6f4780854c8b126e228ea8869f4d7b71260f962fefb57b996b8959ba6b"

[[package]]
name = "sec1"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3be24c1842290c45df0a7bf069e0c268a747ad05a192f2fd7dcfdbc1cba40928"
dependencies = [
 "base16ct 0.1.1",
 "der 0.6.1",
 "generic-array",
 "pkcs8 0.9.0",
 "subtle",
 "zeroize",
]

[[package]]
name = "sec1"
version = "0.7.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d3e97a565f76233a6003f9f5c54be1d9c5bdfa3eccfb189469f11ec4901c47dc"
dependencies = [
 "base16ct 0.2.0",
 "der 0.7.9",
 "generic-array",
 "pkcs8 0.10.2",
 "subtle",
 "zeroize",
]

[[package]]
name = "secret-service"
version = "2.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e1da5c423b8783185fd3fecd1c8796c267d2c089d894ce5a93c280a5d3f780a2"
dependencies = [
 "aes 0.7.5",
 "block-modes",
 "hkdf",
 "lazy_static",
 "num",
 "rand",
 "serde",
 "sha2 0.9.9",
 "zbus",
 "zbus_macros",
 "zvariant",
 "zvariant_derive",
]

[[package]]
name = "security-framework"
version = "2.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c627723fd09706bacdb5cf41499e95098555af3c3c29d014dc3c458ef6be11c0"
dependencies = [
 "bitflags 2.5.0",
 "core-foundation",
 "core-foundation-sys",
 "libc",
 "security-framework-sys",
]

[[package]]
name = "security-framework-sys"
version = "2.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "317936bbbd05227752583946b9e66d7ce3b489f84e11a94a510b4437fef407d7"
dependencies = [
 "core-foundation-sys",
 "libc",
]

[[package]]
name = "semver"
version = "1.0.23"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "61697e0a1c7e512e84a621326239844a24d8207b4669b41bc18b32ea5cbf988b"
dependencies = [
 "serde",
]

[[package]]
name = "serde"
version = "1.0.204"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bc76f558e0cbb2a839d37354c575f1dc3fdc6546b5be373ba43d95f231bf7c12"
dependencies = [
 "serde_derive",
]

[[package]]
name = "serde-wasm-bindgen"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f3b143e2833c57ab9ad3ea280d21fd34e285a42837aeb0ee301f4f41890fa00e"
dependencies = [
 "js-sys",
 "serde",
 "wasm-bindgen",
]

[[package]]
name = "serde_bytes"
version = "0.11.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "387cc504cb06bb40a96c8e04e951fe01854cf6bc921053c954e4a606d9675c6a"
dependencies = [
 "serde",
]

[[package]]
name = "serde_cbor"
version = "0.11.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2bef2ebfde456fb76bbcf9f59315333decc4fda0b2b44b420243c11e0f5ec1f5"
dependencies = [
 "half",
 "serde",
]

[[package]]
name = "serde_derive"
version = "1.0.204"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e0cd7e117be63d3c3678776753929474f3b04a43a080c744d6b0ae2a8c28e222"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "serde_derive_internals"
version = "0.29.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "330f01ce65a3a5fe59a60c82f3c9a024b573b8a6e875bd233fe5f934e71d54e3"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "serde_dhall"
version = "0.11.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "77c01a6b1d6f9f33bb1ad5652249e938297e43a1f43418236f7b72e64837e347"
dependencies = [
 "dhall",
 "dhall_proc_macros",
 "doc-comment",
 "serde",
 "url",
]

[[package]]
name = "serde_json"
version = "1.0.117"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "455182ea6142b14f93f4bc5320a2b31c1f266b66a4a5c858b013302a5d8cbfc3"
dependencies = [
 "itoa",
 "ryu",
 "serde",
]

[[package]]
name = "serde_repr"
version = "0.1.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6c64451ba24fc7a6a2d60fc75dd9c83c90903b19028d4eff35e88fc1e86564e9"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "serde_spanned"
version = "0.6.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eb3622f419d1296904700073ea6cc23ad690adbd66f13ea683df73298736f0c1"
dependencies = [
 "serde",
]

[[package]]
name = "serde_tokenstream"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "797ba1d80299b264f3aac68ab5d12e5825a561749db4df7cd7c8083900c5d4e9"
dependencies = [
 "proc-macro2",
 "serde",
 "syn 1.0.109",
]

[[package]]
name = "serde_urlencoded"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d3491c14715ca2294c4d6a88f15e84739788c1d030eed8c110436aafdaa2f3fd"
dependencies = [
 "form_urlencoded",
 "itoa",
 "ryu",
 "serde",
]

[[package]]
name = "serde_with"
version = "1.14.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "678b5a069e50bf00ecd22d0cd8ddf7c236f68581b03db652061ed5eb13a312ff"
dependencies = [
 "serde",
 "serde_with_macros",
]

[[package]]
name = "serde_with_macros"
version = "1.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e182d6ec6f05393cc0e5ed1bf81ad6db3a8feedf8ee515ecdd369809bcce8082"
dependencies = [
 "darling 0.13.4",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "sha2"
version = "0.9.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4d58a1e1bf39749807d89cf2d98ac2dfa0ff1cb3faa38fbb64dd88ac8013d800"
dependencies = [
 "block-buffer 0.9.0",
 "cfg-if",
 "cpufeatures",
 "digest 0.9.0",
 "opaque-debug",
]

[[package]]
name = "sha2"
version = "0.10.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "793db75ad2bcafc3ffa7c68b215fee268f537982cd901d132f89c6343f3a3dc8"
dependencies = [
 "cfg-if",
 "cpufeatures",
 "digest 0.10.7",
]

[[package]]
name = "sha3"
version = "0.10.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "75872d278a8f37ef87fa0ddbda7802605cb18344497949862c0d4dcb291eba60"
dependencies = [
 "digest 0.10.7",
 "keccak",
]

[[package]]
name = "sharded-slab"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f40ca3c46823713e0d4209592e8d6e826aa57e928f09752619fc696c499637f6"
dependencies = [
 "lazy_static",
]

[[package]]
name = "shell-words"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24188a676b6ae68c3b2cb3a01be17fbf7240ce009799bb56d5b1409051e78fde"

[[package]]
name = "signal-hook-registry"
version = "1.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a9e9e0b4211b72e7b8b6e85c807d36c212bdb33ea8587f7569562a84df5465b1"
dependencies = [
 "libc",
]

[[package]]
name = "signature"
version = "1.6.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "74233d3b3b2f6d4b006dc19dee745e73e2a6bfb6f93607cd3b02bd5b00797d7c"
dependencies = [
 "digest 0.10.7",
 "rand_core",
]

[[package]]
name = "signature"
version = "2.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "77549399552de45a898a580c1b41d445bf730df867cc44e6c0233bbc4b8329de"
dependencies = [
 "digest 0.10.7",
 "rand_core",
]

[[package]]
name = "simdutf8"
version = "0.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f27f6278552951f1f2b8cf9da965d10969b2efdea95a6ec47987ab46edfe263a"

[[package]]
name = "similar"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fa42c91313f1d05da9b26f267f931cf178d4aba455b4c4622dd7355eb80c6640"

[[package]]
name = "simple_asn1"
version = "0.6.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "adc4e5204eb1910f40f9cfa375f6f05b68c3abac4b6fd879c8ff5e7ae8a0a085"
dependencies = [
 "num-bigint 0.4.5",
 "num-traits",
 "thiserror",
 "time",
]

[[package]]
name = "siphasher"
version = "0.3.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "38b58827f4464d87d377d175e90bf58eb00fd8716ff0a62f80356b5e61555d0d"

[[package]]
name = "slab"
version = "0.4.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8f92a496fb766b417c996b9c5e57daf2f7ad3b0bebe1ccfca4856390e3d3bb67"
dependencies = [
 "autocfg",
]

[[package]]
name = "slog"
version = "2.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8347046d4ebd943127157b94d63abb990fcf729dc4e9978927fdf4ac3c998d06"
dependencies = [
 "erased-serde",
]

[[package]]
name = "slog-async"
version = "2.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72c8038f898a2c79507940990f05386455b3a317d8f18d4caea7cbc3d5096b84"
dependencies = [
 "crossbeam-channel",
 "slog",
 "take_mut",
 "thread_local",
]

[[package]]
name = "slog-term"
version = "2.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b6e022d0b998abfe5c3782c1f03551a596269450ccd677ea51c56f8b214610e8"
dependencies = [
 "is-terminal",
 "slog",
 "term",
 "thread_local",
 "time",
]

[[package]]
name = "smallvec"
version = "1.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3c5e1a9a646d36c3599cd173a41282daf47c44583ad367b8e6837255952e5c67"

[[package]]
name = "socket2"
version = "0.4.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9f7916fc008ca5542385b89a3d3ce689953c143e9304a9bf8beec1de48994c0d"
dependencies = [
 "libc",
 "winapi",
]

[[package]]
name = "socket2"
version = "0.5.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ce305eb0b4296696835b71df73eb912e0f1ffd2556a501fcede6e0c50349191c"
dependencies = [
 "libc",
 "windows-sys 0.52.0",
]

[[package]]
name = "spin"
version = "0.5.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6e63cff320ae2c57904679ba7cb63280a3dc4613885beafb148ee7bf9aa9042d"

[[package]]
name = "spin"
version = "0.9.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6980e8d7511241f8acf4aebddbb1ff938df5eebe98691418c4468d0b72a96a67"

[[package]]
name = "spki"
version = "0.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "67cf02bbac7a337dc36e4f5a693db6c21e7863f45070f7064577eb4367a3212b"
dependencies = [
 "base64ct",
 "der 0.6.1",
]

[[package]]
name = "spki"
version = "0.7.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d91ed6c858b01f942cd56b37a94b3e0a1798290327d1236e4d9cf4eaca44d29d"
dependencies = [
 "base64ct",
 "der 0.7.9",
]

[[package]]
name = "stable_deref_trait"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a8f112729512f8e442d81f95a8a7ddf2b7c6b8a1a6f509a95864142b30cab2d3"

[[package]]
name = "stacker"
version = "0.1.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c886bd4480155fd3ef527d45e9ac8dd7118a898a46530b7b94c3e21866259fce"
dependencies = [
 "cc",
 "cfg-if",
 "libc",
 "psm",
 "winapi",
]

[[package]]
name = "static_assertions"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a2eb9349b6444b326872e140eb1cf5e7c522154d69e7a0ffb0fb81c06b37543f"

[[package]]
name = "string_cache"
version = "0.8.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f91138e76242f575eb1d3b38b4f1362f10d3a43f47d182a5b359af488a02293b"
dependencies = [
 "new_debug_unreachable",
 "once_cell",
 "parking_lot 0.12.2",
 "phf_shared",
 "precomputed-hash",
]

[[package]]
name = "strsim"
version = "0.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "73473c0e59e6d5812c5dfe2a064a6444949f089e20eec9a2e5506596494e4623"

[[package]]
name = "strsim"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7da8b5736845d9f2fcb837ea5d9e2628564b3b043a70948a3f0b778838c5fb4f"

[[package]]
name = "strum"
version = "0.24.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "063e6045c0e62079840579a7e47a355ae92f60eb74daaf156fb1e84ba164e63f"

[[package]]
name = "strum"
version = "0.25.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "290d54ea6f91c969195bdbcd7442c8c2a2ba87da8bf60a7ee86a235d4bc1e125"
dependencies = [
 "strum_macros 0.25.3",
]

[[package]]
name = "strum"
version = "0.26.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8fec0f0aef304996cf250b31b5a10dee7980c85da9d759361292b8bca5a18f06"

[[package]]
name = "strum_macros"
version = "0.24.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1e385be0d24f186b4ce2f9982191e7101bb737312ad61c1f2f984f34bcf85d59"
dependencies = [
 "heck 0.4.1",
 "proc-macro2",
 "quote",
 "rustversion",
 "syn 1.0.109",
]

[[package]]
name = "strum_macros"
version = "0.25.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "23dc1fa9ac9c169a78ba62f0b841814b7abae11bdd047b9c58f893439e309ea0"
dependencies = [
 "heck 0.4.1",
 "proc-macro2",
 "quote",
 "rustversion",
 "syn 2.0.61",
]

[[package]]
name = "strum_macros"
version = "0.26.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4c6bee85a5a24955dc440386795aa378cd9cf82acd5f764469152d2270e581be"
dependencies = [
 "heck 0.5.0",
 "proc-macro2",
 "quote",
 "rustversion",
 "syn 2.0.61",
]

[[package]]
name = "subtle"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "81cdd64d312baedb58e21336b31bc043b77e01cc99033ce76ef539f78e965ebc"

[[package]]
name = "subtle-ng"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "734676eb262c623cec13c3155096e08d1f8f29adce39ba17948b18dad1e54142"

[[package]]
name = "supports-color"
version = "2.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d6398cde53adc3c4557306a96ce67b302968513830a77a95b2b17305d9719a89"
dependencies = [
 "is-terminal",
 "is_ci",
]

[[package]]
name = "syn"
version = "1.0.109"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72b64191b275b66ffe2469e8af2c1cfe3bafa67b529ead792a6d0160888b4237"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn"
version = "2.0.61"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c993ed8ccba56ae856363b1845da7266a7cb78e1d146c8a32d54b45a8b831fc9"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn_derive"
version = "0.1.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1329189c02ff984e9736652b1631330da25eaa6bc639089ed4915d25446cbe7b"
dependencies = [
 "proc-macro-error",
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "sync_wrapper"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a7065abeca94b6a8a577f9bd45aa0867a2238b74e8eb67cf10d492bc39351394"
dependencies = [
 "futures-core",
]

[[package]]
name = "sysinfo"
version = "0.28.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b4c2f3ca6693feb29a89724516f016488e9aafc7f37264f898593ee4b942f31b"
dependencies = [
 "cfg-if",
 "core-foundation-sys",
 "libc",
 "ntapi",
 "once_cell",
 "rayon",
 "winapi",
]

[[package]]
name = "take_mut"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f764005d11ee5f36500a149ace24e00e3da98b0158b3e2d53a7495660d3f4d60"

[[package]]
name = "tap"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "55937e1799185b12863d447f42597ed69d9928686b8d88a1df17376a097d8369"

[[package]]
name = "tar"
version = "0.4.40"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b16afcea1f22891c49a00c751c7b63b2233284064f11a200fc624137c51e2ddb"
dependencies = [
 "filetime",
 "libc",
 "xattr",
]

[[package]]
name = "tempfile"
version = "3.10.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "85b77fafb263dd9d05cbeac119526425676db3784113aa9295c88498cbf8bff1"
dependencies = [
 "cfg-if",
 "fastrand 2.1.0",
 "rustix 0.38.34",
 "windows-sys 0.52.0",
]

[[package]]
name = "term"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c59df8ac95d96ff9bede18eb7300b0fda5e5d8d90960e76f8e14ae765eedbf1f"
dependencies = [
 "dirs-next",
 "rustversion",
 "winapi",
]

[[package]]
name = "termcolor"
version = "1.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "06794f8f6c5c898b3275aebefa6b8a1cb24cd2c6c79397ab15774837a0bc5755"
dependencies = [
 "winapi-util",
]

[[package]]
name = "terminal_size"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "21bebf2b7c9e0a515f6e0f8c51dc0f8e4696391e6f1ff30379559f8365fb0df7"
dependencies = [
 "rustix 0.38.34",
 "windows-sys 0.48.0",
]

[[package]]
name = "thiserror"
version = "1.0.60"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "579e9083ca58dd9dcf91a9923bb9054071b9ebbd800b342194c9feb0ee89fc18"
dependencies = [
 "thiserror-impl",
]

[[package]]
name = "thiserror-impl"
version = "1.0.60"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2470041c06ec3ac1ab38d0356a6119054dedaea53e12fbefc0de730a1c08524"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "thousands"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3bf63baf9f5039dadc247375c29eb13706706cfde997d0330d05aa63a77d8820"

[[package]]
name = "thread_local"
version = "1.1.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8b9ef9bad013ada3808854ceac7b46812a6465ba368859a37e2100283d2d719c"
dependencies = [
 "cfg-if",
 "once_cell",
]

[[package]]
name = "time"
version = "0.3.36"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5dfd88e563464686c916c7e46e623e520ddc6d79fa6641390f2e3fa86e83e885"
dependencies = [
 "deranged",
 "itoa",
 "libc",
 "num-conv",
 "num_threads",
 "powerfmt",
 "serde",
 "time-core",
 "time-macros",
]

[[package]]
name = "time-core"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ef927ca75afb808a4d64dd374f00a2adf8d0fcff8e7b184af886c3c87ec4a3f3"

[[package]]
name = "time-macros"
version = "0.2.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3f252a68540fde3a3877aeea552b832b40ab9a69e318efd078774a01ddee1ccf"
dependencies = [
 "num-conv",
 "time-core",
]

[[package]]
name = "tiny-bip39"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "62cc94d358b5a1e84a5cb9109f559aa3c4d634d2b1b4de3d0fa4adc7c78e2861"
dependencies = [
 "anyhow",
 "hmac 0.12.1",
 "once_cell",
 "pbkdf2",
 "rand",
 "rustc-hash 1.1.0",
 "sha2 0.10.8",
 "thiserror",
 "unicode-normalization",
 "wasm-bindgen",
 "zeroize",
]

[[package]]
name = "tiny-keccak"
version = "2.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2c9d3793400a45f954c52e73d068316d76b6f4e36977e3fcebb13a2721e80237"
dependencies = [
 "crunchy",
]

[[package]]
name = "tinyvec"
version = "1.6.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87cc5ceb3875bb20c2890005a4e226a4651264a5c75edb2421b52861a0a0cb50"
dependencies = [
 "tinyvec_macros",
]

[[package]]
name = "tinyvec_macros"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1f3ccbac311fea05f86f61904b462b55fb3df8837a366dfc601a0161d0532f20"

[[package]]
name = "tokio"
version = "1.40.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2b070231665d27ad9ec9b8df639893f46727666c6767db40317fbe920a5d998"
dependencies = [
 "backtrace",
 "bytes",
 "libc",
 "mio",
 "parking_lot 0.12.2",
 "pin-project-lite",
 "signal-hook-registry",
 "socket2 0.5.7",
 "tokio-macros",
 "windows-sys 0.52.0",
]

[[package]]
name = "tokio-macros"
version = "2.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "693d596312e88961bc67d7f1f97af8a70227d9f90c31bba5806eec004978d752"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "tokio-rustls"
version = "0.24.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c28327cf380ac148141087fbfb9de9d7bd4e84ab5d2c28fbc911d753de8a7081"
dependencies = [
 "rustls 0.21.12",
 "tokio",
]

[[package]]
name = "tokio-rustls"
version = "0.26.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0c7bc40d0e5a97695bb96e27995cd3a08538541b0a846f65bba7a359f36700d4"
dependencies = [
 "rustls 0.23.7",
 "rustls-pki-types",
 "tokio",
]

[[package]]
name = "tokio-socks"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "51165dfa029d2a65969413a6cc96f354b86b464498702f174a4efa13608fd8c0"
dependencies = [
 "either",
 "futures-util",
 "thiserror",
 "tokio",
]

[[package]]
name = "tokio-util"
version = "0.7.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9cf6b47b3771c49ac75ad09a6162f53ad4b8088b76ac60e8ec1455b31a189fe1"
dependencies = [
 "bytes",
 "futures-core",
 "futures-sink",
 "pin-project-lite",
 "tokio",
]

[[package]]
name = "toml"
version = "0.5.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f4f7f0dd8d50a853a531c426359045b1998f04219d88799810762cd4ad314234"
dependencies = [
 "serde",
]

[[package]]
name = "toml"
version = "0.7.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dd79e69d3b627db300ff956027cc6c3798cef26d22526befdfcd12feeb6d2257"
dependencies = [
 "serde",
 "serde_spanned",
 "toml_datetime",
 "toml_edit 0.19.15",
]

[[package]]
name = "toml_datetime"
version = "0.6.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3550f4e9685620ac18a50ed434eb3aec30db8ba93b0287467bca5826ea25baf1"
dependencies = [
 "serde",
]

[[package]]
name = "toml_edit"
version = "0.19.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1b5bb770da30e5cbfde35a2d7b9b8a2c4b8ef89548a7a6aeab5c9a576e3e7421"
dependencies = [
 "indexmap 2.2.6",
 "serde",
 "serde_spanned",
 "toml_datetime",
 "winnow",
]

[[package]]
name = "toml_edit"
version = "0.21.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6a8534fd7f78b5405e860340ad6575217ce99f38d4d5c8f2442cb5ecb50090e1"
dependencies = [
 "indexmap 2.2.6",
 "toml_datetime",
 "winnow",
]

[[package]]
name = "tower"
version = "0.4.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b8fa9be0de6cf49e536ce1851f987bd21a43b771b09473c3549a6c853db37c1c"
dependencies = [
 "futures-core",
 "futures-util",
 "pin-project",
 "pin-project-lite",
 "tokio",
 "tower-layer",
 "tower-service",
 "tracing",
]

[[package]]
name = "tower-layer"
version = "0.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c20c8dbed6283a09604c3e69b4b7eeb54e298b8a600d4d5ecb5ad39de609f1d0"

[[package]]
name = "tower-service"
version = "0.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b6bc1c9ce2b5135ac7f93c72918fc37feb872bdc6a5533a8b85eb4b86bfdae52"

[[package]]
name = "tracing"
version = "0.1.40"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c3523ab5a71916ccf420eebdf5521fcef02141234bbc0b8a49f2fdc4544364ef"
dependencies = [
 "log",
 "pin-project-lite",
 "tracing-attributes",
 "tracing-core",
]

[[package]]
name = "tracing-appender"
version = "0.2.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3566e8ce28cc0a3fe42519fc80e6b4c943cc4c8cef275620eb8dac2d3d4e06cf"
dependencies = [
 "crossbeam-channel",
 "thiserror",
 "time",
 "tracing-subscriber",
]

[[package]]
name = "tracing-attributes"
version = "0.1.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "34704c8d6ebcbc939824180af020566b01a7c01f80641264eba0999f6c2b6be7"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "tracing-core"
version = "0.1.32"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c06d3da6113f116aaee68e4d601191614c9053067f9ab7f6edbcb161237daa54"
dependencies = [
 "once_cell",
 "valuable",
]

[[package]]
name = "tracing-log"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ee855f1f400bd0e5c02d150ae5de3840039a3f54b025156404e34c23c03f47c3"
dependencies = [
 "log",
 "once_cell",
 "tracing-core",
]

[[package]]
name = "tracing-serde"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bc6b213177105856957181934e4920de57730fc69bf42c37ee5bb664d406d9e1"
dependencies = [
 "serde",
 "tracing-core",
]

[[package]]
name = "tracing-subscriber"
version = "0.3.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ad0f048c97dbd9faa9b7df56362b8ebcaa52adb06b498c050d2f4e32f90a7a8b"
dependencies = [
 "matchers",
 "nu-ansi-term",
 "once_cell",
 "regex",
 "serde",
 "serde_json",
 "sharded-slab",
 "smallvec",
 "thread_local",
 "tracing",
 "tracing-core",
 "tracing-log",
 "tracing-serde",
]

[[package]]
name = "try-lock"
version = "0.2.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e421abadd41a4225275504ea4d6566923418b7f05506fbc9c0fe86ba7396114b"

[[package]]
name = "typed-arena"
version = "2.0.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6af6ae20167a9ece4bcb41af5b80f8a1f1df981f6391189ce00fd257af04126a"

[[package]]
name = "typenum"
version = "1.17.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "42ff0bf0c66b8238c6f3b578df37d0b7848e55df8577b3f74f92a69acceeb825"

[[package]]
name = "ucd-trie"
version = "0.1.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ed646292ffc8188ef8ea4d1e0e0150fb15a5c2e12ad9b8fc191ae7a8a7f3c4b9"

[[package]]
name = "unarray"
version = "0.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "eaea85b334db583fe3274d12b4cd1880032beab409c0d774be044d4480ab9a94"

[[package]]
name = "unicase"
version = "2.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f7d2d4dafb69621809a81864c9c1b864479e1235c0dd4e199924b9742439ed89"
dependencies = [
 "version_check",
]

[[package]]
name = "unicode-bidi"
version = "0.3.15"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "08f95100a766bf4f8f28f90d77e0a5461bbdb219042e7679bebe79004fed8d75"

[[package]]
name = "unicode-ident"
version = "1.0.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3354b9ac3fae1ff6755cb6db53683adb661634f67557942dea4facebec0fee4b"

[[package]]
name = "unicode-normalization"
version = "0.1.23"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a56d1686db2308d901306f92a263857ef59ea39678a5458e7cb17f01415101f5"
dependencies = [
 "tinyvec",
]

[[package]]
name = "unicode-segmentation"
version = "1.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d4c87d22b6e3f4a18d4d40ef354e97c90fcb14dd91d7dc0aa9d8a1172ebf7202"

[[package]]
name = "unicode-width"
version = "0.1.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "68f5e5f3158ecfd4b8ff6fe086db7c8467a2dfdac97fe420f2b7c4aa97af66d6"

[[package]]
name = "unicode-xid"
version = "0.2.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f962df74c8c05a667b5ee8bcf162993134c104e96440b663c8daa176dc772d8c"

[[package]]
name = "universal-hash"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fc1de2c688dc15305988b563c3854064043356019f97a4b46276fe734c4f07ea"
dependencies = [
 "crypto-common",
 "subtle",
]

[[package]]
name = "untrusted"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a156c684c91ea7d62626509bce3cb4e1d9ed5c4d978f7b4352658f96a4c26b4a"

[[package]]
name = "untrusted"
version = "0.9.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8ecb6da28b8a351d773b68d5825ac39017e680750f980f3a1a85cd8dd28a47c1"

[[package]]
name = "url"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "31e6302e3bb753d46e83516cae55ae196fc0c309407cf11ab35cc51a4c2a4633"
dependencies = [
 "form_urlencoded",
 "idna",
 "percent-encoding",
 "serde",
]

[[package]]
name = "urlencoding"
version = "2.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "daf8dba3b7eb870caf1ddeed7bc9d2a049f3cfdfae7cb521b087cc33ae4c49da"

[[package]]
name = "utf8-width"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "86bd8d4e895da8537e5315b8254664e6b769c4ff3db18321b297a1e7004392e3"

[[package]]
name = "utf8parse"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "711b9620af191e0cdc7468a8d14e709c3dcdb115b36f838e601583af800a370a"

[[package]]
name = "uuid"
version = "1.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a183cf7feeba97b4dd1c0d46788634f6221d87fa961b305bed08c851829efcc0"

[[package]]
name = "valuable"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "830b7e5d4d90034032940e4ace0d9a9a057e7a45cd94e6c007832e39edb82f6d"

[[package]]
name = "version_check"
version = "0.9.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "49874b5167b65d7193b8aba1567f5c7d93d001cafc34600cee003eda787e483f"

[[package]]
name = "wait-timeout"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9f200f5b12eb75f8c1ed65abd4b2db8a6e1b138a20de009dacee265a2498f3f6"
dependencies = [
 "libc",
]

[[package]]
name = "waker-fn"
version = "1.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f3c4517f54858c779bbcbf228f4fca63d121bf85fbecb2dc578cdf4a39395690"

[[package]]
name = "walkdir"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "29790946404f91d9c5d06f9874efddea1dc06c5efe94541a7d6863108e3a5e4b"
dependencies = [
 "same-file",
 "winapi-util",
]

[[package]]
name = "walrus"
version = "0.21.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "467611cafbc8a84834b77d2b4bb191fd2f5769752def8340407e924390c6883b"
dependencies = [
 "anyhow",
 "gimli 0.26.2",
 "id-arena",
 "leb128",
 "log",
 "walrus-macro",
 "wasm-encoder",
 "wasmparser",
]

[[package]]
name = "walrus-macro"
version = "0.19.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0a6e5bd22c71e77d60140b0bd5be56155a37e5bd14e24f5f87298040d0cc40d7"
dependencies = [
 "heck 0.3.3",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "want"
version = "0.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bfa7760aed19e106de2c7c0b581b509f2f25d3dacaf737cb82ac61bc6d760b0e"
dependencies = [
 "try-lock",
]

[[package]]
name = "wasi"
version = "0.11.0+wasi-snapshot-preview1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423"

[[package]]
name = "wasm-bindgen"
version = "0.2.92"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4be2531df63900aeb2bca0daaaddec08491ee64ceecbee5076636a3b026795a8"
dependencies = [
 "cfg-if",
 "wasm-bindgen-macro",
]

[[package]]
name = "wasm-bindgen-backend"
version = "0.2.92"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "614d787b966d3989fa7bb98a654e369c762374fd3213d212cfc0251257e747da"
dependencies = [
 "bumpalo",
 "log",
 "once_cell",
 "proc-macro2",
 "quote",
 "syn 2.0.61",
 "wasm-bindgen-shared",
]

[[package]]
name = "wasm-bindgen-console-logger"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7530a275e7faf7b5b83aabdf78244fb8d9a68a2ec4b26935a05ecc0c9b0185ed"
dependencies = [
 "log",
 "wasm-bindgen",
]

[[package]]
name = "wasm-bindgen-futures"
version = "0.4.42"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "76bc14366121efc8dbb487ab05bcc9d346b3b5ec0eaa76e46594cabbe51762c0"
dependencies = [
 "cfg-if",
 "js-sys",
 "wasm-bindgen",
 "web-sys",
]

[[package]]
name = "wasm-bindgen-macro"
version = "0.2.92"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a1f8823de937b71b9460c0c34e25f3da88250760bec0ebac694b49997550d726"
dependencies = [
 "quote",
 "wasm-bindgen-macro-support",
]

[[package]]
name = "wasm-bindgen-macro-support"
version = "0.2.92"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e94f17b526d0a461a191c78ea52bbce64071ed5c04c9ffe424dcb38f74171bb7"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
 "wasm-bindgen-backend",
 "wasm-bindgen-shared",
]

[[package]]
name = "wasm-bindgen-shared"
version = "0.2.92"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "af190c94f2773fdb3729c55b007a722abb5384da03bc0986df4c289bf5567e96"

[[package]]
name = "wasm-encoder"
version = "0.212.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "501940df4418b8929eb6d52f1aade1fdd15a5b86c92453cb696e3c906bd3fc33"
dependencies = [
 "leb128",
]

[[package]]
name = "wasm-opt"
version = "0.116.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2fd87a4c135535ffed86123b6fb0f0a5a0bc89e50416c942c5f0662c645f679c"
dependencies = [
 "anyhow",
 "libc",
 "strum 0.24.1",
 "strum_macros 0.24.3",
 "tempfile",
 "thiserror",
 "wasm-opt-cxx-sys",
 "wasm-opt-sys",
]

[[package]]
name = "wasm-opt-cxx-sys"
version = "0.116.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8c57b28207aa724318fcec6575fe74803c23f6f266fce10cbc9f3f116762f12e"
dependencies = [
 "anyhow",
 "cxx",
 "cxx-build",
 "wasm-opt-sys",
]

[[package]]
name = "wasm-opt-sys"
version = "0.116.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8a1cce564dc768dacbdb718fc29df2dba80bd21cb47d8f77ae7e3d95ceb98cbe"
dependencies = [
 "anyhow",
 "cc",
 "cxx",
 "cxx-build",
]

[[package]]
name = "wasm-streams"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b65dc4c90b63b118468cf747d8bf3566c1913ef60be765b5730ead9e0a3ba129"
dependencies = [
 "futures-util",
 "js-sys",
 "wasm-bindgen",
 "wasm-bindgen-futures",
 "web-sys",
]

[[package]]
name = "wasmparser"
version = "0.212.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8d28bc49ba1e5c5b61ffa7a2eace10820443c4b7d1c0b144109261d14570fdf8"
dependencies = [
 "ahash 0.8.11",
 "bitflags 2.5.0",
 "hashbrown 0.14.5",
 "indexmap 2.2.6",
 "semver",
 "serde",
]

[[package]]
name = "web-sys"
version = "0.3.69"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "77afa9a11836342370f4817622a2f0f418b134426d91a82dfb48f532d2ec13ef"
dependencies = [
 "js-sys",
 "wasm-bindgen",
]

[[package]]
name = "webpki-roots"
version = "0.25.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5f20c57d8d7db6d3b86154206ae5d8fba62dd39573114de97c2cb0578251f8e1"

[[package]]
name = "webpki-roots"
version = "0.26.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b3de34ae270483955a94f4b21bdaaeb83d508bb84a01435f393818edb0012009"
dependencies = [
 "rustls-pki-types",
]

[[package]]
name = "which"
version = "4.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87ba24419a2078cd2b0f2ede2691b6c66d8e47836da3b6db8265ebad47afbfc7"
dependencies = [
 "either",
 "home",
 "once_cell",
 "rustix 0.38.34",
]

[[package]]
name = "winapi"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
dependencies = [
 "winapi-i686-pc-windows-gnu",
 "winapi-x86_64-pc-windows-gnu",
]

[[package]]
name = "winapi-i686-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"

[[package]]
name = "winapi-util"
version = "0.1.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4d4cc384e1e73b93bafa6fb4f1df8c41695c8a91cf9c4c64358067d15a7b6c6b"
dependencies = [
 "windows-sys 0.52.0",
]

[[package]]
name = "winapi-x86_64-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"

[[package]]
name = "windows-core"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "33ab640c8d7e35bf8ba19b884ba838ceb4fba93a4e8c65a9059d08afcfc683d9"
dependencies = [
 "windows-targets 0.52.6",
]

[[package]]
name = "windows-registry"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e400001bb720a623c1c69032f8e3e4cf09984deec740f007dd2b03ec864804b0"
dependencies = [
 "windows-result",
 "windows-strings",
 "windows-targets 0.52.6",
]

[[package]]
name = "windows-result"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1d1043d8214f791817bab27572aaa8af63732e11bf84aa21a45a78d6c317ae0e"
dependencies = [
 "windows-targets 0.52.6",
]

[[package]]
name = "windows-strings"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4cd9b125c486025df0eabcb585e62173c6c9eddcec5d117d3b6e8c30e2ee4d10"
dependencies = [
 "windows-result",
 "windows-targets 0.52.6",
]

[[package]]
name = "windows-sys"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "677d2418bec65e3338edb076e806bc1ec15693c5d0104683f2efe857f61056a9"
dependencies = [
 "windows-targets 0.48.5",
]

[[package]]
name = "windows-sys"
version = "0.52.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "282be5f36a8ce781fad8c8ae18fa3f9beff57ec1b52cb3de0789201425d9a33d"
dependencies = [
 "windows-targets 0.52.6",
]

[[package]]
name = "windows-targets"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9a2fa6e2155d7247be68c096456083145c183cbbbc2764150dda45a87197940c"
dependencies = [
 "windows_aarch64_gnullvm 0.48.5",
 "windows_aarch64_msvc 0.48.5",
 "windows_i686_gnu 0.48.5",
 "windows_i686_msvc 0.48.5",
 "windows_x86_64_gnu 0.48.5",
 "windows_x86_64_gnullvm 0.48.5",
 "windows_x86_64_msvc 0.48.5",
]

[[package]]
name = "windows-targets"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9b724f72796e036ab90c1021d4780d4d3d648aca59e491e6b98e725b84e99973"
dependencies = [
 "windows_aarch64_gnullvm 0.52.6",
 "windows_aarch64_msvc 0.52.6",
 "windows_i686_gnu 0.52.6",
 "windows_i686_gnullvm",
 "windows_i686_msvc 0.52.6",
 "windows_x86_64_gnu 0.52.6",
 "windows_x86_64_gnullvm 0.52.6",
 "windows_x86_64_msvc 0.52.6",
]

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2b38e32f0abccf9987a4e3079dfb67dcd799fb61361e53e2882c3cbaf0d905d8"

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3"

[[package]]
name = "windows_aarch64_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc35310971f3b2dbbf3f0690a219f40e2d9afcf64f9ab7cc1be722937c26b4bc"

[[package]]
name = "windows_aarch64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469"

[[package]]
name = "windows_i686_gnu"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a75915e7def60c94dcef72200b9a8e58e5091744960da64ec734a6c6e9b3743e"

[[package]]
name = "windows_i686_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b"

[[package]]
name = "windows_i686_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66"

[[package]]
name = "windows_i686_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8f55c233f70c4b27f66c523580f78f1004e8b5a8b659e05a4eb49d4166cca406"

[[package]]
name = "windows_i686_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66"

[[package]]
name = "windows_x86_64_gnu"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "53d40abd2583d23e4718fddf1ebec84dbff8381c07cae67ff7768bbf19c6718e"

[[package]]
name = "windows_x86_64_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0b7b52767868a23d5bab768e390dc5f5c55825b6d30b86c844ff2dc7414044cc"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d"

[[package]]
name = "windows_x86_64_msvc"
version = "0.48.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ed94fce61571a4006852b7389a063ab983c02eb1bb37b47f8272ce92d06d9538"

[[package]]
name = "windows_x86_64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec"

[[package]]
name = "winnow"
version = "0.5.40"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f593a95398737aeed53e489c785df13f3618e41dbcd6718c6addbf1395aa6876"
dependencies = [
 "memchr",
]

[[package]]
name = "wsl"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f8dab7ac864710bdea6594becbea5b5050333cf34fefb0dc319567eb347950d4"

[[package]]
name = "wyz"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "05f360fc0b24296329c78fda852a1e9ae82de9cf7b27dae4b7f62f118f77b9ed"
dependencies = [
 "tap",
]

[[package]]
name = "xattr"
version = "1.3.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8da84f1a25939b27f6820d92aed108f83ff920fdf11a7b19366c27c4cda81d4f"
dependencies = [
 "libc",
 "linux-raw-sys 0.4.13",
 "rustix 0.38.34",
]

[[package]]
name = "yansi"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "09041cd90cf85f7f8b2df60c646f853b7f535ce68f85244eb6731cf89fa498ec"

[[package]]
name = "zbus"
version = "1.9.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9cbeb2291cd7267a94489b71376eda33496c1b9881adf6b36f26cc2779f3fc49"
dependencies = [
 "async-io",
 "byteorder",
 "derivative",
 "enumflags2",
 "fastrand 1.9.0",
 "futures",
 "nb-connect",
 "nix 0.22.3",
 "once_cell",
 "polling",
 "scoped-tls",
 "serde",
 "serde_repr",
 "zbus_macros",
 "zvariant",
]

[[package]]
name = "zbus_macros"
version = "1.9.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fa3959a7847cf95e3d51e312856617c5b1b77191176c65a79a5f14d778bbe0a6"
dependencies = [
 "proc-macro-crate 0.1.5",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "zerocopy"
version = "0.7.34"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ae87e3fcd617500e5d106f0380cf7b77f3c6092aae37191433159dda23cfb087"
dependencies = [
 "zerocopy-derive",
]

[[package]]
name = "zerocopy-derive"
version = "0.7.34"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "15e934569e47891f7d9411f1a451d947a60e000ab3bd24fbb970f000387d1b3b"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "zeroize"
version = "1.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "525b4ec142c6b68a2d10f01f7bbf6755599ca3f81ea53b8431b7dd348f5fdb2d"
dependencies = [
 "zeroize_derive",
]

[[package]]
name = "zeroize_derive"
version = "1.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ce36e65b0d2999d2aafac989fb249189a141aee1f53c612c1f37d72631959f69"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.61",
]

[[package]]
name = "zvariant"
version = "2.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a68c7b55f2074489b7e8e07d2d0a6ee6b4f233867a653c664d8020ba53692525"
dependencies = [
 "byteorder",
 "enumflags2",
 "libc",
 "serde",
 "static_assertions",
 "zvariant_derive",
]

[[package]]
name = "zvariant_derive"
version = "2.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e4ca5e22593eb4212382d60d26350065bf2a02c34b85bc850474a74b589a3de9"
dependencies = [
 "proc-macro-crate 1.3.1",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]


-----------------------

/Cargo.toml:
-----------------------

[workspace]
members = [
    "src/dfx",
    "src/dfx-core",
    "src/lib/apply-patch",
    "src/canisters/frontend/ic-asset",
    "src/canisters/frontend/icx-asset",
    "src/canisters/frontend/ic-certified-assets",
    "src/canisters/frontend/ic-frontend-canister",
]
default-members = ["src/dfx"]
resolver = "2"

[workspace.package]
authors = ["DFINITY Stiftung <sdk@dfinity.org>"]
edition = "2021"
repository = "https://github.com/dfinity/sdk"
rust-version = "1.75.0"
license = "Apache-2.0"

[workspace.dependencies]
candid = "0.10.4"
candid_parser = "0.1.4"
dfx-core = { path = "src/dfx-core", version = "0.1.0" }
ic-agent = "0.38"
ic-asset = { path = "src/canisters/frontend/ic-asset", version = "0.21.0" }
ic-cdk = "0.13.1"
ic-identity-hsm = "0.38"
ic-utils = "0.38"

aes-gcm = "0.10.3"
anyhow = "1.0.56"
anstyle = "1.0.0"
argon2 = "0.4.0"
backoff = { version = "0.4.0", features = ["futures", "tokio"] }
base64 = "0.13.0"
byte-unit = "4.0.14"
bytes = "1.2.1"
clap = "4.5"
clap_complete = "4.5"
dialoguer = "0.11.0"
directories-next = "2.0.0"
flate2 = { version = "1.0.11", default-features = false }
futures = "0.3.21"
handlebars = "4.3.3"
hex = "0.4.3"
humantime = "2.1.0"
itertools = "0.10.3"
keyring = "1.2.0"
lazy_static = "1.4.0"
mime = "0.3.16"
mime_guess = "2.0.4"
num-traits = "0.2.14"
pem = "1.0.2"
proptest = "1.0.0"
reqwest = { version = "0.12.4", default-features = false, features = [
    "rustls-tls",
] }
ring = "0.16.11"
schemars = "0.8"
sec1 = "0.3.0"
serde = "1.0"
serde_bytes = "0.11.5"
serde_cbor = "0.11.1"
serde_json = "1.0.79"
sha2 = "0.10.6"
slog = "2.5.2"
slog-async = "2.4.0"
slog-term = "2.9.0"
tar = "0.4.38"
semver = "1.0.6"
tempfile = "3.3.0"
thiserror = "1.0.24"
time = "0.3.9"
tokio = "1.35"
url = { version = "2.1.0", features = ["serde"] }
walkdir = "2.3.2"

[profile.release]
panic = 'abort'
lto = true

[profile.dev.package.argon2]
opt-level = 3

[profile.release.package.ic-frontend-canister]
opt-level = 'z'


-----------------------

/LICENSE:
-----------------------

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2022 DFINITY Foundation

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


-----------------------

/README.md:
-----------------------

# IC SDK

This repo contains the `IC SDK`: a Software Development Kit for creating and managing [canister smart contracts on the Internet Computer (ICP blockchain)](https://wiki.internetcomputer.org/wiki/Canister_smart_contract).

For further reading:
* [Reference documentation on internetcomputer.org](https://internetcomputer.org/docs/current/developer-docs/developer-tools/cli-tools/cli-reference/)
* [Introduction to the ICP blockchain](https://wiki.internetcomputer.org/wiki/Introduction_to_ICP)
* [Internet Computer dashboard](https://dashboard.internetcomputer.org/)
* [Developer docs for ICP smart contracts](https://internetcomputer.org/docs/current/home)
* [Sample code of ICP smart contracts](https://internetcomputer.org/samples)
* [IC wiki](https://wiki.internetcomputer.org/wiki/Main_Page)

## What gets installed

The `IC SDK` installation script installs several components in default locations on your local computer. The following table describes the development environment components that the installation script installs:

| Component    | Description                                                                                        | Default location                              |
|--------------|----------------------------------------------------------------------------------------------------|-----------------------------------------------|
| dfx          | Command-line interface (CLI)                                                     | `/usr/local/bin/dfx`                          |
| dfxvm        | Command-line interface, version manager                                          | `/usr/local/bin/dfxvm`                          |
| moc          | Motoko runtime compiler                                                                            | `~/.cache/dfinity/versions/<VERSION>/moc`     |
| replica      | Internet Computer local network binary                                                             | `~/.cache/dfinity/versions/<VERSION>/replica` |
| uninstall.sh | Script to remove the SDK and all of its components                                    | `~/.cache/dfinity/uninstall.sh`               |
| versions     | Cache directory that contains a subdirectory for each version of the SDK you install. | `~/.cache/dfinity/versions`                   |

## SDK vs CDK vs `dfx`

There are a few components above worth expanding on:

1. **dfx** - `dfx` is the command-line interface for the `IC SDK`. This is why many commands for the IC SDK start with the command "`dfx ..`" such as `dfx new` or `dfx stop`.

2. **dfxvm** - `dfxvm` is the version manager for `dfx`, i.e. a CLI for selecting and managing installed `dfx` versions.

3. **Canister Development Kit (CDK)** - A CDK is an adapter used by the IC SDK so a programming language has the features needed to create and manage canisters. 
The IC SDK comes with a few CDKs already installed for you so you can use them in the language of your choice. That is why there is a [Rust CDK](https://github.com/dfinity/cdk-rs), [Python CDK](https://demergent-labs.github.io/kybra/), 
[TypeScript CDK](https://demergent-labs.github.io/azle/), etc... Since CDKs are components used the SDK, some developer choose to use the CDK directly (without the `IC SDK`), 
but typically are used as part of the whole `IC SDK`.


## Getting Started

### Prerequisites 

#### Install Rust

To develop Rust projects, you will need to install Rust in your environment with the command:

```
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

#### Install `wasm32-unknown-unknown` target

ICP smart contracts are compiled into WebAssembly modules. To support this compilation, install the `wasm32-unknown-unknown` target:

```
rustup target add wasm32-unknown-unknown
```

### Installing

You can install the `IC SDK` a few different ways.

#### via `curl` (recommended)

``` bash
sh -ci "$(curl -fsSL https://internetcomputer.org/install.sh)"
```

This command will install a binary compatible with your operating system, and add it to `/usr/local/bin`.

#### via GitHub Releases

Find a release for your architecture [here](https://github.com/dfinity/sdk/releases).

#### in GitHub Action, using [`dfinity/setup-dfx`](https://github.com/dfinity/setup-dfx)

```yml
    steps:
    - name: Install dfx
      uses: dfinity/setup-dfx@main
```

### Getting Help

Once the `IC SDK` is installed, get acquainted with its capabilities by entering.

``` bash
dfx help
```

## Contributing to the DFINITY SDK

See our contributing guidelines [here](./CONTRIBUTING.md).

### Building the IC SDK

Building the `IC SDK` is very simple:

``` bash
cargo build
```

## Release Process

`IC SDK` is released in two steps:

1. Publishing a new `IC SDK` release.

2. Publishing a new `manifest.json` and `install.sh` to instruct the installer
   to actually download and install the new `IC SDK` release.

### Publishing the IC SDK

1. The release manager makes sure the `dfx` `stable` branch points to the revision
   that should be released and that the revision is tagged with a version (like
   `0.5.6`).

2. The
   [`sdk-release`](https://hydra.dfinity.systems/jobset/dfinity-ci-build/sdk-release#tabs-configuration)
   jobset on Hydra tracks the `stable` branch and starts evaluating shortly
   after `stable` advances.

3. As you can see it only has the single job `publish.dfx` which is
   defined [here](https://github.com/dfinity-lab/sdk/blob/stable/ci/release.nix)
   in terms of the
   [`dfx`](https://github.com/dfinity-lab/sdk/blob/stable/publish.nix) job. Note
   that the `publish.dfx` job only exists when the revision has a
   proper version tag. This prevents publishing of untagged revisions.

4. Our CD system running at `deployer.dfinity.systems` is configured with the
   [`publish-sdk-dfx-release`](https://github.com/dfinity-lab/infra/blob/1fe63e06135be206d064a74461f739c4fafec3c7/services/nix/publish-sdk-release.nix#L39:L47)
   job. It will monitor the aforementioned `publish.dfx` job for
   new builds, whenever there's a new build it will download the output (the CD
   script) and execute it.

5. As you can see the script also sends a message to the `#build-notifications`
   Slack channel so you can see when and if the SDK has been published.

### Publishing `manifest.json` and `install.sh`

After the `IC SDK` has been released it's available for download but the install
script at https://sdk.dfinity.org/install.sh won't immediately install it. To
make sure the installer actually downloads and installs the new `IC SDK` release the
`manifest.json` file at https://sdk.dfinity.org/manifest.json has to set its
`tags.latest` field to the new version. The following explains how to do that.

1. Edit the `public/manifest.json` file such that it points to the new `IC SDK`
   version and make sure this is merged in `master`.

2. Similarly to releasing the `IC SDK` there's a
   [`install-sh`](https://github.com/dfinity-lab/sdk/blob/stable/publish.nix) job
   that builds a CD script for publishing the `manifest.json` and `install.sh`
   to our CDN.

3. This
   [job](https://hydra.dfinity.systems/job/dfinity-ci-build/sdk/publish.install-sh.x86_64-linux)
   is built on the `sdk` jobset which tracks the `master` branch.

4. `deployer.dfinity.systems` is configured with the
   [`publish-sdk-install-sh`](https://github.com/dfinity-lab/infra/blob/1fe63e06135be206d064a74461f739c4fafec3c7/services/nix/publish-sdk-release.nix#L48:L56)
   job which will monitor the aforementioned `publish.install-sh.x86_64-linux`
   job for new builds, whenever there's a new build it will download the output
   (the CD script) and execute it.


## Troubleshooting
This section provides solutions to problems you might encounter when using the `IC SDK` via `dfx` command line

### Project Reset

This command will remove the build directory and restart your replica:

``` bash
dfx stop && dfx start --clean --background
```

### Using Internet Identity Locally
You can deploy the Internet Identity canister into your replica alongside your project by cloning https://github.com/dfinity/internet-identity. From the `internet-identity` directory, run the following command:

``` bash
II_ENV=development dfx deploy --no-wallet --argument '(null)'
```

There are more notes at https://github.com/dfinity/internet-identity#running-locally that may be helpful.


-----------------------

/clippy.toml:
-----------------------

too-many-arguments-threshold = 16

-----------------------

/deny.toml:
-----------------------

# adapted from https://github.com/dfinity-lab/common/blob/master/pkgs/overlays/packages/cargo-deny/buildtime.toml
# for context, see https://github.com/dfinity-lab/common/blob/master/pkgs/overlays/packages/cargo-deny/runtime.toml
# we allow more licenses in the build-time check. all rust dependencies are statically linked,
# so copyleft licenses like MPL which allow static linking are A-OK
[licenses]
default = "deny"
unlicensed = "allow"

allow = [
    "Apache-2.0",
    "Apache-2.0 WITH LLVM-exception",
    "BSD-2-Clause",
    "BSD-3-Clause",
    "CC0-1.0",
    "ISC",
    "MIT",
    "MPL-2.0",
    "Zlib",
    "Unicode-DFS-2016"
]

deny = [
    "GPL-1.0",
    "GPL-2.0",
    "GPL-3.0",
]

[sources]
allow-git = [ "https://github.com/dfinity/agent-rs.git" ]


-----------------------

/docs/cli-reference/dfx-bootstrap.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx bootstrap {#_dfx_bootstrap}

<MarkdownChipRow labels={["Reference"]} />

> **NOTE**: The bootstrap command has been removed. Please use the [dfx start](./dfx-start.mdx) command instead.


-----------------------

/docs/cli-reference/dfx-build.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx build

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx build` command to compile your program into a WebAssembly module that can be deployed on the IC. You can use this command to compile all of the programs that are defined for a project in the projectâ€™s `dfx.json` configuration file or a specific canister.

Note that you can only run this command from within the project directory structure. For example, if your project name is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its subdirectories.

The `dfx build` command looks for the source code to compile using the information you have configured under the `canisters` section in the `dfx.json` configuration file.

## Basic usage

``` bash
dfx build [flag] [option] [--all | canister_name]
```

## Flags

You can use the following optional flags with the `dfx build` command.

| Flag      | Description                                                                                                                                              |
| --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--check` | Builds canisters using a temporary, hard-coded, locally-defined canister identifier for testing that your program compiles without connecting to the IC. |

## Options

You can specify the following options for the `dfx build` command.

| Option                | Description                                                                                                                                                |
| --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--network <network>` | Specifies the network alias or URL you want to connect to. You can use this option to override the network specified in the `dfx.json` configuration file. |
| `--output-env-file`   | Writes dfx environment variables to a provided path. Overrides the `output_env_file` configuration from `dfx.json` if passed.                              |

## Arguments

You can specify the following arguments for the `dfx build` command.

| Argument        | Description                                                                                                                                                                                                                                                                                                                              |
| --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--all`         | Builds all of the canisters configured in the projectâ€™s `dfx.json` file.                                                                                                                                                                                                                                                                 |
| `canister_name` | Specifies the name of the canister you want to build. If you are not using the `--all` option, you can continue to use `dfx build` or provide a canister name as an argument (the canister name must match at least one name that you have configured in the `canisters` section of the `dfx.json` configuration file for your project.) |

## Examples

You can use the `dfx build` command to build one or more WebAssembly modules from the programs specified in the `dfx.json` configuration file under the `canisters` key. For example, if your `dfx.json` configuration file defines one `hello_world_backend` canister and one `hello_world_frontend` canister, then running `dfx build` compiles two WebAssembly modules.

Note that the file name and path to the programs on your file system must match the information specified in the `dfx.json` configuration file.

In this example, the `hello_world_backend` canister contains the main program code and the `hello_world_frontend` canister store frontend code and assets. If you want to keep the `hello_world_frontend` canister defined in the `dfx.json` file, but only build the backend program, you could run the following command:

``` bash
dfx build hello_world_backend
```

Building a specific canister is useful when you have multiple canisters defined in the dfx.json file, but want to test and debug operations for canisters independently.

To test whether a canister compiles without connecting to the IC or the local canister execution environment, you would run the following command:

``` bash
dfx build --check
```

## Management canister

If `dfx` detects that your Motoko project is importing the Management Canister (e.g. `import Management "ic:aaaaa-aa";`) it will automatically provide the Candid interface for the Management Canister during the build.


-----------------------

/docs/cli-reference/dfx-cache.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx cache

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx cache` command with flags and subcommands to manage the `dfx` version cache.

The basic syntax for running `dfx cache` commands is:

``` bash
dfx cache [subcommand] [flag]
```

Depending on the `dfx cache` subcommand you specify, additional arguments, options, and flags might apply. For reference information and examples that illustrate using `dfx cache` commands, select an appropriate command.

| Command                    | Description                                                                   |
|----------------------------|-------------------------------------------------------------------------------|
| [`delete`](#delete)        | Deletes the specified version of `dfx` from the local cache.                  |
| `help`                     | Displays usage information message for a specified subcommand.                |
| [`install`](#install)      | Installs the specified version of `dfx` from the local cache.                 |
| [`list`](#_dfx_cache_list) | Lists the versions of `dfx` currently installed and used in current projects. |
| [`show`](#_dfx_cache_show) | Show the path of the cache used by this version of the `dfx` executable.      |

To view usage information for a specific subcommand, specify the subcommand and the `--help` flag. For example, to see usage information for `dfx cache delete`, you can run the following command:

``` bash
dfx cache delete --help
```

## dfx cache delete

Use the `dfx cache delete` command to delete a specified version of the IC SDK from the version cache on the local computer.

### Basic usage

``` bash
dfx cache delete [version] [flag]
```

### Arguments

You can specify the following argument for the `dfx cache delete` command.

| Command   | Description                                                        |
|-----------|--------------------------------------------------------------------|
| `version` | Specifies the version of `dfx` you to delete from the local cache. |

### Examples

You can use the `dfx cache delete` command to permanently delete versions of the IC SDK that you no longer want to use. For example, you can run the following command to delete the IC SDK version `0.6.2`:

``` bash
dfx cache delete 0.6.2
```

## dfx cache install

Use the `dfx cache install` command to install the IC SDK using the version currently found in the `dfx` cache.

### Basic usage

``` bash
dfx cache install [flag]
```

### Examples

You can use the `dfx cache install` command to force the installation of `dfx` from the version in the cache. For example, you can run the following command to install `dfx`:

``` bash
dfx cache install
```

## dfx cache list

Use the `dfx cache list` command to list the IC SDK versions you have currently installed and used in projects.

If you have multiple versions of the IC SDK installed, the cache list displays an asterisk (\*) to indicate the currently active version.

### Basic usage

``` bash
dfx cache list [flag]
```

### Examples

You can use the `dfx cache list` command to list the IC SDK versions you have currently installed and used in projects. For example, you can run the following command to list versions of the IC SDK found in the cache:

``` bash
dfx cache list
```

This command displays the list of the IC SDK versions found similar to the following:

``` bash
0.6.4 *
0.6.3
0.6.0
```

## dfx cache show

Use the `dfx cache show` command to display the full path to the cache used by the IC SDK version you are currently using.

### Basic usage

``` bash
dfx cache show [flag]
```

### Examples

You can use the `dfx cache show` command to display the path to the cache used by the IC SDK version you are currently using:

``` bash
dfx cache show
```

This command displays the path to the cache used by the IC SDK version you are currently using:

``` bash
/Users/pubs/.cache/dfinity/versions/0.6.4
```


-----------------------

/docs/cli-reference/dfx-canister.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx canister

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx canister` command with options and subcommands to manage canister operations and interaction with the
Internet Computer or the local canister execution environment. In most cases, you use `dfx canister` subcommands after
you compile a program to manage the canister lifecycle and to perform key tasks such as calling program functions.

The basic syntax for running `dfx canister` commands is:

``` bash
dfx canister <subcommand> [options]
```

Depending on the `dfx canister` subcommand you specify, additional arguments and options might apply or be
required. To view usage information for a specific `dfx canister` subcommand, specify the subcommand and the `--help`
flag. For example, to see usage information for `dfx canister call`, you can run the following command:

``` bash
dfx canister call --help
```

For reference information and examples that illustrate using `dfx canister` commands, select an appropriate command.

| Command                                            | Description                                                                                                                                            |
|----------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|
| [`call`](#dfx-canister-call)                       | Calls a specified method on a deployed canister.                                                                                                       |
| [`create`](#dfx-canister-create)                   | Creates an empty canister and associates the assigned Canister ID to the canister name.                                                                |
| [`delete`](#dfx-canister-delete)                   | Deletes a currently stopped canister.                                                                                                                  |
| [`deposit-cycles`](#dfx-canister-deposit-cycles)   | Deposit cycles into the specified canister.                                                                                                            |
| `help`                                             | Displays usage information message for a specified subcommand.                                                                                         |
| [`id`](#dfx-canister-id)                           | Displays the identifier of a canister.                                                                                                                 |
| [`info`](#dfx-canister-info)                       | Get the hash of a canisterâ€™s Wasm module and its current controller.                                                                                   |
| [`install`](#dfx-canister-install)                 | Installs compiled code in a canister.                                                                                                                  |
| [`logs`](#dfx-canister-logs)                       | Returns the logs from a canister.                                                                                                                      |
| [`metadata`](#dfx-canister-metadata)               | Displays metadata in a canister.                                                                                                                       |
| [`request-status`](#dfx-canister-request-status)   | Requests the status of a call to a canister.                                                                                                           |
| [`send`](#dfx-canister-send)                       | Send a previously-signed message.                                                                                                                      |
| [`sign`](#dfx-canister-send)                       | Sign a canister call and generate message file.                                                                                                        |
| [`start`](#dfx-canister-start)                     | Starts a stopped canister.                                                                                                                             |
| [`status`](#dfx-canister-status)                   | Returns the current status of a canister as defined [here](https://internetcomputer.org/docs/current/references/ic-interface-spec#ic-canister_status). |
| [`stop`](#dfx-canister-stop)                       | Stops a currently running canister.                                                                                                                    |
| [`uninstall-code`](#dfx-canister-uninstall-code)   | Uninstalls a canister, removing its code and state. Does not delete the canister.                                                                      |
| [`update-settings`](#dfx-canister-update-settings) | Update one or more of a canister's settings (i.e its controller, compute allocation, or memory allocation.).                                           |
| [`url`](#dfx-canister-url)                         | Displays the url of a canister.                                                                                                                        |

## Overriding the default deployment environment

By default, `dfx canister` commands run on the local canister execution environment specified in the `dfx.json` file. If
you want to send a `dfx canister` subcommand to the Internet Computer or a testnet without changing the settings in
your `dfx.json` configuration file, you can explicitly specify the URL to connect to using the `--network` option.

For example, to register unique canister identifiers for a project on the local canister execution environment, you can
run the following command:

``` bash
dfx canister create --all
```

If you want to register unique canister identifiers for the same project on the Internet Computer, you can run the
following command:

``` bash
dfx canister create --all --network ic
```

The SDK comes with an alias of `ic`, which is configured to point to the Internet Computer. You can also pass a URL as a
network option, or you can configure additional aliases in `dfx.json` under the `networks` configuration, or
in `$HOME/.config/dfx/networks.json`.

To illustrate, you can call a canister and function running on a testnet using a command similar to the following:

``` bash
dfx canister call counter get --network http://192.168.3.1:5678
```

## Performing a call through the wallet

By default, most `dfx canister` commands to the Internet Computer are signed by and sent from your own principal. (
Exceptions are commands that require cycles: `dfx canister create` and `dfx canister deposit-cycles`. Those
automatically go through the wallet.) Occasionally, you may want to make a call from your wallet, e.g. when only your
wallet is allowed to call a certain function. To send a call through your wallet, you can use the `--wallet` option.
You will either specify the wallet's principal, or the name of an identity associated with a wallet.

### Specifying a wallet by principal

To specify a wallet by principal, pass the principal directly.

``` bash
dfx canister status <canister name> --wallet <wallet id>
```

As a concrete example, if you want to request the status of a canister on the ic that is only controlled by your wallet,
you would do the following:

``` bash
dfx identity get-wallet --network ic
```

This command outputs your wallet's principal (e.g. `22ayq-aiaaa-aaaai-qgmma-cai`) on the `ic` network. Using this id,
you can then query the status of the canister (let's assume the canister is called `my_canister_name`) as follows:

``` bash
dfx canister status --network ic --wallet 22ayq-aiaaa-aaaai-qgmma-cai
```

### Specifying a wallet by identity name

In the previous example, we first looked up the principal of the currently
selected wallet. We can avoid this step by specifying the name of an identity,
in which case dfx will look up the wallet's principal for us.

For example, the following command will query the status of a canister,
using the wallet associated with the default identity on the ic network:

``` bash
dfx canister status --network ic --wallet default
```

As another example, the following commands are equivalent:

``` bash
dfx canister status --network ic --wallet $(dfx identity get-wallet --network ic --identity alice)
dfx canister status --network ic --wallet alice
```

## dfx canister call

Use the `dfx canister call` command to call a specified method on a deployed canister.

### Basic usage

``` bash
dfx canister call [options] <canister_name> <method_name> [argument]
```

### Options

You can use the following options with the `dfx canister call` command.

| Option                            | Description                                                                                                                                                                                                                    |
|-----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--argument-file <argument-file>` | Specifies the file from which to read the argument to pass to the method.  Stdin may be referred to as `-`.                                                                                                                    |
| `--async`                         | Specifies not to wait for the result of the call to be returned by polling the replica. Instead return a response ID.                                                                                                          |
| `--candid <file.did>`             | Provide the .did file with which to decode the response. Overrides value from dfx.json for project canisters.                                                                                                                  |
| `--output <output>`               | Specifies the output format to use when displaying a methodâ€™s return result. The valid values are `idl`, 'json', `pp` and `raw`. The `pp` option is equivalent to `idl`, but is pretty-printed.                                |
| `--query`                         | Sends a query request instead of an update request. For information about the difference between query and update calls, see [Canisters include both program and state](/docs/current/concepts/canisters-code#canister-state). |
| `--random <random>`               | Specifies the config for generating random arguments.                                                                                                                                                                          |
| `--type <type>`                   | Specifies the data format for the argument when making the call using an argument. The valid values are `idl` and `raw`.                                                                                                       |
| `--update`                        | Sends an update request to a canister. This is the default if the method is not a query method.                                                                                                                                |
| `--with-cycles <amount>`          | Specifies the amount of cycles to send on the call. Deducted from the wallet. Requires `--wallet` as an option to `dfx canister`.                                                                                              |

### Arguments

You can specify the following arguments for the `dfx canister call` command.

| Argument        | Description                                                                                                                                                                                                       |
|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `canister_name` | Specifies the name of the canister to call. The canister name is a required argument and should match the name you have configured for a project in the `canisters` section of the `dfx.json` configuration file. |
| `method_name`   | Specifies the method name to call on the canister. The canister method is a required argument.                                                                                                                    |
| `argument`      | Specifies the argument to pass to the method                                                                                                                                                                      |

#### Specifies the argument to pass to the method

Depending on your program logic, the argument can be a required or optional argument. You can specify a data format type using the `--type` option if you pass an argument to the canister. By default, you can specify arguments using the [Candid](/docs/current/references/candid-ref) (`idl`) syntax for data values. For information about using Candid and its supported types, see [Interact with a service in a terminal](/docs/current/developer-docs/smart-contracts/candid/candid-howto#idl-syntax) and [supported types](/docs/current/references/candid-ref#supported-types). You can use `raw` as the argument type if you want to pass raw bytes.

### Examples

You can use the `dfx canister call` command to invoke specific methodsâ€”with or without argumentsâ€”after you have deployed
the canister using the `dfx canister install` command. For example, to invoke the `get` method for a canister with
a `canister_name` of `counter`, you can run the following command:

``` bash
dfx canister call counter get --async
```

In this example, the command includes the `--async` option to indicate that you want to make a separate `request-status`
call rather than waiting to poll the local canister execution environment or the Internet Computer for the result.
The `--async` option is useful when processing an operation might take some time to complete. The option enables you to
continue performing other operations then check for the result using a separate `dfx canister request-status` command.
The returned result will be displayed as the IDL textual format.

#### Using the IDL syntax

You can explicitly specify that you are passing arguments using the IDL syntax by running commands similar to the
following for a Text data type:

``` bash
dfx canister call hello greet --type idl '("Lisa")'
("Hello, Lisa!")

dfx canister call hello greet '("Lisa")' --type idl
("Hello, Lisa!")
```

You can also implicitly use the IDL by running a command similar to the following:

``` bash
dfx canister call hello greet '("Lisa")'
("Hello, Lisa!")
```

To specify multiple arguments using the IDL syntax, use commas between the arguments. For example:

``` bash
dfx canister call contacts insert '("Amy Lu","01 916-335-2042")'

dfx canister call hotel guestroom '("Deluxe Suite",42,true)'
```

You can pass raw data in bytes by running a command similar to the following:

``` bash
dfx canister call hello greet --type raw '4449444c00017103e29883'
```

This example uses the raw data type to pass a hexadecimal to the `greet` function of the `hello` canister.

### JSON output

The `--output json` option formats the output as JSON.

Candid types don't map 1:1 to JSON types.

Notably, the following Candid types map to strings rather than numbers: nat, nat64, int, int64.

These are the mappings:

| Candid type | JSON type                  |
|-------------|----------------------------|
| `null`      | `null`                     |
| `bool`      | `boolean`                  |
| `nat`       | `string`                   |
| `nat8`      | `number`                   |
| `nat16`     | `number`                   |
| `nat32`     | `number`                   |
| `nat64`     | `string`                   |
| `int`       | `string`                   |
| `int8`      | `number`                   |
| `int16`     | `number`                   |
| `int32`     | `number`                   |
| `int64`     | `string`                   |
| `float32`   | float or "NaN"             |
| `float64`   | float or "NaN"             |
| `text`      | `string`                   |
| `opt`       | array with 0 or 1 elements |
| `vec`       | array                      |
| `record`    | object                     |
| `variant`   | object                     |
| `blob`      | array of numbers           |

## dfx canister create

Use the `dfx canister create` command to register one or more canister identifiers without compiled code. The new
canister principals are then recorded in `canister_ids.json` for non-local networks. You must be connected to the local
canister execution environment or the Internet Computer to run this command.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

### Basic usage

``` bash
dfx canister create [options] [--all | canister_name]
```

### Options

You can use the following options with the `dfx canister create` command.

| Option                                    | Description                                                                                                                                                                                                                                                                                                                                                                              |
|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `-c`, `--compute-allocation <allocation>` | Specifies the canister's compute allocation. This should be a percent in the range [0..100].                                                                                                                                                                                                                                                                                             |
| `--controller <principal>`                | Specifies the identity name or the principal of the new controller.                                                                                                                                                                                                                                                                                                                      |
| `--created-at-time <timestamp>`           | Transaction timestamp, in nanoseconds, for use in controlling transaction deduplication, default is system time. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-                                                                                                                                                                |
| `--from-subaccount <subaccount>`          | Subaccount of the selected identity to spend cycles from.                                                                                                                                                                                                                                                                                                                                |
| `--memory-allocation <memory>`            | Specifies how much memory the canister is allowed to use in total. This should be a value in the range [0..12 GiB]. A setting of 0 means the canister will have access to memory on a â€œbest-effortâ€ basis: It will only be charged for the memory it uses, but at any point in time may stop running if it tries to allocate more memory when there isnâ€™t space available on the subnet. |
| `--reserved-cycles-limit <limit>`         | Specifies the upper limit for the canister's reserved cycles.                                                                                                                                                                                                                                                                                                                            |
| `--wasm-memory-limit <limit>`             | Specifies a soft upper limit for the canister's heap memory.                                                                                                                                                                                                                                                                                                                             |
| `--log-viewer <principal>`                | Specifies the principal as an allowed viewers. Can be specified more than once. Cannot be used with `--log-visibility`.                                                                                                                                                                                                                                                                  |
| `--log-visibility <visibility>`           | Specifies who can read the canister's logs: "controllers" or "public". For custom allowed viewers, use `--log-viewer`.                                                                                                                                                                                                                                                                   |
| `--no-wallet`                             | Performs the call with the user Identity as the Sender of messages. Bypasses the Wallet canister. Enabled by default.                                                                                                                                                                                                                                                                    |
| `--with-cycles <number-of-cycles>`        | Specifies the initial cycle balance to deposit into the newly created canister. The specified amount needs to take the canister create fee into account. This amount is deducted from the wallet's cycle balance.                                                                                                                                                                        |
| `--specified-id <PRINCIPAL>`              | Attempts to create the canister with this Canister ID                                                                                                                                                                                                                                                                                                                                    |
| `--subnet-type <subnet-type>`             | Specify the subnet type to create the canister on. If no subnet type is provided, the canister will be created on a random default application subnet.                                                                                                                                                                                                                                   |
| `--subnet <subnet-principal>`             | Specify the subnet to create the canister on. If no subnet is provided, the canister will be created on a random default application subnet.                                                                                                                                                                                                                                             |
| `--next-to <canister-principal>`          | Create canisters on the same subnet as this canister.                                                                                                                                                                                                                                                                                                                                    |

### Arguments

You can use the following argument with the `dfx canister create` command.

| Argument        | Description                                                                                                                                                                                                                                                                                                    |
|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Enables you to create multiple canister identifiers at once if you have a project `dfx.json` file that defines multiple canisters. Note that you must specify `--all` or an individual canister name.                                                                                                          |
| `canister_name` | Specifies the name of the canister for which you want to register an identifier. If you are not using the `--all` option, the canister name is a required argument and must match at least one name that you have configured in the `canisters` section of the `dfx.json` configuration file for your project. |

### Examples

You can use the `dfx canister create` command to register canister identifiers without first compiling any code. For
example, if you want to create the canister identifier for the project `my_counter` before writing the program, you can
run the following command:

``` bash
dfx canister create my_counter
```

You can use the `dfx canister create` command with the `--with-cycles` option to specify the initial balance upon the
creation of one canister or all canisters in a project. For example, to specify an initial balance of 8000000000000
cycles for all canisters, run the following command:

``` bash
dfx canister create --with-cycles 8000000000000 --all
```

#### Allocating message processing

The `--compute-allocation` options allows you to allocate computing resources as a percentage in the range of 0 to 100
to indicate how often your canister should be scheduled for execution.

For example, assume you run the following command:

``` bash
dfx canister create --all --compute-allocation 50
```

With this setting, all of the canisters in the current projects are assigned a 50% allocation. When canisters in the
project receive input messages to process, the messages are scheduled for execution. Over 100 execution cycles, each
canisterâ€™s messages will be scheduled for processing at least 50 times.

The default value for this option is 0â€”indicating that no specific allocation or scheduling is in effect. If all of your
canisters use the default setting, processing occurs in a round-robin fashion.

## dfx canister delete

Use the `dfx canister delete` command to delete a stopped canister from the local canister execution environment or the
Internet Computer. By default, this withdraws remaining cycles to your wallet before deleting the canister.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

### Basic usage

``` bash
dfx canister delete [options] [--all | canister_name]
```

### Options

You can use the following options with the `dfx canister delete` command.

| Option                                            | Description                                                                        |
|---------------------------------------------------|------------------------------------------------------------------------------------|
| `--no-withdrawal`                                 | Do not withdrawal cycles, just delete the canister.                                |
| `--initial-margin <cycles>`                       | Leave this many cycles in the canister when withdrawing cycles.                    |
| `--to-subaccount`                                 | Subaccount of the selected identity to deposit cycles to.                          |
| `--withdraw-cycles-to-dank`                       | Withdraw cycles to dank with the current principal.                                |
| `--withdraw-cycles-to-canister <principal>`       | Withdraw cycles from canister(s) to the specified canister/wallet before deleting. |
| `--withdraw-cycles-to-dank-principal <principal>` | Withdraw cycles to dank with the given principal.                                  |
| `-y, --yes`                                       | Auto-confirm deletion for a non-stopped canister                                   |

### Arguments

You can use the following arguments with the `dfx canister delete` command.

| Argument        | Description                                                                                                                        |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Deletes all of the canisters configured in the `dfx.json` file. Note that you must specify `--all` or an individual canister name. |
| `canister_name` | Specifies the name of the canister you want to delete. Note that you must specify either a canister name or the `--all` option.    |

### Examples

You can use the `dfx canister delete` command to delete a specific canister or all canisters.

To delete the `hello_world` canister, you can run the following command:

``` bash
dfx canister delete hello_world
```

To delete all of the canisters you have deployed on the `ic` Internet Computer and configured in your `dfx.json`, you
can run the following command:

``` bash
dfx canister delete --all --network=ic
```

## dfx canister deposit-cycles

Use the `dfx canister deposit-cycles` command to deposit cycles from your configured wallet into a canister.

Note that you must have your cycles wallet configured for this to work.

### Basic usage

``` bash
dfx canister deposit-cycles <cycles> [--all | canister_name]
```

### Arguments

You can use the following arguments with the `dfx canister deposit-cycles` command.

| Argument        | Description                                                                                                                                             |
|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Deposits the specified amount of cycles into all canisters configured in `dfx.json`. Note that you must specify `--all` or an individual canister name. |
| `canister_name` | Specifies the name of the canister you want to deposit cycles into. Note that you must specify either a canister name or the `--all` option.            |

### Options

You can use the following options with the `dfx canister deposit-cycles` command.

| Option                           | Description                                                                                                                                                                                                               |
|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--created-at-time <timestamp>`  | Transaction timestamp, in nanoseconds, for use in controlling transaction deduplication, default is system time. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication- |
| `--from-subaccount <subaccount>` | Subaccount of the selected identity to spend cycles from.                                                                                                                                                                 |


### Examples

You can use the `dfx canister deposit-cycles` command to add cycles to a specific canister or all canisters.

To add 1T cycles to the canister called `hello`, you can run the following command:

``` bash
dfx canister deposit-cycles 1000000000000 hello
```

To add 2T cycles to each individual canister specified in `dfx.json`, you can run the following command:

``` bash
dfx canister deposit-cycles 2000000000000 --all
```

## dfx canister id

Use the `dfx canister id` command to output the canister identifier/principal for a specific canister name.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

If the canister has been deployed by the local user, the locally stored canister ID will be provided.

If a canister has been deployed by a third party, the user may set the `.canisters[$CANISTER_NAME].remote[$NETWORK]`
entry in `dfx.json` to the canister ID. In this case, the third party is responsible for maintaining the canister and
the local user must ensure that they have the correct canister ID.  `dfx` will return the provided canister ID with no
further checks.

If a canister is typically deployed to the same canister ID on mainnet and all testnets, the user may set a remote
canister ID for the `__default` network. In this case, `dfx canister id $CANISTER_NAME` will return the default canister
ID for all networks that don't have a dedicated entry.

### Basic usage

``` bash
dfx canister id <canister_name>
```

### Arguments

You can use the following argument with the `dfx canister id` command.

| Argument        | Description                                                                     |
|-----------------|---------------------------------------------------------------------------------|
| `canister_name` | Specifies the name of the canister for which you want to display an identifier. |

### Examples

You can use the `dfx canister id` command to display the canister identifier for a specific canister name.

To display the canister identifier for the `hello_world` canister, you can run the following command:

``` bash
dfx canister id hello_world
```

The command displays output similar to the following:

``` bash
75hes-oqbaa-aaaaa-aaaaa-aaaaa-aaaaa-aaaaa-q
```

## dfx canister info

Use the `dfx canister info` command to output a canister's controller and installed Wasm module hash.

### Basic usage

``` bash
dfx canister info <canister>
```

### Arguments

You can use the following argument with the `dfx canister info` command.

| Argument   | Description                                                                  |
|------------|------------------------------------------------------------------------------|
| `canister` | Specifies the name or id of the canister for which you want to display data. |

### Examples

You can use the `dfx canister info` command to display the canister controller and installed Wasm module.

To the data about the `hello_world` canister, you can run the following command:

``` bash
dfx canister info hello_world
```

The command displays output similar to the following:

```
Controllers: owdog-wiaaa-aaaad-qaaaq-cai
Module hash: 0x2cfb6f216fd6ab367364c02960afbbc5c444f5481225ee676992ac9058fd41e3
```

## dfx canister install

Use the `dfx canister install` command to install compiled code as a canister on the Internet Computer or on the local
canister execution environment.

### Basic usage

``` bash
dfx canister install [option] [--all | canister_name]
```

### Options

You can use the following options with the `dfx canister install` command.

| Option                             | Description                                                                                                                                                                                                                                                           |
|------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--argument <argument>`            | Specifies an argument to pass to the canister during installation.                                                                                                                                                                                                    |
| `--argument-type <argument-type>`  | Specifies the data type for the argument when making the call using an argument [possible values: idl, raw]                                                                                                                                                           |
| `--argument-file <argument-file>`  | Specifies the file from which to read the argument to pass to the init method.  Stdin may be referred to as `-`.                                                                                                                                                      |
| `--async-call`                     | Enables you to continue without waiting for the result of the installation to be returned by polling the Internet Computer or the local canister execution environment.                                                                                               |
| `-m`, `--mode <mode>`              | Specifies whether you want to `install`, `reinstall`, or `upgrade` canisters. Defaults to `install`. For more information about installation modes and canister management, see [managing canisters](/docs/current/developer-docs/smart-contracts/maintain/settings). |
| `--no-wallet`                      | Performs the call with the user Identity as the Sender of messages. Bypasses the Wallet canister. Enabled by default.                                                                                                                                                 |
| `--no-asset-upgrade`               | Skips upgrading the asset canister, to only install the assets themselves.                                                                                                                                                                                            |
| `--upgrade-unchanged`              | Upgrade the canister even if the .wasm did not change.                                                                                                                                                                                                                |
| `--wasm <file.wasm>`               | Specifies a particular Wasm file to install, bypassing the dfx.json project settings.                                                                                                                                                                                 |
| `--skip-pre-upgrade`               | Skip the pre_upgrade hook on upgrade. This requires the upgrade/auto mode.                                                                                                                                                                                            |
| `--wasm-memory-persistence <mode>` | Keep or replace the Wasm main memory on upgrade. Possible values: keep, replace. This requires the upgrade/auto mode.                                                                                                                                                 |

#### Specifies the argument to pass to the init entrypoint

With `--argument-type`, you can specify the data format for the argument when you install using the `--argument` option. The valid values are `idl` and `raw`. By default, you can specify arguments using the [Candid](/docs/current/developer-docs/smart-contracts/candid/index) (`idl`) syntax for data values. For information about using Candid and its supported types, see [Interact with a service in a terminal](/docs/current/developer-docs/smart-contracts/candid/candid-howto#idl-syntax) and [Supported types](/docs/current/references/candid-ref). You can use `raw` as the argument type if you want to pass raw bytes to a canister.

### Arguments

You can use the following arguments with the `dfx canister install` command.

| Argument        | Description                                                                                                                                                                                                                                                  |
|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Enables you to install multiple canisters at once if you have a project `dfx.json` file that includes multiple canisters. Note that you must specify `--all` or an individual canister name.                                                                 |
| `canister_name` | Specifies the name of the canister to deploy. If you are not using the `--all` option, the canister name is a required argument and should match the name you have configured for a project in the `canisters` section of the `dfx.json` configuration file. |

### Examples

You can use the `dfx canister install` command to deploy WebAssembly you have compiled using the `dfx build` command as
a canister on the Internet Computer or on the local canister execution environment. The most common use case is to
install all of the canisters by running the following command:

``` bash
dfx canister install --all
```

#### Installing a specific canister

You can also use the `dfx canister install` command to deploy a specific canister instead of all of the canisters in
your project. For example, if you have a project with a `hello_world_backend` canister and a `hello_world_frontend`
canister but only want to deploy the `hello_world_backend` canister, you can deploy just that the canister by running
the following command:

``` bash
dfx canister install hello_world_backend
```

#### Sending an asynchronous request

If you want to submit a request to install the canister and return a request identifier to check on the status of your
request later instead of waiting for the command to complete, you can run a command similar to the following:

``` bash
dfx canister install hello_world_backend --async
```

This command submits a request to install the canister and returns a request identifier similar to the following:

``` bash
0x58d08e785445dcab4ff090463b9e8b12565a67bf436251d13e308b32b5058608
```

You can then use the request identifier to check the status of the request at a later time, much like a tracking number
if you were shipping a package.

#### Overriding the default deployment options

If you want to deploy a canister on a testnet without changing the settings in your `dfx.json` configuration file, you
can explicitly specify the testnet you want to connect to by using the `--network` option.

For example, you can specify a testnet URL by running a command similar to the following:

``` bash
dfx canister install --all --network http://192.168.3.1:5678
```

## dfx canister logs

Use the `dfx canister logs` command to display the logs from a canister.

### Basic usage

``` bash
dfx canister logs <canister-name>
```

### Examples

To display the logs from the `hello_world` canister, you can run the following command:

``` bash
dfx canister logs hello_world
```

The command displays output similar to the following:

``` log
[42. 2021-05-06T19:17:10.000000001Z]: Some text message
[43. 2021-05-06T19:17:10.000000002Z]: (bytes) 0xc0ffee
```

## dfx canister metadata

Use the `dfx canister metadata` command to display metadata stored in a canister's Wasm module.

### Basic usage

``` bash
dfx canister metadata <canister-name> <metadata-name>
```

### Arguments

You can use the following argument with the `dfx canister metadata` command.

| Argument        | Description                                                                      |
|-----------------|----------------------------------------------------------------------------------|
| `canister`      | Specifies the name or id of the canister for which you want to display metadata. |
| `metadata-name` | Specifies the name of the metadata which you want to display.                    |

### Examples

To display the candid service metadata for the `hello_world` canister, you can run the following command:

``` bash
dfx canister metadata hello_world candid:service
```

The command displays output similar to the following:

```
service : {
  greet: (text) -> (text);
}
```

## dfx canister request-status

Use the `dfx canister request-status` command to request the status of a specified call to a canister. This command
requires you to specify the request identifier you received after invoking a method on the canister. The request
identifier is an hexadecimal string starting with `0x`.

### Basic usage

``` bash
dfx canister request-status [options] <request-id> <canister>
```

### Options

You can use the following options with the `dfx canister request-status` command.

| Option              | Description                                                                                                                                                          |
|---------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--output <output>` | Specifies the format for displaying the method's return result. Possible values are `idl`, `raw` and `pp`, where `pp` is equivalent to `idl`, but is pretty-printed. |

### Arguments

You can specify the following argument for the `dfx canister request-status` command.

| Argument     | Description                                                                                                                                                                                                                                                                                                         |
|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `request_id` | Specifies the hexadecimal string returned in response to a `dfx canister call` or `dfx canister install` command. This identifier is an hexadecimal string starting with 0x.                                                                                                                                        |
| `canister`   | Specifies the name or id of the canister onto which the request was made. If the request was made to the Management canister, specify the id of the canister it is updating/querying. If the call was proxied by the wallet, i.e. a `dfx canister call --async --wallet=<ID>` flag, specify the wallet canister id. |

### Examples

You can use the `dfx canister request-status` command to check on the status of a canister state change or to verify
that a call was not rejected by running a command similar to the following:

``` bash
dfx canister request-status 0x58d08e785445dcab4ff090463b9e8b12565a67bf436251d13e308b32b5058608 backend
```

This command displays an error message if the request identifier is invalid or refused by the canister.

## dfx canister send

Use the `dfx canister send` command after signing a message with the `dfx canister sign` command when you want to
separate these steps, rather than using the single `dfx canister call` command. Using separate calls can add security to
the transaction.

For example, when creating your neuron stake, you might want to use the `dfx canister sign` command to create a
signed `message.json` file using an air-gapped computer, then use the `dfx canister send` command to deliver the signed
message.

### Basic usage

``` bash
dfx canister send [options] <file_name>
```

### Options

You can use the following options with the `dfx canister request-status` command.

| Option     | Description                                         |
|------------|-----------------------------------------------------|
| `--status` | Send the signed request-status call in the message. |

### Arguments

You can specify the following argument for the `dfx canister send` command.

| Argument    | Description                             |
|-------------|-----------------------------------------|
| `file_name` | Specifies the file name of the message. |

### Examples

Use the `dfx canister send` command to send a signed message created using the `dfx canister sign` command to the
genesis token canister (GTC) to create a neuron on your behalf by running the following command:

`dfx canister send message.json`

## dfx canister sign

Use the `dfx canister sign` command before sending a message with the `dfx canister send` command when you want to
separate these steps, rather than using the single `dfx canister call` command. Using separate calls can add security to
the transaction. For example, when creating your neuron stake, you might want to use the `dfx canister sign` command to
create a signed `message.json` file using an air-gapped computer, then use the `dfx canister send` command to deliver
the signed message from a computer connected to the Internet Computer.

### Basic usage

``` bash
dfx canister sign [options] <canister-name> <method-name> [argument]
```

### Options

You can specify the following options for the `dfx canister sign` command.

| Option                     | Description                                                                                                                                      |
|----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| `--argument-file <file>`   | Specifies the file from which to read the argument to pass to the method.  Stdin may be referred to as `-`.                                      |
| `--expire-after <seconds>` | Specifies how long the message will be valid before it expires and cannot be sent. Specify in seconds. If not defined, the default is 300s (5m). |
| `--file <output>`          | Specifies the output file name. The default is `message.json`.                                                                                   |
| `--query`                  | Sends a query request to a canister.                                                                                                             |
| `--random <random>`        | Specifies the configuration for generating random arguments.                                                                                     |
| `--type <type>`            | Specifies the data type for the argument when making a call using an argument. Possible values are `idl` and `raw`.                              |
| `--update`                 | Sends an update request to the canister. This is the default method if the `--query` method is not used.                                         |


### Arguments

You can specify the following arguments for the `dfx canister sign` command.

| Argument        | Description                                                                                                                                                                                                       |
|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `canister_name` | Specifies the name of the canister to call. The canister name is a required argument and should match the name you have configured for a project in the `canisters` section of the `dfx.json` configuration file. |
| `method_name`   | Specifies the method name to call on the canister. The canister method is a required argument.                                                                                                                    |
| `argument`      | Specifies the argument to pass to the method                                                                                                                                                                      |

#### Specifies the argument to pass to the method

Depending on your program logic, the argument can be a required or optional argument. You can specify a data format type using the `--type` option if you pass an argument to the canister. By default, you can specify arguments using the [Candid](/docs/current/references/candid-ref) (`idl`) syntax for data values. For information about using Candid and its supported types, see [Interact with a service in a terminal](/docs/current/developer-docs/smart-contracts/candid/candid-howto#idl-syntax) and [supported types](/docs/current/references/candid-ref#supported-types). You can use `raw` as the argument type if you want to pass raw bytes.

### Examples

Use the `dfx canister sign` command to create a signed `message.json` file using the selected identity by running a
command similar to the following:

`dfx canister sign --network=ic --expire-after=1h rno2w-sqaaa-aaaaa-aaacq-cai create_neurons â€˜(â€œPUBLIC_KEYâ€)â€™`

This command illustrates how to creates a `message.json` file to create neurons on the Internet Computer specified by
the `ic` alias, that is signed using your principal identifier as the message sender and with an expiration window that
ends in one hour.

Note that the time allotted to send a signed message is a fixed 5-minute window. The `--expire-after` option enables you
to specify the point in time when the 5-minute window for sending the signed message should end. For example, if you set
the `--expire-after` option to one hour (`1h`), you must wait at least 55 minutes before you send the generated message
and the signature for the message is only valid during the 5-minute window ending in the 60th minute.

In this example, therefore, you would need to send the message after 55 minutes and before 60 minutes for the message to
be recognized as valid.

If you donâ€™t specify the `--expire-after` option, the default expiration is five minutes.

Send the signed message to the genesis token canister (GTC) to create a neuron on your behalf by running the following
command:

`dfx canister send message.json`

## dfx canister snapshot create

Use the `dfx canister snapshot create` command to create a snapshot of a stopped canister.
It can later be applied with `dfx canister snapshot load` to reset the canister to that state.

At both times, the canister must be stopped with `dfx canister stop`.

### Basic usage

```bash
dfx canister snapshot create <canister> [--replace <replace>]
```

### Arguments

You can use the following arguments with the `dfx canister snapshot create` command.

| Argument              | Description                                                                    |
|-----------------------|--------------------------------------------------------------------------------|
| `<canister>`          | The canister to snapshot.                                                      |
| `--replace <replace>` | If a snapshot ID is specified, this snapshot will replace it and reuse the ID. |

### Examples

Use the `dfx canister snapshot create` command to take a snapshot of canister `hello`:

```sh
dfx canister stop hello
dfx canister snapshot create hello
dfx canister start hello
```

To replace an existing snapshot to save space:

```sh
dfx canister stop hello
dfx canister snapshot create hello --replace a1b2c3d4
dfx canister start hello
```

## dfx canister snapshot load

Use the `dfx canister snapshot load` command to load a canister snapshot previously taken with `dfx canister snapshot create`.

At both times, the canister must be stopped with `dfx canister stop`.

### Basic usage

```sh
dfx canister snapshot load <canister> <snapshot>
```

### Arguments

You can use the following arguments with the `dfx canister snapshot load` command.

| Argument     | Description                           |
|--------------|---------------------------------------|
| `<canister>` | The canister to load the snapshot in. |
| `<snapshot>` | The ID of the snapshot to load.       |

### Examples

Use the `dfx canister snapshot load` command to load a previously-taken snapshot in canister `hello`, deleting any data since the snapshot was taken:

```sh
dfx canister snapshot load hello 1a2b3c4d
```

## dfx canister snapshot delete

Use the `dfx canister snapshot delete` command to delete a canister snapshot previously taken with `dfx canister snapshot create`.

### Basic usage 

```sh
dfx canister snapshot delete <canister> <snapshot>
```

### Arguments

You can use the following arguments with the `dfx canister snapshot delete` command.

| Argument     | Description                               |
|--------------|-------------------------------------------|
| `<canister>` | The canister to delete the snapshot from. |
| `<snapshot>` | The ID of the snapshot to delete.         |

### Examples

Use the `dfx canister snapshot delete` command to delete a snapshot from canister `hello`:

```sh
dfx canister snapshot delete hello 1a2b3c4d
```

## dfx canister snapshot list

Use the `dfx canister snapshot list` command to display all saved snapshots for a canister. They can be loaded with `dfx canister snapshot load` or deleted with `dfx canister snapshot delete`.

### Basic usage

```sh
dfx canister snapshot list <canister>
```

### Arguments

You can use the following arguments with the `dfx canister snapshot list` command.

| Argument     | Description                          |
|--------------|--------------------------------------|
| `<canister>` | The canister to list snapshots from. |

### Examples

Use the `dfx canister snapshot list` command to list the snapshots in canister `hello`:

```sh
dfx canister snapshot list hello
```

## dfx canister start

Use the `dfx canister start` command to restart a stopped canister on the Internet Computer or the local canister
execution environment.

In most cases, you run this command after you have stopped a canister to properly terminate any pending requests as a
prerequisite to upgrading the canister.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

### Basic usage

``` bash
dfx canister start [--all | canister_name]
```

### Arguments

You can use the following arguments with the `dfx canister start` command.

| Argument        | Description                                                                                                                       |
|-----------------|-----------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Starts all of the canisters configured in the `dfx.json` file. Note that you must specify `--all` or an individual canister name. |
| `canister_name` | Specifies the name of the canister you want to start. Note that you must specify either a canister name or the `--all` option.    |

### Examples

You can use the `dfx canister start` command to start a specific canister or all canisters.

To start the `hello_world` canister, you can run the following command:

``` bash
dfx canister start hello_world
```

To start all of the canisters you have deployed on the `ic` Internet Computer, you can run the following command:

``` bash
dfx canister start --all --network=ic
```

## dfx canister status

Use the `dfx canister status` command to check whether a canister is currently running, in the process of stopping, or
currently stopped on the Internet Computer or on the local canister execution environment.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

### Basic usage

``` bash
dfx canister status [--all | canister_name]
```

### Arguments

You can use the following arguments with the `dfx canister status` command.

| Argument        | Description                                                                                                                                               |
|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Returns status information for all of the canisters configured in the `dfx.json` file. Note that you must specify `--all` or an individual canister name. |
| `canister_name` | Specifies the name of the canister you want to return information for. Note that you must specify either a canister name or the `--all` option.           |

### Examples

You can use the `dfx canister status` command to check the status of a specific canister or all canisters.

To check the status of the `hello_world` canister, you can run the following command:

``` bash
dfx canister status hello_world
```

To check the status for all of the canisters you have deployed on the `ic` Internet Computer, you can run the following
command:

``` bash
dfx canister status --all --network=ic
```

## dfx canister stop

Use the `dfx canister stop` command to stop a canister that is currently running on the Internet Computer or on the
local canister execution environment.

In most cases, you run this command to properly terminate any pending requests as a prerequisite to upgrading the
canister.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

### Basic usage

``` bash
dfx canister stop [--all | canister_name]
```

### Arguments

You can use the following arguments with the `dfx canister stop` command.

| Argument        | Description                                                                                                                      |
|-----------------|----------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Stops all of the canisters configured in the `dfx.json` file. Note that you must specify `--all` or an individual canister name. |
| `canister_name` | Specifies the name of the canister you want to stop. Note that you must specify either a canister name or the `--all` option.    |

### Examples

You can use the `dfx canister stop` command to stop a specific canister or all canisters.

To stop the `hello_world` canister, you can run the following command:

``` bash
dfx canister stop hello_world
```

To stop all of the canisters you have deployed on the `ic` Internet Computer, you can run the following command:

``` bash
dfx canister stop --all --network=ic
```

## dfx canister uninstall-code

Use the `dfx canister uninstall-code` command to uninstall the code that a canister that is currently running on the
Internet Computer or on the local canister execution environment.

This method removes a canisterâ€™s code and state, making the canister empty again. Only the controller of the canister
can uninstall code. Uninstalling a canisterâ€™s code will reject all calls that the canister has not yet responded to, and
drop the canisterâ€™s code and state. Outstanding responses to the canister will not be processed, even if they arrive
after code has been installed again. The canister is now empty.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

### Basic usage

``` bash
dfx canister uninstall-code [--all | canister_name]
```

### Arguments

You can use the following arguments with the `dfx canister uninstall-code` command.

| Argument        | Description                                                                                                                           |
|-----------------|---------------------------------------------------------------------------------------------------------------------------------------|
| `--all`         | Uninstalls all of the canisters configured in the `dfx.json` file. Note that you must specify `--all` or an individual canister name. |
| `canister_name` | Specifies the name of the canister you want to uninstall. Note that you must specify either a canister name or the `--all` option.    |

### Examples

You can use the `dfx canister uninstall-code` command to uninstall a specific canister or all canisters.

To uninstall the `hello_world` canister, you can run the following command:

``` bash
dfx canister uninstall-code hello_world
```

To uninstall all of the canisters you have deployed on the `ic` Internet Computer, you can run the following command:

``` bash
dfx canister uninstall-code --all --network=ic
```

## dfx canister update-settings

Use the `dfx canister update-settings` command to update the settings of a canister running in the local execution
environment.

In most cases, you run this command to tune the amount of resources allocated to your canister.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

### Basic usage

``` bash
dfx canister update-settings [options] [canister_name | --all]
```

### Options

You can specify the following options for the `dfx canister update-settings` command.

| Option                                    | Description                                                                                                                                                                                                                                                                                                                                                                              |
|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--add-controller <principal>`            | Add a principal to the list of controllers of the canister.                                                                                                                                                                                                                                                                                                                              |
| `--add-log-viewer <principal>`            | Add a principal to the list of log viewers of the canister. Can be specified more than once to add multiple log viewers. If current log visibility is `public` or `controllers`, it will be changed to the custom allowed viewer list.                                                                                                                                                   |
| `-c`, `--compute-allocation <allocation>` | Specifies the canister's compute allocation. This should be a percent in the range [0..100].                                                                                                                                                                                                                                                                                             |
| `--confirm-very-long-freezing-threshold`  | Freezing thresholds above ~1.5 years require this option as confirmation.                                                                                                                                                                                                                                                                                                                |
| `--set-controller <principal>`            | Specifies the identity name or the principal of the new controller. Can be specified more than once, indicating the canister will have multiple controllers. If any controllers are set with this parameter, any other controllers will be removed.                                                                                                                                      |
| `--set-log-viewer <principal>`            | Specifies the the principal of the log viewer of the canister. Can be specified more than once, indicating the canister will have multiple log viewers. If any log viewers are set with this parameter, any other log viewers will be removed. If current log visibility is `public` or `controllers`, it will be changed to the custom allowed viewer list.                             |
| `--memory-allocation <allocation>`        | Specifies how much memory the canister is allowed to use in total. This should be a value in the range [0..12 GiB]. A setting of 0 means the canister will have access to memory on a â€œbest-effortâ€ basis: It will only be charged for the memory it uses, but at any point in time may stop running if it tries to allocate more memory when there isnâ€™t space available on the subnet. |
| `--reserved-cycles-limit <limit>`         | Specifies the upper limit of the canister's reserved cycles.                                                                                                                                                                                                                                                                                                                             |
| `--wasm-memory-limit <limit>`             | Specifies a soft upper limit for the canister's heap memory.                                                                                                                                                                                                                                                                                                                             |
| `--log-visibility <visibility>`           | Specifies who is allowed to read the canister's logs. Can be either "controllers" or "public". For custom allowed viewers, use `--set-log-viewer` or `--add-log-viewer`.                                                                                                                                                                                                                 |
| `--remove-controller <principal>`         | Removes a principal from the list of controllers of the canister.                                                                                                                                                                                                                                                                                                                        |
| `--remove-log-viewer <principal>`         | Removes a principal from the list of log viewers of the canister. Can be specified more than once to remove multiple log viewers.                                                                                                                                                                                                                                                        |
| `--freezing-threshold <seconds>`          | Set the [freezing threshold](https://internetcomputer.org/docs/current/references/ic-interface-spec/#ic-create_canister) in seconds for a canister. This should be a value in the range [0..2^64^-1]. Very long thresholds require the `--confirm-very-long-freezing-threshold` option.                                                                                                  |
| `-y`, `--yes`                             | Skips yes/no checks by answering 'yes'. Such checks can result in loss of control, so this is not recommended outside of CI.                                                                                                                                                                                                                                                             |

### Arguments

You can use the following arguments with the `dfx canister update-settings` command.

| Argument        | Description                                                                                                           |
|-----------------|-----------------------------------------------------------------------------------------------------------------------|
| `--all`         | Updates all canisters you have specified in `dfx.json`. You must specify either canister name/id or the --all option. |
| `canister_name` | Specifies the name of the canister you want to update. You must specify either canister name/id or the --all option.  |

### Examples

You can use the `dfx canister update-settings` command to update settings of a specific canister.

To update the settings of the `hello_world` canister, you can run the following command:

``` bash
dfx canister update-settings --freezing-threshold 2592000 --compute-allocation 99 hello_world
```

## dfx canister url

Use the `dfx canister url` command to output the canister url for a specific canister.

Note that you can only run this command from within the project directory structure. For example, if your project name
is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its
subdirectories.

If the canister has been deployed by the local user, the locally stored canister ID will be provided within the url. 
You can check the `.dfx/local/canister_ids.json` file in the project directory for details.

If a canister has been deployed by a third party, the user may set the `.canisters[$CANISTER_NAME].remote[$NETWORK]`
entry in `dfx.json` to the canister ID. In this case, the third party is responsible for maintaining the canister and
the local user must ensure that they have the correct canister ID.  `dfx` will use the provided canister ID with no
further checks.

### Basic usage

``` bash
dfx canister url <canister>
```

### Arguments

You can use the following argument with the `dfx canister url` command.

| Argument   | Description                                                                     |
|------------|---------------------------------------------------------------------------------|
| `canister` | Specifies the name or id of the canister for which you want to display the url. |

### Examples

To display the canister url for the `hello_world_backend` canister, you can run the following command:

``` bash
dfx canister url hello_world_backend
```

The command displays output similar to the following:

``` bash
http://127.0.0.1:4943/?canisterId=br5f7-7uaaa-aaaaa-qaaca-cai&id=bkyz2-fmaaa-aaaaa-qaaaq-cai
```

The `br5f7-7uaaa-aaaaa-qaaca-cai` is the local canister id for `__Candid_UI` canister. You need to run `dfx deploy`
to generate the local `__Candid_UI` entry in `.dfx/local/canister_ids.json` first.

If you run: 

``` bash
dfx canister url hello_world_backend --network ic
```

The command displays output similar to the following:

``` bash
https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io/?id=bkyz2-fmaaa-aaaaa-qaaaq-cai
```

The `a4gq6-oaaaa-aaaab-qaa4q-cai` is the canister id for `__Candid_UI` canister on the IC mainnet.


-----------------------

/docs/cli-reference/dfx-completion.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx completion

<MarkdownChipRow labels={["Reference"]} />

Generate a shell completion script for `dfx`.

## Basic usage

``` bash
dfx completion [option] SHELL
```

The command takes installed extensions into account when generating the
completion script, so you should regenerate it after installing or removing
extensions.

The values supported for SHELL are `bash`, `elvish`, `fish`, `powershell`,
and `zsh`. The default value is `bash`.

## Options

You can use the following option with the `dfx completion` command.

| Option                | Description                        -|
|-----------------------|-------------------------------------|
| `--bin-name <name>`   | Name of the command. Only needed if referring to dfx by another name, such as with an alias. Default `dfx`. |

## Examples

Generate a bash completion script:

``` bash
dfx completion
```

Generate a zsh completion script:

``` bash
dfx completion zsh
```

Enable completion in zsh:
```bash
source <(dfx completion zsh)
```

Enable completion in bash:
```bash
source <(dfx completion)
```


-----------------------

/docs/cli-reference/dfx-cycles.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx cycles

<MarkdownChipRow labels={["Reference"]} />

**NOTE**: The cycles ledger is in development and the dfx cycles command is not expected to work on mainnet at this time.

Use the `dfx cycles` command to manage cycles associated with an identity's principal.

The basic syntax for running `dfx cycles` commands is:

``` bash
dfx cycles [subcommand] [options]
```

The following subcommands are available:

| Command                               | Description                                                                          |
|---------------------------------------|--------------------------------------------------------------------------------------|
| [`balance`](#dfx-cycles-balance)      | Prints the account balance of the user.                                              |
| [`convert`](#dfx-cycles-convert)      | Convert some of the user's ICP balance into cycles.                                  |
| [`transfer`](#dfx-cycles-transfer)    | Send cycles to another account.                                                      |
| `help`                                | Displays usage information message for a specified subcommand.                       |

To view usage information for a specific subcommand, specify the subcommand and the `--help` flag. For example, to see usage information for `dfx cycles balance`, you can run the following command:

`dfx cycles balance --help`

## dfx cycles approve

Use the `dfx cycles approve` command to approve a principal to spend cycles on your behalf.

### Basic usage

``` bash
dfx cycles approve [options] <spender> <amount>
```

### Arguments

You must specify the following arguments for the `dfx cycles approve` command.

| Argument   | Description                           |
|------------|---------------------------------------|
| `<spender>`| Allow this principal to spend cycles. |
| `<amount>` | The number of cycles to approve.      |

### Options

You can specify the following options for the `dfx cycles approve` command.

| Option                              | Description                                                                            |
|-------------------------------------|----------------------------------------------------------------------------------------|
| `--created-at-time <timestamp>`     | Specify the timestamp-nanoseconds for the `created_at_time` field on the transfer request. Useful for controlling [transaction-de-duplication.](https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication) |
| `--expected-allowance <cycles>`     | The number of previously approved cycles. See [ICRC-2 specification](https://github.com/dfinity/ICRC-1/blob/main/standards/ICRC-2/README.md) for details. |
| `--expires-at <timestamp>`        | Timestamp-nanoseconds until which the approval is valid. None means that the approval is valid indefinitely. |
| `--from-subaccount <subaccount>`    | Approve cycles to be spent from this subaccount.                                       |
| `--memo <memo>`                     | Specifies a numeric memo for this transaction. |
| `--spender-subaccount <subaccount>` | Allow this subaccount to spend cycles.                                                 |

### Examples

Approve the principal `raxcz-bidhr-evrzj-qyivt-nht5a-eltcc-24qfc-o6cvi-hfw7j-dcecz-kae` to spend 1 billion cycles:

``` bash
dfx cycles approve raxcz-bidhr-evrzj-qyivt-nht5a-eltcc-24qfc-o6cvi-hfw7j-dcecz-kae 1000000000 --network ic
```

Approve from a subaccount:

``` bash
dfx cycles approve raxcz-bidhr-evrzj-qyivt-nht5a-eltcc-24qfc-o6cvi-hfw7j-dcecz-kae 1000000000 --from-subaccount 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f --network ic
```

## dfx cycles balance

Use the `dfx cycles balance` command to print your account balance or that of another user.

### Basic usage

``` bash
dfx cycles balance [flag] --network ic
```

### Options

You can specify the following arguments for the `dfx cycles balance` command.

| Option                                      | Description                                                         |
|---------------------------------------------|---------------------------------------------------------------------|
| `--owner <principal>`                       | Display the balance of this principal                               |
| `--subaccount <subaccount>`                 | Display the balance of this subaccount                              |
| `--precise`                                 | Displays the exact balance, without scaling to trillions of cycles. |

### Examples

Check the cycles balance of the selected identity.

```
$ dfx cycles balance --network ic
89.000 TC (trillion cycles).
```

To see the exact amount of cycles, you can use the `--precise` option:
```
$ dfx cycles balance --network ic --precise
89000000000000 cycles.
```

You can use the `dfx cycles balance` command to check the balance of another principal:

``` bash
dfx cycles balance --owner raxcz-bidhr-evrzj-qyivt-nht5a-eltcc-24qfc-o6cvi-hfw7j-dcecz-kae --network ic
```

## dfx cycles convert

Use the `dfx cycles convert` command to convert ICP into cycles that are stored on the cycles ledger.

### Basic usage

``` bash
dfx cycles convert [flag] --network ic
```

### Options

You can specify the following arguments for the `dfx cycles convert` command.

| Option                                      | Description |
|---------------------------------------------|-------------|
| `--amount <amount>`            | Specify the number of ICP tokens to convert to cycles and deposit into your cycles ledger account. You can specify an amount as a number with up to eight (8) decimal places. |
| `--created-at-time <timestamp>`| Specify the timestamp-nanoseconds for the `created_at_time` field on the ledger transfer request. Useful for transaction deduplication. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-  |
| `--e8s <e8s>`                  | Specify ICP token fractional unitsâ€”called e8sâ€”as a whole number, where one e8 is the smallest fraction of an ICP token. For example, 1.05000000 is 1 ICP and 5000000 e8s. You can use this option on its own or in conjunction with the `--icp` option. |
| `--fee <fee>`                  | Specify a transaction fee. The default is 10000 e8s. |
| `--icp <icp>`                  | Specify ICP tokens as a whole number. You can use this option on its own or in conjunction with `--e8s`. |
| `--memo <memo>`                | Memo used when depositing the minted cycles. |
| `--to-subaccount <subaccount>` | Subaccount where the cycles are deposited. |

### Examples

Convert 10 ICP into cycles.

```
$ dfx cycles convert --network ic --amount 10
Account was topped up with 1_234_567_000_000_000 cycles! New balance is 1_234_567_000_000_000 cycles.
```

To have the cycles deposited into a different subaccount, use the `--to-subaccount` option.

```
$ dfx cycles convert --network ic --amount 10 --to-subaccount 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
Account was topped up with 1_234_567_000_000_000 cycles! New balance is 1_234_567_000_000_000 cycles.
```

## dfx cycles transfer

Use the `dfx cycles transfer` command to transfer cycles from your account to another account.

### Basic usage

``` bash
dfx cycles transfer [options] <to> <amount>
```

### Arguments

You must specify the following arguments for the `dfx cycles transfer` command.

| Argument   | Description                       |
|------------|-----------------------------------|
| `<to>`     | The principal of the account to which you want to transfer cycles. |
| `<amount>` | The number of cycles to transfer. |

### Options

You can specify the following options for the `dfx cycles transfer` command.

| Option                              | Description                                                                            |
|-------------------------------------|----------------------------------------------------------------------------------------|
| `--from <principal>`                |  Transfer cycles from this principal. Requires that principal's approval.              |
| `--to-subaccount <subaccount>`      | The subaccount to which you want to transfer cycles.                                   |
| `--from-subaccount <subaccount>`    | The subaccount from which you want to transfer cycles.                                 |
| `--spender-subaccount <subaccount>` | Deduct allowance from this subaccount. Requires `--from` to be specified.              |
| `--memo <memo>`                     | Specifies a numeric memo for this transaction. |
| `--created-at-time <timestamp>`     | Specify the timestamp-nanoseconds for the `created_at_time` field on the transfer request. Useful for controlling transaction-de-duplication. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication- |

### Examples

Transfer 1 billion cycles to another account:

``` bash
dfx cycles transfer raxcz-bidhr-evrzj-qyivt-nht5a-eltcc-24qfc-o6cvi-hfw7j-dcecz-kae 1000000000 --network ic
```

Transfer from a subaccount:

``` bash
dfx cycles transfer raxcz-bidhr-evrzj-qyivt-nht5a-eltcc-24qfc-o6cvi-hfw7j-dcecz-kae 1000000000 --from-subaccount 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f --network ic
```

## dfx cycles top-up

Use the `dfx cycles top-up` command to send cycles from your account to a canister.

### Basic usage

``` bash
dfx cycles top-up [options] <to> <amount>
```

### Arguments

You must specify the following arguments for the `dfx cycles transfer` command.

| Argument   | Description                                                             |
|------------|-------------------------------------------------------------------------|
| `<to>`     | The name of a canister in the current project, or a canister principal. |
| `<amount>` | The number of cycles to transfer.                                       |

### Options

You can specify the following options for the `dfx cycles top-up` command.

| Option                           | Description                                                                            |
|----------------------------------|----------------------------------------------------------------------------------------|
| `--from-subaccount <subaccount>` | The subaccount from which you want to transfer cycles.                                 |
| `--created-at-time <timestamp>`  | Specify the timestamp-nanoseconds for the `created_at_time` field on the transfer request. Useful for controlling transaction deduplication. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication- |

### Examples

Send cycles to a canister in your project:

``` bash
dfx cycles top-up my_backend 1000000000 --network ic
```

Send cycles to a canister by principal:

``` bash
dfx cycles top-up bkyz2-fmaaa-aaaaa-qaaaq-cai 1000000000 --network ic
```


-----------------------

/docs/cli-reference/dfx-deploy.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx deploy

<MarkdownChipRow labels={["Reference"]}/>

Use the `dfx deploy` command to register, build, and deploy a dapp on the local canister execution environment, on the IC or on a specified testnet. By default, all canisters defined in the project `dfx.json` configuration file are deployed.

This command simplifies the developer workflow by enabling you to run one command instead of running the following commands as separate steps:

```shell
dfx canister create --all
dfx build
dfx canister install --all
```

Note that you can only run this command from within the project directory structure. For example, if your project name is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its subdirectories.

## Basic usage

``` bash
dfx deploy [options] [canister_name]
```

## Options

You can use the following options with the `dfx deploy` command.

| Option                             | Description                                                                                                                                                                                                                                                        |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--network <network>`              | Overrides the environment to connect to. By default, the local canister execution environment is used.                                                                                                                                                             |
| `--playground       `              | Alias for `--network playground`. By default, canisters on this network are borrowed from the Motoko Playground.                                                                                                                                                   |
| `--ic               `              | Alias for `--network ic`.                                                                                                                                                                                                                                          |
| `--argument <argument>`            | Specifies an argument to pass to the canister during installation.                                                                                                                                                                                                 |
| `--argument-type <argument-type>`  | Specifies the data type for the argument when making the call using an argument [possible values: idl, raw]                                                                                                                                                        |
| `--argument-file <argument-file>`  | Specifies the file from which to read the argument to pass to the init method.  Stdin may be referred to as `-`.                                                                                                                                                   |
| `--created-at-time <timestamp>`    |  Transaction timestamp, in nanoseconds, for use in controlling transaction deduplication, default is system time. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-                                         |
| `--from-subaccount <subaccount>`   | Subaccount of the selected identity to spend cycles from.                                                                                                                                                                                                          |
| `-m`, `--mode <mode>`              | Specifies whether you want to `install`, `reinstall`, or `upgrade` canisters. Defaults to `auto`. For more information about installation modes and canister management, see [managing canisters](/docs/current/developer-docs/smart-contracts/maintain/settings). |
| `--with-cycles <number-of-cycles>` | Enables you to specify the initial number of cycles for a canister in a project.                                                                                                                                                                                   |
| `--specified-id <PRINCIPAL>`       | Attempts to create the canister with this Canister ID                                                                                                                                                                                                              |
| `--by-proposal`                    | Upload proposed changed assets, but do not commit them.  Follow up by calling either commit_proposed_batch() or delete_batch().                                                                                                                                    |
| `--compute-evidence`               | Build a frontend canister, determine batch operations required to synchronize asset canister contents, and compute a hash over those operations.  Displays this hash ("evidence"), which should match the evidence displayed by `dfx deploy --by-proposal`.        |
| `--subnet-type <subnet-type>`      | Specify the subnet type to create the canister on. If no subnet type is provided, the canister will be created on a random default application subnet.                                                                                                             |
| `--subnet <subnet-principal>`      | Specify the subnet to create the canister on. If no subnet is provided, the canister will be created on a random default application subnet.                                                                                                                       |
| `--next-to <canister-principal>`   | Create canisters on the same subnet as this canister.                                                                                                                                                                                                              |
| `--skip-pre-upgrade`               | Skip the pre_upgrade hook on upgrade. This requires the upgrade/auto mode.                                                                                                                                                                                         |
| `--wasm-memory-persistence <mode>` | Keep or replace the Wasm main memory on upgrade. Possible values: keep, replace. This requires the upgrade/auto mode.                                                                                                                                              |

### Specifies the argument to pass to the init entrypoint

With `--argument-type`, you can specify the data format for the argument when you install using the `--argument` option. The valid values are `idl` and `raw`. By default, you can specify arguments using the [Candid](/docs/current/developer-docs/smart-contracts/candid/index) (`idl`) syntax for data values. For information about using Candid and its supported types, see [Interact with a service in a terminal](/docs/current/developer-docs/smart-contracts/candid/candid-howto#idl-syntax) and [Supported types](/docs/current/references/candid-ref). You can use `raw` as the argument type if you want to pass raw bytes to a canister.

## Arguments

You can specify the following arguments for the `dfx deploy` command.

| Argument        | Description                                                                                                                                                                                                                                                                                                                                    |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `canister_name` | Specifies the name of the canister you want to register, build, and deploy. Note that the canister name you specify must match at least one name in the `canisters` section of the `dfx.json` configuration file for the project. If you donâ€™t specify a canister name, `dfx deploy` will deploy all canisters defined in the `dfx.json` file. |

## Examples

You can use the `dfx deploy` command to deploy all or specific canisters on the local canister execution environment, on the IC or on a specified testnet.

For example, to deploy the `hello` project on the hypothetical `ic-pubs` testnet configured in the `dfx.json` configuration file, you can run the following command:

``` bash
dfx deploy hello_backend --network ic-pubs
```

To deploy a project on the local canister execution environment and pass a single argument to the installation step, you can run a command similar to the following:

``` bash
dfx deploy hello_actor_class --argument '("from DFINITY")'
```

Note that currently you must use an actor class in your Motoko dapp. In this example, the `dfx deploy` command specifies an argument to pass to the `hello_actor_class` canister. The main program for the `hello_actor_class` canister looks like this:
```motoko
actor class Greet(name: Text) {
    public query func greet() : async Text {
        return "Hello, " # name # "!";
    };
};
```

You can use the `dfx deploy` command with the `--with-cycles` option to specify the initial balance of a canister created by your wallet. If you donâ€™t specify a canister, the number of cycles you specify will be added to all canisters by default. To avoid this, specify a specific canister by name. For example, to add an initial balance of 8000000000000 cycles to a canister called "hello-assets", run the following command:

``` bash
dfx deploy --with-cycles 8000000000000 hello-assets
```


-----------------------

/docs/cli-reference/dfx-deps.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx deps

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx deps` command with flags and subcommands to pull dependencies from the mainnet and deploy locally.

The basic syntax for running `dfx deps` commands is:

``` bash
dfx deps [subcommand] [options]
```

Depending on the `dfx deps` subcommand you specify, additional arguments, options, and flags might apply. For reference information and examples that illustrate using `dfx deps` commands, select an appropriate command.

| Command                      | Description                                    |
| ---------------------------- | ---------------------------------------------- |
| [`pull`](#dfx-deps-pull)     | Pull canisters upon which the project depends. |
| [`init`](#dfx-deps-init)     | Set init arguments for pulled dependencies.    |
| [`deploy`](#dfx-deps-deploy) | Deploy pulled dependencies.                    |

To view usage information for a specific subcommand, specify the subcommand and the `--help` flag. For example, to see usage information for `dfx deps pull`, you can run the following command:

``` bash
dfx deps pull --help
```

## dfx deps pull

Use the `dfx deps pull` command to pull dependencies as defined in `dfx.json`.

### Basic usage

``` bash
dfx deps pull [options]
```

### Arguments

You can specify the following argument for the `dfx deps delete` command.

| Command   | Description                                                     |
| --------- | --------------------------------------------------------------- |
| `network` | Specify the network to pull dependencies from, default is "ic". |

### Examples

You can use the `dfx deps pull` command to pull the dependencies as defined in `dfx.json` from the mainnet. It will resolve all indirect dependencies.

``` bash
dfx deps pull
```

For testing, you may want to pull from local replica, then run:

```bash
dfx deps pull --network local
```

## dfx deps init

Use the `dfx deps init` command to set init arguments for pulled dependencies.

### Basic usage

``` bash
dfx deps init [options] [canister]
```

### Examples

You can use the `dfx deps init` command to set empty init arguments for all pulled dependencies.

``` bash
dfx deps init
```

If any of the dependencies require init arguments, the above command will alarm you with their canister ID and names if exist. Then you can specify canister ID or name to set init argument for individual dependency.

```bash
`dfx deps init <CANISTER_ID/NAME> --argument <ARGUMENT> [--argument-type <TYPE>]`
```

The command below set number `1` for canister `dep_a` as the argument type is the default `idl` (candid).

```bash
dfx deps init dep_a --argument 1
```

The command below set the hex-encoded raw bytes for canister `dep_b`.

```bash
dfx deps init dep_b --argument "4449444c00017103616263" --argument-type raw
```

The command below set the init argument for canister `dep_c` with the content of file `init_c.txt`.

```bash
dfx deps init dep_c --argument-file init_c.txt
```

## dfx deps deploy

Use the `dfx deps deploy` command to deploy all dependencies.

### Basic usage

``` bash
dfx deps deploy [flag]
```

### Examples

You can use the `dfx deps deploy` command to deploy dependencies on local replica.

``` bash
dfx deps deploy
```

If some of the dependencies haven't been pulled or set init arguments, the command will fail. And the error message will help you to fix it.


-----------------------

/docs/cli-reference/dfx-envars.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# Environment variables

<MarkdownChipRow labels={["Reference"]} />

You can configure certain properties for your SDK execution environment using environment variables.

This section lists the environment variables that are currently supported with examples of how to use them. In most cases, you can set environment variables for a session by executing an command in the terminal or by adding a line similar to the following to your `.profile` file:

```shell
export DFX_NETWORK=ic
```

## CANISTER_CANDID_PATH\_\{canister.name\}

Use environment variables with the `CANISTER_CANDID_PATH` prefix to reference the path to the Candid description file for the canisters that are listed as dependencies in the `dfx.json` file for your project.

For example, if you have a `whoami_frontend` canister that lists `whoami` under the `dependencies` key, you could use the `CANISTER_CANDID_PATH_whoami_frontend` environment variable to refer to the location of the `whoami.did` file, which for local development might be:

    $PROJECT_ROOT/.dfx/local/canisters/whoami/whoami.did

## CANISTER_ID\_\{canister.name\}

Use environment variables with the `CANISTER_ID` prefix to reference the canister identifier for each canister in the `dfx.json` file for your project. Hyphens are invalid in environment variables and are replaced by underscores.  Lowercase characters are replaced by uppercase characters.

For example, if you have a `linkedup` project that consists of the `linkedup` and `connect-d` canisters, you could use the `CANISTER_ID_LINKEDUP` and `CANISTER_ID_CONNECT_D` environment variables to refer to the canister identifiersâ€”for example `ryjl3-tyaaa-aaaaa-aaaba-cai` and `rrkah-fqaaa-aaaaa-aaaaq-cai`â€”created for your project.

## DFX_CONFIG_ROOT

Use the `DFX_CONFIG_ROOT` environment variable to specify a different location for storing the `.cache` and `.config` subdirectories for `dfx`.

By default, the `.cache` and `.config` directories are located in the home directory for your development environment. For example, on macOS the default location is in the `/Users/<YOUR-USER-NAME>` directory. Use the `DFX_CONFIG_ROOT` environment variable to specify a different location for these directories.

    DFX_CONFIG_ROOT=~/ic-root

## DFX_INSTALLATION_ROOT

Use the `DFX_INSTALLATION_ROOT` environment variable to specify a different location for the `dfx` binary if you are not using the default location for your operating system.

The `.cache/dfinity/uninstall.sh` script uses this environment variable to identify the root directory for your SDK installation.

## DFX_VERSION

Use the `DFX_VERSION` environment variable to identify a specific version of the SDK that you want to install.

    DFX_VERSION=0.10.0 sh -ci "$(curl -fsSL https://internetcomputer.org/install.sh)"

## DFX_MOC_PATH

Use the `DFX_MOC_PATH` environment variable to use a different version of the Motoko compiler than the one bundled with a given dfx version.

## DFX_WARNING

Use the `DFX_WARNING` environment variable to disable one or more warnings that dfx may display. The value is a comma-separated list of warning names, each prefixed with a "-" to disable. The following warning names are currently supported:

- `mainnet_plaintext_identity`: Disables the warning message that is displayed when you use an insecure identity on the Internet Computer mainnet.

```bash
export DFX_WARNING="-mainnet_plaintext_identity"
dfx deploy --network ic

# disable multiple warnings, though dfx 0.13.1 does not know about the mainnet_plaintext_identity warning and version_check no longer exists
export DFX_WARNING="-version_check,-mainnet_plaintext_identity"
DFX_VERSION=0.13.1 dfx deploy --network ic
```

## DFX_DISABLE_QUERY_VERIFICATION

Set this to a non-empty value to disable verification of replica-signed queries.

## DFX_REPLICA_PATH

Use the `DFX_REPLICA_PATH` environment variable to specify a file path to a local version of the replica. If this option is used, `canister_sandbox` and `sandbox_launcher` must be in the same directory with the desired replica version.

## DFX_IC_STARTER_PATH

Use the `DFX_IC_STARTER_PATH` environment variable to specify a file path to a local version of `ic-starter`. 


-----------------------

/docs/cli-reference/dfx-generate.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx generate

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx generate` command to generate canister type declarations for supported programming languages. Currently, `dfx generate` supports four languages: Motoko, Candid, JavaScript, and TypeScript.

You can use this command to generate type declarations for all canisters that are defined for a project in the projectâ€™s `dfx.json` configuration file or a specific canister.

Note that you can only run this command from within the project directory structure. For example, if your project name is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its subdirectories.

The `dfx generate` command looks for the configuration under the `declarations` section of a canister in the `dfx.json` configuration file.

## Basic usage

``` bash
dfx generate [canister_name]
```

## Arguments

You can specify the following arguments for the `dfx generate` command.

| Argument        | Description                                                                                                                                                                                                                                                                                                                                                        |
|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `canister_name` | Specifies the name of the canister for which to generate type declarations. The canister name must match at least one name that you have configured in the `canisters` section of the `dfx.json` configuration file for your project. If you donâ€™t specify this argument, `dfx generate` will generate type declarations for all canisters declared in `dfx.json`. |

## Configuration

The behavior of `dfx generate` is controlled by the `dfx.json` configuration file. Under `dfx.json` â†’ `canisters` â†’ `<canister_name>`, you can add a `declarations` section. In this section, you can specify the following fields:

| Field          | Description                                                                                                                                  |
|----------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| `output`       | Directory to place declarations for the canister. Default is `src/declarations/<canister_name>`.                                             |
| `bindings`     | List of languages to generate type declarations. Options are `"js", "ts", "did", "mo"`. Default is `["js", "ts", "did"]`.                    |
| `env_override` | String that will replace `process.env.CANISTER_ID_{canister_name_uppercase}` in the `src/dfx/assets/language_bindings/canister.js` template. |

Outputs from `dfx generate`:

| Language         | File                                     |
|------------------|------------------------------------------|
| `JavaScript(js)` | `index.js` and `<canister_name>.did.js`  |
| `TypeScript(ts)` | `index.d.ts` and `<canister_name>.did.ts`|
| `Candid(did)`    | `<canister_name>.did`                    |
| `Motoko(mo)`     | `<canister_name>.mo`                     |

## Examples

Note that the file name and path to the programs on your file system must match the information specified in the `dfx.json` configuration file.

In this example, the `hello_world` canister itself is written in Motoko. The `declarations` section specifies that type declarations for all four languages will be generated and stored at `src/declarations/`.

``` bash
dfx generate hello_world
```

Since there is only one canister in `dfx.json`, calling `dfx generate` without an argument will have the same effect as the above command. If there were multiple canisters defined in `dfx.json`, this would generate type declarations for all defined canisters.

``` bash
dfx generate
```


-----------------------

/docs/cli-reference/dfx-help.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx help

<MarkdownChipRow labels={["Reference"]} />

Use this command to view usage information for the `dfx` parent command or for any specified subcommand.

## Basic usage

``` bash
dfx help [subcommand]
```

## Arguments

You can specify any `dfx` subcommand as an argument to view usage information for that subcommand using the `dfx help` command.

| Argument     | Description                                                     |
|--------------|-----------------------------------------------------------------|
| `subcommand` | Specifies the subcommand usage information you want to display. |

## Examples

To display the usage information for `dfx`, run the following command:

``` bash
dfx help
```

To display the usage information for `dfx new`, run the following command:

``` bash
dfx help new
```


-----------------------

/docs/cli-reference/dfx-identity.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx identity

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx identity` command with subcommands and flags to manage the identities used to execute commands and communicate with the IC or the local canister execution environment. Creating multiple user identities enables you to test user-based access controls.

The basic syntax for running `dfx identity` commands is:

``` bash
dfx identity [subcommand] [flag]
```

Depending on the `dfx identity` subcommand you specify, additional arguments, options, and flags might apply or be required. To view usage information for a specific `dfx identity` subcommand, specify the subcommand and the `--help` flag. For example, to see usage information for `dfx identity new`, you can run the following command:

``` bash
dfx identity new --help
```

For reference information and examples that illustrate using `dfx identity` commands, select an appropriate command.

| Command                                         | Description                                                                                                               |
|-------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|
| [`deploy-wallet`](#dfx-identity-deploy-wallet) | Installs the wallet Wasm to the provided canister id.                                                                     |
| [`get-principal`](#dfx-identity-get-principal) | Shows the textual representation of the principal associated with the current identity.                                   |
| [`get-wallet`](#dfx-identity-get-wallet)       | Shows the canister identifier for the wallet associated with your current identity principal.                             |
| `help`                                          | Displays this usage message or the help of the given subcommand(s).                                                       |
| [`export`](#dfx-identity-export)               | Exports the PEM definition for an identity. |
| [`import`](#dfx-identity-import)               | Creates a new identity by importing a PEM file that contains the key information or security certificate for a principal. |
| [`list`](#dfx-identity-list)                   | Lists existing identities.                                                                                                |
| [`new`](#dfx-identity-new)                     | Creates a new identity.                                                                                                   |
| [`remove`](#dfx-identity-remove)               | Removes an existing identity.                                                                                             |
| [`rename`](#dfx-identity-rename)               | Renames an existing identity.                                                                                             |
| [`set-wallet`](#dfx-identity-set-wallet)       | Sets the wallet canister identifier to use for your current identity principal.                                           |
| [`use`](#dfx-identity-use)                     | Specifies the identity to use.                                                                                            |
| [`whoami`](#dfx-identity-whoami)               | Displays the name of the current identity user context.                                                                   |

## Creating a default identity

The first time you run the `dfx canister create` command to register an identifier, your public/private key pair credentials are used to create a `default` user identity. The credentials for the `default` user are migrated from `$HOME/.dfinity/identity/creds.pem` to `$HOME/.config/dfx/identity/default/identity.pem`.

You can then use `dfx identity new` to create new user identities and store credentials for those identities in `$HOME/.config/dfx/identity/<identity_name>/identity.pem` files. For example, you can create an identity named `ic_admin` by running the following command:

    dfx identity new ic_admin

This command adds a private key for the `ic_admin` user identity in the `~/.config/dfx/identity/ic_admin/identity.pem` file.

## dfx identity deploy-wallet

Use the `dfx identity deploy-wallet` command to turn a canister into a wallet canister by installing the wallet Wasm to it.

Note that you must be connected to the IC or the local canister execution environment to run this command. In addition, you must be a controller of the canister you want to deploy the wallet to.

### Basic usage

``` bash
dfx identity deploy-wallet [flag] <canister id>
```

### Arguments

You must specify the following argument for the `dfx identity deploy-wallet` command.

| Argument        | Description                                                    |
|-----------------|----------------------------------------------------------------|
| `<canister id>` | The ID of the canister where the wallet Wasm will be deployed. |

## dfx identity get-principal

Use the `dfx identity get-principal` command to display the textual representation of a principal associated with the current user identity context.

If you havenâ€™t created any user identities, you can use this command to display the principal for the `default` user. The textual representation of a principal can be useful for establishing and testing role-based authorization scenarios.

### Basic usage

``` bash
dfx identity get-principal [flag]
```

### Example

If you want to display the textual representation of a principal associated with a specific user identity context, you can run commands similar to the following:

``` bash
dfx identity use ic_admin
dfx identity get-principal
```

In this example, the first command sets the user context to use the `ic_admin` identity. The second command then returns the principal associated with the `ic_admin` identity.

## dfx identity get-wallet

Use the `dfx identity get-wallet` command to display the canister identifier for the wallet associated with your current identity principal.

Note that you must be connected to the IC or the local canister execution environment to run this command. In addition, you must be in a project directory to run the command. For example, if your project name is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its subdirectories to run the `dfx identity get-wallet` command.

### Basic usage

``` bash
dfx identity get-wallet [flag]
```

### Example

If you want to display the canister identifier for the wallet canister associated with your identity, you can run the following command:

``` bash
dfx identity get-wallet
```

To display the canister identifier for the wallet canister associated with your identity on a specific testnet, you might run a command similar to the following:

``` bash
dfx identity get-wallet --network=https://192.168.74.4
```

## dfx identity export

Use the `dfx identity export` command to export the PEM definition of an existing identity.  You can import this definition elsewhere using [`dfx identity import`](#dfx-identity-import).

### Basic usage

``` bash
dfx identity export identity-name
```

### Examples

The following will export a PEM file for an existing identity.  If you transferred this file to another computer, the
example below for `dfx identity import` would import it.

``` bash
dfx identity export alice >generated-id.pem
```

## dfx identity import

Use the `dfx identity import` command to create a user identity by importing the userâ€™s key information or security certificate from a PEM file.

*Password policy*: If an identity is imported using `--storage-mode password-protected`, the following requirements apply to the password:
- The password needs to be longer than 8 characters.

### Basic usage

``` bash
dfx identity import [options] identity-name pem_file-name
```

### Options

You can specify the following options for the `dfx identity import` command.

|Argument|Description|
|--------|-----------|
|`--storage-mode` | By default, PEM files are stored in the OS-provided keyring. If that is not available, they are encrypted with a password when writing them to disk. Plaintext PEM files are still available (e.g. for use in non-interactive situations like CI), but not recommended for use since they put the keys at risk. To force the use of one specific storage mode, use the `--storage-mode` flag with either `--storage-mode password-protected` or `--storage-mode plaintext`.|
|`--force` |If the identity already exists, remove and re-import it.|

### Examples

You can use the `dfx identity import` command to import a PEM file that contains the security certificate to use for an identity. For example, you can run the following command to import the `generated-id.pem` file to create the user identity `alice`:

``` bash
dfx identity import alice generated-id.pem
```

The command adds the `generated-id.pem` file to the `~/.config/dfx/identity/alice` directory.

## dfx identity list

Use the `dfx identity list` command to display the list of user identities available. When you run this command, the list displays an asterisk (\*) to indicate the currently active user context. You should note that identities are global. They are not confined to a specific project context. Therefore, you can use any identity listed by the `dfx identity list` command in any project.

### Basic usage

``` bash
dfx identity list [flag]
```

### Examples

You can use the `dfx identity list` command to list all of the identities you have currently available and to determine which identity is being used as the currently-active user context for running `dfx` commands. For example, you can run the following command to list the identities available:

``` bash
dfx identity list
```

This command displays the list of identities found similar to the following:

``` bash
alice_auth
anonymous
bob_standard *
default
ic_admin
```

In this example, the `bob_standard` identity is the currently-active user context. After you run this command to determine the active user, you know that any additional `dfx` commands you run are executed using the principal associated with the `bob_standard` identity.

## dfx identity new

Use the `dfx identity new` command to add new user identities. You should note that the identities you add are global. They are not confined to a specific project context. Therefore, you can use any identity you add using the `dfx identity new` command in any project.
Only the characters `ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.-_@0123456789` are valid in identity names.

*Password policy*: If an identity is created using `--storage-mode password-protected`, the following requirements apply to the password:
- The password needs to be longer than 8 characters.

### Basic usage

``` bash
dfx identity new [options] _identity-name_
```

### Arguments

You must specify the following argument for the `dfx identity new` command.

| Argument          | Description                                                                                                                             |
|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| `<identity_name>` | Specifies the name of the identity to create. This argument is required. Valid characters are letters, numbers, and these symbols: .-_@ |

### Options

You can specify the following options for the `+dfx identity new+` command.

|Argument|Description|
|--------|-----------|
|`--storage-mode` |By default, PEM files are stored in the OS-provided keyring. If that is not available, they are encrypted with a password when writing them to disk. Plaintext PEM files are still available (e.g. for use in non-interactive situations like CI), but not recommended for use since they put the keys at risk. To force the use of one specific storage mode, use the `--storage-mode` flag with either `--storage-mode password-protected` or `--storage-mode plaintext`.|
|`--force` |If the identity already exists, remove and re-import it.|
|`--hsm-key-id <hsm key id>` |A sequence of pairs of hex digits.|
|`--hsm-pkcs11-lib-path <hsm pkcs11 lib path>` |The file path to the opensc-pkcs11 library e.g. "/usr/local/lib/opensc-pkcs11.so"|

### Examples

You can then use `dfx identity new` to create new user identities and store credentials for those identities in `$HOME/.config/dfx/identity/<identity_name>/identity.pem` files. For example, you can create an identity named `ic_admin` by running the following command:

    dfx identity new ic_admin

This command adds a private key for the `ic_admin` user identity in the `~/.config/dfx/identity/ic_admin/identity.pem` file.

After adding the private key for the new identity, the command displays confirmation that the identity has been created:

    Creating identity: "ic_admin".
    Created identity: "ic_admin".

## dfx identity remove

Use the `dfx identity remove` command to remove an existing user identity. You should note that the identities you add are global. They are not confined to a specific project context. Therefore, any identity you remove using the `dfx identity remove` command will no longer be available in any project.

### Basic usage

``` bash
dfx identity remove [flag] _identity-name_
```

### Flags

You can use the following optional flags with the `dfx identity remove` command.

| Flag              | Description                   |
|-------------------|-------------------------------|
| `--drop-wallets`  | Required if the identity has wallets configured so that users do not accidentally lose access to wallets.   |

### Arguments

You must specify the following argument for the `dfx identity remove` command.

| Argument          | Description                                                              |
|-------------------|--------------------------------------------------------------------------|
| `<identity_name>` | Specifies the name of the identity to remove. This argument is required. |

### Examples

You can use the `dfx identity remove` command to remove any previously-created identity, including the `default` user identity. For example, if you have added named user identities and want to remove the `default` user identity, you can run the following command:

    dfx identity remove default

The command displays confirmation that the identity has been removed:

    Removing identity "default".
    Removed identity "default".

Although you can delete the `default` identity if you have created other identities to replace it, you must always have at least one identity available. If you attempt to remove the last remaining user context, the `dfx identity remove` command displays an error similar to the following:

    Identity error:
      Cannot delete the default identity

If you have an identity with one or more wallets configured, it will only be deleted if you call it with `--drop-wallets`. This is made so that users don't accidentally lose access to their cycles wallets. If you try to delete an identity with at least one wallet configured, it will display the attached wallets like this:

    This identity is connected to the following wallets:
        identity 'mainnet' on network 'ic' has wallet rwlgt-iiaaa-aaaaa-aaaaa-cai

## dfx identity rename

Use the `dfx identity rename` command to rename an existing user identity. You should note that the identities you add are global. They are not confined to a specific project context. Therefore, any identity you rename using the `dfx identity rename` command is available using the new name in any project.

### Basic usage

``` bash
dfx identity rename [flag] _from_identity-name_ _to_identity-name_
```

### Arguments

You must specify the following arguments for the `dfx identity rename` command.

| Argument               | Description                                                                               |
|------------------------|-------------------------------------------------------------------------------------------|
| `<from_identity_name>` | Specifies the current name of the identity you want to rename. This argument is required. |
| `<to_identity_name>`   | Specifies the new name of the identity you want to rename. This argument is required.     |

### Example

You can rename the `default` user or any identity you have previously created using the `dfx identity rename` command. For example, if you want to rename a `test_admin` identity that you previously created, you would specify the current identity name you want to change **from** and the new name you want to change **to** by running a command similar to the following:

    dfx identity rename test_admin devops

## dfx identity set-wallet

Use the `dfx identity set-wallet` command to specify the wallet canister identifier to use for your identity.

### Basic usage

``` bash
dfx identity set-wallet [flag] [--canister-name canister-name]
```

### Flags

You can use the following optional flags with the `dfx identity set-wallet` command.

| Flag              | Description                                                                                                                                                |
|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--force`           | Skips verification that the canister you specify is a valid wallet canister. This option is only useful if you are connecting to the IC running locally. |

### Example

If you use more than one principal for your identity, you might have access to more than one wallet canister identifier. You can use the `dfx identity set-wallet` command to specify the wallet canister identifier to use for a given identity.

For example, you might store the wallet canister identifier in an environment variable, then invoke the `dfx identity set-wallet` command to use that wallet canister for additional operations by running the following:

```shell
export WALLET_CANISTER_ID=$(dfx identity get-wallet)
dfx identity set-wallet --canister-name ${WALLET_CANISTER_ID} --network=https://192.168.74.4
```

## dfx identity use

Use the `dfx identity use` command to specify the user identity you want to active. You should note that the identities you have available to use are global. They are not confined to a specific project context. Therefore, you can use any identity you have previously created in any project.

The identity used by a command is:
- the identity specified in the command with `--identity <NAME>`, if defined
- else the identity specified by the environment variable `export DFX_IDENTITY=<NAME>`, if defined
- the identity specified by `dfx identity use <NAME>`.

### Basic usage

``` bash
dfx identity use [flag] _identity-name_
```

### Arguments

You must specify the following argument for the `dfx identity use` command.

| Argument          | Description                                                                                                    |
|-------------------|----------------------------------------------------------------------------------------------------------------|
| `<identity_name>` | Specifies the name of the identity you want to make active for subsequent commands. This argument is required. |

### Examples

If you want to run multiple commands with the same user identity context, you can run a command similar to the following:

    dfx identity use ops

After running this command, subsequent commands use the credentials and access controls associated with the `ops` user.

## dfx identity whoami

Use the `dfx identity whoami` command to display the name of the currently-active user identity context.

### Basic usage

``` bash
dfx identity whoami [flag]
```

### Example

If you want to display the name of the currently-active user identity, you can run the following command:

``` bash
dfx identity whoami
```

The command displays the name of the user identity. For example, you had previously run the command `dfx identity use bob_standard`, the command would display:

    bob_standard


-----------------------

/docs/cli-reference/dfx-info.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx info

<MarkdownChipRow labels={["Reference"]} />

## Basic usage

``` bash
dfx info [type] [flag]
```

## Information Types

These are the types of information that the `dfx info` command can display.

| Information        | Description                                                                                                        |
|--------------------|--------------------------------------------------------------------------------------------------------------------|
| candid-ui-url      | The URL of the Candid UI canister.                                                                                 |
| networks-json-path | Path to network definition file networks.json.                                                                     |
| replica-port       | The listening port of the replica.                                                                                 |
| replica-rev        | The revision of the bundled replica.                                                                               |
| security-policy    | Show the headers that gets applied to assets in .ic-assets.json5 if "security_policy" is "standard" or "hardened". |
| webserver-port     | The local webserver port.                                                                                          |

## Options

You can use the following options with the `dfx info` command.

| Option                | Description                                                                                            |
|-----------------------|--------------------------------------------------------------------------------------------------------|
| `--network <network>` | Overrides the environment to connect to. By default, the local canister execution environment is used. |

## Examples

You can display the webserver port by running the following command:

``` bash
$ dfx info webserver-port
4943
```

You can display the URL of Candid UI on mainnet by running the following command:

``` bash
$ dfx info candid-ui-url --network ic
https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io
```


-----------------------

/docs/cli-reference/dfx-killall.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx stop

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx killall` command to forcibly kill every DFX-related process.

## Basic usage

``` bash
dfx killall [flag]
```

## Examples

You can kill a stuck process that is not responding to `dfx stop` with this command:

```bash
dfx killall
```

This may impact IDE plugins, including from other versions of DFX. For ordinary usage `dfx stop` should be preferred.


-----------------------

/docs/cli-reference/dfx-ledger.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx ledger

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx ledger` command to interact with the ledger canister.

This command can be used to make ICP utility token transactions from one canister to another, or top up canisters with cycles from ICP.

The basic syntax for running `dfx ledger` commands is:

``` bash
dfx ledger [subcommand] [options]
```

Depending on the `dfx ledger` subcommand you specify, additional arguments, options, and flags might apply. For reference information and examples that illustrate using `dfx ledger` commands, select an appropriate command.

| Command                               | Description                                                                          |
|---------------------------------------|--------------------------------------------------------------------------------------|
| [`account-id`](#dfx-ledger-account-id)           | Prints the selected identityâ€™s Account Identifier.                                   |
| [`balance`](#dfx-ledger-balance)                 | Prints the account balance of the user.                                              |
| [`create-canister`](#dfx-ledger-create-canister) | Creates a canister from ICP.                                                         |
| [`fabricate-cycles`](#dfx-ledger-fabricate-cycles) | Local development only: Fabricate cycles out of thin air and deposit them into the specified canister(s) |
| `help`                                | Displays usage information message for a specified subcommand.                       |
| [`notify`](#dfx-ledger-notify)                   | Notifies the ledger when there is a send transaction to the cycles minting canister. |
| [`top-up`](#dfx-ledger-top-up)                   | Tops up a canister with cycles minted from ICP.                                      |
| [`transfer`](#dfx-ledger-transfer)               | Transfers ICP from the user to the destination Account Identifier.                   |

To view usage information for a specific subcommand, specify the subcommand and the `--help` flag. For example, to see usage information for `dfx ledger transfer`, you can run the following command:

`dfx ledger transfer --help`

## dfx ledger account-id

Use the `dfx ledger account-id` command to display the account identifier associated with the currently-active identity. Like the textual representation of your developer identity principal, the account identifier is derived from your private key and used to represent your identity in the ledger canister. The command can also be used to compute and display the account ids of other principals, canister aliases and subaccounts of your account (also subaccounts derived from principals).

### Basic usage

``` bash
dfx ledger account-id [flag]
```

### Flags

You can use the following optional flags with the `dfx ledger account-id` command.

| Flag                         | Description                                            |
|------------------------------|--------------------------------------------------------|
| `-h`, `--help`               | Displays usage information.                            |
| `-V`, `--version`            | Displays version information.                          |
| `--of-canister <ALIAS>`      | Alias or principal of the canister controlling the account  |
| `--of-principal <PRINCIPAL>` | Principal controlling the account                      |
| `--subaccount <SUBACCOUNT>`   | Subaccount identifier (64 character long hex string)   |
| `--subaccount-of-principal <PRINCIPAL>`   | Principal from which the subaccount identifier is derived.   |

### Examples

If you have created more than one identity, check the identity you are currently using by running the `dfx identity whoami` command or the `dfx identity get-principal` command. You can then check the account identifier for your currently-selected developer identity by running the following command:

``` bash
dfx ledger account-id
```

The following command computes the account identifier controlled by the Subnet Rental Canister and for the subaccount derived from Alice's principal:

``` bash
dfx ledger account-id --of-canister qvhpv-4qaaa-aaaaa-aaagq-cai --subaccount-from-principal fdsgv-62ihb-nbiqv-xgic5-iefsv-3cscz-tmbzv-63qd5-vh43v-dqfrt-pae
```

The commands display output similar to the following:

    03e3d86f29a069c6f2c5c48e01bc084e4ea18ad02b0eec8fccadf4487183c223

## dfx ledger balance

Use the `dfx ledger balance` command to print your account balance or that of another user.

### Basic usage

``` bash
dfx ledger balance [of] [flag] --network ic
```

### Arguments

You can specify the following argument for the `dfx ledger balance` command.

| Argument | Description                                                                                                                                                                 |
|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `<of>`   | Specify an Account Identifier to get the balance. If this command is not specified, the command returns the balance of ICP tokens for the currently-selected user identity. |

### Examples

You can use the `dfx ledger balance` command to check the balance of another user. For example, you can run the following command to see the ICP utlity tokens associated with a known Account Identifier:

``` bash
dfx ledger balance 03e3d86f29a069c6f2c5c48e01bc084e4ea18ad02b0eec8fccadf4487183c223 --network ic
```

This command displays an ICP amount similar to the following:

    2.49798000 ICP

## dfx ledger create-canister

Use the `dfx ledger create-canister` command to convert ICP tokens to cycles and to register a new canister identifier on the IC.

### Basic usage

``` bash
dfx ledger create-canister <controller> [options]  [flag] --network ic
```

### Arguments

You can specify the following argument for the `dfx ledger create-canister` command.

| Argument       | Description                                                                      |
|----------------|----------------------------------------------------------------------------------|
| `<controller>` | Specifies the principal identifier to set as the controller of the new canister. |

### Options

You can specify the following argument for the `dfx ledger create-canister` command.

| Option                | Description                                                                                                                                                                                                                                          |
|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--amount <amount>`           | Specify the number of ICP tokens to mint into cycles and deposit into destination canister. You can specify an amount as a number with up to eight (8) decimal places.                                                                               |
| `--e8s <e8s>`                 | Specify ICP token fractional unitsâ€”called e8sâ€”as a whole number, where one e8 is smallest partition of an ICP token. For example, 1.05000000 is 1 ICP and 5000000 e8s. You can use this option on its own or in conjunction with the `--icp` option. |
| `--fee <fee>`                 | Specify a transaction fee. The default is 10000 e8s.                                                                                                                                                                                                 |
| `--icp <icp>`                 | Specify ICP tokens as a whole number. You can use this option on its own or in conjunction with `--e8s`.                                                                                                                                             |
| `--max-fee <max-fee>`         | Specify a maximum transaction fee. The default is 10000 e8s.                                                                                                                                                                                         |
| `--subnet-type <subnet-type>` | Specify the optional subnet type to create the canister on. If no subnet type is provided, the canister will be created on a random default application subnet.                                                                                      |
| `--subnet <subnet-principal>` | Specify the optional subnet to create the canister on. If no subnet is provided, the canister will be created on a random default application subnet.                                                                                                |
| `--next-to <canister-principal>` | Create canisters on the same subnet as this canister. |
| `--created-at-time <timestamp>`| Specify the timestamp-nanoseconds for the `created_at_time` field on the ledger transfer request. Useful for controlling transaction-de-duplication. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-  |

### Examples

To create a new canister with cycles, transfer ICP tokens from your ledger account by running a command similar to the following:

``` bash
dfx ledger create-canister tsqwz-udeik-5migd-ehrev-pvoqv-szx2g-akh5s-fkyqc-zy6q7-snav6-uqe --amount 1.25 --network ic
```

This command converts the number of ICP tokens you specify for the `--amount` argument into cycles, and associates the cycles with a new canister identifier controlled by the principal you specify.

In this example, the command converts 1.25 ICP tokens into cycles and specifies the principal identifier for the default identity as the controller of the new canister.

If the transaction is successful, the ledger records the event and you should see output similar to the following:

    Transfer sent at BlockHeight: 20
    Canister created with id: "53zcu-tiaaa-aaaaa-qaaba-cai"

You can create a new canister by specifying separate values for ICP tokens and e8s by running a command similar to the following:

``` bash
dfx ledger create-canister tsqwz-udeik-5migd-ehrev-pvoqv-szx2g-akh5s-fkyqc-zy6q7-snav6-uqe --icp 3 --e8s 5000 --network ic
```

## dfx ledger fabricate-cycles

Use the `dfx ledger fabricate-cycles` add cycles to a canister while developing locally. The cycles are created out of thin air and are not deducted from anywhere.

### Basic usage

```
dfx ledger fabricate-cycles [options]
```

### Options

You can specify the following options for the `dfx ledger fabricate-cycles` command.
If no amount is specified, 10T cycles are used by default.


|Option |Description|
|-------|-----------|
|`--all` |Deposit cycles to all of the canisters configured in the dfx.json file. |
|`--amount <icp>` |ICP to mint into cycles and deposit into destination canister Can be specified as a Decimal with the fractional portion up to 8 decimal places i.e. 100.012 |
|`--canister <canister name/id>` |Specifies the name or id of the canister to receive the cycles deposit. You must specify either a canister name/id or the --all option. |
|`--cycles <cycles>` | Specifies the amount of cycles to fabricate. |
|`--e8s <e8s>` | Specify e8s as a whole number, helpful for use in conjunction with `--icp`|
|`--icp`|  Specify ICP as a whole number, helpful for use in conjunction with `--e8s`|
|`--t <trillion cycles>` |Specifies the amount of cycles to fabricate in trillion cycles. Only accepts whole numbers.|

### Examples

If you are developing locally and want to add 8T cycles to all your canisters in your procject, you can do so like this:

```
dfx ledger fabricate-cycles --all --cycles 8000000000000
```

The command displays output similar to the following:

```
Fabricating 8000000000000 cycles onto hello_backend
Fabricated 8000000000000 cycles, updated balance: 11_899_662_119_932 cycles
```

If you would rather only add the cycles to the canister called 'hello' and don't want to type all the zeros, you can do it like this:

```
dfx ledger fabricate-cycles --canister hello --t 8
```

The command displays output similar to the following:

```
Fabricating 8000000000000 cycles onto hello
Fabricated 8000000000000 cycles, updated balance: 11_899_662_119_932 cycles
```


## dfx ledger notify

Use the `dfx ledger notify` command to notify the ledger about a transaction sent to the cycles minting canister. This command should only be used if `dfx ledger create-canister`, `dfx ledger top-up`, or `dfx cycles convert` successfully sent a message to the ledger, and a transaction was recorded at some block height, but for some reason the subsequent notify failed.

### Basic usage

``` bash
dfx ledger notify [options] _block-height_ _destination-principal_
```

### Arguments

You can specify the following argument for the `dfx ledger notify` command.

| Argument                  | Description                                                                                                                                                                                                                                                                                                     |
|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `<block-height>`          | Specifies the block height at which the send transaction was recorded.                                                                                                                                                                                                                                          |
| `<destination-principal>` | Specifies the principal of the destination, either a canister identifier or the textual representation of a user principal. If the send transaction was for the `create-canister` command, specify the `controller` principal. If the send transaction was for the `top-up` command, specify the `canister ID`. |

### Examples

The following example illustrates sending a `notify` message to the ledger in response to a `_send+` transaction that was recorded at the block height `75948`.

``` bash
dfx ledger notify 75948 tsqwz-udeik-5migd-ehrev-pvoqv-szx2g-akh5s-fkyqc-zy6q7-snav6-uqe --network ic
```

## dfx ledger show-subnet-types

Use the `dfx ledger show-subnet-types` command to list the available subnet types that can be chosen to create a canister on.

### Basic usage

``` bash
dfx ledger show-subnet-types [options] [flag]
```

### Options

You can specify the following options for the `dfx ledger show-subnet-types` command.

| Option                | Description                                                                                                                                                                                                                                                 |
|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--cycles-minting-canister-id <cycles-minting-canister-id>`   | Canister id of the cycles minting canister. Useful if you want to test locally with a different id for the cycles minting canister. |

### Examples

You can use the `dfx ledger show-subnet-types` command to list the available subnet types that can be chosen to create a canister on. If a specific cycles minting canister id is not provided, then the mainnet cycles minting canister id will be used.

For example, you can run the following command to get the subnet types available on mainnet:

``` bash
dfx ledger show-subnet-types
```

This command displays output similar to the following:

    ["Type1", "Type2", ..., "TypeN"]

## dfx ledger top-up

Use the `dfx ledger top-up` command to top up a canister with cycles minted from ICP tokens.

### Basic usage

``` bash
dfx ledger top-up [options] canister [flag] --network ic
```

### Arguments

You can specify the following argument for the `dfx ledger top-up` command.

| Argument   | Description                                                              |
|------------|--------------------------------------------------------------------------|
| `canister` | Specifies the canister identifier or name that you would like to top up. |

### Options

You can specify the following options for the `dfx ledger top-up` command.

| Option                | Description                                                                                                                                                                                                                                                 |
|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--amount <amount>`   | Specifies the number of ICP tokens to mint into cycles and deposit into the destination canister. You can specify the amount as a number with up to eight (8) decimal places.                                                                               |
| `--e8s <e8s>`         | Specifies fractional units of an ICP tokenâ€”called e8sâ€”as a whole number, where one e8 is the smallest unit of an ICP token. For example, 1.05000000 is 1 ICP and 5000000 e8s. You can use this option on its own or in conjunction with the `--icp` option. |
| `--fee <fee>`         | Specifies the transaction fee for the operation. The default is 10000 e8s.                                                                                                                                                                                  |
| `--icp <icp>`         | Specifies ICP tokens as a whole number. You can use this option on its own or in conjunction with `--e8s`.                                                                                                                                                  |
| `--max-fee <max-fee>` | Specifies a maximum transaction fee. The default is 10000 e8s.                                                                                                                                                                                              |
| `--created-at-time <timestamp>`| Specify the timestamp-nanoseconds for the `created_at_time` field on the ledger transfer request. Useful for controlling transaction-de-duplication. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-  |

### Examples

You can use the `dfx ledger top-up` command to top up the cycles of a specific canister from the balance of ICP tokens you control. The canister identifier must be associated with a cycles wallet canister that is able to receive cycles. Alternatively, you can modify a non-cycles wallet canister to implement a method to receive cycles using system APIs described in the [Internet Computer Interface Specification](/docs/current/references/ic-interface-spec).

For example, you can run the following command to top-up a cycles wallet canister deployed on the Internet Computer with 1 ICP worth of cycles:

``` bash
dfx ledger top-up --icp 1 5a46r-jqaaa-aaaaa-qaadq-cai --network ic
```

This command displays output similar to the following:

    Transfer sent at BlockHeight: 59482
    Canister was topped up!

## dfx ledger transfer

Use the `dfx ledger transfer` command to transfer ICP tokens from your account address in the ledger canister to a destination address.

### Basic usage

``` bash
dfx ledger transfer [options] to --memo memo
```

### Arguments

You can specify the following argument for the `dfx ledger transfer` command.

| Argument            | Description                                                                         |
|---------------------|-------------------------------------------------------------------------------------|
| `<to>`              | Specify the Account Identifier or address to which you want to transfer ICP tokens. |
| `--memo <memo>`     | Specifies a numeric memo for this transaction.                                      |

### Options

You can specify the following argument for the `dfx ledger transfer` command.

| Option              | Description                                                                                                                                                                                                     |
|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--amount <amount>` | Specifies the number of ICP tokens to transfer. Can be specified as a number with up to eight (8) decimal places.                                                                                               |
| `--e8s <e8s>`       | Specifies e8s as a whole number, where one e8 is smallest partition of an ICP token. For example, 1.05000000 is 1 ICP and 5000000 e8s. You can use this option alone or in conjunction with the `--icp` option. |
| `--fee <fee>`       | Specifies a transaction fee. The default is 10000 e8s.                                                                                                                                                          |
| `--icp <icp>`       | Specifies ICP as a whole number. You can use this option alone or in conjunction with `--e8s`.                                                                                                                  |
| `--created-at-time <timestamp>`| Specify the timestamp-nanoseconds for the `created_at_time` field on the ledger transfer request. Useful for controlling transaction-de-duplication. https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-  |

### Examples

You can use the `dfx ledger transfer` command to send ICP to the Account Identifier of the destination.

For example, you can run the following command to check the account identifier associated with the principal you are currently using:

``` bash
dfx ledger account-id
```

This command displays output similar to the following:

    30e596fd6c5ff5ad7b7d70bbbda1187c833e646c6251464da7f82bc217bba397

You can check the balance of this account by running the following command:

``` bash
dfx ledger balance --network ic
```

This command displays output similar to the following:

    64.89580000 ICP

Use the `dfx ledger transfer` command to send some of your ICP balance to another known destination using the following command:

``` bash
dfx ledger transfer dd81336dbfef5c5870e84b48405c7b229c07ad999fdcacb85b9b9850bd60766f --memo 12345 --icp 1 --network ic
```

This command displays output similar to the following:

    Transfer sent at BlockHeight: 59513

You can then use the `dfx ledger balance --network ic` command to check that your account balance reflects the transaction you just made.


-----------------------

/docs/cli-reference/dfx-new.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx new

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx new` command to create a new project for the IC. This command creates a default project structure with template files that you can modify to suit your dapp. You must specify the name of the project to you want to create.

You can use the `--dry-run` option to preview the directories and files to be created without adding them to the file system.

If called without any arguments besides the project name, it will interactively prompt you for the values of `--type`, `--frontend`, and `--extras`.

## Basic usage

``` bash
dfx new _project_name_ [flag]
```

## Flags

You can use the following optional flags with the `dfx new` command:

| Flag                    | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|-------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--dry-run`             | Generates a preview of the directories and files to be created for a new project without adding them to the file system.                                                                                                                                                                                                                                                                                                                                                                                  |
| `--extras <EXTRAS>`     | Comma-separated list of additional features to add to the project template. `bitcoin` and `internet-identity` will insert the appropriate boilerplate into `dfx.json`, and `frontend-tests` adds a `vitest` skeleton to the frontend project.                                                                                                                                                                                                                                                             |
| `--frontend <FRONTEND>` | Installs the template frontend code for the default project canister. The default value for the flag is `vanilla` if `node.js` is currently installed on your local computer. If `node.js` is not currently installed, you can set this flag to attempt to install `node.js` and the template file when creating the project or you can set the flag to `none` to skip the installation of template frontend code entirely. Possible values: `svelte`, `react`, `vue`, `vanilla`, `plain-assets`, `none`. |
| `--no-frontend`         | Skips installing the frontend template code. This is the default behavior if `node.js` is currently not installed on your computer. Equivalent to `--frontend none`.                                                                                                                                                                                                                                                                                                                                      |
| `--type <TYPE>`         | Selects the template backend code for the default project canister. The default value for the flag is `motoko`. Possible values: `motoko`, `rust`, `azle`, `kybra`.                                                                                                                                                                                                                                                                                                                                       |

## Arguments

You can specify the following argument for the `dfx new` command.

| Argument       | Description                                                             |
|----------------|-------------------------------------------------------------------------|
| `project_name` | Specifies the name of the project to create. This argument is required. |
| `--type`       | Choose the canister type in the starter project, motoko and rust are available. The default is motoko. |

## Examples

You can use `dfx new` to create a new project named `my_social_network` by running the following command:

``` bash
dfx new my_social_network
```

The command creates a new project, including a default project directory structure under the new project name and a Git repository for your project.

If you want to preview the directories and files to be created without adding them to the file system, you can run the following command:

``` bash
dfx new my_social_network --dry-run
```


-----------------------

/docs/cli-reference/dfx-nns.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx nns

<MarkdownChipRow labels={["Reference"]} />

:::caution
The `dfx nns` command is no longer part of dfx, and has now been turned into the dfx extension. In order to obtain it, please run `dfx extension install nns`.
:::

Use the `dfx nns` subcommands to interact with the Network Nervous System.

The basic syntax for running `dfx nns` commands is:

``` bash
dfx nns [subcommand] [flag]
```

Depending on the `dfx nns` subcommand you specify, additional arguments, options, and flags might apply. For reference information and examples that illustrate using `dfx nns` commands, select an appropriate command.

| Command                             | Description                                                                   |
|-------------------------------------|-------------------------------------------------------------------------------|
| [`import`](#_dfx_nns_import)        | Adds the NNS canisters to the local dfx.json as remote canisters.             |
| [`install`](#_dfx_nns_install)      | Deploys NNS canisters to the local dfx server.                                 |
| `help`                              | Displays usage information message for a specified subcommand.                |

To view usage information for a specific subcommand, specify the subcommand and the `--help` flag. For example, to see usage information for `dfx nns install`, you can run the following command:

``` bash
$ dfx nns install --help
```


## dfx nns import

Use the `dfx nns import` command to add the NNS canisters to the local `dfx.json`.  It also downloads the did files and sets the canister IDs of the NNS cansiters so that you can make API calls to NNS canisters.

### Basic usage

``` bash
$ dfx nns import
```

### Flags

You can use the following optional flags with the `dfx nns import` command.

| Flag                | Description                                    |
|---------------------|------------------------------------------------|
| `--network-mapping` | Renames networks when installing canister IDs. |

### Examples

You can use the `dfx nns import` command to get did files and so query NNS canisters.

``` bash
$ dfx nns import
$ dfx canister call --network ic nns-governance get_pending_proposals '()'
```

You can rename a network on import.  For example, if you have `test-ic` set up as an alias of the `ic` network then you can set NNS canister IDs for `test-ic` with:

``` bash
$ dfx nns import --network-mapping test-ic=ic
```

## dfx nns install

Use the `dfx nns install` command to install a local NNS. This provides local ledger and governance canisters as well as the GUI canisters Internet Identity and NNS-Dapp.

### Basic usage
The local network needs to be set up with a very specific configuration:
```
$ cat ~/.config/dfx/networks.json
{
  "local": {
    "bind": "127.0.0.1:8080",
    "type": "ephemeral",
    "replica": {
      "subnet_type": "system"
    }
  }
}
```

This is because:

* The NNS canisters need to run on a system subnet.
* Some canisters are compiled to run on only very specific canister IDs and hostname/port pairs.


In addition, the local dfx server needs to be clean:

``` bash
$ dfx start --clean --background
$ dfx nns install
```

This is because NNS canisters need to be installed before any others.


### Examples

#### Example: Making API calls to the local NNS.

``` bash
$ dfx stop
$ dfx start --clean --background
$ dfx nns install
$ dfx nns import
$ dfx canister call --network ic nns-governance get_pending_proposals '()'
```

You can view the API calls that can be made for each NNS canister by looking at the interface definition files installed by `dfx nns import` in `candid/*.did`.  The API methods are in the `service` section, which is usually located at the end of a `.did` file.  It is easiest to start experimenting with methods that take no arguments.

#### Example: Accessing ICP on the command line
Two accounts in the local ledger is initialized with ICP that can be used for testing.  One uses a secp256k1 key, which is convenient for command line usage, another uses an ed25519 key, which is more convenient in web applications.



To use ICP on the command line:
* Start dfx and install the NNS, as described in [`install`](#_dfx_nns_install).
* Put this secret key into a file called `ident-1.pem`:
``` bash
$ cat <<EOF >ident-1.pem
-----BEGIN EC PRIVATE KEY-----
MHQCAQEEICJxApEbuZznKFpV+VKACRK30i6+7u5Z13/DOl18cIC+oAcGBSuBBAAK
oUQDQgAEPas6Iag4TUx+Uop+3NhE6s3FlayFtbwdhRVjvOar0kPTfE/N8N6btRnd
74ly5xXEBNSXiENyxhEuzOZrIWMCNQ==
-----END EC PRIVATE KEY-----
EOF
```
* Check the key: (optional)
```
$ openssl ec -in ident-1.pem -noout -text
```
* Create an identity with that secret key:
``` bash
$ dfx identity import ident-1 ident-1.pem
```
* Now you can use the (toy) funds:
``` bash
$ dfx ledger balance
```

To use ICP in an existing web application:
* Install the [@dfinity/agent npm module](https://www.npmjs.com/package/@dfinity/agent).
* Create an identity with this key pair:
```
  const publicKey = "Uu8wv55BKmk9ZErr6OIt5XR1kpEGXcOSOC1OYzrAwuk=";
  const privateKey =
    "N3HB8Hh2PrWqhWH2Qqgr1vbU9T3gb1zgdBD8ZOdlQnVS7zC/nkEqaT1kSuvo4i3ldHWSkQZdw5I4LU5jOsDC6Q==";
  const identity = Ed25519KeyIdentity.fromKeyPair(
    base64ToUInt8Array(publicKey),
    base64ToUInt8Array(privateKey)
  );

  // If using node:
  const base64ToUInt8Array = (base64String: string): Uint8Array => {
    return Buffer.from(base64String, 'base64')
  };
  // If in a browser:
  const base64ToUInt8Array = (base64String: string): Uint8Array => {
    return Uint8Array.from(window.atob(base64String), (c) => c.charCodeAt(0));
  };
```
* That identity can now make API calls, including sending ICP.


-----------------------

/docs/cli-reference/dfx-parent.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx

<MarkdownChipRow labels={["Reference"]} />

The DFINITY command-line execution environment (`dfx`) is the primary tool for creating, deploying, and managing the
dapps you develop for the IC.

Use the `dfx` parent command with flags and subcommands to specify the operations you want to perform with or without
optional arguments.

## Basic usage

``` bash
dfx [subcommand] [flag]
```

## Flags

You can use the following optional flags with the `dfx` parent command or with any of the `dfx` subcommands.

| Flag              | Description                                     |
|-------------------|-------------------------------------------------|
| `-h`, `--help`    | Displays usage information.                     |
| `-q`, `--quiet`   | Suppresses informational messages.              |
| `-v`, `--verbose` | Displays detailed information about operations. |
| `-V`, `--version` | Displays version information.                   |

## Options

You can use the following options with the `dfx` command.

| Option                  | Description                                                                                                                                                                                                                                                                                                                                    |
|-------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--identity <identity>` | Specifies the user identity to use when running a command.                                                                                                                                                                                                                                                                                     |
| `--logfile <logfile>`   | Writes log file messages to the specified log file name if you use the `--log file` logging option.                                                                                                                                                                                                                                            |
| `--log <logmode>`       | Specifies the logging mode to use. + You can set the log mode to one of the following:<br />- `stderr` to log messages to the standard error facility.<br />- `tee` to write messages to both standard output and to a specified file name.<br />- `file` to write messages to a specified file name.<br />The default logging mode is stderr. |

## Subcommands

Use the following subcommands to specify the operation you want to perform or to view usage information for a specific
command.

For reference information and examples, select an appropriate subcommand.

| Command                          | Description                                                                                                                                                                    |
|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| bootstrap                        | Removed.  Use the `start` command instead.                                                                                                                                     |
| [`build`](./dfx-build.mdx)       | Builds canister output from the source code in your project.                                                                                                                   |
| [`cache`](./dfx-cache.mdx)       | Manages the `dfx` cache on the local computer.                                                                                                                                 |
| [`canister`](./dfx-canister.mdx) | Manages deployed canisters .                                                                                                                                                   |
| [`deploy`](./dfx-deploy.mdx)     | Deploys all or a specific canister from the code in your project. By default, all canisters are deployed.                                                                      |
| diagnose                         | Detects known problems in the current environment caused by upgrading DFX, and suggests commands to fix them. These commands can be batch-run automatically via `dfx fix`      |
| fix                              | Applies one-time fixes for known problems in the current environment caused by upgrading DFX. Makes no changes that would not have been suggested by `dfx diagnose`            |
| generate                         | Generate type declarations for canisters from the code in your project                                                                                                         |
| [`help`](./dfx-help.mdx)         | Displays usage information for a specified subcommand.                                                                                                                         |
| [`identity`](./dfx-identity.mdx) | Enables you to create and manage the identities used to communicate with the IC.                                                                                               |
| info                             | Displays information like port numbers and version numbers.                                                                                                                    |
| [`killall`](./dfx-killall.mdx)   | Kills any dfx-related processes that have gotten stuck                                                                                                                         |
| [`ledger`](./dfx-ledger.mdx)     | Enables you to interact with accounts in the ledger canister running on the Internet Computer.                                                                                 |
| [`new`](./dfx-new.mdx)           | Creates a new project.                                                                                                                                                         |
| [`ping`](./dfx-ping.mdx)         | Sends a response request to the IC or the local canister execution environment to determine network connectivity. If the connection is successful, a status reply is returned. |
| quickstart                       | Perform initial one time setup for your identity and/or wallet.                                                                                                                |
| remote                           | Commands used to work with remote canisters.                                                                                                                                   |
| [`replica`](./dfx-replica.mdx)   | Removed.  Use the `start` command instead.                                                                                                                                     |
| [`schema`](./dfx-schema.mdx)     | Prints the schema for `dfx.json`.                                                                                                                                              |
| [`start`](./dfx-start.mdx)       | Starts the local canister execution environment a web server for the current project.                                                                                          |
| [`stop`](./dfx-stop.mdx)         | Stops the local canister execution environment.                                                                                                                                |
| [`upgrade`](./dfx-upgrade.mdx)   | Upgrades the version of `dfx` installed on the local computer to the latest version available.                                                                                 |
| [`wallet`](./dfx-wallet.mdx)     | Enables you to manage cycles, controllers, custodians, and addresses for the default cycles wallet associated with the currently-selected identity.                            |

## Examples

You can use the `dfx` parent command to display usage information or version information. For example, to display
information about the version of `dfx` you currently have installed, you can run the following command:

``` bash
dfx --version
```

To view usage information for a specific subcommand, specify the subcommand and the `--help` flag. For example, to see
usage information for `dfx build`, you can run the following command:

``` bash
dfx build --help
```

### Using logging options

You can use the `--verbose` and `--quiet` flags to increment or decrement the logging level. If you donâ€™t specify any
logging level, CRITICAL, ERROR, WARNING, and INFO messages are logged by default. Specifying one verbose flag (`-v`)
increases the log level to include DEBUG messages. Specifying two verbose flags (`-vv`)increases the logging level to
include both DEBUG and TRACE messages.

Adding a `--quiet` flag decreases the logging level. For example, to remove all messages, you can run a command similar
the following:

``` bash
dfx build -qqqq
```

Keep in mind that using TRACE level logging (`--vv`) generates a lot of log messages that can affect performance and
should only be used when required for troubleshooting or analysis.

To output log messages to a file named `newlog.txt` and display the messages on your terminal when creating a new
project, you can run a command similar to the following:

``` bash
dfx new hello_world --log tee --logfile newlog.txt
```

### Specifying a user identity

If you create user identities with the `dfx identity new` command, you can then use the `--identity` comment-line option
to change the user context when running other `dfx` commands.

In the most common use case, you use the `--identity` option to call specific canister functions to test access controls
for specific operations.

For example, you might want to test whether the `devops` user identity can call the `modify_profile` function for
the `accounts` canister by running the following command:

    dfx canister call accounts modify_profile '("Kris Smith")' --identity devops


-----------------------

/docs/cli-reference/dfx-ping.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx ping

<MarkdownChipRow labels={["Reference"]}/>

Use the `dfx ping` command to check connectivity to the IC or a testnet. This command enables you to verify that you can connect to the environment where you want to deploy to.

To ping your local server, please note that you can only run this command from within the project directory structure. For example, if your project name is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its subdirectories.

To ping mainnet, you can run `dfx ping ic` from any directory.

## Basic usage

``` bash
dfx ping [provider] [flag]
```

## Arguments

You can specify the following argument for the `dfx ping` command.

| Argument | Description                                                   |
|----------|---------------------------------------------------------------|
| provider | Specifies the IC or testnet URL that you want to use. |

## Examples

You can use the `dfx ping` command to check whether the IC is currently available at a specific network address by running a command similar to the following:

``` bash
dfx ping https://icp-api.io
```

If the IC is running on the specified network provider address, the command returns output similar to the following:

```json
{
  "ic_api_version": "0.8"
}
```

-----------------------

/docs/cli-reference/dfx-quickstart.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx quickstart

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx quickstart` command to perform initial one time setup for your identity and/or wallet. This command
can be run anytime to repeat the setup process or to be used as an informational command, printing
information about
your ICP balance, current ICP to XDR conversion rate, and more.

## Basic usage

``` bash
dfx quickstart
```

## Flags

You can use the following optional flags with the `dfx quickstart` command.

| Flag              | Description                                                                                                                            |
|-------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| `--identity`      | The user identity to run this command as. It contains your principal as well as some things DFX associates with it such as the wallet. |

## Options

You can specify the following options for the `dfx quickstart` command.

| Option                | Description                                                                                                             |
|-----------------------|-------------------------------------------------------------------------------------------------------------------------|
| `--log <LOGMODE>`     | The logging mode to use. You can log to stderr, a file, or both [default: stderr] [possible values: stderr, tee, file]. |
| `--logfile <LOGFILE>` | The file to log to, if logging to a file (see --logmode).                                                               |

## Examples

To run a guided setup, run `dfx quickstart`

``` bash
dfx quickstart
```
```shell
Your DFX user principal: xxxxx-xxxxx-xxxxx-xxxxx-xxxxx-xxxxx-xxxxx-xxxxx-xxxxx-xxxxx-xxx
Your ledger account address: xxxxx
Your ICP balance: 1234.567 ICP
Conversion rate: 1 ICP <> 3.7950 XDR
Import an existing wallet? no
Spend 2.63504611 ICP to create a new wallet with 10 TC? no
Run this command again at any time to continue from here.
```


-----------------------

/docs/cli-reference/dfx-replica.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx replica

<MarkdownChipRow labels={["Reference"]} />

**NOTE**: The replica command has been removed. Please use the [dfx start](./dfx-start.mdx) command instead.


-----------------------

/docs/cli-reference/dfx-schema.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx schema

<MarkdownChipRow labels={["Reference"]} />

The `dfx schema` command prints the schema for `dfx.json`.

## Basic usage

``` bash
dfx schema [option] [flag]
```

## Options

You can use the following option with the `dfx schema` command.

| Option                 | Description                                                                |
|------------------------|----------------------------------------------------------------------------|
| `--for <which>`        | Display schema for which JSON file. (default: dfx)                         |
| `--outfile <outfile>`  | Specifies a file to output the schema to instead of printing it to stdout. |

The parameter passed to `--for` can be one of the following:

| `<for>` option         | Corresponds To                                         |
|------------------------|--------------------------------------------------------|
| dfx                    | dfx.json                                               |
| networks               | networks.json                                          |
| dfx-metadata           | dfx-related metadata stored in canister and accessible with `dfx canister metadata <canister> dfx` |
| extension-manifest     | extension.json [example][example-extension-json]       |
| extension-dependencies | dependencies.json [example][example-dependencies-json] |

## Examples

You can print the schema for `dfx.json` by running the following command:

``` bash
dfx schema
```

You can print the schema for `networks.json` by running the following command:

``` bash
dfx schema --for networks
```

You can print the schema for the "dfx" metadata JSON (which contains fields such as "pullable", "tech_stack", etc) by running the following command:

``` bash
dfx schema --for dfx-metadata
```

If you want to write the schema for dfx.json to `path/to/file/schema.json`, you can do so by running the following command:

``` bash
dfx schema --outfile path/to/file/schema.json
```

[example-extension-json]: https://raw.githubusercontent.com/dfinity/dfx-extensions/main/extensions/nns/extension.json
[example-dependencies-json]: https://raw.githubusercontent.com/dfinity/dfx-extensions/main/extensions/nns/dependencies.json


-----------------------

/docs/cli-reference/dfx-sns.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx sns

<MarkdownChipRow labels={["Reference"]} />
:::caution
The `dfx sns` command is no longer part of dfx, and has now been turned into the dfx extension. In order to obtain it, please run `dfx extension install sns`.
:::

Use the `dfx sns` subcommands to simulate decentralizing a dapp.

The basic syntax for running `dfx sns` commands is:

``` bash
dfx sns [subcommand] [flag]
```

Depending on the `dfx sns` subcommand you specify, additional arguments, options, and flags might apply. For reference information and examples that illustrate using `dfx sns` commands, select an appropriate command.

| Command                             | Description                                                                   |
|-------------------------------------|-------------------------------------------------------------------------------|
| [`create`](#_dfx_sns_create)        | Creates an SNS configuration template.                                        |
| [`validate`](#_dfx_sns_validate)    | Checks whether the sns config file is valid.                                  |
| [`deploy`](#_dfx_sns_deploy)        | Deploys SNS canisters according to the local config.                          |
| `help`                              | Displays usage information message for a specified subcommand.                |

To view usage information for a specific subcommand, specify the subcommand and the `--help` flag. For example, to see usage information for `dfx sns validate`, you can run the following command:

``` bash
dfx sns validate --help
```


## dfx sns create

Use the `dfx sns create` command to create an SNS configuration file. The configuration file specifies important, legally and financially relevant details about dapp decentralization.  The file leaves blank parameters such as token name; you will need to fill these in.

### Basic usage

``` bash
dfx sns create
```

### Examples

You can use the `dfx sns create` command to create and view a configuration file:

``` bash
dfx sns create
less sns.yml
```

## dfx sns validate

Use the `dfx sns validate` command to verify that an SNS configuration file is well formed.

### Basic usage

``` bash
dfx sns validate
```

### Examples

You can use the `dfx sns validate` command to verify that a configuration template is valid.  It is not; it needs details such as token name:

``` bash
dfx sns config create
```
Fill in the blank fields, then:
``` bash
dfx sns config validate
```

## dfx sns deploy

Use the `dfx sns deploy` command to create SNS canisters according to the local configuration file.

Note:  Deploying SNS canisters does not require a proposal, however there is a hefty fee.  Please don't create canisters on mainnet until you have tested your configuration locally and are sure that you are happy with it.

### Basic usage

``` bash
dfx sns deploy
```

### Examples

Create an SNS on the local testnet:
``` bash
dfx sns config create
```
Fill in the blank fields, then:
``` bash
dfx sns config validate
dfx sns deploy
```
You can now verify that the sns canisters have been created.  E.g.:
```
dfx canister info sns_root
dfx canister info sns_ledger
```



-----------------------

/docs/cli-reference/dfx-start.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx start

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx start` command to start a local canister execution environment and web server processes. This command enables you to deploy canisters to the local canister execution environment to test your dapps during development.

By default, all local dfx projects will use this single local canister execution environment, and you can run `dfx start` and `dfx stop` from any directory.  See [Local Server Configuration](#local-server-configuration) and [Project-Specific Local Networks](#project-specific-local-networks) below for exceptions.

## Basic usage

``` bash
dfx start [option] [flag]
```

## Flags

You can use the following optional flags with the `dfx start` command.

| Flag                     | Description                                                                                                                                                                                                                                             |
|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--background`           | Starts the local canister execution environment and web server processes in the background and waits for a reply before returning to the shell.                                                                                                         |
| `--clean`                | Starts the local canister execution environment and web server processes in a clean state by removing checkpoints from your project cache. You can use this flag to set your project cache to a new state when troubleshooting or debugging.            |
| `--enable-bitcoin`       | Enables bitcoin integration.                                                                                                                                                                                                                            |
| `--enable-canister-http` | Enables canister HTTP requests. (deprecated: now enabled by default)                                                                                                                                                                                    |
| `--pocketic`             | Runs [PocketIC](https://github.com/dfinity/pocketic) instead of the replica.                                                                                                                                                                            |

## Options

You can use the following option with the `dfx start` command.

| Option                            | Description                                                                                                                                                                                                          |
|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `--host host`                     | Specifies the host interface IP address and port number to bind the frontend to. The default for the local shared network is `127.0.0.1:4943`, while the default for a project-specific network is '127.0.0.1:8000'. |
| `--bitcoin-node host:port`        | Specifies the address of a bitcoind node. Implies `--enable-bitcoin`.                                                                                                                                                |
| `--artificial-delay milliseconds` | Specifies the delay that an update call should incur. Default: 600ms                                                                                                                                                 |
| `--domain domain`                 | A domain that can be served. Can be specified more than once.  These are used for canister resolution [default: localhost]                                                                                           |

## Examples

You can start the local canister execution environment and web server processes in the current shell by running the following command:

``` bash
dfx start
```

If you start the local canister execution environment in the current shell, you need to open a new terminal shell to run additional commands. Alternatively, you can start the local canister execution environment in the background by running the following command:

``` bash
dfx start --background
```

If you run the local canister execution environment in the background, however, be sure to stop the local canister execution environment before uninstalling or reinstalling the `dfx` execution environment by running the following command:

``` bash
dfx stop
```

You can view the current process identifier (`pid`) for the local canister execution environment process started by `dfx` by running the following command:

``` bash
more .dfx/pid
```

## Local Server Configuration

### The Shared Local Network

By default, `dfx start` manages a single replica that is independent of any given local project.  Running `dfx deploy` and other commands will manage canisters on this single local network, in the same way that `dfx deploy --network ic` deploys separate projects to mainnet.

If run from outside any dfx project, or if dfx.json does not define the `local` network, then `dfx start` looks for the `local` network definition in `$HOME/.config/dfx/networks.json`. If this file does not exist or does not contain a definition for the `local` network, then dfx uses the following default definition:

```
{
  "local": {
    "bind": "127.0.0.1:4943",
    "type": "ephemeral"
  }
}
```

dfx stores data for the shared local network in one of the following locations, depending on your operating system:
- `$HOME/.local/share/dfx/network/local` (Linux)
- `$HOME/Library/Application Support/org.dfinity.dfx/network/local` (Macos)

### Project-Specific Local Networks

If dfx.json defines the `local` network, then `dfx start` will use this definition and store network data files under `<project dir>/.dfx/network/local`.

Note that for projects that define the `local` network in dfx.json, you can only run the `dfx start` and `dfx stop` commands from within the project directory structure. For example, if your project name is `hello_world`, your current working directory must be the `hello_world` top-level project directory or one of its subdirectories.


-----------------------

/docs/cli-reference/dfx-stop.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx stop

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx stop` command to stop the local canister execution environment processes that you currently have running on your computer. In most cases, you run the canister execution environment locally so that you can deploy canisters and test your dapps during development. To simulate the connection to the IC, these processes run continuously either in a terminal shell where you started them or the in the background until you stop or kill them.

You can run this command from any directory unless you are working with a dfx.json project that defines a project-specific local network.  See [Local Server Configuration](dfx-start.mdx#local-server-configuration) for details.

## Basic usage

``` bash
dfx stop [flag]
```

## Examples

You can stop the local canister execution environment processes that are running in the background by changing to a project directory then running the following command:

``` bash
dfx stop
```

If the local canister execution environment is running in a current shell rather than in the background, open a new terminal shell, change to a project directory, then run the `dfx stop` command.

The current process identifier (`pid`) for the canister execution environment process started by `dfx` is recorded in a file named `pid`. You can view the process identifier before running the `dfx stop` command by running one of the following commands:

For a project-specific local network:
``` bash
cat .dfx/network/local/pid
```

For the shared local network, on Linux:
``` bash
cat $HOME/.local/share/dfx/network/local/pid
```

For the shared local network, on MacOS:
``` bash
cat '$HOME/Library/Application Support/org.dfinity.dfx/network/local/pid'
```

This command displays a process identifier similar to the following:

``` bash
1896
```

If you are still having trouble with a persistent service running after attempting to stop, you can terminate all running jobs with:

``` bash
dfx killall
```


-----------------------

/docs/cli-reference/dfx-upgrade.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx upgrade

<MarkdownChipRow labels={["Reference"]} />

> **NOTE**: The upgrade command has been removed.  Please use the [dfx version manager][dfxvm] instead.

[dfxvm]: https://github.com/dfinity/dfxvm


-----------------------

/docs/cli-reference/dfx-wallet.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# dfx wallet

<MarkdownChipRow labels={["Reference"]} />

Use the `dfx wallet` command with subcommands and flags to manage the cycles wallets of your identities and to send cycles to the wallets of other account cycles wallet canisters.

The basic syntax for running the `dfx wallet` commands is:


```bash
dfx wallet [option] <subcommand> [flag]
```

Depending on the `dfx wallet` subcommand you specify, additional arguments, options, and flags might apply or be required.
To view usage information for a specific `dfx wallet` subcommand, specify the subcommand and the `--help` flag.
For example, to see usage information for `dfx wallet send`, you can run the following command:

```bash
dfx wallet send --help
```

For reference information and examples that illustrate using `dfx wallet` commands, select an appropriate command.


|Command |Description
---------|-----------------------------------
|[`add-controller`](#dfx-wallet-add-controller) | Add a controller using the selected identity's principal. |
|[`addresses`](#dfx-wallet-addresses)|Displays the address book of the cycles wallet.|
|[`authorize`](#dfx-wallet-authorize)|Authorize a custodian by principal for the selected identity's cycles wallet|
|[`balance`](#dfx-wallet-balance)|Displays the cycles wallet balance of the selected identity.
|[`controllers`](#dfx-wallet-controllers) |Displays a list of the selected identity's cycles wallet controllers. 
|[`custodians`](#dfx-wallet-custodians) |Displays a list of the selected identity's cycles wallet custodians.
|[`deauthorize`](#dfx-wallet-deauthorize) | Deauthorize a cycles wallet custodian using the custodian's principal.
|`help`|Displays a usage message and the help of the given subcommand(s).
|[`name`](#dfx-wallet-name) |Returns the name of the cycles wallet if you've used the `dfx wallet set-name` command.
|[`redeem-faucet-coupon`](#redeem-faucet-coupon) | Redeem a code at the cycles faucet. |
|[`remove-controller`](#dfx-wallet-remove-controller) |Removes a specified controller from the selected identity's cycles wallet. 
|[`send`](#dfx-wallet-send) |Sends a specified amount of cycles from the selected identity's cycles wallet to another canister.
|[`set-name`](#dfx-wallet-set-name) |Specify a name for your cycles wallet. 
|[`upgrade`](#dfx-wallet-upgrade) |Upgrade the cycles wallet's Wasm module to the current Wasm bundled with DFX.


## Using your wallet

After you have used the `dfx identity deploy-wallet` command to create a cycles wallet canister tied to an identity, you can use `dfx wallet` commands to modify your cycles wallet settings, send cycles to other cycles wallets, and add or remove controllers and custodians. 

## dfx wallet add-controller

Use the `dfx wallet add-controller` to add a controller to the wallet. An identity assigned the role of Controller has the most privileges and can perform the following actions on the selected identity's cycles wallet:

* Rename the cycles wallet.

* Add entries to the address book.

* Add and remove controllers.

* Authorize and deauthorize custodians.

A controller is also a custodian and can perform the following actions associated with that role:

* Access wallet information.

* Send cycles.

* Forward calls.

* Create canisters. 


### Basic usage


```
dfx wallet add-controller [option] <controller> [flag]
```

### Options

You can use the following options with the `dfx canister call` command.


|Option |Description|
--------|------------|
|`--network <network>` |Specifies the environment (e.g., Internet Computer or testnet) of the controller you want to add.

### Arguments

You can specify the following arguments for the `dfx wallet add-controller` command.


|Argument |Description
----------|-------------
|`controller` |Specifies the principal of the controller to add to the wallet. 

### Examples

You can use the `dfx wallet add-controller` command to add a controller to your wallet. If the controller you want to add is on a different environment, specify it using the `--network` option. For example:


```
dfx wallet add-controller b5quc-npdph-l6qp4-kur4u-oxljq-7uddl-vfdo6-x2uo5-6y4a6-4pt6v-7qe
```

The command displays output similar to the following:

```
Added b5quc-npdph-l6qp4-kur4u-oxljq-7uddl-vfdo6-x2uo5-6y4a6-4pt6v-7qe as a controller.
```

## dfx wallet addresses

Use the `dfx wallet addresses` command to display the wallet's address book.The address entries contain the principal and `role` (`Contact`, `Custodian`, or `Controller`), and might contain a `name`, and `kind` (`Unknown`, `User`, or `Canister`) associated with the address.

### Basic usage

```
dfx wallet addresses
```

### Examples

You can use the `dfx wallet addresses` command to retrieve information on the addresses in your wallet's address book. For example:

```
dfx wallet addresses --network ic
```

The command displays the controllers and custodians for the cycles wallet with output similar to the following:

```
dfx wallet addresses
Id: hpff-grjfd-tg7cj-hfeuj-olrjd-vbego-lpcax-ou5ld-oh7kr-kl9kt-yae, Kind: Unknown, Role: Controller, Name: ic_admin.
Id: e7ptl-4x43t-zxcvh-n6s6c-k2dre-doy7l-bbo6h-ok8ik-msiz3-eoxhl-6qe, Kind: Unknown, Role: Custodian, Name: alice_auth.
```

## dfx wallet authorize

Use the `dfx wallet authorize` command to authorize a custodian for the wallet. An identity authorized as a custodian can perform the following actions on the cycles wallet:

* Access wallet information.

* Send cycles.

* Forward calls.

* Create canisters. 

### Basic usage


```
dfx wallet authorize <custodian> [flag]
```

### Arguments

Use the following necessary argument with the `dfx wallet authorize` command.

|Argument |Description
----------|--------
|`<custodian>` | Specify the principal of the identity you would like to add as a custodian to the selected identity's cycles wallet.

### Example

For example, to add alice_auth as a custodian, specify her principal in the following command:

```
dfx wallet authorize dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae
```

This command outputs something similar to the following:

```
Authorized dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae as a custodian.
```

## dfx wallet balance

Use the `dfx wallet balance` command to display the balance of the cycles wallet of the selected identity. 

### Basic usage


```
dfx wallet balance
```

### Flags

You can use the following optional flags with the `dfx wallet balance` command.

|Flag |Description
-------|---------
|`--precise` |Displays the exact balance, without scaling to trillions of cycles.

### Examples

Check the balance of the selected identity's cycles wallet.

```
dfx wallet balance
```

This command displays the number of cycles in your cycles wallet. For example: 

```
89.000 TC (trillion cycles).
```

## dfx wallet controllers

Use the `dfx wallet controllers` command to list the principals of the identities that are controllers of the selected identity's cycles wallet. 

### Basic usage


```
dfx wallet controllers
```

### Examples

List the controllers of your selected identity's cycles wallet. 


```
dfx wallet controllers
```

The information returned should look similar to the following if there are two controllers:

```
dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae
hpnmi-qgxsv-tgecj-hmjyn-gmfft-vbego-lpcax-ou4ld-oh7kr-l3nu2-yae
```

## dfx wallet custodians

Use the `dfx wallet custodians` command to list the principals of the identities that are custodians of the selected identity's cycles wallet. Identities that are added as controllers are also listed as custodians.

### Basic usage


```
dfx wallet custodians
```

### Examples

List the custodians of your selected identity's cycles wallet. 


```
dfx wallet custodians
```

The information returned should look similar to the following if there are two custodians:

```
dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae
hpnmi-qgxsv-tgecj-hmjyn-gmfft-vbego-lpcax-ou4ld-oh7kr-l3nu2-yae
```


## dfx wallet deauthorize

Use the `dfx wallet deauthorize` command to remove a custodian from the cycles wallet. 

NOTE:  that this will also remove the role of controller if the custodian is also a controller.

### Basic usage


```
dfx wallet deauthorize <custodian> [flag]
```

### Arguments

Use the following necessary argument with the `dfx wallet deauthorize` command.


|Argument |Description
----------|--------------
|`<custodian>` | Specify the principal of the custodian you want to remove.

### Example

For example, to remove "alice_auth" as a custodian, specify her principal in the following command:


```
dfx wallet deauthorize dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae
```

This command will output something similar to:

```
Deauthorized dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae as a custodian.
```

## dfx wallet name

Use the `dfx wallet name` command to display the name of the selected identity's cycles wallet if it has been set using the `dfx wallet set-name` command. 

### Basic usage


```
dfx wallet name [flag] 
```

### Example

If you have named your cycles wallet "Terrances_wallet", then the command would return the following:

```
Terrances_wallet
```

## dfx wallet redeem-faucet-coupon

Use the `dfx wallet redeem-faucet-coupon` command to redeem a cycles faucet coupon.
If you have no wallet set, this will create a wallet for you.
If you have a wallet set already, this will add the coupon's cycles to your existing wallet.

### Basic usage
```
dfx wallet redeem-faucet-coupon <your faucet coupon>
```

### Arguments

Use the following necessary argument with the `dfx wallet redeem-faucet-coupon` command.


|Argument |Description
----------|--------------
|`<your faucet coupon>` | The coupon code to redeem at the faucet.|


### Flags

You can use the following optional flags with the `dfx wallet redeem-faucet-coupon` command.


|Flag |Description|
|-----|-----------|
|`--faucet`|Alternative faucet address. If not set, this uses the DFINITY faucet.|
|`--yes`|Skips yes/no checks by answering 'yes'. Not recommended outside of CI.|

### Example

If you have no wallet yet and a coupon code `ABCDE-ABCDE-ABCDE`, you can redeem it like this:
``` bash
dfx wallet redeem-faucet-coupon 'ABCDE-ABCDE-ABCDE'
```

This will print something similar to this:
```
Redeemed coupon ABCDE-ABCDE-ABCDE for a new wallet: rdmx6-jaaaa-aaaaa-aaadq-cai
New wallet set.
```

If you have a wallet already and a coupon code `ABCDE-ABCDE-ABCDE`, you can redeem it like this:
``` bash
dfx wallet redeem-faucet-coupon 'ABCDE-ABCDE-ABCDE'
```

This will print something similar to this:
```
Redeemed coupon code ABCDE-ABCDE-ABCDE for 20.000 TC (trillion cycles).
```

## dfx wallet remove-controller

Use the `dfx wallet remove-controller` command to remove a controller of your selected identity's cycles wallet.

### Basic usage


```
dfx wallet remove-controller <controller> [flag]
```

### Arguments

Use the following necessary argument with the `dfx wallet remove-controller` command.

|Argument |Description

|`<controller>` | Specify the principal of the controller you want to remove.


### Example

For example, to remove alice_auth as a controller, specify her principal in the following command:


```
dfx wallet remove-controller dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae
```
The command outputs something similar to the following:
```
Removed dheus-mqf6t-xafkj-d3tuo-gh4ng-7t2kn-7ikxy-vvwad-dfpgu-em25m-2ae as a controller.
```

## dfx wallet send

Use the `dfx wallet send` command to send cycles from the selected identity's cycles wallet to canister.

### Basic usage


```
dfx wallet [network] send [flag] <destination> <amount> 
```

### Options

You can use the following option with the `dfx wallet send` command.

|Option |Description
-----------|----------
|`--network` |Override the environment to connect to. By default, the local canister execution environment is used. A valid URL (starting with `http:` or `https:`) can be specified here. E.g. "http://localhost:12345/" is a valid network name.

### Arguments

You must specify the following arguments for the `dfx wallet send` command.

|Argument |Description
-----------|----------
|`<destination>` |Specify the destination canister using its Canister ID.
|`<amount>` |Specify the number of cycles to send.

### Examples

Send cycles from the selected identity's cycles wallet to another cycles wallet.

For example, to send 2,000,000,000 cycles from the cycles wallet of the selected identity, `<ic_admin>`, to the cycles wallet of the destination identity, `<buffy_standard>` with a wallet address `r7inp-6aaaa-aaaaa-aaabq-cai`, run the following command:


```
dfx wallet send r7inp-6aaaa-aaaaa-aaabq-cai 2000000000
```

If the transfer is successful, the command does not displays any output.

## dfx wallet set-name

Use the `dfx wallet set-name` command to assign a name to the selected identity's cycles wallet.

### Basic usage

```
    dfx wallet set-name [flag] <name> 
```

### Arguments

You must specify the following arguments for the `dfx wallet set-name` command.

|Argument |Description
--------|------------
|`<name>` |Specify a name for the cycles wallet.


### Example

If you want to set the name of the current identity's cycles wallet to "Terrances_wallet" you can run the following command:


```
dfx wallet set-name Terrances_wallet
```

## dfx wallet upgrade

Use the `dfx wallet upgrade` command to upgrade the cycle wallet's Wasm module to the current Wasm bundled with DFX.

### Basic usage


```
    dfx wallet upgrade [flag] 
```

### Example
To upgrade the Wasm module to the latest version, run the following command:

```
dfx wallet upgrade
```


-----------------------

/docs/cli-reference/index.mdx:
-----------------------

import { MarkdownChipRow } from "/src/components/Chip/MarkdownChipRow";

# Command-line reference

<MarkdownChipRow labels={["Reference"]} />

The DFINITY command-line execution environment (`dfx`) is the primary tool for creating, deploying, and managing the dapps for the Internet Computer platform.

You can use the `dfx` parent command with different flags and subcommands to perform different types of operations.

The basic syntax for running `dfx` commands is:

``` bash
dfx [subcommand] [flag]
```

Depending on the subcommand, the options and flags you specify might apply to the parent command or to a specific subcommand. For example, the flags for enabling or suppressing verbose logging are specified for the `dfx` parent command, then applied to any subcommands.

When you have the SDK installed, you can use the following commands to specify the operation you want to perform. For reference information and examples that illustrate using these commands, select an appropriate command.

-   [dfx](./dfx-parent.mdx)

-   [dfx bootstrap](./dfx-bootstrap.mdx)

-   [dfx build](./dfx-build.mdx)

-   [dfx cache](./dfx-cache.mdx)

-   [dfx canister](./dfx-canister.mdx)

-   [dfx completion](./dfx-completion.mdx)

-   [dfx cycles](./dfx-cycles.mdx)

-   [dfx deploy](./dfx-deploy.mdx)

-   [dfx deps](./dfx-deps.mdx)

-   [dfx help](./dfx-help.mdx)

-   [dfx identity](./dfx-identity.mdx)

-   [dfx info](./dfx-info.mdx)

-   [dfx ledger](./dfx-ledger.mdx)

-   [dfx new](./dfx-new.mdx)

-   [dfx nns](./dfx-nns.mdx)

-   [dfx ping](./dfx-ping.mdx)

-   [dfx quickstart](./dfx-quickstart.mdx)

-   [dfx replica](./dfx-replica.mdx)

-   [dfx schema](./dfx-schema.mdx)

-   [dfx sns](./dfx-sns.mdx)

-   [dfx start](./dfx-start.mdx)

-   [dfx stop](./dfx-stop.mdx)

-   [dfx upgrade](./dfx-upgrade.mdx)

-   [dfx wallet](./dfx-wallet.mdx)


-----------------------

/docs/concepts/canister-metadata.md:
-----------------------

# Canister Metadata

## Overview

Canisters can store custom metadata, which is available from the state tree at `/canister/<canister id>/metadata/<name>`.

You can configure this metadata in dfx.json, per canister, in the `metadata` array.

Here is a simple example:

```json
{
  "canisters": {
    "app_backend": {
      "main": "src/app_backend/main.mo",
      "type": "motoko"
    },
    "app_frontend": {
      "dependencies": [
        "app_backend"
      ],
      "frontend": {
        "entrypoint": "src/app_frontend/src/index.html"
      },
      "source": [
        "src/app_frontend/assets",
        "dist/app_frontend/"
      ],
      "type": "assets",
      "metadata": [
        {
          "name": "alternative-domains",
          "visibility": "public",
          "path": "src/app_frontend/metadata/alternative-domains.cbor"
        }
      ]
    }
  },
  "version": 1
}
```
## Fields

The JSON schema also documents these fields.

### name

A string containing the name of the wasm section.

### visibility

A string containing either `private` or `public` (the default).

Anyone can read the public metadata of a canister.

Only a controller of the canister can read its private metadata.

It is not possible to define metadata with the same name with both `private` and `public` visibility, unless they are for different networks.

### networks

An array of strings containing the names of the networks that this metadata applies to.

If this field is absent, it applies to all networks.

If this field is present as an empty array, it does not apply to any networks.

If dfx.json contains more than one metadata entry with a given name, dfx will use the first entry that matches the current network and ignore any that follow.

### path

A string containing the path of a file containing the wasm section contents. Conflicts with "content".

### content

A string containing the wasm section content directly. Conflicts with "path".

## Canister Metadata Standard

Contents should be valid UTF-8 text.

### `candid:args`

Parsed with `candid:service` to get the init arguments type of the canister.

### `candid:service`

The candid interface of the canister which excludes the init arguments type.

Dfx automatically adds `candid:service` metadata, with public visibility, for Rust and Motoko canisters.

You can, however, override this behavior by defining a metadata entry with `"name": "candid:service"`.  You can change the visibility or the contents.

For Motoko canisters, if you specify a `path` for candid:service metadata (replacing the candid:service definition generated by `moc`), dfx will verify that the candid:service definition you provide is a valid subtype of the definition that `moc` generated.

### `dfx`

A json text for `dfx` usage. It consists of independent optional objects.

#### `pullable`

The `pullable` object is necessary for the canister to be pullable.

Check [pull-dependencies](pull-dependencies.md#service-provider-workflow) for more details of each field
and how to set it in `dfx.json`.

```json
{
  "pullable": {
    "wasm_url": "http://example.com/a.wasm",
    "wasm_hash": "d180f1e232bafcee7d4879d8a2260ee7bcf9a20c241468d0e9cf4aa15ef8f312",
    "dependencies": [
      "yofga-2qaaa-aaaaa-aabsq-cai"
    ],
    "init_guide": "A natural number, e.g. 10."
  }
}
```

#### `tech_stack`

The `tech_stack` object provides a standard format to show the technologies involved to build the canister.

Check [tech-stack](tech-stack.md) for more details and how to set it in `dfx.json`.

```json
{
  "tech_stack": {
    "language": {
      "rust": {
        "version": "1.75.0"
      }
    },
    "cdk": {
      "ic-cdk": {
        "version": "0.13.0"
      }
    },
    "lib": {
      "ic-cdk-timers": {},
      "ic-stable-structures": {}
    },
    "other": {
      "bitcoin": {
        "address": "bcrt1qfe264m0ycx2vcqvqyhs0gpxk6tw8ug6hqeps2d"
      }
    },
    "tool": {
      "dfx": {}
    }
  }
}
```

## A more complex example

In this example, we change the visibility of the `candid:service` metadata on the ic and staging networks to private, but leave it public for the local network.

```json
{
  "canisters": {
    "app_backend": {
      "main": "src/app_backend/main.mo",
      "type": "motoko",
      "dependencies": [
        "dep1",
        "dep2"
      ],
      "metadata": [
        {
          "name": "candid:service",
          "networks": [ "ic", "staging" ],
          "visibility": "private"
        },
        {
          "name": "candid:service",
          "networks": [ "local" ],
          "visibility": "public"
        }
      ]
    },
    "app_frontend": {
      "dependencies": [
        "app_backend"
      ],
      "frontend": {
        "entrypoint": "src/app_frontend/src/index.html"
      },
      "source": [
        "src/app_frontend/assets",
        "dist/app_frontend/"
      ],
      "type": "assets"
    },
    "dep1": {
      "type": "pull",
      "id": "rrkah-fqaaa-aaaaa-aaaaq-cai"
    },
    "dep2": {
      "type": "pull",
      "id": "ryjl3-tyaaa-aaaaa-aaaba-cai"
    }
  },
  "version": 1
}
```


-----------------------

/docs/concepts/extension-defined-canister-types.md:
-----------------------

# Extension-Defined Canister Types

## Overview

An extension can define a canister type.

# Specification

The `canister_type` field in an extension's `extension.json` defines the
characteristics of the canister type.  It has the following fields:

| Field | Type | Description                                      |
|-------|------|--------------------------------------------------|
| `defaults` | Object | Default values for canister fields.       |
| `evaluation_order` | Array | Default fields to evaluate first.  |

The `canister_type.defaults` field is an object that defines canister properties,
as if they were found in dfx.json.  Any fields present in dfx.json
override those found in the extension-defined canister type.

The `metadata` and `tech_stack` fields have special handling.

All elements defined in the `metadata` array in the canister type are appended
to the `metadata` array found in dfx.json. This has the effect that any
metadata specified in dfx.json will take precedence over those that the
extension defines.

If the `tech_stack` field is present in both extension.json and dfx.json,
then dfx merges the two together.  Individual items found in dfx.json
will take precedence over those found in extension.json.

## Handlebar replacement

dfx will perform [handlebars] string replacement on every string field in the
canister type definition. The following data are available for replacement:

| Handlebar              | Description                                                                                       |
|------------------------|---------------------------------------------------------------------------------------------------|
| `{{canister_name}}`    | The name of the canister.                                                                         |
| `{{canister.<field>}}` | Any field from the canister definition in dfx.json, or `canister_type.defaults` in extension.json |

# Examples

Suppose a fictional extension called `addyr` defined a canister type in its
extension.json as follows:
```json
{
  "name": "addyr",
  "canister_type": {
    "defaults": {
      "build": "python -m addyr {{canister_name}} {{canister.main}} {{canister.candid}}",
      "gzip": true,
      "post_install": ".addyr/{{canister_name}}/post_install.sh",
      "wasm": ".addyr/{{canister_name}}/{{canister_name}}.wasm"
    }
  }
}
```

And dfx.json contained this canister definition:
```json
{
  "canisters": {
    "backend": {
      "type": "addyr",
      "candid": "src/hello_backend/hello_backend.did",
      "main": "src/hello_backend/src/main.py"
    }
  }
}
```
This would be treated as if dfx.json defined the following custom canister:
```json
{
  "canisters": {
    "hello_backend": {
      "build": "python -m addyr hello_backend src/hello_backend/src/main.py src/hello_backend/hello_backend.did",
      "candid": "src/hello_backend/hello_backend.did",
      "gzip": true,
      "post_install": ".addyr/hello_backend/post_install.sh",
      "type": "custom",
      "wasm": ".addyr/hello_backend/hello_backend.wasm"
    }
  }
}
```

[handlebars]: https://handlebarsjs.com/


-----------------------

/docs/concepts/index.md:
-----------------------

# DFX Concepts

- [Asset Canister Interface](../design/asset-canister-interface.md)
- [Canister metadata](./canister-metadata.md)


-----------------------

/docs/concepts/pull-dependencies.md:
-----------------------

# Pull Dependencies

## Overview

The interoperability of canisters on the Internet Computer (IC) is an important feature. 

`dfx` provides a consistent developer workflow for integrating third-party canisters.

A service provider prepares the canister to be `pullable` and deploys it on the IC mainnet.

A service consumer then can pull dependencies directly from mainnet and easily deploy them on a local replica.

This document describes the workflow and explains what happens behind the scenes.

## Service Provider Workflow

Below is an example provider `dfx.json` which has a `pullable` "service" canister:

```json
{
    "canisters": {
        "service": {
            "type": "motoko",
            "main": "src/main.mo",
            "pullable": {
                "wasm_url": "http://example.com/a.wasm",
                "wasm_hash": "d180f1e232bafcee7d4879d8a2260ee7bcf9a20c241468d0e9cf4aa15ef8f312",
                "dependencies": [
                    "yofga-2qaaa-aaaaa-aabsq-cai"
                ],
                "init_guide": "A natural number, e.g. 10."
            }
        }
    }
}
```

The `pullable` object will be serialized as a part of the [`dfx` metadata](canister-metadata.md#dfx) and attached to the wasm.

Let's go through the properties of the `pullable` object.

### `wasm_url`

A URL to download canister wasm module which will be deployed locally.

### `wasm_hash` 

SHA256 hash of the wasm module located at `wasm_url`.

This field is optional.

In most cases, the wasm module at `wasm_url` will be the same as the on-chain wasm module. This means that dfx can read the state tree to obtain and verify the module hash.

In other cases, the wasm module at `wasm_url` is not the same as the on-chain wasm module. For example, the Internet Identity canister provides Development flavor to be integrated locally. In these cases, `wasm_hash` provides the expected hash, and dfx verifies the downloaded wasm against this.

### `wasm_hash_url` 

A URL to get the SHA256 hash of the wasm module located at `wasm_url`.

The content of this URL can be the SHA256 hash only.

It can also be the output of `shasum` or `sha256sum` which contains the hash and the file name.

This field is optional.

Aside from specifying SHA256 hash of the wasm module directly using `wasm_hash`, providers can also specify the hash with this URL. If both are defined, the `wasm_hash_url` field will be ignored.

### `dependencies`

An array of Canister IDs (`Principal`) of direct dependencies.

### `init_guide`

A message to guide consumers how to initialize the canister.

### `init_arg`

A default initialization argument for the canister that consumers can use.

This field is optional.

## Fetch the metadata

To retrieve the metadata and validate the generated information, execute the following command:

```sh
> dfx canister metadata <canister_name> dfx
```

Please note that the `"pullable"` object is part of the public metadata under the key [`"dfx"`](canister-metadata.md#dfx).

Given that the content is in JSON format, you can utilize tools such as `jq` to manipulate the output and extract the pertinent information.

```sh
> dfx canister metadata <canister_name> dfx | jq -r ".pullable.wasm_url"
http://example.com/a.wasm
```

## Canister Metadata Requirements

The "production" canister running on the mainnet should have public `dfx` metadata.

The canister wasm downloaded from `wasm_url` should have the following [metadata](canister-metadata.md#canister-metadata-standard) (public or private):

- `candid:service`
- `candid:args`
- `dfx`

All metadata sections are handled by `dfx` during canister building.

## Service Consumer Workflow

### 1. Declare "pull" dependencies in `dfx.json`

Below is an example `dfx.json` in which the service consumer is developing the "app" canister which has two pull dependencies:

- "dep_b" which has canister ID of "yhgn4-myaaa-aaaaa-aabta-cai" on mainnet.
- "dep_c" which has canister ID of "yahli-baaaa-aaaaa-aabtq-cai" on mainnet.

```json
{
    "canisters": {
        "app": {
            "type": "motoko",
            "main": "src/main.mo",
            "dependencies": [
                "dep_b", "dep_c"
            ]
        },
        "dep_b": {
            "type": "pull",
            "id": "yhgn4-myaaa-aaaaa-aabta-cai"
        },
        "dep_c": {
            "type": "pull",
            "id": "yahli-baaaa-aaaaa-aabtq-cai"
        }
    }
}
```

### 2. Pull the dependencies using `dfx deps pull`

Running `dfx deps pull` will:

1. resolve the dependency graph by fetching `dependencies` field in `dfx` metadata recursively;
2. fetch the expected hash from `wasm_hash`,  `wasm_hash_url` in `dfx` metadata or canister status call;
3. download wasm of all direct and indirect dependencies from their `wasm_url` into shared cache, skip if cached wasm has match hash;
4. extract `candid:args`, `candid:service`, `dfx` metadata from the downloaded wasm;
5. create `deps/` folder in project root;
6. save `candid:service` of direct dependencies as `deps/candid/<CANISTER_ID>.did`;
7. save `deps/pulled.json` which contains major info of all direct and indirect dependencies;

For the example project, you will find following files in `deps/`:

- `candid/yhgn4-myaaa-aaaaa-aabta-cai.did` and `candid/yahli-baaaa-aaaaa-aabtq-cai.did`: candid files that can be imported by "app";
- `pulled.json` which has following content:

```json
{
  "canisters": {
    "yofga-2qaaa-aaaaa-aabsq-cai": {
      "dependencies": [
        "yofga-2qaaa-aaaaa-aabsq-cai"
      ],
      "wasm_hash": "616af3b750c80787f5f123cf7860206db3bb352ef1efe77afcce4d3ee9f2c7ab",
      "wasm_hash_download": "6f053bb3d53d64409c6bddc9355eea658f3d79d510cf57c587bcc809c804bdea",
      "init_guide": "A natural number, e.g. 10.",
      "init_arg": "10",
      "candid_args": "(nat)",
      "gzip": false
    },
    "yhgn4-myaaa-aaaaa-aabta-cai": {
      "name": "dep_b",
      "dependencies": [
        "yofga-2qaaa-aaaaa-aabsq-cai"
      ],
      "wasm_hash": "5642fc8c6fcc0e975a48c87d8e5f21ad0781cb740e5230647754bde14e7f1569",
      "wasm_hash_download": "5642fc8c6fcc0e975a48c87d8e5f21ad0781cb740e5230647754bde14e7f1569",
      "init_guide": "No init arguments required",
      "init_arg": null,
      "candid_args": "()",
      "gzip": true
    },
    "yahli-baaaa-aaaaa-aabtq-cai": {
      "name": "dep_c",
      "dependencies": [
        "yofga-2qaaa-aaaaa-aabsq-cai"
      ],
      "wasm_hash": "6f053bb3d53d64409c6bddc9355eea658f3d79d510cf57c587bcc809c804bdea",
      "wasm_hash_download": "6f053bb3d53d64409c6bddc9355eea658f3d79d510cf57c587bcc809c804bdea",
      "init_guide": "An optional natural number, e.g. \"(opt 20)\".",
      "init_arg": null,
      "candid_args": "(opt nat)",
      "gzip": false
    }
  }
}
```

There are three dependencies:

- "yhgn4-myaaa-aaaaa-aabta-cai": "dep_b" in `dfx.json`;
- "yahli-baaaa-aaaaa-aabtq-cai": "dep_c" in `dfx.json`;
- "yofga-2qaaa-aaaaa-aabsq-cai": an indirect dependency that both "dep_b" and "dep_c" depend on;

**Note**

- In `pulled.json`, every dependency canister has the `wasm_hash` and `wasm_hash_download` fields.
  - They are likely to be the same which means that the downloaded wasm passed integrity check.
  - They can be different in one major circumstance:
    the canister provider serves a customized wasm at `wasm_url` to be deployed locally.
    But the corresponding `wasm_hash` or `wasm_hash_url` is not provided (or the content is wrong).
    `dfx deps` is designed to accept the mismatch hash and will proceed in the following `dfx deps init/deploy`.
-  `dfx deps pull` connects to the IC mainnet by default (`--network ic`).
You can choose other network as usual, e.g. `--network local`.

### 3. Set init arguments using `dfx deps init`

Running `dfx deps init` will iterate over all dependencies in `pulled.json`, try to set init arguments in the following order:

- For canisters that require no init argument, set empty
- For canisters that do require init arguments:
  - Use `init_arg` in `pulled.json` if it is set
  - use `"(null)"` if the canister's init type has a top-level `opt`

The command will also print the list of dependencies that do require an init argument.

Then running `dfx deps init <CANISTER> --argument <ARGUMENT>` will set the init argument for an individual dependency.

The init arguments will be recorded in `deps/init.json`.

For the example, simply running `dfx deps init` to set init arguments for all three pulled canisters.

- "yofga-2qaaa-aaaaa-aabsq-cai" ("a"): set with `init_arg`;
- "yhgn4-myaaa-aaaaa-aabta-cai" ("dep_b"): requires no argument, set empty;
- "yahli-baaaa-aaaaa-aabtq-cai" ("dep_c"): init type `(opt nat)` which has a top-level `opt`, set `"(null)"`;

The init arguments can be overwritten:

```
> dfx deps init yofga-2qaaa-aaaaa-aabsq-cai --argument 11
> dfx deps init deps_c --argument "(opt 22)"
```

The generated `init.json` has following content:

```json
{
  "canisters": {
    "yofga-2qaaa-aaaaa-aabsq-cai": {
      "arg_str": "11",
      "arg_raw": "4449444c00017d0b"
    },
    "yhgn4-myaaa-aaaaa-aabta-cai": {
      "arg_str": null,
      "arg_raw": null
    },
    "yahli-baaaa-aaaaa-aabtq-cai": {
      "arg_str": "(opt 22)",
      "arg_raw": "4449444c016e7d01000116"
    }
  }
}
```

### 4. Deploy pull dependencies on local replica using `dfx deps deploy`

Running `dfx deps deploy` will:

1. create the dependencies on the local replica with the same mainnet canister ID;
2. install the downloaded wasm with the init arguments in `init.json`;

You can also specify the name or principal to deploy one particular dependency.

For our example:
```
> dfx deps deploy
Creating canister: yofga-2qaaa-aaaaa-aabsq-cai
Installing canister: yofga-2qaaa-aaaaa-aabsq-cai
Creating canister: yhgn4-myaaa-aaaaa-aabta-cai (dep_b)
Installing canister: yhgn4-myaaa-aaaaa-aabta-cai (dep_b)
Creating canister: yahli-baaaa-aaaaa-aabtq-cai (dep_c)
Installing canister: yahli-baaaa-aaaaa-aabtq-cai (dep_c)
> dfx deps deploy yofga-2qaaa-aaaaa-aabsq-cai
Installing canister: yofga-2qaaa-aaaaa-aabsq-cai
> dfx deps deploy dep_b
Installing canister: yhgn4-myaaa-aaaaa-aabta-cai (dep_b)
```

**Note**

- `dfx deps deploy` always creates the canister with the anonymous identity so that dependencies and application canisters will have different controllers;
- `dfx deps deploy` always installs the canister in "reinstall" mode so that the canister status will be discarded;

## Q&A

### Why download wasm into shared cache instead of a project subfolder?

We don't want to encourage including binary files in version control.

On the Internet Computer, every canister only has one latest version running on mainnet. Service consumers should integrate with that latest version.

So `dfx deps pull` always gets the latest dependencies instead of locking on a particular run.

Every pulled canister has the latest version in the shared cache and can be reused by different projects.

### Should I include `deps/` folder in version control?

Yes.

`deps/` files enable the dependent canister to build and get IDE support.

If the required wasm files are also available in the shared cache, all application and dependencies can be deployed and tested integrally.

Considering a canister developer team:

1.  Dev1 follows the [workflow](#workflow) and include all generated `deps/` files in source control;
2.  Dev2 pulls the branch by Dev1 and runs `dfx deps pull` again
    1.  If the `pulled.json` has no change, then all dependencies are still up to date. Dev2 can `dfx deps deploy` without setting init arguments again;
    2.  If there are changes in `pulled.json`, Dev2 can try `dfx deps deploy` to see if all init arguments are still valid. Then Dev2 run `dfx deps init` if necessary and update source control;

These files also helps CI to detect outdated dependencies.


-----------------------

/docs/concepts/tech-stack.md:
-----------------------

# Tech Stack

## Overview

Canister authors can opt in to display the tech stack of the canister.

Providing a standard format of such information makes it easier to build tools like a Canister Explorer.

## JSON schema

`tech_stack` has 5 top-level optional categories.

- cdk
- language
- lib
- tool
- other

Each category is a map keyed by the names of tech stack items, where each value is a map containing optional fields.

The `"tech_stack"` object will be serialized as a part of the [`"dfx"` metadata](canister-metadata.md#dfx) and attached to the wasm.

### Example

```json
{
  "tech_stack": {
    "language": {
      "rust": {
        "version": "1.75.0"
      }
    },
    "cdk": {
      "ic-cdk": {
        "version": "0.13.0"
      }
    },
    "lib": {
      "ic-cdk-timers": {},
      "ic-stable-structures": {}
    },
    "other": {
      "bitcoin": {
        "address": "bcrt1qfe264m0ycx2vcqvqyhs0gpxk6tw8ug6hqeps2d"
      }
    },
    "tool": {
      "dfx": {}
    }
  }
}
```

## Configuration in `dfx.json`

While the only way to configure this is in `dfx.json`, we don't envision that canister developers will define these values in `dfx.json` by themselves.

Upcoming work will enable CDK providers to set `tech_stack` fields for their users. Please check the Q&A below for more explanation.

The example above was generated from the `dfx.json` configuration below.

```json
{
  "canisters": {
    "canister_foo": {
      "type": "custom",
      "tech_stack": {
        "cdk": {
          "ic-cdk": {
            "version": "0.13.0"
          }
        },
        "language": {
          "rust": {
            "version": "$(rustc --version | cut -d ' ' -f 2)"
          }
        },
        "lib": {
          "ic-cdk-timers": {},
          "ic-stable-structures": {}
        },
        "tool": {
          "dfx": {}
        },
        "other": {
          "bitcoin": {
            "address": "bcrt1qfe264m0ycx2vcqvqyhs0gpxk6tw8ug6hqeps2d"
          }
        }
      }
    }
  }
}
```

The `"tech_stack"` object in `dfx.json` is almost the same as the generated metadata.

The only difference is that the `language->rust->version` field is `"$(rustc --version | cut -d ' ' -f 2)"` instead of `"1.75.0"`.

Besides directly setting the value of custom fields, it's also possible to obtain the value by executing a command.

If the content of a custom field value begins with the prefix `$(` and ends with the postfix `)`, the inner text will be interpreted as a command.

- The command will be executed in the workspace root directory, which contains the `dfx.json` file.
- The stdout should be a valid UTF-8 string.
- The field value will be obtained by trimming the stdout, removing any leading and trailing whitespace.

## Fetch the metadata

To retrieve the metadata and validate the generated information, execute the following command:

```sh
> dfx canister metadata <canister_name> dfx
```

Please note that the `"tech_stack"` object is part of the public metadata under the key [`"dfx"`](canister-metadata.md#dfx).

Given that the content is in JSON format, you can utilize tools such as `jq` to manipulate the output and extract the pertinent information.

```sh
> dfx canister metadata <canister_name> dfx | jq -r ".tech_stack.language.rust.version"
1.75.0
```

## Q&A

### Who should set `tech_stack`?

In the near future, CDK will be able to set `tech_stack` without requiring extra configuration in `dfx.json`.

Currently, `dfx` sets `tech_stack` for Rust and Motoko canisters if they don't define `tech_stack` explicitly in `dfx.json`.

For Azle and Kybra projects created with `dfx new`, the corresponding `tech_stack` configuration will be added `dfx.json` by default.

Canister developers can always add/overwrite/remove the `tech_stack` fields set by CDK.


-----------------------

= FeatureTemplate Design Doc
// Author field:
Jonh Smith <email@email.org>
v0.1, 2020-01-01
:draft:
:toc:

== Overview
////
:required:

In a few sentences, describe the key objectives. Why do we need this feature?
What are we trying to accomplish?

Just a few sentences.
////

== Background
////
:required:

Include as much information as necessary here to understand the design. Include
glossary if necessary in this section. Links to examples, related projects
or other design docs. Any previous/current version of this feature.

Do not write ideas about how to solve the problem here.
////

=== Problem Statement
////
:required:

State the problem this design solves, in a concise way. A few sentences, not more.
////

=== Requirements
////
:optional:

State which requirements are necessary to consider this problem solved. This is in
relation to the solution, not the problem.
////

== Expected User/Developer Experience
////
:required: Either User and/or Developer experience should be explained.

Describe what
////

== Prior Art
////
:optional: But recommended.

Link to other products available as comparatives to this design. For example,
if another tool has a similar feature, list pros/cons/shortcomings of that tool.
////

== Detailed Design
////
:required:

Full design on how to implement the feature. Use this section to also describe
why some simpler approaches did not work. Mention other things to watch out
for during implementation.

Keep in mind that you might not be the person implementing the design.
////

=== Considered Solutions
////
:required:

What solutions were considered, with a list of pros and cons of each solutions.
////

=== Recommended Solution
////
:required:

What solution from the above are you recommending, and most importantly, WHY?
////

=== Public API
////
:optional: Required if there is any public API changes

List any new or current API changes. List traits, methods, arguments and any
types. A good way is to paste an example of the API in the language it will be
implemented, for example (with Rust):

[source,rust]
----
/// Confabulate the splines using reverse polarity. Can return an error if
/// the space is asynchronously stochastic.
pub fn confabulate(spline: &mut [&Spline], polarity: bool) -> Result<(), Error> {}
----

It's important to avoid implementations here and speak in general terms.
////

=== Prototype
////
:optional:

If a proof of concept is available, include a link to the files here (even if
it's in the same PR).
////

=== Security Considerations
////
:optional:

How will this feature impact security, and what needs to be done to keep it
secure. Considerations should include:
  - User input sanitization
  - Existing security protocols and standards
  - Permissions, Access Control and capabilities
  - Privacy, GDPR considerations, etc.
  - Anything else that can affect security and privacy.
////

=== Performance Considerations
////
:optional:

How will the feature affect speed and performance. Will there be a need to
benchmark the feature (and if so, how)? Is there any considerations to keep
in mind for avoiding and preventing future regressions?
////

== Breaking Changes
////
:optional:

Does this feature create or require breaking changes?
////

=== Deprecation
////
:optional:

Does this feature deprecates any existing APIs?
////

== Documentation
////
:required:

How will this feature be documented? Which people need to be involved?
////

== Lifecycle

=== Integration Plan
////
:optional: Required if there are interactions with other tools.

How will this feature interact with other tools? Is there any changes outside
of the SDK that are required to make this feature work? Does this feature
have integration with `dfx`?
////

=== Publishing Plan
////
:optional: Required if there are new packages.

Explain which new packages will be released and published with this feature.
Include any changes to current released packages.
////

=== Rollout / Migration
////
:optional:

How can we minimize impact to users? How do we maximize adoption?
////

=== Rollback Plan
////
:optional:

How do you plan to rollback the change if a major issue is found?
////

=== Maintenance Plan
////
:required:

How do you plan to maintain this feature for the next years? Can the
APIs be cleanly evolved? Can Breaking Changes in the future be avoided?

If this is a service, what is the update and monitoring strategy?

If this is a package, how do we plan to publish and deploy it? This includes
version numbering.
////

== Work Breakdown
////
:required:

Description of the various phases and milestones. This is supposed to be a
bullet point list of high level stories and tasks. It is not meant to be a
1:1 ratio of PRs.
////


-----------------------

/docs/design/asset-canister-interface.md:
-----------------------

# Asset Canister Interface

## Introduction

The asset canister interface provides for storage and retrieval of static assets, such as HTML, CSS, JavaScript, images, and other media files. It can store different content encodings of an asset's contents, such as `identity` and `gzip`.

A canister that implements this interface can also return dynamic results from the [http_request](#method-http_request) method.

This document is meant to describe the interface with enough detail to aid in understanding how the asset canister works and in interacting with the asset canister at the code level.  It does not describe the interface in sufficient detail to rise to the level of a specification.

This document describes an interface, not an implementation. The IC SDK bundles one such possible implementation, the [IC Frontend Canister](https://github.com/dfinity/sdk/tree/master/src/canisters/frontend/ic-frontend-canister).

For brevity, this document does not reproduce the candid signatures for every method. 

## Table of Contents

| Section                                                  |
|----------------------------------------------------------|
| [Retrieving Assets](#retrieving-assets)                  |
| [Storing Assets](#storing-assets)                        |
| [Type Reference](#type-reference)                        |
| [Method Reference](#method-reference)                    |
| [Batch Operation Reference](#batch-operation-reference)  |
| [Configuration Reference](#configuration-reference)      |
| [Permission Reference](#permission-reference)            |

## Retrieving Assets

Asset retrieval begins with a call to either [get()](#method-get) or [http_request()](#method-http_request).

### Asset Lookup

The asset canister looks up assets by [key](#key).  An asset key is a unique, case-sensitive identifier.  Asset keys are usually pathnames, and by convention begin with a forward slash.

Examples:
- `/index.html`
- `/img/how-it-works/chain-key-signature.jpg`

#### Aliasing

If no asset with the requested key exists, the asset canister will look for an asset with a different key, where the alternate asset has the [enable_aliasing](#enable-aliasing) field set to `true`.

The aliasing rules are as follows:

- an attempt to retrieve `{some key}/` can instead retrieve `{some key}/index.html`
- an attempt to retrieve `{some key}`, where `{some key}` does not end with `.html`, can instead retrieve either `{some key}.html` or `{some key}/index.html`

Examples:
- an attempt to retrieve `/` can instead retrieve `/index.html`
- an attempt to retrieve `/docs/language-guide/about-this-guide/` can instead retrieve `/docs/language-guide/about-this-guide/index.html`
- an attempt to retrieve `/docs/language-guide/about-this-guide` can instead retrieve `/docs/language-guide/about-this-guide/index.html` or `/docs/language-guide/about-this-guide.html`

### Content Encoding Selection

When retrieving an asset, the caller specifies a list of acceptable [content encodings](#content_encoding). The asset canister will select the first suitable[^1] content encoding from this list.

### Large Assets

While the size of any given asset content encoding is limited only by the canister's available memory, the amount of data that can be passed or returned in a single method call is limited. For this reason, the interface provides for data upload and retrieval in smaller pieces, called "chunks".

The size of each chunk is limited by the message ingress limit.

## Storing Assets

### Batch Updates

The usual method of updating data in the asset canister is by calling the following methods:
1. [create_batch()](#method-create_batch) once.
2. [create_chunk()](#method-create_chunk) one or more times, which can occur concurrently.
3. [commit_batch()](#method-commit_batch) zero or more times with `batch_id: 0`.
4. [commit_batch()](#method-commit_batch) once with the batch ID from step 1, which indicates the batch is complete.

The reason for multiple rather than single calls to [commit_batch][#method-commit_batch] is that certificate computation for an entire batch may exceed per-message computation limits.

### Batch Updates By Proposal

If a [Service Nervous System](https://internetcomputer.org/docs/current/developer-docs/integrations/sns/)  (SNS) controls an asset canister, it can update the assets by proposal. In this scenario, there are two principals:
- The Preparer, which must have the [Prepare](#permission-prepare) permission. This principal prepares the proposal by uploading data and proposing changes to be committed.
- The Committer, which must have the [Commit](#permission-commit) permission. This principal commits the previously-proposed changes.

In this scenario, the Preparer calls the following methods:
1. [create_batch()](#method-create_batch) once.
2. [create_chunk()](#method-create_chunk) one or more times, which can occur concurrently.
3. [propose_commit_batch()](#method-propose_commit_batch) once.
4. [compute_evidence()](#method-compute_evidence) until the method returns `Some(evidence)`.

The Preparer then furnishes the Committer with the following information:
- the batch ID
- the computed evidence

The Committer then calls the following method upon approval of the proposal:
1. [commit_proposed_batch()](#method-commit_proposed_batch)

If the proposal is not approved, the Preparer must call [delete_batch()](#method-delete_batch).  Until this is done, all calls to [create_batch()](#method-create_batch) will fail.

### Individual Updates

It is also possible to upload a single content encoding of a single asset by calling the [store()](#method-store) method.  The size of the content encoding must not exceed the message ingress limit.

## Type Reference

### Asset

#### Key

The `key` is a case-sensitive string that identifies the asset. By convention, all asset keys begin with a forward slash. For example, `/index.html`.

#### Content Type

The `content_type` field is a string that identifies the type of the asset, such as `text/plain` or `image/jpeg`. It is used to set the `Content-Type` header when serving the asset over HTTP.

#### Content Encodings

The asset canister can store and serve multiple encodings of the same asset. Each encoding is identified by a `content_encoding` string, such as `identity` or `gzip`. It is used to set the `Content-Encoding` header when serving the asset over HTTP.

The `identity` encoding corresponds to the original, unencoded asset contents.

#### Content Chunks

Each encoding contains one or more "chunks" of data. The size of each chunk is limited by the message ingress limit.

Content chunks can have any size that fits within the message ingress limit, but for a given asset encoding, all chunks except the last must have the same size.

#### Content Hash

The `sha256` field contains the SHA-256 hash of the entire asset encoding. It is used to set the `ETag` header when serving the asset over HTTP, and to ensure coherence when retrieving a content encoding with more than one query call.

#### Max Age

The `max_age` field is the maximum number of seconds that the asset can be cached by a browser or CDN. It is used to set the `max-age` value of the `Cache-Control` header when serving the asset over HTTP.

#### Headers

The `headers` field is a list of additional headers to set when serving the asset over HTTP.

#### Enable Aliasing

This field enables retrieval of this asset by a different key, according to the [aliasing](#aliasing) rules.

> **NOTE** The interface uses more than one name for this field:
> - `enable_aliasing` in [CreateAsset](#operation-createasset) arguments
> - `is_aliased` in [SetAssetProperties](#operation-setassetproperties) arguments and in the return value of [get_asset_properties()](#method-get_asset_properties).
>
> In all cases, it indicates that the asset's key _might be_ an alias for another asset, not that it is _definitely the case_ for the asset in question.  It will often be `true` for assets which are not an alias for another asset.

#### Raw Access

The `allow_raw_access` field controls whether an asset can be retrieved from `raw.ic0.app` or `raw.icp0.io`. If false (which is the default), then the asset canister will redirect any such attempts to the non-raw URL.

### Batch

The asset canister holds related changes in a batch before committing those changes to assets in its state. The asset canister must retain all data in a batch for at least the [Minimum Batch Retention Duration](#constant-minimum-batch-retention-duration) after creation of the batch itself or creation of any chunk in the batch. 

### Chunk

A chunk is sequence of bytes comprising all or part of a content encoding for an asset.

The size of any chunk cannot exceed the message ingress limit.

## Method Reference

### Method: `init` and `post_upgrade`

```candid
service: (asset_canister_args: variant {
  Init: record {};
  Upgrade: record {
    set_permissions: opt record {
      prepare: vec principal;
      commit: vec principal;
      manage_permissions: vec principal;
    };
  };
})
```

The methods `init` and `post_upgrade` are called automatically by the system after new code is installed in the canister.

Both methods take the same argument type by definition. Therefore, to be able to have different arguments for the two cases, an enum is used to make the distinction.
If `init` is called with the `Upgrade` variant or if `post_upgrade` is called with the `Init` variant the asset canister traps and thereby reverts the code changes.

In `Upgrade`, the field `set_permissions` can be used to (re)set the list of principals with the listed permissions.
If `set_permissions` that is not `null`, then all permissions are set to the newly provided list of principals and the previous lists of principals are discarded.

### Method: `get`

```candid
  get: (record {
    key: Key;
    accept_encodings: vec text;
  }) -> (record {
    content: blob; // may be the entirety of the content, or just chunk index 0
    content_type: text;
    content_encoding: text;
    sha256: opt blob; // sha256 of entire asset encoding
    total_length: nat; // all chunks except last have size == content.size()
  }) query;
```

This method looks up the asset with the given key, using [aliasing](#aliasing) rules if the key is not found.

Then, it searches the asset's [content encodings](#content-encodings) in the order specified in `accept_encodings`.  If none are found, it returns an error.  A typical value for `accept_encodings` would be `["gzip", "identity"]`.

Finally, it returns the first chunk of the content encoding.

If `total_length` exceeds the length of the returned `content` blob, this means that there is more than one chunk.  The caller can then call [get_chunk()](#method-get_chunk) to retrieve the remaining chunks.  Note that since all chunks except the last have the same length as the first chunk, the caller can determine the number of chunks by dividing `total_length` by the length of the first chunk.

The `sha256` field is `opt` only because it was added after the initial release of the asset canister.  It must always be present in the response.

### Method: `get_chunk`

```candid
  get_chunk: (record {
    key: Key;
    content_encoding: text;
    index: nat;
    sha256: opt blob;
  }) -> (record { content: blob }) query;
```

This method looks up the asset with the given key, using [aliasing](#aliasing) rules if the key is not found.

It returns the chunk with the given index of the specified content encoding of the asset.

The asset canister returns an error if the `sha256` field is not present, or if the `sha256` field does not match the hash of the content encoding.  This protects against changes to the content encoding in between calls to [get()](#method-get) and [get_chunk()](#method-get_chunk).

### Method: `list`

```candid
  list : (record {}) -> (vec record {
    key: Key;
    content_type: text;
    encodings: vec record {
      content_encoding: text;
      sha256: opt blob; // sha256 of entire asset encoding
      length: nat;
      modified: Time;
    };
  }) query;
```

This method returns a list of all assets.

The `sha256` field is `opt` only because it was added after the initial release of the asset canister.  It must always be present in the response.

### Method: `http_request`

This method returns an HTTP response for the given HTTP request.

### Method: `http_request_streaming_callback`

If the response to an `http_request` call includes a `streaming_strategy`, then this will be the value of the `callback`.

### Method: `create_batch`

This method creates a new [batch](#batch) and returns its ID.

Preconditions:
- No batch exists for which [propose_commit_batch()](#method-propose_commit_batch) has been called.
- Creation of a new batch would not exceed batch creation limits.

Required Permission: [Prepare](#permission-prepare)

### Method: `create_chunk`

```candid
  create_chunk: (
    record { 
      batch_id: BatchId;
      content: blob
    }
  ) -> (record { 
    chunk_id: ChunkId
  });
```

This method stores a content chunk and extends the batch expiry time.

When creating chunks for a given content encoding, the size of each chunk except the last must be the same.

The asset canister must retain all data related to a batch for at least the [Minimum Batch Retention Duration](#constant-minimum-batch-retention-duration) after creating a chunk in a batch.

Preconditions:
- The batch exists.
- Creation of the chunk would not exceed chunk creation limits.

Required Permission: [Prepare](#permission-prepare)

### Method: `create_chunks`

```candid
  create_chunks: (
    record {
      batch_id: BatchId;
      content: vec blob;
    }
  ) -> (
    chunk_ids: vec ChunkId;
  );
```

This method stores a number of chunks and extends the batch expiry time.

When creating chunks for a given content encoding, the size of each chunk except the last must be the same.

The asset canister must retain all data related to a batch for at least the [Minimum Batch Retention Duration](#constant-minimum-batch-retention-duration) after creating a chunk in a batch.

Preconditions:
- The batch exists.
- Creation of the chunk would not exceed chunk creation limits.

Required Permission: [Prepare](#permission-prepare)

### Method: `commit_batch`

```candid
  commit_batch: (record {
    batch_id: BatchId;
    operations: vec BatchOperationKind
  }) -> ();
```

The `commit_batch` method executes the specified batch operations in the order listed. The method traps if there is an error executing any operation, so either all or none of the operations will be applied.

After executing the operations, this method deletes the batch associated with `batch_id`. It is valid to pass `0` for batch_id, in which case this method does not delete any batch. This allows multiple calls to `commit_batch` to execute operations from a large batch, such that no call to `commit_batch` exceeds per-call computation limits.  The final call to `commit_batch` should include the batch ID, in order to delete the batch.

| Operation                                           | Description                           |
|-----------------------------------------------------|---------------------------------------|
| [CreateAsset](#operation-createasset)               | Creates a new asset.                  |
| [SetAssetContent](#operation-setassetcontent)       | Adds or changes content for an asset. |
| [SetAssetProperties](#operation-setassetproperties) | Changes properties for an asset.      |
| [UnsetAssetContent](#operation-unsetassetcontent)   | Removes content for an asset.         |
| [DeleteAsset](#operation-deleteasset)               | Deletes an asset.                     |
| [Clear](#operation-clear)                           | Deletes all assets.                   |

Required Permission: [Commit](#permission-commit)

### Method: `delete_batch`

The `delete_batch` method deletes a single batch and any related chunks.

Required Permission: [Prepare](#permission-prepare)

### Method: `propose_commit_batch`

This method takes the same arguments as `commit_batch`, but does not execute the operations. Instead, it stores the operations in a "proposed batch" for later execution by the `commit_proposed_batch` method.

Required permission: [Prepare](#permission-prepare)

### Method: `compute_evidence`

The `compute_evidence` method computes a hash over the proposed commit batch arguments.

Since calculation of this hash may exceed per-message computation limits, this method computes the hash iteratively, saving its work as it goes. Once it completes the computation, it saves the hash as `evidence` to be checked later.

The method will return `None` if the hash computation has not yet completed, or `Some(evidence)` if the hash computation has been completed.

The returned `evidence` value must be passed to the `commit_proposed_batch` method.

After the hash computation has completed, the batch will no longer expire. The batch will remain until one of the following occurs:
- a call to [commit_proposed_batch()]
- a call to [delete_batch()]
- the canister is upgraded

Required permission: [Prepare](#permission-prepare)

### Method: `commit_proposed_batch`

This method executes the operations previously supplied by [propose_commit_batch()](#method-propose_commit_batch), and deletes the batch.

Preconditions:
- The batch exists.
- The batch has proposed commit batch arguments.
- Evidence computation has completed.
- Evidence passed in the arguments matches the evidence previously computed by [compute_evidence()](#method-compute_evidence).

Required permission: [Commit](#permission-commit)

### Method: `grant_permission`

This method grants a permission to a principal.

Callable by: Principals with [ManagePermissions](#permission-managepermissions) permission, and canister controllers.

### Method: `revoke_permission`

This method revokes a permission from a principal.

Callable by: Principals with [ManagePermissions](#permission-managepermissions) permission, and canister controllers. Also, any principal can revoke any of its own permissions.

### Method: `list_permitted`

This method returns a list of principals that have the given permission.

### Method: `authorize`

> **NOTE**: This method is deprecated. Use [grant_permission()](#method-grant_permission) instead.

This method grants the [Commit](#Permission_commit) permission to a principal.

Callable by: Principals with [ManagePermissions](#permission-managepermissions) permission, and canister controllers.

### Method: `deauthorize`

> **NOTE**: This method is deprecated. Use [revoke_permission()](#method-revoke_permission) instead.

This method revokes the [Commit](#Permission_commit) permission from a principal.

Callable by: Principals with [ManagePermissions](#permission-managepermissions) permission, and canister controllers. Also, any principal can revoke anny of its own permissions.

### Method: `list_authorized`

> **NOTE**: This method is deprecated. Use [list_permitted()](#method-list_permitted) instead.

This method returns a list of principals that have the [Commit](#Permission_commit) permission.

### Method: `configure`

This method configures the asset canister. See [Configuration Reference](#configuration-reference) for details.

### Method: `get_configuration`

This method returns the configuration of the asset canister.

### Method: `get_asset_properties`

This method returns the properties of the asset with the given key.

### Method: `certified_tree`

This method returns the certified tree.

### Convenience Methods

Each of these methods is the equivalent of its respective batch operation.

> **NOTE**: While they are provided for "convenience," some don't actually make sense.  For example, [set_asset_content()](#operation-setassetcontent) requires chunk ids, but [create_chunk()](#method-createchunk) requires a batch.
>
> These methods may be deprecated in the future.  It is recommended to instead call [commit_batch()](#method-commitbatch) with a single operation, specifying batch ID 0.

| Method                   | Operation                                           |
|--------------------------|-----------------------------------------------------|
| `create_asset()`         | [CreateAsset](#operation-createasset)               |
| `delete_asset()`         | [DeleteAsset](#operation-deleteasset)               |
| `set_asset_content()`    | [SetAssetContent](#operation-setassetcontent)       |
| `set_asset_properties()` | [SetAssetProperties](#operation-setassetproperties) |
| `unset_asset_content()`  | [UnsetAssetContent](#operation-unsetassetcontent)   |
| `clear()`                | [Clear](#operation-clear)                           |

Each of these methods requires permission: [Commit](#permission-commit)

### Validation Methods

These methods validate the arguments for the corresponding methods.  They are required for the SNS to be able to call the corresponding methods.

- `validate_grant_permission()`
- `validate_revoke_permission()`
- `validate_take_ownership()`
- `validate_commit_proposed_batch()`
- `validate_configure()`

## Batch Operation Reference

### Operation: `CreateAsset`

```candid
type CreateAssetArguments = record {
  key: Key;
  content_type: text;
  max_age: opt nat64;
  headers: opt vec HeaderField;
  enable_aliasing: opt bool;
  allow_raw_access: opt bool;
};
```

This operation creates a new asset.  An asset with the given key must not already exist.

### Operation: `SetAssetContent`

```candid
type SetAssetContentArguments = record {
  key: Key;
  content_encoding: text;
  chunk_ids: vec ChunkId;
  sha256: opt blob;
};
```

This operation adds or changes a single content encoding for an asset.  It also updates the modification time of the content encoding.

If `sha256` is not passed, the asset canister will compute the hash of the content.

### Operation: `SetAssetProperties`

```candid
type SetAssetPropertiesArguments = record {
  key: Key;
  max_age: opt opt nat64;
  headers: opt opt vec HeaderField;
  allow_raw_access: opt opt bool;
  is_aliased: opt opt bool;
};
```

This operation sets some or all properties of an asset.

### Operation: `UnsetAssetContent`  

```candid
type UnsetAssetContentArguments = record {
  key: Key;
  content_encoding: text;
};
```

This operation removes a single content encoding for an asset.

### Operation: `DeleteAsset`    

```candid
type DeleteAssetArguments = record {
  key: Key;
};
```

This operation deletes a single asset.

### Operation: `Clear`

```candid
type ClearArguments = record {};
```

This operation deletes all assets.

## Configuration Reference

These are set by the [configure()](#method-configure) method.  All limits default to unlimited.

| Configuration | Description                                                                         |
|---------------|-------------------------------------------------------------------------------------|
| `max_batches` | The maximum number of batches being uploaded at one time.                           |
| `max_chunks`  | The maximum number of chunks across all batches being uploaded.                     |
| `max_bytes`   | The maximum number of total size of content bytes across all chunks being uploaded. |

## API Versions

### API Version 1

This version added `SetAssetProperties` to `BatchOperationKind`.

## Permissions

### Permission: `Commit`

Permits changes to the assets served by the asset canister.

Any principal with this permission can also call any method callable with the [Prepare](#permission-prepare) permission.

### Permission: `Prepare`

Permits upload of data to the canister to be committed later by a principal with the [Commit](#permission-commit) permission.

### Permission: `ManagePermissions`

Permits a principal to grant and revoke permissions to other principals.

## Constants

### Constant: Minimum Batch Retention Duration

The asset canister must retain all data related to a batch for at least 5 minutes after creating that batch or creating a chunk within it.

[^1] This term is intentionally vague.



-----------------------

/docs/dfx-json-schema.json:
-----------------------

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "dfx.json",
  "type": "object",
  "properties": {
    "canisters": {
      "description": "Mapping between canisters and their settings.",
      "type": [
        "object",
        "null"
      ],
      "additionalProperties": {
        "$ref": "#/definitions/ConfigCanistersCanister"
      }
    },
    "defaults": {
      "description": "Defaults for dfx start.",
      "anyOf": [
        {
          "$ref": "#/definitions/ConfigDefaults"
        },
        {
          "type": "null"
        }
      ]
    },
    "dfx": {
      "title": "dfx version",
      "description": "Pins the dfx version for this project.",
      "type": [
        "string",
        "null"
      ]
    },
    "networks": {
      "description": "Mapping between network names and their configurations. Networks 'ic' and 'local' are implicitly defined.",
      "type": [
        "object",
        "null"
      ],
      "additionalProperties": {
        "$ref": "#/definitions/ConfigNetwork"
      }
    },
    "output_env_file": {
      "description": "If set, environment variables will be output to this file (without overwriting any user-defined variables, if the file already exists).",
      "type": [
        "string",
        "null"
      ]
    },
    "profile": {
      "anyOf": [
        {
          "$ref": "#/definitions/Profile"
        },
        {
          "type": "null"
        }
      ]
    },
    "version": {
      "description": "Used to keep track of dfx.json versions.",
      "type": [
        "integer",
        "null"
      ],
      "format": "uint32",
      "minimum": 0.0
    }
  },
  "definitions": {
    "BitcoinAdapterLogLevel": {
      "description": "Represents the log level of the bitcoin adapter.",
      "type": "string",
      "enum": [
        "critical",
        "error",
        "warning",
        "info",
        "debug",
        "trace"
      ]
    },
    "Byte": {
      "title": "Byte Count",
      "description": "A quantity of bytes. Representable either as an integer, or as an SI unit string",
      "examples": [
        72,
        "2KB",
        "4 MiB"
      ],
      "type": [
        "integer",
        "string"
      ],
      "pattern": "^[0-9]+( *([KkMmGgTtPpEeZzYy]i?)?[Bb])?$"
    },
    "CanisterDeclarationsConfig": {
      "title": "Declarations Configuration",
      "description": "Configurations about which canister interface declarations to generate, and where to generate them.",
      "type": "object",
      "properties": {
        "bindings": {
          "title": "Languages to generate",
          "description": "A list of languages to generate type declarations. Supported options are 'js', 'ts', 'did', 'mo'. Default is ['js', 'ts', 'did'].",
          "type": [
            "array",
            "null"
          ],
          "items": {
            "type": "string"
          }
        },
        "env_override": {
          "title": "Canister ID ENV Override",
          "description": "A string that will replace process.env.CANISTER_ID_{canister_name_uppercase} in the 'src/dfx/assets/language_bindings/canister.js' template.",
          "type": [
            "string",
            "null"
          ]
        },
        "node_compatibility": {
          "title": "Node compatibility flag",
          "description": "Flag to pre-populate generated declarations with better defaults for various types of projects Default is false",
          "default": false,
          "type": "boolean"
        },
        "output": {
          "title": "Declaration Output Directory",
          "description": "Directory to place declarations for that canister. Default is 'src/declarations/<canister_name>'.",
          "type": [
            "string",
            "null"
          ]
        }
      }
    },
    "CanisterLogVisibility": {
      "oneOf": [
        {
          "type": "string",
          "enum": [
            "controllers",
            "public"
          ]
        },
        {
          "type": "object",
          "required": [
            "allowed_viewers"
          ],
          "properties": {
            "allowed_viewers": {
              "type": "array",
              "items": {
                "type": "string"
              }
            }
          },
          "additionalProperties": false
        }
      ]
    },
    "CanisterMetadataSection": {
      "title": "Canister Metadata Configuration",
      "description": "Configures a custom metadata section for the canister wasm. dfx uses the first definition of a given name matching the current network, ignoring any of the same name that follow.",
      "type": "object",
      "required": [
        "name"
      ],
      "properties": {
        "content": {
          "title": "Content",
          "description": "Content of this metadata section. Conflicts with `path`.",
          "type": [
            "string",
            "null"
          ]
        },
        "name": {
          "title": "Name",
          "description": "The name of the wasm section",
          "type": "string"
        },
        "networks": {
          "title": "Networks",
          "description": "Networks this section applies to. If this field is absent, then it applies to all networks. An empty array means this element will not apply to any network.",
          "type": [
            "array",
            "null"
          ],
          "items": {
            "type": "string"
          },
          "uniqueItems": true
        },
        "path": {
          "title": "Path",
          "description": "Path to file containing section contents. Conflicts with `content`. For sections with name=`candid:service`, this field is optional, and if not specified, dfx will use the canister's candid definition. If specified for a Motoko canister, the service defined in the specified path must be a valid subtype of the canister's actual candid service definition.",
          "type": [
            "string",
            "null"
          ]
        },
        "visibility": {
          "title": "Visibility",
          "default": "public",
          "allOf": [
            {
              "$ref": "#/definitions/MetadataVisibility"
            }
          ]
        }
      }
    },
    "ConfigCanistersCanister": {
      "title": "Canister Configuration",
      "description": "Configurations for a single canister.",
      "type": "object",
      "oneOf": [
        {
          "title": "Rust-Specific Properties",
          "type": "object",
          "required": [
            "candid",
            "package",
            "type"
          ],
          "properties": {
            "candid": {
              "title": "Candid File",
              "description": "Path of this canister's candid interface declaration.",
              "type": "string"
            },
            "crate": {
              "title": "Crate name",
              "description": "Name of the Rust crate that compiles to this canister's Wasm. If left unspecified, defaults to the crate with the same name as the package.",
              "type": [
                "string",
                "null"
              ]
            },
            "package": {
              "title": "Package Name",
              "description": "Name of the Rust package that compiles this canister's Wasm.",
              "type": "string"
            },
            "type": {
              "type": "string",
              "enum": [
                "rust"
              ]
            }
          }
        },
        {
          "title": "Asset-Specific Properties",
          "type": "object",
          "required": [
            "source",
            "type"
          ],
          "properties": {
            "build": {
              "title": "Build Commands",
              "description": "Commands that are executed in order to produce this canister's assets. Expected to produce assets in one of the paths specified by the 'source' field. Optional if there is no build necessary or the assets can be built using the default `npm run build` command.",
              "default": [],
              "allOf": [
                {
                  "$ref": "#/definitions/SerdeVec_for_String"
                }
              ]
            },
            "source": {
              "title": "Asset Source Folder",
              "description": "Folders from which assets are uploaded.",
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "type": {
              "type": "string",
              "enum": [
                "assets"
              ]
            },
            "workspace": {
              "title": "NPM workspace",
              "description": "The workspace in package.json that this canister is in, if it is not in the root workspace.",
              "type": [
                "string",
                "null"
              ]
            }
          }
        },
        {
          "title": "Custom-Specific Properties",
          "type": "object",
          "required": [
            "candid",
            "type",
            "wasm"
          ],
          "properties": {
            "build": {
              "title": "Build Commands",
              "description": "Commands that are executed in order to produce this canister's Wasm module. Expected to produce the Wasm in the path specified by the 'wasm' field. No build commands are allowed if the `wasm` field is a URL. These commands are executed in the root of the project.",
              "default": [],
              "allOf": [
                {
                  "$ref": "#/definitions/SerdeVec_for_String"
                }
              ]
            },
            "candid": {
              "title": "Candid File",
              "description": "Path to this canister's candid interface declaration.  A URL to a candid file is also acceptable.",
              "type": "string"
            },
            "type": {
              "type": "string",
              "enum": [
                "custom"
              ]
            },
            "wasm": {
              "title": "Wasm Path",
              "description": "Path to Wasm to be installed. URLs to a Wasm module are also acceptable. A canister that has a URL to a Wasm module can not also have `build` steps.",
              "type": "string"
            }
          }
        },
        {
          "title": "Motoko-Specific Properties",
          "type": "object",
          "required": [
            "type"
          ],
          "properties": {
            "type": {
              "type": "string",
              "enum": [
                "motoko"
              ]
            }
          }
        },
        {
          "title": "Pull-Specific Properties",
          "type": "object",
          "required": [
            "id",
            "type"
          ],
          "properties": {
            "id": {
              "title": "Canister ID",
              "description": "Principal of the canister on the ic network.",
              "type": "string"
            },
            "type": {
              "type": "string",
              "enum": [
                "pull"
              ]
            }
          }
        }
      ],
      "properties": {
        "args": {
          "title": "Canister-Specific Build Argument",
          "description": "This field defines an additional argument to pass to the Motoko compiler when building the canister.",
          "type": [
            "string",
            "null"
          ]
        },
        "declarations": {
          "title": "Declarations Configuration",
          "description": "Defines which canister interface declarations to generate, and where to generate them.",
          "default": {
            "bindings": null,
            "env_override": null,
            "node_compatibility": false,
            "output": null
          },
          "allOf": [
            {
              "$ref": "#/definitions/CanisterDeclarationsConfig"
            }
          ]
        },
        "dependencies": {
          "title": "Dependencies",
          "description": "Defines on which canisters this canister depends on.",
          "default": [],
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "frontend": {
          "title": "Force Frontend URL",
          "description": "Mostly unused. If this value is not null, a frontend URL is displayed after deployment even if the canister type is not 'asset'.",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "string"
          }
        },
        "gzip": {
          "title": "Gzip Canister Wasm",
          "description": "Disabled by default.",
          "type": [
            "boolean",
            "null"
          ]
        },
        "init_arg": {
          "title": "Init Arg",
          "description": "The Candid initialization argument for installing the canister. If the `--argument` or `--argument-file` argument is also provided, this `init_arg` field will be ignored.",
          "type": [
            "string",
            "null"
          ]
        },
        "init_arg_file": {
          "title": "Init Arg File",
          "description": "The Candid initialization argument file for installing the canister. If the `--argument` or `--argument-file` argument is also provided, this `init_arg_file` field will be ignored.",
          "type": [
            "string",
            "null"
          ]
        },
        "initialization_values": {
          "title": "Resource Allocation Settings",
          "description": "Defines initial values for resource allocation settings.",
          "default": {
            "compute_allocation": null,
            "freezing_threshold": null,
            "log_visibility": null,
            "memory_allocation": null,
            "reserved_cycles_limit": null,
            "wasm_memory_limit": null
          },
          "allOf": [
            {
              "$ref": "#/definitions/InitializationValues"
            }
          ]
        },
        "main": {
          "title": "Path to Canister Entry Point",
          "description": "Entry point for e.g. Motoko Compiler.",
          "type": [
            "string",
            "null"
          ]
        },
        "metadata": {
          "title": "Metadata",
          "description": "Defines metadata sections to set in the canister .wasm",
          "default": [],
          "type": "array",
          "items": {
            "$ref": "#/definitions/CanisterMetadataSection"
          }
        },
        "optimize": {
          "title": "Optimize Canister Wasm",
          "description": "Invoke wasm level optimizations after building the canister. Optimization level can be set to \"cycles\" to optimize for cycle usage, \"size\" to optimize for binary size, or any of \"O4, O3, O2, O1, O0, Oz, Os\". Disabled by default. If this option is specified, the `shrink` option will be ignored.",
          "anyOf": [
            {
              "$ref": "#/definitions/WasmOptLevel"
            },
            {
              "type": "null"
            }
          ]
        },
        "post_install": {
          "title": "Post-Install Commands",
          "description": "One or more commands to run post canister installation. These commands are executed in the root of the project.",
          "default": [],
          "allOf": [
            {
              "$ref": "#/definitions/SerdeVec_for_String"
            }
          ]
        },
        "pullable": {
          "title": "Pullable",
          "description": "Defines required properties so that this canister is ready for `dfx deps pull` by other projects.",
          "anyOf": [
            {
              "$ref": "#/definitions/Pullable"
            },
            {
              "type": "null"
            }
          ]
        },
        "remote": {
          "title": "Remote Configuration",
          "description": "Used to mark the canister as 'remote' on certain networks.",
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigCanistersCanisterRemote"
            },
            {
              "type": "null"
            }
          ]
        },
        "shrink": {
          "title": "Shrink Canister Wasm",
          "description": "Whether run `ic-wasm shrink` after building the Canister. Enabled by default for Rust/Motoko canisters. Disabled by default for custom canisters.",
          "type": [
            "boolean",
            "null"
          ]
        },
        "specified_id": {
          "title": "Specified Canister ID",
          "description": "Attempts to create the canister with this Canister ID. This option only works with non-mainnet replica. If the `--specified-id` argument is also provided, this `specified_id` field will be ignored.",
          "type": [
            "string",
            "null"
          ]
        },
        "tech_stack": {
          "title": "Tech Stack",
          "description": "Defines the tech stack used to build this canister.",
          "anyOf": [
            {
              "$ref": "#/definitions/TechStack"
            },
            {
              "type": "null"
            }
          ]
        }
      }
    },
    "ConfigCanistersCanisterRemote": {
      "title": "Remote Canister Configuration",
      "description": "This field allows canisters to be marked 'remote' for certain networks. On networks where this canister contains a remote ID, the canister is not deployed. Instead it is assumed to exist already under control of a different project.",
      "type": "object",
      "required": [
        "id"
      ],
      "properties": {
        "candid": {
          "title": "Remote Candid File",
          "description": "On networks where this canister is marked 'remote', this candid file is used instead of the one declared in the canister settings.",
          "type": [
            "string",
            "null"
          ]
        },
        "id": {
          "title": "Network to Remote ID Mapping",
          "description": "This field contains mappings from network names to remote canister IDs (Principals). For all networks listed here, this canister is considered 'remote'.",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        }
      }
    },
    "ConfigDefaults": {
      "description": "Defaults to use on dfx start.",
      "type": "object",
      "properties": {
        "bitcoin": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsBitcoin"
            },
            {
              "type": "null"
            }
          ]
        },
        "bootstrap": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsBootstrap"
            },
            {
              "type": "null"
            }
          ]
        },
        "build": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsBuild"
            },
            {
              "type": "null"
            }
          ]
        },
        "canister_http": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsCanisterHttp"
            },
            {
              "type": "null"
            }
          ]
        },
        "proxy": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsProxy"
            },
            {
              "type": "null"
            }
          ]
        },
        "replica": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsReplica"
            },
            {
              "type": "null"
            }
          ]
        }
      }
    },
    "ConfigDefaultsBitcoin": {
      "title": "Bitcoin Adapter Configuration",
      "type": "object",
      "properties": {
        "canister_init_arg": {
          "title": "Initialization Argument",
          "description": "The initialization argument for the bitcoin canister.",
          "default": "(record { stability_threshold = 0 : nat; network = variant { regtest }; blocks_source = principal \"aaaaa-aa\"; fees = record { get_utxos_base = 0 : nat; get_utxos_cycles_per_ten_instructions = 0 : nat; get_utxos_maximum = 0 : nat; get_balance = 0 : nat; get_balance_maximum = 0 : nat; get_current_fee_percentiles = 0 : nat; get_current_fee_percentiles_maximum = 0 : nat;  send_transaction_base = 0 : nat; send_transaction_per_byte = 0 : nat; }; syncing = variant { enabled }; api_access = variant { enabled }; disable_api_if_not_fully_synced = variant { enabled }})",
          "type": "string"
        },
        "enabled": {
          "title": "Enable Bitcoin Adapter",
          "default": false,
          "type": "boolean"
        },
        "log_level": {
          "title": "Logging Level",
          "description": "The logging level of the adapter.",
          "default": "info",
          "allOf": [
            {
              "$ref": "#/definitions/BitcoinAdapterLogLevel"
            }
          ]
        },
        "nodes": {
          "title": "Available Nodes",
          "description": "Addresses of nodes to connect to (in case discovery from seeds is not possible/sufficient).",
          "type": [
            "array",
            "null"
          ],
          "items": {
            "type": "string"
          }
        }
      }
    },
    "ConfigDefaultsBootstrap": {
      "title": "Bootstrap Server Configuration",
      "description": "The bootstrap command has been removed.  All of these fields are ignored.",
      "type": "object",
      "properties": {
        "ip": {
          "description": "Specifies the IP address that the bootstrap server listens on. Defaults to 127.0.0.1.",
          "default": "127.0.0.1",
          "type": "string",
          "format": "ip"
        },
        "port": {
          "description": "Specifies the port number that the bootstrap server listens on. Defaults to 8081.",
          "default": 8081,
          "type": "integer",
          "format": "uint16",
          "minimum": 0.0
        },
        "timeout": {
          "description": "Specifies the maximum number of seconds that the bootstrap server will wait for upstream requests to complete. Defaults to 30.",
          "default": 30,
          "type": "integer",
          "format": "uint64",
          "minimum": 0.0
        }
      }
    },
    "ConfigDefaultsBuild": {
      "title": "Build Process Configuration",
      "type": "object",
      "properties": {
        "args": {
          "description": "Arguments for packtool.",
          "type": [
            "string",
            "null"
          ]
        },
        "packtool": {
          "description": "Main command to run the packtool. This command is executed in the root of the project.",
          "type": [
            "string",
            "null"
          ]
        }
      }
    },
    "ConfigDefaultsCanisterHttp": {
      "title": "HTTP Adapter Configuration",
      "type": "object",
      "properties": {
        "enabled": {
          "title": "Enable HTTP Adapter",
          "default": true,
          "type": "boolean"
        },
        "log_level": {
          "title": "Logging Level",
          "description": "The logging level of the adapter.",
          "default": "error",
          "allOf": [
            {
              "$ref": "#/definitions/HttpAdapterLogLevel"
            }
          ]
        }
      }
    },
    "ConfigDefaultsProxy": {
      "description": "Configuration for the HTTP gateway.",
      "type": "object",
      "properties": {
        "domain": {
          "description": "A list of domains that can be served. These are used for canister resolution [default: localhost]",
          "anyOf": [
            {
              "$ref": "#/definitions/SerdeVec_for_String"
            },
            {
              "type": "null"
            }
          ]
        }
      }
    },
    "ConfigDefaultsReplica": {
      "title": "Local Replica Configuration",
      "type": "object",
      "properties": {
        "log_level": {
          "description": "Run replica with the provided log level. Default is 'error'. Debug prints still get displayed",
          "anyOf": [
            {
              "$ref": "#/definitions/ReplicaLogLevel"
            },
            {
              "type": "null"
            }
          ]
        },
        "port": {
          "description": "Port the replica listens on.",
          "type": [
            "integer",
            "null"
          ],
          "format": "uint16",
          "minimum": 0.0
        },
        "subnet_type": {
          "title": "Subnet Type",
          "description": "Determines the subnet type the replica will run as. Affects things like cycles accounting, message size limits, cycle limits. Defaults to 'application'.",
          "anyOf": [
            {
              "$ref": "#/definitions/ReplicaSubnetType"
            },
            {
              "type": "null"
            }
          ]
        }
      }
    },
    "ConfigLocalProvider": {
      "title": "Local Replica Configuration",
      "type": "object",
      "properties": {
        "bind": {
          "description": "Bind address for the webserver. For the shared local network, the default is 127.0.0.1:4943. For project-specific local networks, the default is 127.0.0.1:8000.",
          "type": [
            "string",
            "null"
          ]
        },
        "bitcoin": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsBitcoin"
            },
            {
              "type": "null"
            }
          ]
        },
        "bootstrap": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsBootstrap"
            },
            {
              "type": "null"
            }
          ]
        },
        "canister_http": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsCanisterHttp"
            },
            {
              "type": "null"
            }
          ]
        },
        "playground": {
          "anyOf": [
            {
              "$ref": "#/definitions/PlaygroundConfig"
            },
            {
              "type": "null"
            }
          ]
        },
        "proxy": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsProxy"
            },
            {
              "type": "null"
            }
          ]
        },
        "replica": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsReplica"
            },
            {
              "type": "null"
            }
          ]
        },
        "type": {
          "description": "Persistence type of this network.",
          "default": "ephemeral",
          "allOf": [
            {
              "$ref": "#/definitions/NetworkType"
            }
          ]
        }
      }
    },
    "ConfigNetwork": {
      "anyOf": [
        {
          "$ref": "#/definitions/ConfigNetworkProvider"
        },
        {
          "$ref": "#/definitions/ConfigLocalProvider"
        }
      ]
    },
    "ConfigNetworkProvider": {
      "title": "Custom Network Configuration",
      "type": "object",
      "required": [
        "providers"
      ],
      "properties": {
        "playground": {
          "anyOf": [
            {
              "$ref": "#/definitions/PlaygroundConfig"
            },
            {
              "type": "null"
            }
          ]
        },
        "providers": {
          "description": "The URL(s) this network can be reached at.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "type": {
          "description": "Persistence type of this network.",
          "default": "persistent",
          "allOf": [
            {
              "$ref": "#/definitions/NetworkType"
            }
          ]
        }
      }
    },
    "HttpAdapterLogLevel": {
      "description": "Represents the log level of the HTTP adapter.",
      "type": "string",
      "enum": [
        "critical",
        "error",
        "warning",
        "info",
        "debug",
        "trace"
      ]
    },
    "InitializationValues": {
      "title": "Initial Resource Allocations",
      "type": "object",
      "properties": {
        "compute_allocation": {
          "title": "Compute Allocation",
          "description": "Must be a number between 0 and 100, inclusively. It indicates how much compute power should be guaranteed to this canister, expressed as a percentage of the maximum compute power that a single canister can allocate.",
          "anyOf": [
            {
              "$ref": "#/definitions/PossiblyStr_for_uint64"
            },
            {
              "type": "null"
            }
          ]
        },
        "freezing_threshold": {
          "title": "Freezing Threshold",
          "description": "Freezing threshould of the canister, measured in seconds. Valid inputs are numbers (seconds) or strings parsable by humantime (e.g. \"15days 2min 2s\").",
          "type": [
            "string",
            "null"
          ]
        },
        "log_visibility": {
          "title": "Log Visibility",
          "description": "Specifies who is allowed to read the canister's logs.\n\nCan be \"public\", \"controllers\" or \"allowed_viewers\" with a list of principals.",
          "anyOf": [
            {
              "$ref": "#/definitions/CanisterLogVisibility"
            },
            {
              "type": "null"
            }
          ]
        },
        "memory_allocation": {
          "title": "Memory Allocation",
          "description": "Maximum memory (in bytes) this canister is allowed to occupy. Can be specified as an integer, or as an SI unit string (e.g. \"4KB\", \"2 MiB\")",
          "anyOf": [
            {
              "$ref": "#/definitions/Byte"
            },
            {
              "type": "null"
            }
          ]
        },
        "reserved_cycles_limit": {
          "title": "Reserved Cycles Limit",
          "description": "Specifies the upper limit of the canister's reserved cycles balance.\n\nReserved cycles are cycles that the system sets aside for future use by the canister. If a subnet's storage exceeds 450 GiB, then every time a canister allocates new storage bytes, the system sets aside some amount of cycles from the main balance of the canister. These reserved cycles will be used to cover future payments for the newly allocated bytes. The reserved cycles are not transferable and the amount of reserved cycles depends on how full the subnet is.\n\nA setting of 0 means that the canister will trap if it tries to allocate new storage while the subnet's memory usage exceeds 450 GiB.",
          "type": [
            "integer",
            "null"
          ],
          "format": "uint128",
          "minimum": 0.0
        },
        "wasm_memory_limit": {
          "title": "Wasm Memory Limit",
          "description": "Specifies a soft limit (in bytes) on the Wasm memory usage of the canister.\n\nUpdate calls, timers, heartbeats, installs, and post-upgrades fail if the Wasm memory usage exceeds this limit. The main purpose of this setting is to protect against the case when the canister reaches the hard 4GiB limit.\n\nMust be a number of bytes between 0 and 2^48 (i.e. 256 TiB), inclusive. Can be specified as an integer, or as an SI unit string (e.g. \"4KB\", \"2 MiB\")",
          "anyOf": [
            {
              "$ref": "#/definitions/Byte"
            },
            {
              "type": "null"
            }
          ]
        }
      }
    },
    "MetadataVisibility": {
      "oneOf": [
        {
          "description": "Anyone can query the metadata",
          "type": "string",
          "enum": [
            "public"
          ]
        },
        {
          "description": "Only the controllers of the canister can query the metadata.",
          "type": "string",
          "enum": [
            "private"
          ]
        }
      ]
    },
    "NetworkType": {
      "title": "Network Type",
      "description": "Type 'ephemeral' is used for networks that are regularly reset. Type 'persistent' is used for networks that last for a long time and where it is preferred that canister IDs get stored in source control.",
      "type": "string",
      "enum": [
        "ephemeral",
        "persistent"
      ]
    },
    "PlaygroundConfig": {
      "description": "Playground config to borrow canister from instead of creating new canisters.",
      "type": "object",
      "required": [
        "playground_canister"
      ],
      "properties": {
        "playground_canister": {
          "description": "Canister ID of the playground canister",
          "type": "string"
        },
        "timeout_seconds": {
          "description": "How many seconds a canister can be borrowed for",
          "default": 1200,
          "type": "integer",
          "format": "uint64",
          "minimum": 0.0
        }
      }
    },
    "PossiblyStr_for_uint64": {
      "type": "integer",
      "format": "uint64",
      "minimum": 0.0
    },
    "Profile": {
      "type": "string",
      "enum": [
        "Debug",
        "Release"
      ]
    },
    "Pullable": {
      "type": "object",
      "required": [
        "dependencies",
        "init_guide",
        "wasm_url"
      ],
      "properties": {
        "dependencies": {
          "title": "dependencies",
          "description": "Canister IDs (Principal) of direct dependencies.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "init_arg": {
          "title": "init_arg",
          "description": "A default initialization argument for the canister that consumers can use.",
          "type": [
            "string",
            "null"
          ]
        },
        "init_guide": {
          "title": "init_guide",
          "description": "A message to guide consumers how to initialize the canister.",
          "type": "string"
        },
        "wasm_hash": {
          "title": "wasm_hash",
          "description": "SHA256 hash of the wasm module located at wasm_url. Only define this if the on-chain canister wasm is expected not to match the wasm at wasm_url. The hash can also be specified via a URL using the `wasm_hash_url` field. If both are defined, the `wasm_hash_url` field will be ignored.",
          "type": [
            "string",
            "null"
          ]
        },
        "wasm_hash_url": {
          "title": "wasm_hash_url",
          "description": "Specify the SHA256 hash of the wasm module via this URL. Only define this if the on-chain canister wasm is expected not to match the wasm at wasm_url. The hash can also be specified directly using the `wasm_hash` field. If both are defined, the `wasm_hash_url` field will be ignored.",
          "type": [
            "string",
            "null"
          ]
        },
        "wasm_url": {
          "title": "wasm_url",
          "description": "The Url to download canister wasm.",
          "type": "string"
        }
      }
    },
    "ReplicaLogLevel": {
      "type": "string",
      "enum": [
        "critical",
        "error",
        "warning",
        "info",
        "debug",
        "trace"
      ]
    },
    "ReplicaSubnetType": {
      "type": "string",
      "enum": [
        "system",
        "application",
        "verifiedapplication"
      ]
    },
    "SerdeVec_for_String": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "array",
          "items": {
            "type": "string"
          }
        }
      ]
    },
    "TechStack": {
      "title": "Tech Stack",
      "description": "The tech stack used to build a canister.",
      "type": "object",
      "properties": {
        "cdk": {
          "title": "cdk",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "language": {
          "title": "language",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "lib": {
          "title": "lib",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "other": {
          "title": "other",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "tool": {
          "title": "tool",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        }
      }
    },
    "WasmOptLevel": {
      "title": "Wasm Optimization Levels",
      "description": "Wasm optimization levels that are passed to `wasm-opt`. \"cycles\" defaults to O3, \"size\" defaults to Oz. O4 through O0 focus on performance (with O0 performing no optimizations), and Oz and Os focus on reducing binary size, where Oz is more aggressive than Os. O3 and Oz empirically give best cycle savings and code size savings respectively.",
      "type": "string",
      "enum": [
        "cycles",
        "size",
        "O4",
        "O3",
        "O2",
        "O1",
        "O0",
        "Oz",
        "Os"
      ]
    }
  }
}

-----------------------

/docs/dfx-metadata-schema.json:
-----------------------

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "\"dfx\" metadata.",
  "description": "Standardized metadata for dfx usage.",
  "type": "object",
  "properties": {
    "pullable": {
      "title": "Pullable",
      "description": "The required information so that the canister can be pulled using `dfx deps pull`.",
      "anyOf": [
        {
          "$ref": "#/definitions/Pullable"
        },
        {
          "type": "null"
        }
      ]
    },
    "tech_stack": {
      "title": "Tech Stack",
      "description": "The tech stack information of the canister.",
      "anyOf": [
        {
          "$ref": "#/definitions/TechStack"
        },
        {
          "type": "null"
        }
      ]
    }
  },
  "definitions": {
    "Pullable": {
      "type": "object",
      "required": [
        "dependencies",
        "init_guide",
        "wasm_url"
      ],
      "properties": {
        "dependencies": {
          "title": "dependencies",
          "description": "Canister IDs (Principal) of direct dependencies.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "init_arg": {
          "title": "init_arg",
          "description": "A default initialization argument for the canister that consumers can use.",
          "type": [
            "string",
            "null"
          ]
        },
        "init_guide": {
          "title": "init_guide",
          "description": "A message to guide consumers how to initialize the canister.",
          "type": "string"
        },
        "wasm_hash": {
          "title": "wasm_hash",
          "description": "SHA256 hash of the wasm module located at wasm_url. Only define this if the on-chain canister wasm is expected not to match the wasm at wasm_url. The hash can also be specified via a URL using the `wasm_hash_url` field. If both are defined, the `wasm_hash_url` field will be ignored.",
          "type": [
            "string",
            "null"
          ]
        },
        "wasm_hash_url": {
          "title": "wasm_hash_url",
          "description": "Specify the SHA256 hash of the wasm module via this URL. Only define this if the on-chain canister wasm is expected not to match the wasm at wasm_url. The hash can also be specified directly using the `wasm_hash` field. If both are defined, the `wasm_hash_url` field will be ignored.",
          "type": [
            "string",
            "null"
          ]
        },
        "wasm_url": {
          "title": "wasm_url",
          "description": "The Url to download canister wasm.",
          "type": "string"
        }
      }
    },
    "TechStack": {
      "title": "Tech Stack",
      "description": "The tech stack used to build a canister.",
      "type": "object",
      "properties": {
        "cdk": {
          "title": "cdk",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "language": {
          "title": "language",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "lib": {
          "title": "lib",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "other": {
          "title": "other",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        },
        "tool": {
          "title": "tool",
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "type": "object",
            "additionalProperties": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}

-----------------------

/docs/extension-catalog-schema.json:
-----------------------

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "ExtensionCatalog",
  "type": "object",
  "additionalProperties": {
    "type": "string"
  }
}

-----------------------

/docs/extension-dependencies-schema.json:
-----------------------

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "ExtensionDependencies",
  "type": "object",
  "additionalProperties": {
    "type": "object",
    "additionalProperties": {
      "$ref": "#/definitions/DependencyRequirement"
    }
  },
  "definitions": {
    "DependencyRequirement": {
      "oneOf": [
        {
          "description": "A SemVer version requirement, for example \">=0.17.0, <0.19.0\".",
          "type": "object",
          "required": [
            "version"
          ],
          "properties": {
            "version": {
              "type": "string"
            }
          },
          "additionalProperties": false
        }
      ]
    }
  }
}

-----------------------

/docs/extension-manifest-schema.json:
-----------------------

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "ExtensionManifest",
  "type": "object",
  "required": [
    "categories",
    "homepage",
    "name",
    "summary",
    "version"
  ],
  "properties": {
    "authors": {
      "type": [
        "string",
        "null"
      ]
    },
    "canister_type": {
      "anyOf": [
        {
          "$ref": "#/definitions/ExtensionCanisterType"
        },
        {
          "type": "null"
        }
      ]
    },
    "categories": {
      "type": "array",
      "items": {
        "type": "string"
      }
    },
    "dependencies": {
      "type": [
        "object",
        "null"
      ],
      "additionalProperties": {
        "$ref": "#/definitions/ExtensionDependency"
      }
    },
    "description": {
      "type": [
        "string",
        "null"
      ]
    },
    "download_url_template": {
      "description": "Components of the download url template are: - `{{tag}}`: the tag of the extension release, which will follow the form \"<extension name>-v<extension version>\" - `{{basename}}`: The basename of the release filename, which will follow the form \"<extension name>-<arch>-<platform>\", for example \"nns-x86_64-unknown-linux-gnu\" - `{{archive-format}}`: the format of the archive, for example \"tar.gz\"",
      "default": "https://github.com/dfinity/dfx-extensions/releases/download/{{tag}}/{{basename}}.{{archive-format}}",
      "type": [
        "string",
        "null"
      ]
    },
    "homepage": {
      "type": "string"
    },
    "keywords": {
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    },
    "name": {
      "type": "string"
    },
    "subcommands": {
      "anyOf": [
        {
          "$ref": "#/definitions/ExtensionSubcommandsOpts"
        },
        {
          "type": "null"
        }
      ]
    },
    "summary": {
      "type": "string"
    },
    "version": {
      "type": "string"
    }
  },
  "additionalProperties": false,
  "definitions": {
    "ArgNumberOfValues": {
      "oneOf": [
        {
          "description": "zero or more values",
          "type": "object",
          "required": [
            "Number"
          ],
          "properties": {
            "Number": {
              "type": "integer",
              "format": "uint",
              "minimum": 0.0
            }
          },
          "additionalProperties": false
        },
        {
          "description": "non-inclusive range",
          "type": "object",
          "required": [
            "Range"
          ],
          "properties": {
            "Range": {
              "$ref": "#/definitions/Range_of_uint"
            }
          },
          "additionalProperties": false
        },
        {
          "description": "unlimited values",
          "type": "string",
          "enum": [
            "Unlimited"
          ]
        }
      ]
    },
    "ExtensionCanisterType": {
      "type": "object",
      "properties": {
        "defaults": {
          "description": "Default values for the canister type. These values are used when the user does not provide values in dfx.json. The \"metadata\" field, if present, is appended to the metadata field from dfx.json, which has the effect of providing defaults. The \"tech_stack field, if present, it merged with the tech_stack field from dfx.json, which also has the effect of providing defaults.",
          "default": {},
          "type": "object",
          "additionalProperties": true
        },
        "evaluation_order": {
          "description": "If one field depends on another and both specify a handlebars expression, list the fields in the order that they should be evaluated.",
          "default": [],
          "type": "array",
          "items": {
            "type": "string"
          }
        }
      }
    },
    "ExtensionDependency": {
      "anyOf": [
        {
          "description": "A SemVer version requirement, for example \">=0.17.0\".",
          "type": "string"
        }
      ]
    },
    "ExtensionSubcommandArgOpts": {
      "type": "object",
      "properties": {
        "about": {
          "type": [
            "string",
            "null"
          ]
        },
        "long": {
          "type": [
            "string",
            "null"
          ]
        },
        "multiple": {
          "default": false,
          "deprecated": true,
          "type": "boolean"
        },
        "short": {
          "type": [
            "string",
            "null"
          ],
          "maxLength": 1,
          "minLength": 1
        },
        "values": {
          "default": 1,
          "allOf": [
            {
              "$ref": "#/definitions/ArgNumberOfValues"
            }
          ]
        }
      },
      "additionalProperties": false
    },
    "ExtensionSubcommandOpts": {
      "type": "object",
      "properties": {
        "about": {
          "type": [
            "string",
            "null"
          ]
        },
        "args": {
          "type": [
            "object",
            "null"
          ],
          "additionalProperties": {
            "$ref": "#/definitions/ExtensionSubcommandArgOpts"
          }
        },
        "subcommands": {
          "anyOf": [
            {
              "$ref": "#/definitions/ExtensionSubcommandsOpts"
            },
            {
              "type": "null"
            }
          ]
        }
      },
      "additionalProperties": false
    },
    "ExtensionSubcommandsOpts": {
      "type": "object",
      "additionalProperties": {
        "$ref": "#/definitions/ExtensionSubcommandOpts"
      }
    },
    "Range_of_uint": {
      "type": "object",
      "required": [
        "end",
        "start"
      ],
      "properties": {
        "end": {
          "type": "integer",
          "format": "uint",
          "minimum": 0.0
        },
        "start": {
          "type": "integer",
          "format": "uint",
          "minimum": 0.0
        }
      }
    }
  }
}

-----------------------

/docs/migration/dfx-0.12.0-migration-guide.md:
-----------------------

# dfx 0.12.0 Migration Guide

This guide assumes you have installed dfx 0.12.0.

```bash
$ dfx --version
dfx 0.12.0
```

## Start the system-wide dfx server

Run this command from a directory that is not within any dfx project.  Run this once (not per-project).

```bash
[~] $ dfx start --background
Running dfx start for version 0.12.0
Using the default definition for the 'local' shared network because ~/.config/dfx/networks.json does not exist.
Dashboard: http://localhost:61605/_/dashboard
```

## Migrate each individual project

This section assumes you have a dfx project functioning with 0.11.2.  Follow these directions for each such project.

1. Change into the project's directory
```bash
$ cd ~/w/migration/0.11.2-to-0.12.0/basic
```

2. Remove the `dfx` version specification from dfx.json
If your project's dfx.json contains a `dfx` key, remove it.  It looks like this:
```json
{
  "dfx": "0.11.2"
}
```
Note that dfx will still respect the dfx version if you leave it in (or change it to `"0.12.0"`, but we recommend to remove it.

3. Stop any running dfx server
```bash
$ dfx stop
Using project-specific network 'local' defined in ~/w/migration/0.11.2-to-0.12.0/basic/dfx.json
WARN: Project-specific networks are deprecated and will be removed after February 2023.
```

4. Remove the `local` network, or the entire `networks` field, from dfx.json

If your project's dfx.json `networks` field contains only the `local` network, remove the `networks` key altogether.

```json
{
  "networks": {
    "local": {
      "bind": "127.0.0.1:8000",
      "type": "ephemeral"
    }
  }
}
```

5. Update the webserver port in `webpack.config.js`

Change port 8000 to port 4943 in this section of `webpack.config.js`.  When you're done, it should look like this:

```
  // proxy /api to port 4943 during development
  devServer: {
    proxy: {
      "/api": {
        target: "http://127.0.0.1:4943",
        changeOrigin: true,
        pathRewrite: {
          "^/api": "/api",
        },
      },
    },
```

dfx uses port `4943` by default for the shared local network in order to ensure that it does not accidentally connect to a project local network.
```bash
$ dfx info webserver-port
4943
```

6. Deploy your project

Notice that the Frontend canister URL will have changed:
```bash
$ dfx deploy
...
  Frontend canister via browser
    basic_frontend: http://127.0.0.1:4943/?canisterId=qoctq-giaaa-aaaaa-aaaea-cai
```

## Usage Changes

### Replace any usage of `dfx config`

dfx 0.12.0 removes the `dfx config` command. Please update Bash scripts to use `jq`, PowerShell scripts to use `ConvertTo-Json`, nushell scripts to use `to json`, etc.

### Use `dfx canister update-settings --set-controller` to set a canister's controller

When using `dfx canister update-settings`, it is easy to mistake `--controller` for `--add-controller`. For this reason `--controller` has been renamed to `--set-controller`.

### Use `dfx canister metadata` to retrieve the candid service definition

dfx used to provide the `/_/candid` endpoint to retrieve a canister's candid service definition. We've removed this endpoint. Please use `dfx canister metadata <canister> candid:service` instead.


-----------------------

/docs/migration/dfx-0.14.0-migration-guide.md:
-----------------------

# 0.14.0 Migration Guide - Environment Variables

This is not an immediate breaking change, but there will be a standardization around environment variables. Backwards compatibility will be supported until 0.16.0, but we recommend updating your projects to use the new environment variables.

## Context

Previously, `webpack.config.js` included an `initCanisterEnv` that parsed canister\*ids.json files and mapped them to environment variables. The pattern used there was `<CANISTER_NAME_UPPERCASE>_CANISTER_ID`. This was not consistent with the pattern used in the `dfx` internals, which used `CANISTER_ID_<canister
_name_case_agnostic>`. This was confusing for users, and since uppercase environment variables is a best practice, we have decided to standardize on the `CANISTER_ID_<CANISTER_NAME_UPPERCASE>` pattern across the board.

## Changes

Starting in `dfx 0.14.0`, the auto-generated `index.js` from `dfx generate` will use the new `CANISTER_ID_<CANISTER_NAME_UPPERCASE>`, with a fallback to the old `<CANISTER_NAME_UPPERCASE>_CANISTER_ID` pattern. This will look like this:

```js
/* CANISTER_ID is replaced by webpack based on node environment
 * Note: canister environment variable will be standardized as
 * process.env.CANISTER_ID_<CANISTER_NAME_UPPERCASE>
 * beginning in dfx 0.16.0
 */
export const canisterId =
  process.env.CANISTER_ID_HELLO_BACKEND ||
  process.env.HELLO_BACKEND_CANISTER_ID;
```

This will not break an existing application, but if you are using the environment variables elsewhere throughout your application, you should switch to the new pattern.

## Webpack / Bundler Migration

If your frontend project is using the old environment variables, you will need to update your `webpack.config.js` file to use the new `CANISTER_ID_<CANISTER_NAME_UPPERCASE>` variables, or to use the new webpack config provided in the starter project. We recommend using the new, simplified webpack config, which is provided below:

```js
require("dotenv").config();
const path = require("path");
const webpack = require("webpack");
const HtmlWebpackPlugin = require("html-webpack-plugin");
const TerserPlugin = require("terser-webpack-plugin");
const CopyPlugin = require("copy-webpack-plugin");

const isDevelopment = process.env.NODE_ENV !== "production";

const frontendDirectory = "<your_frontend_directory>";

const frontend_entry = path.join("src", frontendDirectory, "src", "index.html");

module.exports = {
  target: "web",
  mode: isDevelopment ? "development" : "production",
  entry: {
    // The frontend.entrypoint points to the HTML file for this build, so we need
    // to replace the extension to `.js`.
    index: path.join(__dirname, frontend_entry).replace(/\.html$/, ".js"),
  },
  devtool: isDevelopment ? "source-map" : false,
  optimization: {
    minimize: !isDevelopment,
    minimizer: [new TerserPlugin()],
  },
  resolve: {
    extensions: [".js", ".ts", ".jsx", ".tsx"],
    fallback: {
      assert: require.resolve("assert/"),
      buffer: require.resolve("buffer/"),
      events: require.resolve("events/"),
      stream: require.resolve("stream-browserify/"),
      util: require.resolve("util/"),
    },
  },
  output: {
    filename: "index.js",
    path: path.join(__dirname, "dist", frontendDirectory),
  },

  // Depending in the language or framework you are using for
  // front-end development, add module loaders to the default
  // webpack configuration. For example, if you are using React
  // modules and CSS as described in the "Adding a stylesheet"
  // tutorial, uncomment the following lines:
  // module: {
  //  rules: [
  //    { test: /\.(ts|tsx|jsx)$/, loader: "ts-loader" },
  //    { test: /\.css$/, use: ['style-loader','css-loader'] }
  //  ]
  // },
  plugins: [
    new HtmlWebpackPlugin({
      template: path.join(__dirname, frontend_entry),
      cache: false,
    }),
    new webpack.EnvironmentPlugin([
      ...Object.keys(process.env).filter((key) => {
        if (key.includes("CANISTER")) return true;
        if (key.includes("DFX")) return true;
        return false;
      }),
    ]),
    new webpack.ProvidePlugin({
      Buffer: [require.resolve("buffer/"), "Buffer"],
      process: require.resolve("process/browser"),
    }),
    new CopyPlugin({
      patterns: [
        {
          from: `src/${frontendDirectory}/src/.ic-assets.json*`,
          to: ".ic-assets.json5",
          noErrorOnMissing: true,
        },
      ],
    }),
  ],
  // proxy /api to port 4943 during development.
  // if you edit dfx.json to define a project-specific local network, change the port to match.
  devServer: {
    proxy: {
      "/api": {
        target: "http://127.0.0.1:4943",
        changeOrigin: true,
        pathRewrite: {
          "^/api": "/api",
        },
      },
    },
    static: path.resolve(__dirname, "src", frontendDirectory, "assets"),
    hot: true,
    watchFiles: [path.resolve(__dirname, "src", frontendDirectory)],
    liveReload: true,
  },
};
```

If you prefer to modify an existing webpack config, you will need to update the `initCanisterEnv` function to use the new `CANISTER_ID_<CANISTER_NAME_UPPERCASE>` variables.

For example, if your frontend project is using the following mapping of environment variables inside of `initCanisterEnv`:

```js
prev[canisterName.toUpperCase() + "_CANISTER_ID"] = canisterDetails[network];
````

You should update your `webpack.config.js` file to use the new `CANISTER_ID_<CANISTER_NAME_UPPERCASE>` variables:

```js
prev[`CANISTER_ID_${canisterName.toUpperCase()}`] = canisterDetails[network];
```

Alternately, if you are using the new `.env` file support, you can install `dotenv` and imitate the new webpack config provided above.


-----------------------

/docs/migration/dfx-0.15-migration-guide.md:
-----------------------

# dfx 0.15.0 Migration Guide

## Restrict new identity names to safe characters

New identities like `dfx identity new my/identity` or `dfx identity new 'my identity'` can easily lead to problems, either for dfx internals or for usability.
New identities are now restricted to the characters `ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.-_@0123456789`.
Existing identities are not affected by this change.

## Use `dfx start` rather than `dfx bootstrap` and `dfx replica`

If you have been using `dfx bootstrap` and `dfx replica`, use `dfx start` instead.  If you have a good reason why we should keep these commands, please contribute to the discussion at https://github.com/dfinity/sdk/discussions/3163

## Install nns and sns extensions if using this functionality

We've removed the `dfx nns` and `dfx sns` commands from dfx. Both have now been turned into dfx extensions. In order to obtain them, please run `dfx extension install nns` and `dfx extension install sns` respectively. After the installation, you can use them as you did before: `dfx nns ...`, and `dfx sns ...`.

## frontend canister: Delete assets before re-creating them

You don't have to change anything if you are using `dfx deploy`, icx-asset sync`, or the ic-asset crate `sync` methods.  All of these already take care of this, in current and previous versions.

The CreateAsset operation and create_asset method now fail if the asset already exists.  Previously, these were no-op if the content type matched.

For an asset that already exists:
- before adding a `CreateAssetOperation`, add a `DeleteAssetOperation`
- before calling `create_asset()`, call `delete_asset()`

## frontend canister: Always pass Some(sha256) parameter to `http_request_streaming_callback` and `get_chunk`

For http_request and http_request_streaming_callback, there should be no change: all callers of http_request_streaming_callback should be passing the entire token returned by http_request, which includes the sha256 parameter.

Any callers of `get_chunk` should make sure to always pass the `sha256` value returned by the `get` method.  It will always be present.


-----------------------

/docs/migration/dfx-0.17.0-migration-guide.md:
-----------------------

# dfx 0.17.0 Migration Guide

## Use dfxvm rather than `dfx toolchain`

We've removed the `dfx toolchain` command.

Please use the [dfx version manager][dfxvm] to manage dfx versions instead.

## Deprecation of `dfx upgrade`

The `dfx upgrade` command is deprecated.  After the release of the
[dfx version manager][dfxvm], the `dfx upgrade` command will no longer be
available for use. It will instead print an error message and exit.

## Release tarball format

If you've been manually downloading release tarballs, please note that
we have begun to release tarballs in a new format and will stop releasing
the old format in the future.

The new format contains a top-level directory with the same name as the
basename of the tarball. The filenames no longer contain the version number.

Example links to tarballs in the new format:

- [dfx-x86_64-apple-darwin.tar.gz](https://github.com/dfinity/sdk/releases/download/0.16.1/dfx-x86_64-apple-darwin.tar.gz_)
- [dfx-x86_64-unknown-linux-gnu.tar.gz](https://github.com/dfinity/sdk/releases/download/0.16.1/dfx-x86_64-unknown-linux-gnu.tar.gz)

[dfxvm]: https://github.com/dfinity/dfxvm


-----------------------

/docs/migration/dfx-0.18.0-migration-guide.md:
-----------------------

# dfx 0.18.0 Migration Guide

## Use dfxvm rather than `dfx upgrade`

We've removed the `dfx upgrade` command.  Please use the [dfx version manager][dfxvm] to manage dfx versions instead.

[dfxvm]: https://github.com/dfinity/dfxvm

## Use standard canister ID and candid path environment variable names

This version no longer provides environment variable name formats
that we deprecated in dfx 0.14.0.

If you are using environment variables to reference canister IDs and candid paths,
you may need to update your environment variable names.

The only variable names now provided are the following,
all uppercase, with any '-' replaced by '_':
- `CANISTER_CANDID_PATH_<CANISTER_NAME>`
- `CANISTER_ID_<CANISTER_NAME>`

Rust canisters may need to upgrade their dependencies to at least:

```toml
candid = "0.10"
ic-cdk = "0.12"
```


-----------------------

/docs/migration/dfx-0.23.0-migration-guide.md:
-----------------------

# dfx 0.23.0 Migration Guide

## New frontend security header mechanism

It is possible to define custom headers for frontend assets in `.ic-assets.json5` files.
Previously, the default projects provided by `dfx new` included a default set of security headers like [this](https://github.com/dfinity/sdk/blob/b8ff661785e6979b2a155c525f91737ad058482a/src/dfx/assets/project_templates/vue/src/__frontend_name__/public/.ic-assets.json5).
While this is a solid default, it is likely that these headers will receive updates in the future and many projects will forget to update their security headers.
Additionally, it is unlikely that many projects actually improve over the defaults with the individually defined headers.

`dfx` version `0.23.0` introduces a new field `"security_policy"` to `.ic-assets.json5`,
which can automatically provide all the default security headers and updates that will be shipped in future versions of `dfx`.
You can check the default security headers with `dfx info security-policy`.

### If your `.ic-assets.json5` does not exist or does not use any security headers

If your `.ic-assets.json5` does not exist at all or does not use any security headers and you want to keep it that way, you don't need to do anything.
However, `dfx` will print a warning when you deploy your frontend that you didn't define a security policy.
If you want to get rid of the warning, you can add this to your `.ic-assets.json5`:

```json5
{
    "match": "**/*",
    "security_policy": "disabled"
}
```

This will instruct dfx to not add any security headers and to not print the warning anymore.

If your `.ic-assets.json5` does not exist or does not use any security headers and you would like to add the default set of headers, you can add this to your `.ic-assets.json5`:

```json5
{
    "match": "**/*",
    "security_policy": "standard",

    // Uncomment to disable the warning about using the
    // standard security policy, if you understand the risk
    // "disable_security_policy_warning": true,
}
```

### If your `.ic-assets.json5` uses the default security headers

`dfx info security-policy` will print the default security headers.

If your `.ic-assets.json5` uses the default security headers without modification, replace the headers (looks like [this](https://github.com/dfinity/sdk/blob/b8ff661785e6979b2a155c525f91737ad058482a/src/dfx/assets/project_templates/vue/src/__frontend_name__/public/.ic-assets.json5#L5-L48)) with this:

```json5
{
    "match": "**/*",
    "security_policy": "standard",

    // Uncomment to disable the warning about using the
    // standard security policy, if you understand the risk
    // "disable_security_policy_warning": true,
}
```

### If your `.ic-assets.json5` uses custom security headers

`dfx info security-policy` will print the default security headers.

If your `.ic-assets.json5` uses the default security headers with custom improvements, remove the headers that are the same as the default security headers.
Leave the headers that have custom improvements, and set `"security_policy": "hardened"`, like this:

```json5
{
    "match": "**/*",
    "security_policy": "hardened",
    "headers": {
        // this is an improvement over the default CSP
        "Content-Security-Policy": "super-secure-csp"
    }
}
```

-----------------------

/docs/migration/index.md:
-----------------------

# DFX Migration Guide

- [dfx 0.23.0](./dfx-0.23.0-migration-guide.md)
- [dfx 0.18.0](./dfx-0.18.0-migration-guide.md)
- [dfx 0.17.0](./dfx-0.17.0-migration-guide.md)
- [dfx 0.15.0](./dfx-0.15.0-migration-guide.md)
- [dfx 0.14.0](./dfx-0.14.0-migration-guide.md)
- [dfx 0.12.0](./dfx-0.12.0-migration-guide.md)


-----------------------

/docs/networks-json-schema.json:
-----------------------

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Map_of_ConfigNetwork",
  "type": "object",
  "additionalProperties": {
    "$ref": "#/definitions/ConfigNetwork"
  },
  "definitions": {
    "BitcoinAdapterLogLevel": {
      "description": "Represents the log level of the bitcoin adapter.",
      "type": "string",
      "enum": [
        "critical",
        "error",
        "warning",
        "info",
        "debug",
        "trace"
      ]
    },
    "ConfigDefaultsBitcoin": {
      "title": "Bitcoin Adapter Configuration",
      "type": "object",
      "properties": {
        "canister_init_arg": {
          "title": "Initialization Argument",
          "description": "The initialization argument for the bitcoin canister.",
          "default": "(record { stability_threshold = 0 : nat; network = variant { regtest }; blocks_source = principal \"aaaaa-aa\"; fees = record { get_utxos_base = 0 : nat; get_utxos_cycles_per_ten_instructions = 0 : nat; get_utxos_maximum = 0 : nat; get_balance = 0 : nat; get_balance_maximum = 0 : nat; get_current_fee_percentiles = 0 : nat; get_current_fee_percentiles_maximum = 0 : nat;  send_transaction_base = 0 : nat; send_transaction_per_byte = 0 : nat; }; syncing = variant { enabled }; api_access = variant { enabled }; disable_api_if_not_fully_synced = variant { enabled }})",
          "type": "string"
        },
        "enabled": {
          "title": "Enable Bitcoin Adapter",
          "default": false,
          "type": "boolean"
        },
        "log_level": {
          "title": "Logging Level",
          "description": "The logging level of the adapter.",
          "default": "info",
          "allOf": [
            {
              "$ref": "#/definitions/BitcoinAdapterLogLevel"
            }
          ]
        },
        "nodes": {
          "title": "Available Nodes",
          "description": "Addresses of nodes to connect to (in case discovery from seeds is not possible/sufficient).",
          "type": [
            "array",
            "null"
          ],
          "items": {
            "type": "string"
          }
        }
      }
    },
    "ConfigDefaultsBootstrap": {
      "title": "Bootstrap Server Configuration",
      "description": "The bootstrap command has been removed.  All of these fields are ignored.",
      "type": "object",
      "properties": {
        "ip": {
          "description": "Specifies the IP address that the bootstrap server listens on. Defaults to 127.0.0.1.",
          "default": "127.0.0.1",
          "type": "string",
          "format": "ip"
        },
        "port": {
          "description": "Specifies the port number that the bootstrap server listens on. Defaults to 8081.",
          "default": 8081,
          "type": "integer",
          "format": "uint16",
          "minimum": 0.0
        },
        "timeout": {
          "description": "Specifies the maximum number of seconds that the bootstrap server will wait for upstream requests to complete. Defaults to 30.",
          "default": 30,
          "type": "integer",
          "format": "uint64",
          "minimum": 0.0
        }
      }
    },
    "ConfigDefaultsCanisterHttp": {
      "title": "HTTP Adapter Configuration",
      "type": "object",
      "properties": {
        "enabled": {
          "title": "Enable HTTP Adapter",
          "default": true,
          "type": "boolean"
        },
        "log_level": {
          "title": "Logging Level",
          "description": "The logging level of the adapter.",
          "default": "error",
          "allOf": [
            {
              "$ref": "#/definitions/HttpAdapterLogLevel"
            }
          ]
        }
      }
    },
    "ConfigDefaultsProxy": {
      "description": "Configuration for the HTTP gateway.",
      "type": "object",
      "properties": {
        "domain": {
          "description": "A list of domains that can be served. These are used for canister resolution [default: localhost]",
          "anyOf": [
            {
              "$ref": "#/definitions/SerdeVec_for_String"
            },
            {
              "type": "null"
            }
          ]
        }
      }
    },
    "ConfigDefaultsReplica": {
      "title": "Local Replica Configuration",
      "type": "object",
      "properties": {
        "log_level": {
          "description": "Run replica with the provided log level. Default is 'error'. Debug prints still get displayed",
          "anyOf": [
            {
              "$ref": "#/definitions/ReplicaLogLevel"
            },
            {
              "type": "null"
            }
          ]
        },
        "port": {
          "description": "Port the replica listens on.",
          "type": [
            "integer",
            "null"
          ],
          "format": "uint16",
          "minimum": 0.0
        },
        "subnet_type": {
          "title": "Subnet Type",
          "description": "Determines the subnet type the replica will run as. Affects things like cycles accounting, message size limits, cycle limits. Defaults to 'application'.",
          "anyOf": [
            {
              "$ref": "#/definitions/ReplicaSubnetType"
            },
            {
              "type": "null"
            }
          ]
        }
      }
    },
    "ConfigLocalProvider": {
      "title": "Local Replica Configuration",
      "type": "object",
      "properties": {
        "bind": {
          "description": "Bind address for the webserver. For the shared local network, the default is 127.0.0.1:4943. For project-specific local networks, the default is 127.0.0.1:8000.",
          "type": [
            "string",
            "null"
          ]
        },
        "bitcoin": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsBitcoin"
            },
            {
              "type": "null"
            }
          ]
        },
        "bootstrap": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsBootstrap"
            },
            {
              "type": "null"
            }
          ]
        },
        "canister_http": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsCanisterHttp"
            },
            {
              "type": "null"
            }
          ]
        },
        "playground": {
          "anyOf": [
            {
              "$ref": "#/definitions/PlaygroundConfig"
            },
            {
              "type": "null"
            }
          ]
        },
        "proxy": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsProxy"
            },
            {
              "type": "null"
            }
          ]
        },
        "replica": {
          "anyOf": [
            {
              "$ref": "#/definitions/ConfigDefaultsReplica"
            },
            {
              "type": "null"
            }
          ]
        },
        "type": {
          "description": "Persistence type of this network.",
          "default": "ephemeral",
          "allOf": [
            {
              "$ref": "#/definitions/NetworkType"
            }
          ]
        }
      }
    },
    "ConfigNetwork": {
      "anyOf": [
        {
          "$ref": "#/definitions/ConfigNetworkProvider"
        },
        {
          "$ref": "#/definitions/ConfigLocalProvider"
        }
      ]
    },
    "ConfigNetworkProvider": {
      "title": "Custom Network Configuration",
      "type": "object",
      "required": [
        "providers"
      ],
      "properties": {
        "playground": {
          "anyOf": [
            {
              "$ref": "#/definitions/PlaygroundConfig"
            },
            {
              "type": "null"
            }
          ]
        },
        "providers": {
          "description": "The URL(s) this network can be reached at.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "type": {
          "description": "Persistence type of this network.",
          "default": "persistent",
          "allOf": [
            {
              "$ref": "#/definitions/NetworkType"
            }
          ]
        }
      }
    },
    "HttpAdapterLogLevel": {
      "description": "Represents the log level of the HTTP adapter.",
      "type": "string",
      "enum": [
        "critical",
        "error",
        "warning",
        "info",
        "debug",
        "trace"
      ]
    },
    "NetworkType": {
      "title": "Network Type",
      "description": "Type 'ephemeral' is used for networks that are regularly reset. Type 'persistent' is used for networks that last for a long time and where it is preferred that canister IDs get stored in source control.",
      "type": "string",
      "enum": [
        "ephemeral",
        "persistent"
      ]
    },
    "PlaygroundConfig": {
      "description": "Playground config to borrow canister from instead of creating new canisters.",
      "type": "object",
      "required": [
        "playground_canister"
      ],
      "properties": {
        "playground_canister": {
          "description": "Canister ID of the playground canister",
          "type": "string"
        },
        "timeout_seconds": {
          "description": "How many seconds a canister can be borrowed for",
          "default": 1200,
          "type": "integer",
          "format": "uint64",
          "minimum": 0.0
        }
      }
    },
    "ReplicaLogLevel": {
      "type": "string",
      "enum": [
        "critical",
        "error",
        "warning",
        "info",
        "debug",
        "trace"
      ]
    },
    "ReplicaSubnetType": {
      "type": "string",
      "enum": [
        "system",
        "application",
        "verifiedapplication"
      ]
    },
    "SerdeVec_for_String": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "array",
          "items": {
            "type": "string"
          }
        }
      ]
    }
  }
}

-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/docs/process/automerge-label.png

-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/docs/process/is-master-green.png

-----------------------

/docs/process/release.md:
-----------------------

# IC SDK Release Process

## Stage 1: Preparation - day 1

### Update the Replica Version

Click the "Run workflow" button on the [Update Replica page](https://github.com/dfinity/sdk/actions/workflows/update-replica-version.yml) workflow.

Obtain approval and merge the PR.

### Update the changelog

Open a PR to `master`. Roll the changelog by adding a new header for the
new dfx version underneath the "# Unreleased" header.  Further changes to dfx
should be added under the "#Unreleased" header, unless they are ported to
the release branch.

[Sample PR](https://github.com/dfinity/sdk/pull/3486)

### Create the Release Branch

Create a release branch from `master`, for example `release-0.15.3`.

If you create a new patch version make sure there will be no breaking changes included.

This branch will be used to create beta releases as well as the final release.

## Stage 2: Beta releases - day 1 ~ 2

1. Check out the release branch;
1. Run the release script, for example `./scripts/release.sh 0.15.3-beta.0`.
   It will:
    - Build `dfx` from clean;
    - Validate the default project interactively;
    - Create a beta release branch which update the version number in manifest files and push to GitHub;
    - Wait for you to:
        - Create a PR from the new branch to the release branch,
      e.g. into `release-0.15.3` from `adam/release-0.15.3-beta.0`;
        - Obtain approval and merge the PR;
    - Push a tag which triggers the [publish][publish-workflow] workflow;
1. Wait for the [publish][publish-workflow] workflow to create the GitHub release;
1. Update the GitHub release:
    - Copy/paste the changelog section for the new version into the release notes;
    - Make sure that the "Pre-release" flag **is** set and the "Latest" flag is **NOT** set;
1. Announce the release to #eng-sdk with a message like this:
    > dfx 0.15.3-beta.1 is available for manual installation and testing.
    >
    > ```bash
    > dfxvm install 0.15.3-beta.1
    > ```
    >
    > See also release notes.
1. Post a message to the forum about availability of the not-yet-promoted beta, linking to the GitHub release notes.

[Sample PR](https://github.com/dfinity/sdk/pull/3477)

[publish-workflow]: https://github.com/dfinity/sdk/blob/master/.github/workflows/publish.yml

## Stage 3: Final Release - day 3

Once the beta releases are ready to be promoted:

1. Check out the release branch;
1. Run the release script, for example `./scripts/release.sh 0.15.3`;
1. Follow the same steps as for the beta releases;

[Sample PR](https://github.com/dfinity/sdk/pull/3490)

## Stage 4: Draft PRs to prepare for promotion - day 3

All following PRs should be created as "draft".

Obtain approval, but do not merge them yet.

### Promote the release in [sdk](https://github.com/dfinity/sdk)

1. Create a new branch from the release branch, for example `release-0.15.3-promote`;
1. Update the [version manifest](https://github.com/dfinity/sdk/blob/master/public/manifest.json):
    - Set `.tags.latest` to the new dfx version;
    - Remove the beta releases from the `versions` array;
1. Open a PR from this branch to `master`;

[Sample PR](https://github.com/dfinity/sdk/pull/3491)

### Update the [portal](https://github.com/dfinity/portal) release notes and sdk submodule

- Add a link to the [release-notes-table](https://github.com/dfinity/portal/blob/master/docs/other/updates/release-notes/release-notes.md);
    - Also include the link of the migration guide if it is available;
- Update the sdk submodule:
    1. Change to the sdk submodule: `cd submodules/sdk`
    1. Checkout the release branch, e.g. `git checkout release-0.18.0`
    1. Go back to project root and commit the submodule change.

[Sample PR](https://github.com/dfinity/portal/pull/2330)

### Update the [motoko-playground][motoko-playground] frontend canister hash whitelist

- Click the "Run workflow" button on the [Broadcast Frontend Hash page](https://github.com/dfinity/sdk/actions/workflows/broadcast-frontend-hash.yml).
- Fill "Release version of dfx" with the version of this release.
- The workflow will create a PR in the [motoko-playground][motoko-playground] repo.

[Sample PR](https://github.com/dfinity/motoko-playground/pull/217)

[motoko-playground]: https://github.com/dfinity/motoko-playground

### Update the [examples](https://github.com/dfinity/examples) default dfx

Modify `DFX_VERSION` in these two files:

- [provision-darwin.sh](https://github.com/dfinity/examples/blob/master/.github/workflows/provision-darwin.sh)
- [provision-linux.sh](https://github.com/dfinity/examples/blob/master/.github/workflows/provision-linux.sh)

[Sample PR](https://github.com/dfinity/examples/pull/704)

## Stage 5: Promote the release - day 4

### Update the GitHub release

- Unset the "Pre-release" flag
- Set the "Latest" flag

### Merge PRs

Merge all 4 PRs created in the previous stage:

- Promote the release
- Update the portal
- Update the motoko-playground
- Update the examples

### Post to the forum

Post a message to the forum, linking to the GitHub release notes.

[Sample Post](https://forum.dfinity.org/t/dfx-0-17-0-is-promoted)


-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/archive/frontend/0.12.1/assetstorage.wasm.gz

-----------------------

/e2e/archive/wallet/0.10.0/wallet.did:
-----------------------

type EventKind = variant {
  CyclesSent: record {
    to: principal;
    amount: nat64;
    refund: nat64;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat64;
    memo: opt text;
  };
  AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat64;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat64;
  };
  WalletDeployed: record {
    canister: principal;
  }
};

type EventKind128 = variant {
  CyclesSent: record {
    to: principal;
    amount: nat;
    refund: nat;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat;
    memo: opt text;
  };
    AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat;
  };
  WalletDeployed: record {
    canister: principal;
  };
};

type Event = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind;
};

type Event128 = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind128;
};

type Role = variant {
  Contact;
  Custodian;
  Controller;
};

type Kind = variant {
  Unknown;
  User;
  Canister;
};

// An entry in the address book. It must have an ID and a role.
type AddressEntry = record {
  id: principal;
  name: opt text;
  kind: Kind;
  role: Role;
};

type ManagedCanisterInfo = record {
  id: principal;
  name: opt text;
  created_at: nat64;
};

type ManagedCanisterEventKind = variant {
  CyclesSent: record {
    amount: nat64;
    refund: nat64;
  };
  Called: record {
    method_name: text;
    cycles: nat64;
  };
  Created: record {
    cycles: nat64;
  };
};

type ManagedCanisterEventKind128 = variant {
  CyclesSent: record {
    amount: nat;
    refund: nat;
  };
  Called: record {
    method_name: text;
    cycles: nat;
  };
  Created: record {
    cycles: nat;
  };
};

type ManagedCanisterEvent = record {
  id: nat32;
  timestamp: nat64;
  kind: ManagedCanisterEventKind;
};

type ManagedCanisterEvent128 = record {
  id: nat32;
  timestamp: nat64;
  kind: ManagedCanisterEventKind128;
};

type ReceiveOptions = record {
  memo: opt text;
};

type WalletResultCreate = variant {
  Ok : record { canister_id: principal };
  Err: text;
};

type WalletResult = variant {
  Ok : null;
  Err : text;
};

type WalletResultCall = variant {
  Ok : record { return: blob };
  Err : text;
};

type CanisterSettings = record {
  controller: opt principal;
  controllers: opt vec principal;
  compute_allocation: opt nat;
  memory_allocation: opt nat;
  freezing_threshold: opt nat;
};

type CreateCanisterArgs = record {
  cycles: nat64;
  settings: CanisterSettings;
};

type CreateCanisterArgs128 = record {
  cycles: nat;
  settings: CanisterSettings;
};

// Assets
type HeaderField = record { text; text; };

type HttpRequest = record {
  method: text;
  url: text;
  headers: vec HeaderField;
  body: blob;
};

type HttpResponse = record {
  status_code: nat16;
  headers: vec HeaderField;
  body: blob;
  streaming_strategy: opt StreamingStrategy;
};

type StreamingCallbackHttpResponse = record {
  body: blob;
  token: opt Token;
};

type Token = record {};

type StreamingStrategy = variant {
  Callback: record {
    callback: func (Token) -> (StreamingCallbackHttpResponse) query;
    token: Token;
  };
};

service : {
  wallet_api_version: () -> (text) query;

  // Wallet Name
  name: () -> (opt text) query;
  set_name: (text) -> ();

  // Controller Management
  get_controllers: () -> (vec principal) query;
  add_controller: (principal) -> ();
  remove_controller: (principal) -> (WalletResult);

  // Custodian Management
  get_custodians: () -> (vec principal) query;
  authorize: (principal) -> ();
  deauthorize: (principal) -> (WalletResult);

  // Cycle Management
  wallet_balance: () -> (record { amount: nat64 }) query;
  wallet_balance128: () -> (record { amount: nat }) query;
  wallet_send: (record { canister: principal; amount: nat64 }) -> (WalletResult);
  wallet_send128: (record { canister: principal; amount: nat }) -> (WalletResult);
  wallet_receive: (opt ReceiveOptions) -> ();  // Endpoint for receiving cycles.

  // Managing canister
  wallet_create_canister: (CreateCanisterArgs) -> (WalletResultCreate);
  wallet_create_canister128: (CreateCanisterArgs128) -> (WalletResultCreate);

  wallet_create_wallet: (CreateCanisterArgs) -> (WalletResultCreate);
  wallet_create_wallet128: (CreateCanisterArgs128) -> (WalletResultCreate);

  wallet_store_wallet_wasm: (record {
    wasm_module: blob;
  }) -> ();

  // Call Forwarding
  wallet_call: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat64;
  }) -> (WalletResultCall);
  wallet_call128: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat;
  }) -> (WalletResultCall);

  // Address book
  add_address: (address: AddressEntry) -> ();
  list_addresses: () -> (vec AddressEntry) query;
  remove_address: (address: principal) -> (WalletResult);

  // Events
  // If `from` is not specified, it will start 20 from the end; if `to` is not specified, it will stop at the end
  get_events: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event) query;
  get_events128: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event128) query;
  get_chart: (opt record { count: opt nat32; precision: opt nat64; } ) -> (vec record { nat64; nat64; }) query;

  // Managed canisters
  list_managed_canisters: (record { from: opt nat32; to: opt nat32; }) -> (vec ManagedCanisterInfo, nat32) query;
  // If `from` is not specified, it will start 20 from the end; if `to` is not specified, it will stop at the end
  get_managed_canister_events: (record { canister: principal; from: opt nat32; to: opt nat32; }) -> (opt vec ManagedCanisterEvent) query;
  get_managed_canister_events128: (record { canister: principal; from: opt nat32; to: opt nat32; }) -> (opt vec ManagedCanisterEvent128) query;
  set_short_name: (principal, opt text) -> (opt ManagedCanisterInfo);

  // Assets
  http_request: (request: HttpRequest) -> (HttpResponse) query;
}


-----------------------

/e2e/archive/wallet/0.10.0/wallet.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/archive/wallet/0.10.0/wallet.wasm

-----------------------

/e2e/archive/wallet/0.12.0/wallet.did:
-----------------------

type EventKind = variant {
  CyclesSent: record {
    to: principal;
    amount: nat64;
    refund: nat64;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat64;
    memo: opt text;
  };
  AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat64;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat64;
  };
  WalletDeployed: record {
    canister: principal;
  }
};

type EventKind128 = variant {
  CyclesSent: record {
    to: principal;
    amount: nat;
    refund: nat;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat;
    memo: opt text;
  };
    AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat;
  };
  WalletDeployed: record {
    canister: principal;
  };
};

type Event = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind;
};

type Event128 = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind128;
};

type Role = variant {
  Contact;
  Custodian;
  Controller;
};

type Kind = variant {
  Unknown;
  User;
  Canister;
};

// An entry in the address book. It must have an ID and a role.
type AddressEntry = record {
  id: principal;
  name: opt text;
  kind: Kind;
  role: Role;
};

type ManagedCanisterInfo = record {
  id: principal;
  name: opt text;
  created_at: nat64;
};

type ManagedCanisterEventKind = variant {
  CyclesSent: record {
    amount: nat64;
    refund: nat64;
  };
  Called: record {
    method_name: text;
    cycles: nat64;
  };
  Created: record {
    cycles: nat64;
  };
};

type ManagedCanisterEventKind128 = variant {
  CyclesSent: record {
    amount: nat;
    refund: nat;
  };
  Called: record {
    method_name: text;
    cycles: nat;
  };
  Created: record {
    cycles: nat;
  };
};

type ManagedCanisterEvent = record {
  id: nat32;
  timestamp: nat64;
  kind: ManagedCanisterEventKind;
};

type ManagedCanisterEvent128 = record {
  id: nat32;
  timestamp: nat64;
  kind: ManagedCanisterEventKind128;
};

type ReceiveOptions = record {
  memo: opt text;
};

type WalletResultCreate = variant {
  Ok : record { canister_id: principal };
  Err: text;
};

type WalletResult = variant {
  Ok : null;
  Err : text;
};

type WalletResultCall = variant {
  Ok : record { return: blob };
  Err : text;
};

type CanisterSettings = record {
  controller: opt principal;
  controllers: opt vec principal;
  compute_allocation: opt nat;
  memory_allocation: opt nat;
  freezing_threshold: opt nat;
};

type CreateCanisterArgs = record {
  cycles: nat64;
  settings: CanisterSettings;
};

type CreateCanisterArgs128 = record {
  cycles: nat;
  settings: CanisterSettings;
};

// Assets
type HeaderField = record { text; text; };

type HttpRequest = record {
  method: text;
  url: text;
  headers: vec HeaderField;
  body: blob;
};

type HttpResponse = record {
  status_code: nat16;
  headers: vec HeaderField;
  body: blob;
  streaming_strategy: opt StreamingStrategy;
};

type StreamingCallbackHttpResponse = record {
  body: blob;
  token: opt Token;
};

type Token = record {};

type StreamingStrategy = variant {
  Callback: record {
    callback: func (Token) -> (StreamingCallbackHttpResponse) query;
    token: Token;
  };
};

service : {
  wallet_api_version: () -> (text) query;

  // Wallet Name
  name: () -> (opt text) query;
  set_name: (text) -> ();

  // Controller Management
  get_controllers: () -> (vec principal) query;
  add_controller: (principal) -> ();
  remove_controller: (principal) -> (WalletResult);

  // Custodian Management
  get_custodians: () -> (vec principal) query;
  authorize: (principal) -> ();
  deauthorize: (principal) -> (WalletResult);

  // Cycle Management
  wallet_balance: () -> (record { amount: nat64 }) query;
  wallet_balance128: () -> (record { amount: nat }) query;
  wallet_send: (record { canister: principal; amount: nat64 }) -> (WalletResult);
  wallet_send128: (record { canister: principal; amount: nat }) -> (WalletResult);
  wallet_receive: (opt ReceiveOptions) -> ();  // Endpoint for receiving cycles.

  // Managing canister
  wallet_create_canister: (CreateCanisterArgs) -> (WalletResultCreate);
  wallet_create_canister128: (CreateCanisterArgs128) -> (WalletResultCreate);

  wallet_create_wallet: (CreateCanisterArgs) -> (WalletResultCreate);
  wallet_create_wallet128: (CreateCanisterArgs128) -> (WalletResultCreate);

  wallet_store_wallet_wasm: (record {
    wasm_module: blob;
  }) -> ();

  // Call Forwarding
  wallet_call: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat64;
  }) -> (WalletResultCall);
  wallet_call128: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat;
  }) -> (WalletResultCall);

  // Address book
  add_address: (address: AddressEntry) -> ();
  list_addresses: () -> (vec AddressEntry) query;
  remove_address: (address: principal) -> (WalletResult);

  // Events
  // If `from` is not specified, it will start 20 from the end; if `to` is not specified, it will stop at the end
  get_events: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event) query;
  get_events128: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event128) query;
  get_chart: (opt record { count: opt nat32; precision: opt nat64; } ) -> (vec record { nat64; nat64; }) query;

  // Managed canisters
  list_managed_canisters: (record { from: opt nat32; to: opt nat32; }) -> (vec ManagedCanisterInfo, nat32) query;
  // If `from` is not specified, it will start 20 from the end; if `to` is not specified, it will stop at the end
  get_managed_canister_events: (record { canister: principal; from: opt nat32; to: opt nat32; }) -> (opt vec ManagedCanisterEvent) query;
  get_managed_canister_events128: (record { canister: principal; from: opt nat32; to: opt nat32; }) -> (opt vec ManagedCanisterEvent128) query;
  set_short_name: (principal, opt text) -> (opt ManagedCanisterInfo);

  // Assets
  http_request: (request: HttpRequest) -> (HttpResponse) query;
}

-----------------------

/e2e/archive/wallet/0.12.0/wallet.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/archive/wallet/0.12.0/wallet.wasm

-----------------------

/e2e/archive/wallet/0.7.0-beta.5/wallet.did:
-----------------------

type EventKind = variant {
  CyclesSent: record {
    to: principal;
    amount: nat64;
    refund: nat64;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat64;
  };
  AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat64;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat64;
  };
  WalletDeployed: record {
    canister: principal;
  }
};

type Event = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind;
};

type Role = variant {
  Contact;
  Custodian;
  Controller;
};

type Kind = variant {
  Unknown;
  User;
  Canister;
};

// An entry in the address book. It must have an ID and a role.
type AddressEntry = record {
  id: principal;
  name: opt text;
  kind: Kind;
  role: Role;
};

type ResultCreate = variant {
  Ok : record { canister_id: principal };
  Err: text;
};

type ResultSend = variant {
  Ok : null;
  Err : text;
};

type ResultCall = variant {
  Ok : record { return: blob };
  Err : text;
};

type CanisterSettings = record {
  controller: opt principal;
  compute_allocation: opt nat;
  memory_allocation: opt nat;
  freezing_threshold: opt nat;
};

type CreateCanisterArgs = record {
  cycles: nat64;
  settings: CanisterSettings;
};

service : {
  // Wallet Name
  name: () -> (opt text) query;
  set_name: (text) -> ();

  // Controller Management
  get_controllers: () -> (vec principal) query;
  add_controller: (principal) -> ();
  remove_controller: (principal) -> ();

  // Custodian Management
  get_custodians: () -> (vec principal) query;
  authorize: (principal) -> ();
  deauthorize: (principal) -> ();

  // Cycle Management
  wallet_balance: () -> (record { amount: nat64 }) query;
  wallet_send: (record { canister: principal; amount: nat64 }) -> (ResultSend);
  wallet_receive: () -> ();  // Endpoint for receiving cycles.

  // Managing canister
  wallet_create_canister: (CreateCanisterArgs) -> (ResultCreate);

  wallet_create_wallet: (CreateCanisterArgs) -> (ResultCreate);

  wallet_store_wallet_wasm: (record {
    wasm_module: blob;
  }) -> ();

  // Call Forwarding
  wallet_call: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat64;
  }) -> (ResultCall);

  // Address book
  add_address: (address: AddressEntry) -> ();
  list_addresses: () -> (vec AddressEntry) query;
  remove_address: (address: principal) -> ();

  // Events
  get_events: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event) query;
  get_chart: (opt record { count: opt nat32; precision: opt nat64; } ) -> (vec record { nat64; nat64; }) query;
}


-----------------------

/e2e/archive/wallet/0.7.0-beta.5/wallet.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/archive/wallet/0.7.0-beta.5/wallet.wasm

-----------------------

/e2e/archive/wallet/0.7.0/wallet.did:
-----------------------

type EventKind = variant {
  CyclesSent: record {
    to: principal;
    amount: nat64;
    refund: nat64;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat64;
  };
  AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat64;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat64;
  };
  WalletDeployed: record {
    canister: principal;
  }
};

type Event = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind;
};

type Role = variant {
  Contact;
  Custodian;
  Controller;
};

type Kind = variant {
  Unknown;
  User;
  Canister;
};

// An entry in the address book. It must have an ID and a role.
type AddressEntry = record {
  id: principal;
  name: opt text;
  kind: Kind;
  role: Role;
};

type ResultCreate = variant {
  Ok : record { canister_id: principal };
  Err: text;
};

type ResultSend = variant {
  Ok : null;
  Err : text;
};

type ResultCall = variant {
  Ok : record { return: blob };
  Err : text;
};

type CanisterSettings = record {
  controller: opt principal;
  compute_allocation: opt nat;
  memory_allocation: opt nat;
  freezing_threshold: opt nat;
};

type CreateCanisterArgs = record {
  cycles: nat64;
  settings: CanisterSettings;
};

service : {
  // Wallet Name
  name: () -> (opt text) query;
  set_name: (text) -> ();

  // Controller Management
  get_controllers: () -> (vec principal) query;
  add_controller: (principal) -> ();
  remove_controller: (principal) -> ();

  // Custodian Management
  get_custodians: () -> (vec principal) query;
  authorize: (principal) -> ();
  deauthorize: (principal) -> ();

  // Cycle Management
  wallet_balance: () -> (record { amount: nat64 }) query;
  wallet_send: (record { canister: principal; amount: nat64 }) -> (ResultSend);
  wallet_receive: () -> ();  // Endpoint for receiving cycles.

  // Managing canister
  wallet_create_canister: (CreateCanisterArgs) -> (ResultCreate);

  wallet_create_wallet: (CreateCanisterArgs) -> (ResultCreate);

  wallet_store_wallet_wasm: (record {
    wasm_module: blob;
  }) -> ();

  // Call Forwarding
  wallet_call: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat64;
  }) -> (ResultCall);

  // Address book
  add_address: (address: AddressEntry) -> ();
  list_addresses: () -> (vec AddressEntry) query;
  remove_address: (address: principal) -> ();

  // Events
  get_events: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event) query;
  get_chart: (opt record { count: opt nat32; precision: opt nat64; } ) -> (vec record { nat64; nat64; }) query;
}


-----------------------

/e2e/archive/wallet/0.7.0/wallet.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/archive/wallet/0.7.0/wallet.wasm

-----------------------

/e2e/archive/wallet/0.7.2/wallet.did:
-----------------------

type EventKind = variant {
  CyclesSent: record {
    to: principal;
    amount: nat64;
    refund: nat64;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat64;
  };
  AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat64;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat64;
  };
  WalletDeployed: record {
    canister: principal;
  }
};

type Event = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind;
};

type Role = variant {
  Contact;
  Custodian;
  Controller;
};

type Kind = variant {
  Unknown;
  User;
  Canister;
};

// An entry in the address book. It must have an ID and a role.
type AddressEntry = record {
  id: principal;
  name: opt text;
  kind: Kind;
  role: Role;
};

type ResultCreate = variant {
  Ok : record { canister_id: principal };
  Err: text;
};

type ResultSend = variant {
  Ok : null;
  Err : text;
};

type ResultCall = variant {
  Ok : record { return: blob };
  Err : text;
};

type CanisterSettings = record {
  controller: opt principal;
  compute_allocation: opt nat;
  memory_allocation: opt nat;
  freezing_threshold: opt nat;
};

type CreateCanisterArgs = record {
  cycles: nat64;
  settings: CanisterSettings;
};

service : {
  // Wallet Name
  name: () -> (opt text) query;
  set_name: (text) -> ();

  // Controller Management
  get_controllers: () -> (vec principal) query;
  add_controller: (principal) -> ();
  remove_controller: (principal) -> ();

  // Custodian Management
  get_custodians: () -> (vec principal) query;
  authorize: (principal) -> ();
  deauthorize: (principal) -> ();

  // Cycle Management
  wallet_balance: () -> (record { amount: nat64 }) query;
  wallet_send: (record { canister: principal; amount: nat64 }) -> (ResultSend);
  wallet_receive: () -> ();  // Endpoint for receiving cycles.

  // Managing canister
  wallet_create_canister: (CreateCanisterArgs) -> (ResultCreate);

  wallet_create_wallet: (CreateCanisterArgs) -> (ResultCreate);

  wallet_store_wallet_wasm: (record {
    wasm_module: blob;
  }) -> ();

  // Call Forwarding
  wallet_call: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat64;
  }) -> (ResultCall);

  // Address book
  add_address: (address: AddressEntry) -> ();
  list_addresses: () -> (vec AddressEntry) query;
  remove_address: (address: principal) -> ();

  // Events
  get_events: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event) query;
  get_chart: (opt record { count: opt nat32; precision: opt nat64; } ) -> (vec record { nat64; nat64; }) query;
}


-----------------------

/e2e/archive/wallet/0.7.2/wallet.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/archive/wallet/0.7.2/wallet.wasm

-----------------------

/e2e/archive/wallet/0.8.2/wallet.did:
-----------------------

type EventKind = variant {
  CyclesSent: record {
    to: principal;
    amount: nat64;
    refund: nat64;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat64;
  };
  AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat64;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat64;
  };
  WalletDeployed: record {
    canister: principal;
  }
};

type Event = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind;
};

type Role = variant {
  Contact;
  Custodian;
  Controller;
};

type Kind = variant {
  Unknown;
  User;
  Canister;
};

// An entry in the address book. It must have an ID and a role.
type AddressEntry = record {
  id: principal;
  name: opt text;
  kind: Kind;
  role: Role;
};

type WalletResultCreate = variant {
  Ok : record { canister_id: principal };
  Err: text;
};

type WalletResult = variant {
  Ok : null;
  Err : text;
};

type WalletResultCall = variant {
  Ok : record { return: blob };
  Err : text;
};

type CanisterSettings = record {
  controller: opt principal;
  controllers: opt vec principal;
  compute_allocation: opt nat;
  memory_allocation: opt nat;
  freezing_threshold: opt nat;
};

type CreateCanisterArgs = record {
  cycles: nat64;
  settings: CanisterSettings;
};


// Assets
type HeaderField = record { text; text; };

type HttpRequest = record {
  method: text;
  url: text;
  headers: vec HeaderField;
  body: blob;
};

type HttpResponse = record {
  status_code: nat16;
  headers: vec HeaderField;
  body: blob;
  streaming_strategy: opt StreamingStrategy;
};

type StreamingCallbackHttpResponse = record {
  body: blob;
  token: opt Token;
};

type Token = record {};

type StreamingStrategy = variant {
  Callback: record {
    callback: func (Token) -> (StreamingCallbackHttpResponse) query;
    token: Token;
  };
};

service : {
  wallet_api_version: () -> (text) query;

  // Wallet Name
  name: () -> (opt text) query;
  set_name: (text) -> ();

  // Controller Management
  get_controllers: () -> (vec principal) query;
  add_controller: (principal) -> ();
  remove_controller: (principal) -> (WalletResult);

  // Custodian Management
  get_custodians: () -> (vec principal) query;
  authorize: (principal) -> ();
  deauthorize: (principal) -> (WalletResult);

  // Cycle Management
  wallet_balance: () -> (record { amount: nat64 }) query;
  wallet_send: (record { canister: principal; amount: nat64 }) -> (WalletResult);
  wallet_receive: () -> ();  // Endpoint for receiving cycles.

  // Managing canister
  wallet_create_canister: (CreateCanisterArgs) -> (WalletResultCreate);

  wallet_create_wallet: (CreateCanisterArgs) -> (WalletResultCreate);

  wallet_store_wallet_wasm: (record {
    wasm_module: blob;
  }) -> ();

  // Call Forwarding
  wallet_call: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat64;
  }) -> (WalletResultCall);

  // Address book
  add_address: (address: AddressEntry) -> ();
  list_addresses: () -> (vec AddressEntry) query;
  remove_address: (address: principal) -> (WalletResult);

  // Events
  get_events: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event) query;
  get_chart: (opt record { count: opt nat32; precision: opt nat64; } ) -> (vec record { nat64; nat64; }) query;

  // Assets
  http_request: (request: HttpRequest) -> (HttpResponse) query;
}


-----------------------

/e2e/archive/wallet/0.8.2/wallet.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/archive/wallet/0.8.2/wallet.wasm

-----------------------

/e2e/assets/allocate_memory/src/e2e_project_backend/src/lib.rs:
-----------------------

#[ic_cdk::query]
fn greet(s: String) -> String {
    format!("Hello, {s}!")
}

#[ic_cdk::update]
fn greet_update(s: String) -> String {
    format!("Hello, {s}!")
}


-----------------------

/e2e/assets/assetscanister/src/e2e_project_frontend/assets/binary/noise.txt:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/assets/assetscanister/src/e2e_project_frontend/assets/binary/noise.txt

-----------------------

/e2e/assets/assetscanister/src/e2e_project_frontend/assets/text-with-newlines.txt:
-----------------------

cherries
it's cherry season
CHERRIES

-----------------------

/e2e/assets/base/main.mo:
-----------------------

import Char "mo:base/Char";
import Option "mo:base/Option";

actor IsDigit {

    // Could not figure out how to have a Char parameter.
    public query func is_digit(text: Text) : async Bool {
        let ch = Option.unwrap<Char>(text.chars().next());
        Char.isDigit(ch);
    };

}


-----------------------

/e2e/assets/base/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/bitcoin/bitcoin.did:
-----------------------

type satoshi = nat64;
type bitcoin_network = variant {
    mainnet;
    testnet;
    regtest;
};
type bitcoin_address = text;
type block_hash = blob;
type outpoint = record {
    txid : blob;
    vout : nat32;
};
type utxo = record {
    outpoint : outpoint;
    value : satoshi;
    height : nat32;
};
type bitcoin_get_utxos_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    filter : opt variant {
        min_confirmations : nat32;
        page : blob;
    };
};
type bitcoin_get_utxos_query_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    filter : opt variant {
        min_confirmations : nat32;
        page : blob;
    };
};
type bitcoin_get_current_fee_percentiles_args = record {
    network : bitcoin_network;
};
type bitcoin_get_utxos_result = record {
    utxos : vec utxo;
    tip_block_hash : block_hash;
    tip_height : nat32;
    next_page : opt blob;
};
type bitcoin_get_utxos_query_result = record {
    utxos : vec utxo;
    tip_block_hash : block_hash;
    tip_height : nat32;
    next_page : opt blob;
};
type bitcoin_get_balance_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    min_confirmations : opt nat32;
};
type bitcoin_get_balance_query_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    min_confirmations : opt nat32;
};
type bitcoin_send_transaction_args = record {
    transaction : blob;
    network : bitcoin_network;
};
type millisatoshi_per_byte = nat64;
type bitcoin_get_balance_result = satoshi;
type bitcoin_get_balance_query_result = satoshi;
type bitcoin_get_current_fee_percentiles_result = vec millisatoshi_per_byte;
service ic : {
    // bitcoin interface
    bitcoin_get_balance : (bitcoin_get_balance_args) -> (bitcoin_get_balance_result);
    bitcoin_get_balance_query : (bitcoin_get_balance_query_args) -> (bitcoin_get_balance_query_result) query;
    bitcoin_get_utxos : (bitcoin_get_utxos_args) -> (bitcoin_get_utxos_result);
    bitcoin_get_utxos_query : (bitcoin_get_utxos_query_args) -> (bitcoin_get_utxos_query_result) query;
    bitcoin_send_transaction : (bitcoin_send_transaction_args) -> ();
    bitcoin_get_current_fee_percentiles : (bitcoin_get_current_fee_percentiles_args) -> (bitcoin_get_current_fee_percentiles_result);
};


-----------------------

/e2e/assets/call/README.md:
-----------------------

`call.wasm` is generated by building from `call.mo`, and manually updating the metadata to be `partial.did`. 


-----------------------

/e2e/assets/call/call.mo:
-----------------------

actor {
  public query func make_struct(a: Text, b: Text) : async { c: Text; d: Text; } {
    let result = { c = a; d = b; };
    result
  };
  public query func make_struct2(a: Text, b: Text) : async { c: Text; d: Text; } {
    let result = { c = a; d = b; };
    result
  };
};


-----------------------

/e2e/assets/call/call.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/assets/call/call.wasm

-----------------------

/e2e/assets/call/dfx.json:
-----------------------

{
  "canisters": {
    "hello_backend": {
      "type": "custom",
      "candid": "partial.did",
      "wasm": "call.wasm"
    }
  }
}


-----------------------

/e2e/assets/call/full.did:
-----------------------

service : {
  make_struct: (text, text) -> (record {
                                  c: text;
                                  d: text;
                                }) query;
  make_struct2: (text, text) -> (record {
                                   c: text;
                                   d: text;
                                 }) query;
}


-----------------------

/e2e/assets/call/partial.did:
-----------------------

service : {
  make_struct: (text, text) -> (record {
                                  c: text;
                                  d: text;
                                }) query;
}


-----------------------

/e2e/assets/canister_args/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.args="--compacting-gcY" | .defaults.build.args="--compacting-gcX"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/canister_http/main.mo:
-----------------------

import Types "types";
import Cycles "mo:base/ExperimentalCycles";
import Nat64 "mo:base/Nat64";
import Text "mo:base/Text";
import Blob "mo:base/Blob";
import Nat "mo:base/Nat";

shared actor class HttpQuery() = this {
    let MAX_RESPONSE_BYTES : Nat64 = 800000; // last seen ~90k
    let CYCLES_TO_PAY : Nat = 16_000_000_000;

    public func get_url(host : Text, url : Text) : async Text {
        let request_headers = [
            { name = "Host"; value = host },
            { name = "User-Agent"; value = "sdk-e2e-test" },
        ];

        let transform_context : Types.TransformContext = {
            function = transform;
            context = Blob.fromArray([]);
        };


        let request : Types.CanisterHttpRequestArgs = {
            url = url;
            max_response_bytes = ?MAX_RESPONSE_BYTES;
            headers = request_headers;
            body = null;
            method = #get;
            transform = ?transform_context;
        };

        Cycles.add(CYCLES_TO_PAY);
        let ic : Types.IC = actor ("aaaaa-aa");
        let response : Types.CanisterHttpResponsePayload = await ic.http_request(request);
        let result : Text = switch (Text.decodeUtf8(Blob.fromArray(response.body))) {
            case null "";
            case (?decoded) decoded;
        };
        result
    };

    public query func transform(raw : Types.TransformArgs) : async Types.CanisterHttpResponsePayload {
        let transformed : Types.CanisterHttpResponsePayload = {
            status = raw.response.status;
            body = raw.response.body;
            headers = [];
        };
        transformed;
    };
};

-----------------------

/e2e/assets/canister_http/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/canister_http/types.mo:
-----------------------

import Blob "mo:base/Blob";
import HashMap "mo:base/HashMap";
import Principal "mo:base/Principal";

module Types {
    public type Timestamp = Nat64;
    public type Rate = Text;

    public type TimeRange = {
        start : Timestamp;
        end : Timestamp;
    };

    public type RatesWithInterval = {
        interval : Nat64;
        rates : [(Timestamp, Rate)];
    };

    public type HttpHeader = {
        name : Text;
        value : Text;
    };

    public type HttpMethod = {
        #get;
        #post;
        #head;
    };

    public type TransformArgs = {
        response : CanisterHttpResponsePayload;
        context : Blob;
    };

    public type TransformContext = {
        function : shared query TransformArgs -> async CanisterHttpResponsePayload;
        context : Blob;
    };

    public type CanisterHttpRequestArgs = {
        url : Text;
        max_response_bytes : ?Nat64;
        headers : [HttpHeader];
        body : ?[Nat8];
        method : HttpMethod;
        transform : ?TransformContext;
    };

    public type CanisterHttpResponsePayload = {
        status : Nat;
        headers : [HttpHeader];
        body : [Nat8];
    };

    public type IC = actor {
        http_request : Types.CanisterHttpRequestArgs -> async Types.CanisterHttpResponsePayload;
    };
};

-----------------------

/e2e/assets/certificate/certificate.mo:
-----------------------

actor Certificate {

    public query func hello_query(name: Text) : async Text {
        "Hello, " # name # "!"
    };

    public func hello_update(name: Text) : async Text {
        "Hello, " # name # "!"
    }

}


-----------------------

/e2e/assets/certificate/patch.bash:
-----------------------

jq '.canisters.certificate_backend.main="certificate.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/cmc/main.mo:
-----------------------

import Array "mo:base/Array";
import P "mo:base/Principal";
import Text "mo:base/Text";
import Prim "mo:â›”";

actor Call {
  type SubnetTypesToSubnetsResponse = {
    data: [(Text, [Principal])];
  };

  public query func get_subnet_types_to_subnets() : async SubnetTypesToSubnetsResponse {
    let type1 = "type1";
    let type2 = "type2";
    {
      data = [
        (type1, [Prim.principalOfBlob("\00")]),
        (type2, [Prim.principalOfBlob("\01"), Prim.principalOfBlob("\02")]),
      ];
    }
  };

}


-----------------------

/e2e/assets/cmc/patch.bash:
-----------------------

jq '.canisters.cmc.main="main.mo"' dfx.json | sponge dfx.json

-----------------------

/e2e/assets/counter/counter.mo:
-----------------------

actor Counter {
    var cell : Nat = 0;

    public func inc() : async () {
        cell += 1;
    };

    public query func read() : async Nat {
        cell
    };

    public func inc_read() : async Nat {
        cell += 1;
        cell
    };

    public func write(n: Nat) : async () {
        cell := n;
    };
}


-----------------------

/e2e/assets/counter/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="counter.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/counter_idl/counter_idl.mo:
-----------------------

import A "mo:base/Array";
import C "mo:base/Char";
import T "mo:base/Text";

actor {
    public type List<T> = ?{head : T; tail : List<T>};
    public type List2<T> = { #nil; #cons: (T, List2<T>) };

    func map(l: List<Int>) : List<Int> {
        switch l {
          case null { null };
          case (?v) { ?{head=v.head+1; tail=map(v.tail)} };
        }
    };
    func map2(l: List2<Int>) : List2<Int> {
         switch l {
           case (#nil) { #nil };
           case (#cons(h, tl)) { #cons(h+1, map2 tl) };
         }
    };
    public func inc(i: Int, b: Bool, str: Text, vec: [Nat], l: List<Int>, l2: List2<Int>) : async (Int, Bool, Text, [Nat], List<Int>, List2<Int>) {
        let arr = A.tabulate<Nat>(
          vec.size(),
          func (i : Nat) : Nat {
              vec[i]+1;
          });

        var text = "";
        for (c in str.chars()) {
            let c2 = C.fromNat32(C.toNat32(c)+1);
            text := text # T.fromChar(c2);
        };
        return (i+1, not b, text, arr, map(l), map2(l2));
    };
};


-----------------------

/e2e/assets/counter_idl/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="counter_idl.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/custom_canister/build.sh:
-----------------------

#!/usr/bin/env bash
echo "working directory of build script: '$(pwd)'"
echo CUSTOM_CANISTER2_BUILD_DONE

-----------------------

/e2e/assets/custom_canister/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "custom": {
      "type": "custom",
      "candid": "main.did",
      "wasm": "main.wasm",
      "build": "echo CUSTOM_CANISTER_BUILD_DONE"
    },
    "custom2": {
      "type": "custom",
      "candid": "main.did",
      "wasm": "main.wasm",
      "build": "build.sh"
    },
    "custom3": {
      "type": "custom",
      "candid": "main.did",
      "wasm": "main.wasm"
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}


-----------------------

/e2e/assets/cycles-ledger/dfx.json:
-----------------------

{
  "canisters": {
    "cycles-ledger": {
      "type": "custom",
      "wasm": "cycles-ledger.wasm.gz",
      "candid": "cycles-ledger.did"
    },
    "depositor": {
      "type": "custom",
      "wasm": "depositor.wasm.gz",
      "candid": "depositor.did"
    }
  }
}

-----------------------

/e2e/assets/default_args/patch.bash:
-----------------------

jq '.defaults.build.args="--compacting-gcX"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/deploy_deps/dependency.mo:
-----------------------

actor class Dependency(name: Text) {
    public query func greet() : async Text {
        return "Hello, " # name # "!";
    }
}

-----------------------

/e2e/assets/deploy_deps/dependent.mo:
-----------------------

actor {
  public func greet(name : Text) : async Text {
    return "Hello, " # name # "!";
  };
};


-----------------------

/e2e/assets/deploy_deps/dfx.json:
-----------------------

{
    "version": 1,
    "canisters": {
        "dependency": {
            "main": "dependency.mo"
        },
        "dependent": {
            "main": "dependent.mo",
            "dependencies": [
                "dependency"
            ]
        }
    },
    "networks": {
        "local": {
            "bind": "127.0.0.1:8000"
        }
    }
}

-----------------------

/e2e/assets/deps/app/dfx.json:
-----------------------

{
    "canisters": {
        "app": {
            "type": "motoko",
            "main": "src/main.mo",
            "dependencies": [
                "dep_b", "dep_c"
            ]
        },
        "dep_b": {
            "type": "pull",
            "id": "yhgn4-myaaa-aaaaa-aabta-cai"
        },
        "dep_c": {
            "type": "pull",
            "id": "yahli-baaaa-aaaaa-aabtq-cai"
        }
    }
}

-----------------------

/e2e/assets/deps/app/src/main.mo:
-----------------------

import dep_b "canister:dep_b";
import dep_c "canister:dep_c";

actor {
    public query func get() : async Nat {
        return 4;
    };

    public func get_b() : async Nat {
        let res = await dep_b.get();
        return res;
    };

    public func get_c() : async Nat {
        let res = await dep_c.get();
        return res;
    };

    public func get_b_times_a() : async Nat {
        let res = await dep_b.times_a();
        return res;
    };

    public func get_c_times_a() : async Nat {
        let res = await dep_c.times_a();
        return res;
    };
};


-----------------------

/e2e/assets/deps/onchain/dfx.json:
-----------------------

{
    "canisters": {
        "a": {
            "type": "motoko",
            "main": "src/a.mo",
            "pullable": {
                "wasm_url": "http://httpbin.org/status/404",
                "dependencies": [],
                "init_guide": "A natural number, e.g. 10."
            }
        },
        "b": {
            "type": "motoko",
            "main": "src/b.mo",
            "dependencies": [
                "a"
            ],
            "pullable": {
                "wasm_url": "http://httpbin.org/status/404",
                "dependencies": [
                    "yofga-2qaaa-aaaaa-aabsq-cai"
                ],
                "init_guide": "No init arguments required"
            },
            "gzip": true
        },
        "c": {
            "type": "motoko",
            "main": "src/c.mo",
            "dependencies": [
                "a"
            ],
            "pullable": {
                "wasm_url": "http://httpbin.org/status/404",
                "dependencies": [
                    "yofga-2qaaa-aaaaa-aabsq-cai"
                ],
                "init_guide": "An optional natural number, e.g. \"(opt 20)\"."
            }
        }
    }
}

-----------------------

/e2e/assets/deps/onchain/src/a.mo:
-----------------------

actor class a (num : Nat) {
    stable var NUM : Nat = num;

    public query func get() : async Nat {
        return NUM;
    };
};


-----------------------

/e2e/assets/deps/onchain/src/b.mo:
-----------------------

import a "canister:a";

actor {
    public query func get() : async Nat {
        return 2;
    };

    public func times_a() : async Nat {
        let res = 2 * (await a.get());
        return res;
    };
};


-----------------------

/e2e/assets/deps/onchain/src/c.mo:
-----------------------

import a "canister:a";

actor class c(num : ?Nat) {
    stable var NUM : Nat = switch (num) {
        case (?n) { n };
        case (null) { 3 };
    };

    public query func get() : async Nat {
        return NUM;
    };

    public func times_a() : async Nat {
        let res = NUM * (await a.get());
        return res;
    };
};


-----------------------

/e2e/assets/echo/echo.mo:
-----------------------

actor class C(p: { x: Nat; y: Int }) {
  type Profile = { name: Principal; kind: {#admin; #user; #guest }; age: ?Nat8; };
  type List = (Profile, ?List);
  public query func echo(x: List) : async List {
    x
  }
};


-----------------------

/e2e/assets/echo/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="echo.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/ed25519/identity.pem:
-----------------------

-----BEGIN PRIVATE KEY-----
MFMCAQEwBQYDK2VwBCIEIODVjQrIvbt3PO3FPDKCZs2FarAbsRrLuiQZ+NBslV9U
oSMDIQDVkl2stdaeyBDvfb0t4qy9vhsv6xLl1v7p7i0NO1+9pw==
-----END PRIVATE KEY-----


-----------------------

/e2e/assets/empty_canister_args/patch.bash:
-----------------------

jq '.defaults.build.args="--error-detail 5 --compacting-gcX" | .canisters.e2e_project_backend.args=""' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/error_context/dfx.json:
-----------------------

{
  "canisters": {
    "packtool_missing": {
      "type": "motoko",
      "main": "main.mo"
    },
    "m_o_c_missing": {
      "type": "motoko",
      "main": "main.mo"
    },
    "npm_missing": {
      "type": "assets",
      "source": []
    },
    "asset_bad_source_path": {
      "type": "assets",
      "source": [
        "src/does/not/exist"
      ]
    },
    "custom_bad_build_step": {
      "type": "custom",
      "wasm": "not used",
      "candid": "not used",
      "build": "not-the-name-of-an-executable-that-exists oh no certainly not"
    },
    "bad_optimization_level": {
      "type": "motoko",
      "main": "main.mo"
    }
  }
}

-----------------------

/e2e/assets/error_context/main.mo:
-----------------------

actor {
}


-----------------------

/e2e/assets/expect_scripts/candid_assist.exp:
-----------------------

/e2e/assets/expect_scripts/create_identity_with_invalid_password.exp:
-----------------------

#!/usr/bin/expect -df

match_max 100000
set timeout 30

spawn dfx identity new bob --storage-mode password-protected
expect "Please enter a passphrase for your identity: "
send -- "1234\r"
expect "error: Password must be longer than 8 characters."
send \x03
expect eof

-----------------------

/e2e/assets/expect_scripts/create_identity_with_password.exp:
-----------------------

#!/usr/bin/expect -df
#
# This Expect script was generated by autoexpect on Wed Mar  9 13:51:53 2022
# Expect and autoexpect were both written by Don Libes, NIST.

match_max 100000
set timeout 30

# ASSUMPTION: init_alice_with_pw.exp run before this script

spawn dfx identity use alice
expect eof

spawn dfx identity get-principal
expect {
	"Please enter the passphrase for your identity: " {
		send -- "testpassword\r"
	}
	timeout {
		puts stderr "Not asked for a password when decrypting identity!"
		exit 1
	}
}
expect "Decryption complete."
expect eof

-----------------------

/e2e/assets/expect_scripts/ctrl_c_right_after_dfx_start.exp:
-----------------------

#!/usr/bin/expect -df

match_max 100000
set timeout 330

spawn dfx cache install
expect "installed successfully."

spawn dfx start --clean
set pid [exp_pid]
expect "Using" {
    sleep 1
    send \x03
}

expect {
    -re "panic" {
        puts "Panic detected."
        exit 2
    }
    eof {
        catch wait result
        set exit_status [lindex $result 3]
        if {$exit_status == 0} {
            puts "Success: Program exited with 0 error code"
            exit 0
        } else {
            puts "Error: Program exited with non-zero error code"
            exit 2
        }
    }
    timeout {
        puts "Didn't complete before timeout."
        exec kill -9 $pid
        exit 2
    }
}


-----------------------

/e2e/assets/expect_scripts/get_ledger_balance.exp:
-----------------------

#!/usr/bin/expect -df

# ASSUMPTION: init_alice_with_pw.exp run before this script

match_max 100000
set timeout 30

spawn dfx ledger balance --network ic --identity alice
expect "Please enter the passphrase for your identity: "
send -- "testpassword\r"
expect "Decryption complete.\r"
expect {
    "WARN" {
        puts stderr "Warned incorrectly for an encrypted identity"
        exit 1
    }
    eof {}
}


-----------------------

/e2e/assets/expect_scripts/import_export_identity_with_password.exp:
-----------------------

#!/usr/bin/expect -df
#
# This Expect script was generated by autoexpect on Wed Mar  9 13:51:53 2022
# Expect and autoexpect were both written by Don Libes, NIST.

match_max 100000
set timeout 30

# try creating the identity alice until it is created
while {1} {
	spawn dfx identity import alice import.pem
	expect {
		"Please enter a passphrase for your identity: " {
			send -- "testpassword\r"
			expect {
				"Imported identity: \"alice\"." {
					expect eof {
						break
					}
				}
			}
		}
		# expect sometimes kills off the process before it has written the identity file.
		# When this happens, the folder for alice is created, but no identity.pem file within, which leaves dfx in a broken state.
		# For those cases we have to clean it up like this:
		"Error: Identity already exists." {
			puts stderr "Previous identity creation attempt was botched. Clean up broken identity"
			exec rm -rf "$env(DFX_CONFIG_ROOT)/.config/dfx/identity/alice"
		}
		timeout {
			puts stderr "Not asked for a password when creating new identity!"
			exit 2
		}
	}
}

while {1} {
	# use 'bash -c' to use output redirection
	spawn bash -c "dfx identity export alice > export.pem"
	expect {
		"Please enter the passphrase for your identity: " {
			send -- "testpassword\r"
		}
		timeout {
			puts stderr "Not asked for a password when exporting encrypted identity!"
			exit 3
		}
	}
	expect {
		"Failed during decryption." {
			expect eof
		}
		"Decryption complete." {
			expect eof {
				break
			}
		}
	}	
}

-----------------------

/e2e/assets/expect_scripts/init_alice_with_pw.exp:
-----------------------

#!/usr/bin/expect -df
#
# This Expect script was generated by autoexpect on Wed Mar  9 13:51:53 2022
# Expect and autoexpect were both written by Don Libes, NIST.

match_max 100000
set timeout 30

# repeat until identity properly created
while {1} {
	spawn dfx identity new alice
	expect {
		"Please enter a passphrase for your identity: " {
			send -- "testpassword\r"
			expect {
				"Created identity: \"alice\"." {
					expect eof {
						break
					}
				}
			}
		}
		# expect sometimes kills off the process before it has written the identity file.
		# When this happens, the folder for alice is created, but no identity.pem file within, which leaves dfx in a broken state.
		# For those cases we have to clean it up like this:
		"Error: Identity already exists." {
			puts stderr "Previous identity creation attempt was botched. Clean up broken identity"
			exec rm -rf "$env(DFX_CONFIG_ROOT)/.config/dfx/identity/alice"
		}
		timeout {
			puts stderr "Not asked for a password when creating new identity!"
			exit 1
		}
	}
}


-----------------------

/e2e/assets/expect_scripts/init_alice_with_storage_mode_pwprotected.exp:
-----------------------

#!/usr/bin/expect -df
#
# This Expect script was generated by autoexpect on Wed Mar  9 13:51:53 2022
# Expect and autoexpect were both written by Don Libes, NIST.

match_max 100000
set timeout 30

# repeat until identity properly created
while {1} {
	spawn dfx identity new alice --storage-mode password-protected
	expect {
		"Please enter a passphrase for your identity: " {
			send -- "testpassword\r"
			expect {
				"Created identity: \"alice\"." {
					expect eof {
						break
					}
				}
			}
		}
		# expect sometimes kills off the process before it has written the identity file.
		# When this happens, the folder for alice is created, but no identity.pem file within, which leaves dfx in a broken state.
		# For those cases we have to clean it up like this:
		"Error: Identity already exists." {
			puts stderr "Previous identity creation attempt was botched. Clean up broken identity"
			exec rm -rf "$env(DFX_CONFIG_ROOT)/.config/dfx/identity/alice"
		}
		timeout {
			puts stderr "Not asked for a password when creating new identity!"
			exit 1
		}
	}
}


-----------------------

/e2e/assets/expect_scripts/remove_identity_with_password.exp:
-----------------------

#!/usr/bin/expect -df
#
# This Expect script was generated by autoexpect on Wed Mar  9 13:51:53 2022
# Expect and autoexpect were both written by Don Libes, NIST.

match_max 100000
set timeout 30

# ASSUMPTION: init_alice_with_pw.exp run before this script

spawn dfx identity remove alice
expect {
	"Removed identity \"alice\"." {}
	timeout {
		puts stderr "Unable to remove encrypted identity!"
		exit 1
	}
}
expect eof


-----------------------

/e2e/assets/expect_scripts/rename_identity_with_password.exp:
-----------------------

#!/usr/bin/expect -df
#
# This Expect script was generated by autoexpect on Wed Mar  9 13:51:53 2022
# Expect and autoexpect were both written by Don Libes, NIST.

match_max 100000
set timeout 30

# ASSUMPTION: init_alice_with_pw.exp run before this script

spawn dfx identity rename alice bob
expect {
	-regex "Renamed identity" {}
	timeout {
		puts stderr "Failed to rename encrypted identity!"
		exit 1
	}
}
expect eof

spawn dfx identity use bob
expect eof

spawn dfx identity get-principal
expect -exact "\rPlease enter the passphrase for your identity: "
send -- "testpassword\r"
expect {
	"Decryption complete." {
		expect eof
	}
	"Decryption failed." {
		puts stderr "Failed to decrypt renamed identity."
		exit 1
	}
}


-----------------------

/e2e/assets/expect_scripts/rust_svelte_with_tests_and_ii.exp:
-----------------------

#!/usr/bin/expect -df

match_max 100000
set timeout 30

spawn dfx new e2e_project
expect "Select a backend language:"
# down arrow, Rust should be option 2
send "\033\[B"
send "\r"

expect "Select a frontend framework:"
# no down arrow, Svelte should be option 1
send "\r"
# first and third, should be II and frontend tests respectively
expect "Add extra features"
send " "
send "\033\[B\033\[B"
send " "
send "\r"
expect eof


-----------------------

/e2e/assets/expect_scripts/wrong_password_rejected.exp:
-----------------------

#!/usr/bin/expect -df
#
# This Expect script was generated by autoexpect on Wed Mar  9 13:51:53 2022
# Expect and autoexpect were both written by Don Libes, NIST.

match_max 100000
set timeout 30

# ASSUMPTION: init_alice_with_pw.exp run before this script

spawn dfx identity use alice
expect eof

spawn dfx identity get-principal
expect "Please enter the passphrase for your identity: "
send -- "wrong_password\r"
expect {
	"Failed to decrypt PEM file" {}
	"Decryption complete." {
		puts stderr "Wrong password not rejected!"
		exit 1
	}
}
expect eof


-----------------------

/e2e/assets/fake_cmc/dfx.json:
-----------------------

{
  "canisters": {
    "fake-cmc": {
      "type": "custom",
      "wasm": "fake-cmc.wasm.gz",
      "candid": "fake-cmc.did",
      "specified_id": "rkp4c-7iaaa-aaaaa-aaaca-cai"
    }
  }
}

-----------------------

/e2e/assets/fake_registry/dfx.json:
-----------------------

{
    "canisters": {
        "fake_registry": {
            "type": "motoko",
            "main": "fake_registry.mo",
            "specified_id": "rwlgt-iiaaa-aaaaa-aaaaa-cai"
        }
    }
}

-----------------------

/e2e/assets/fake_registry/fake_registry.mo:
-----------------------

import Principal "mo:base/Principal";
import List "mo:base/List";
import Array "mo:base/Array";
import Debug "mo:base/Debug";

actor FakeRegistry {

    // list of (canister id -> subnet id) mappings
    var subnet_per_canister : [(Principal, Principal)] = [];

    public func set_subnet_for_canister(mappings : [(Principal, Principal)]) {
        subnet_per_canister := mappings;
    };

    public query func get_subnet_for_canister(arg : { principal : ?Principal }) : async ({
        #Ok : { subnet_id : ?Principal };
        #Err : Text;
    }) {
        switch (Array.find<(Principal, Principal)>(subnet_per_canister, func pair { ?pair.0 == arg.principal })) {
            case (null) { #Err("mapping not defined") };
            case (?mapping) { #Ok { subnet_id = ?mapping.1 } };
        };
    };

};


-----------------------

/e2e/assets/faucet/dfx.json:
-----------------------

{
  "canisters": {
    "faucet": {
      "type": "motoko",
      "main": "main.mo"
    }
  }
}

-----------------------

/e2e/assets/faucet/main.mo:
-----------------------

import Cycles "mo:base/ExperimentalCycles";
import Error "mo:base/Error";
import Principal "mo:base/Principal";
import Text "mo:base/Text";
import Debug "mo:base/Debug";

actor class Coupon() = self {
    type Management = actor {
        deposit_cycles : ({ canister_id : Principal }) -> async ();
    };
    type CyclesLedger = actor {
        deposit : (DepositArgs) -> async (DepositResult);
    };
    type Account = {
        owner : Principal;
        subaccount : ?Blob;
    };
    type DepositArgs = {
        to : Account;
        memo : ?Blob;
    };
    type DepositResult = { balance : Nat; block_index : Nat };
    type DepositToCyclesLedgerResult = {
        cycles : Nat;
        balance : Nat;
        block_index : Nat;
    };

    // Uploading wasm is hard. This is much easier to handle.
    var wallet_to_hand_out : ?Principal = null;
    public func set_wallet_to_hand_out(wallet : Principal) : async () {
        wallet_to_hand_out := ?wallet;
    };

    // Redeem coupon code to create a cycle wallet
    public shared (args) func redeem(code : Text) : async Principal {
        if (code == "invalid") {
            throw (Error.reject("Code is expired or not redeemable"));
        };
        switch (wallet_to_hand_out) {
            case (?wallet) {
                return wallet;
            };
            case (_) {
                throw (Error.reject("Set wallet to return before calling this!"));
            };
        };
    };

    // Redeem coupon code to top up an existing wallet
    public func redeem_to_wallet(code : Text, wallet : Principal) : async Nat {
        if (code == "invalid") {
            throw (Error.reject("Code is expired or not redeemable"));
        };
        let IC0 : Management = actor ("aaaaa-aa");
        var amount = 10000000000000; // 10T
        Cycles.add(amount);
        await IC0.deposit_cycles({ canister_id = wallet });
        return amount;
    };

    // Redeem coupon code to cycle ledger
    public shared (args) func redeem_to_cycles_ledger(code : Text, account : Account) : async DepositToCyclesLedgerResult {
        if (code == "invalid") {
            throw (Error.reject("Code is expired or not redeemable"));
        };
        let CyclesLedgerCanister : CyclesLedger = actor ("um5iw-rqaaa-aaaaq-qaaba-cai");
        var amount = 10000000000000; // 10T
        Cycles.add(amount);
        let result = await CyclesLedgerCanister.deposit({
            to = account;
            memo = null;
        });
        return {
            cycles = amount;
            balance = result.balance;
            block_index = result.block_index;
        };
    };
};


-----------------------

/e2e/assets/faucet/patch.bash:
-----------------------

# noop


-----------------------

/e2e/assets/greet/greet.mo:
-----------------------

actor Greet {

    public query func greet(name: Text) : async Text {
        "Hello, " # name # "!"
    }

}


-----------------------

/e2e/assets/greet/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="greet.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/greet_arg/greet.mo:
-----------------------

actor class Greet(name: Text) {

    public query func greet() : async Text {
        "Hello, " # name # "!"
    }

}


-----------------------

/e2e/assets/greet_arg/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="greet.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/gzip/dfx.json:
-----------------------

{
    "version": 1,
    "canisters": {
        "gzipped": {
            "type": "custom",
            "candid": "main.did",
            "wasm": "main.wasm.gz",
            "build": "sh -c 'gzip -c main.wasm >main.wasm.gz'"
        }
    }
}

-----------------------

/e2e/assets/id/id.py:
-----------------------

#!/usr/bin/python

import struct
import sys

args = sys.argv
if len(args) != 2:
   print("usage: python %s <file>" % args[0])
   sys.exit(1)

table = [
    0x00, 0x07, 0x0E, 0x09, 0x1C, 0x1B, 0x12, 0x15,
    0x38, 0x3F, 0x36, 0x31, 0x24, 0x23, 0x2A, 0x2D,
    0x70, 0x77, 0x7E, 0x79, 0x6C, 0x6B, 0x62, 0x65,
    0x48, 0x4F, 0x46, 0x41, 0x54, 0x53, 0x5A, 0x5D,
    0xE0, 0xE7, 0xEE, 0xE9, 0xFC, 0xFB, 0xF2, 0xF5,
    0xD8, 0xDF, 0xD6, 0xD1, 0xC4, 0xC3, 0xCA, 0xCD,
    0x90, 0x97, 0x9E, 0x99, 0x8C, 0x8B, 0x82, 0x85,
    0xA8, 0xAF, 0xA6, 0xA1, 0xB4, 0xB3, 0xBA, 0xBD,
    0xC7, 0xC0, 0xC9, 0xCE, 0xDB, 0xDC, 0xD5, 0xD2,
    0xFF, 0xF8, 0xF1, 0xF6, 0xE3, 0xE4, 0xED, 0xEA,
    0xB7, 0xB0, 0xB9, 0xBE, 0xAB, 0xAC, 0xA5, 0xA2,
    0x8F, 0x88, 0x81, 0x86, 0x93, 0x94, 0x9D, 0x9A,
    0x27, 0x20, 0x29, 0x2E, 0x3B, 0x3C, 0x35, 0x32,
    0x1F, 0x18, 0x11, 0x16, 0x03, 0x04, 0x0D, 0x0A,
    0x57, 0x50, 0x59, 0x5E, 0x4B, 0x4C, 0x45, 0x42,
    0x6F, 0x68, 0x61, 0x66, 0x73, 0x74, 0x7D, 0x7A,
    0x89, 0x8E, 0x87, 0x80, 0x95, 0x92, 0x9B, 0x9C,
    0xB1, 0xB6, 0xBF, 0xB8, 0xAD, 0xAA, 0xA3, 0xA4,
    0xF9, 0xFE, 0xF7, 0xF0, 0xE5, 0xE2, 0xEB, 0xEC,
    0xC1, 0xC6, 0xCF, 0xC8, 0xDD, 0xDA, 0xD3, 0xD4,
    0x69, 0x6E, 0x67, 0x60, 0x75, 0x72, 0x7B, 0x7C,
    0x51, 0x56, 0x5F, 0x58, 0x4D, 0x4A, 0x43, 0x44,
    0x19, 0x1E, 0x17, 0x10, 0x05, 0x02, 0x0B, 0x0C,
    0x21, 0x26, 0x2F, 0x28, 0x3D, 0x3A, 0x33, 0x34,
    0x4E, 0x49, 0x40, 0x47, 0x52, 0x55, 0x5C, 0x5B,
    0x76, 0x71, 0x78, 0x7F, 0x6A, 0x6D, 0x64, 0x63,
    0x3E, 0x39, 0x30, 0x37, 0x22, 0x25, 0x2C, 0x2B,
    0x06, 0x01, 0x08, 0x0F, 0x1A, 0x1D, 0x14, 0x13,
    0xAE, 0xA9, 0xA0, 0xA7, 0xB2, 0xB5, 0xBC, 0xBB,
    0x96, 0x91, 0x98, 0x9F, 0x8A, 0x8D, 0x84, 0x83,
    0xDE, 0xD9, 0xD0, 0xD7, 0xC2, 0xC5, 0xCC, 0xCB,
    0xE6, 0xE1, 0xE8, 0xEF, 0xFA, 0xFD, 0xF4, 0xF3,
]

data = []
with open(args[1], "rb") as file:
    crc = 0x00
    buf = file.read(1)
    while buf != b"":
        byte = struct.unpack("=B", buf)[0]
        data.append(byte)
        crc = table[crc ^ byte]
        buf = file.read(1)
    data.append(crc)
id = "ic:" + "".join("{:02X}".format(byte) for byte in data)
print(id)


-----------------------

/e2e/assets/identity/identity.mo:
-----------------------

import P "mo:base/Principal";

shared({caller}) actor class () = Self {
    private let initializer : Principal = caller;

    public shared(msg) func fromCall(): async Principal {
        msg.caller
    };
    public shared query(msg) func fromQuery() : async Principal {
        msg.caller
    };
    public query func getCanisterId() : async Principal {
        P.fromActor(Self)
    };
    public query func isMyself(id: Principal) : async Bool {
        id == P.fromActor(Self)
    };

    public shared query(msg) func amInitializer() : async Bool {
        msg.caller == initializer
    };
};


-----------------------

/e2e/assets/identity/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="identity.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/import/friend.mo:
-----------------------

module Friend {

    public func greet(name: Text) : Text {
        "0" # name
    }

}


-----------------------

/e2e/assets/import/main.mo:
-----------------------

import Friend "./friend"

actor Greet {

    public query func greet(name: Text) : async Text {
        "1" # Friend.greet(name)
    }

}


-----------------------

/e2e/assets/import/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/import_canister/dfx.json:
-----------------------

{
    "canisters": {
        "origin": {
            "main": "origin.mo",
            "type": "motoko"
        },
        "importer": {
            "dependencies": [
                "origin"
            ],
            "main": "importer.mo",
            "type": "motoko"
        }
    }
}

-----------------------

/e2e/assets/import_canister/importer.mo:
-----------------------

import Hello "canister:origin";

actor {
    public func greet_piped(name : Text) : async Text {
        let response = await Hello.greet(name);
        return response;
    };
};


-----------------------

/e2e/assets/import_canister/origin.mo:
-----------------------

actor {
    public query func greet(name : Text) : async Text {
        return "Hello, " # name # "!";
    };
};


-----------------------

/e2e/assets/import_error/main.mo:
-----------------------

import X "canister:random";
actor {};



-----------------------

/e2e/assets/import_error/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/inspect_message/main.mo:
-----------------------

import Principal = "mo:base/Principal";

actor {
   public func always_accepted() : async () { };
   public func always_rejected() : async () { };

   system func inspect(
     {
      caller : Principal;
      arg : Blob;
      msg : {
        #always_accepted : () -> ();
        #always_rejected : () -> ();
      }
    }) : Bool {
         switch (msg) {
           case (#always_accepted _) { true };
           case (#always_rejected _) { false };
         }
     };
};


-----------------------

/e2e/assets/inspect_message/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/inspect_message_rs/src/hello_backend/src/lib.rs:
-----------------------

use ic_cdk::api::call;

#[ic_cdk::update]
fn always_accepted() {
}

#[ic_cdk::update]
fn always_rejected() {
}

#[ic_cdk::inspect_message]
fn inspect_message() {
  if call::method_name().as_str() == "always_accepted"{
    call::accept_message();
  }
}


-----------------------

/e2e/assets/inter/Cargo.toml:
-----------------------

[workspace]
members = ["src/counter_rs", "src/inter_rs", "src/inter2_rs"]

[workspace.dependencies]
candid = "0.10"
ic-cdk = "0.12"
ic-cdk-bindgen = "0.1"


-----------------------

/e2e/assets/inter/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "counter_rs": {
      "type": "rust",
      "package": "counter_rs",
      "candid": "src/counter_rs/counter.did"
    },
    "counter_mo": {
      "type": "motoko",
      "main": "src/counter_mo/main.mo"
    },
    "inter_rs": {
      "type": "rust",
      "package": "inter_rs",
      "candid": "src/inter_rs/lib.did",
      "dependencies": [
        "counter_mo"
      ]
    },
    "inter_mo": {
      "type": "motoko",
      "main": "src/inter_mo/main.mo"
    },
    "inter2_rs": {
      "type": "rust",
      "package": "inter2_rs",
      "candid": "src/inter2_rs/lib.did",
      "dependencies": [
        "inter_mo"
      ]
    },
    "inter2_mo": {
      "type": "motoko",
      "main": "src/inter2_mo/main.mo"
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    },
    "start": {
      "address": "127.0.0.1",
      "port": 8000,
      "serve_root": "canisters/eeoo/assets"
    }
  }
}


-----------------------

/e2e/assets/inter/src/counter_mo/main.mo:
-----------------------

actor Counter {
    var cell : Nat = 0;

    public func inc() : async () {
        cell += 1;
    };

    public query func read() : async Nat {
        cell
    };

    public func write(n: Nat) : async () {
        cell := n;
    };
}


-----------------------

/e2e/assets/inter/src/counter_rs/Cargo.toml:
-----------------------

[package]
name = "counter_rs"
version = "0.1.0"
authors = ["Hans Larsen <hans@larsen.online>"]
edition = "2018"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
path = "lib.rs"
crate-type = ["cdylib"]

[dependencies]
candid.workspace = true
ic-cdk.workspace = true


-----------------------

/e2e/assets/inter/src/counter_rs/counter.did:
-----------------------

service : {
  "inc": () -> ();
  "read": () -> (nat) query;
  "write": (nat) -> ();
}


-----------------------

/e2e/assets/inter/src/counter_rs/lib.rs:
-----------------------

use candid::Principal;
use ic_cdk::api::call::ManualReply;
use ic_cdk::{init, query, update};
use std::cell::{Cell, RefCell};

thread_local! {
    static COUNTER: RefCell<candid::Nat> = RefCell::new(candid::Nat::from(0u8));
    static OWNER: Cell<Principal> = Cell::new(Principal::from_slice(&[]));
}

#[init]
fn init() {
    OWNER.with(|owner| owner.set(ic_cdk::api::caller()));
}

#[update]
fn inc() {
    ic_cdk::println!("{:?}", OWNER.with(|owner| owner.get()));
    COUNTER.with(|counter| *counter.borrow_mut() += 1u64);
}

#[query(manual_reply = true)]
fn read() -> ManualReply<candid::Nat> {
    COUNTER.with(|counter| ManualReply::one(counter))
}

#[update]
fn write(input: candid::Nat) {
    COUNTER.with(|counter| *counter.borrow_mut() = input);
}


-----------------------

/e2e/assets/inter/src/inter2_mo/main.mo:
-----------------------

import CounterRs "canister:inter_rs";

actor Counter {
    public func inc() : async () {
        await CounterRs.inc()
    };

    public func read() : async Nat {
        await CounterRs.read()
    };

    public func write(n: Nat) : async () {
        await CounterRs.write(n)
    };
}


-----------------------

/e2e/assets/inter/src/inter2_rs/Cargo.toml:
-----------------------

[package]
name = "inter2_rs"
version = "0.1.0"
authors = ["Hans Larsen <hans@larsen.online>"]
edition = "2018"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
path = "lib.rs"
crate-type = ["cdylib"]

[dependencies]
candid.workspace = true
ic-cdk.workspace = true

[build-dependencies]
ic-cdk-bindgen.workspace = true


-----------------------

/e2e/assets/inter/src/inter2_rs/build.rs:
-----------------------

use ic_cdk_bindgen::{Builder, Config};
use std::path::PathBuf;

fn main() {
    let manifest_dir =
        PathBuf::from(std::env::var("CARGO_MANIFEST_DIR").expect("Cannot find manifest dir"));
    let counter = Config::new("inter_mo");
    let mut builder = Builder::new();
    builder.add(counter);
    builder.build(Some(manifest_dir.join("declarations")));
}


-----------------------

/e2e/assets/inter/src/inter2_rs/lib.did:
-----------------------

service : {
  "inc": () -> ();
  "read": () -> (nat);
  "write": (nat) -> ();
}


-----------------------

/e2e/assets/inter/src/inter2_rs/lib.rs:
-----------------------

use ic_cdk::update;

mod declarations;
use declarations::inter_mo::inter_mo;

#[update]
async fn read() -> candid::Nat {
    inter_mo.read().await.unwrap().0
}

#[update]
async fn inc() {
    inter_mo.inc().await.unwrap()
}

#[update]
async fn write(input: candid::Nat) {
    inter_mo.write(input).await.unwrap()
}


-----------------------

/e2e/assets/inter/src/inter_mo/main.mo:
-----------------------

import CounterRs "canister:counter_rs";

actor Counter {
    public func inc() : async () {
        await CounterRs.inc()
    };

    public func read() : async Nat {
        await CounterRs.read()
    };

    public func write(n: Nat) : async () {
        await CounterRs.write(n)
    };
}


-----------------------

/e2e/assets/inter/src/inter_rs/Cargo.toml:
-----------------------

[package]
name = "inter_rs"
version = "0.1.0"
authors = ["Hans Larsen <hans@larsen.online>"]
edition = "2018"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
path = "lib.rs"
crate-type = ["cdylib"]

[dependencies]
candid.workspace = true
ic-cdk.workspace = true

[build-dependencies]
ic-cdk-bindgen.workspace = true


-----------------------

/e2e/assets/inter/src/inter_rs/build.rs:
-----------------------

use ic_cdk_bindgen::{Builder, Config};
use std::path::PathBuf;

fn main() {
    let manifest_dir =
        PathBuf::from(std::env::var("CARGO_MANIFEST_DIR").expect("Cannot find manifest dir"));
    let counter = Config::new("counter_mo");
    let mut builder = Builder::new();
    builder.add(counter);
    builder.build(Some(manifest_dir.join("declarations")));
}


-----------------------

/e2e/assets/inter/src/inter_rs/lib.did:
-----------------------

service : {
  "inc": () -> ();
  "read": () -> (nat);
  "write": (nat) -> ();
}


-----------------------

/e2e/assets/inter/src/inter_rs/lib.rs:
-----------------------

use ic_cdk::update;

mod declarations;
use declarations::counter_mo::counter_mo;

#[update]
async fn read() -> candid::Nat {
    counter_mo.read().await.unwrap().0
}

#[update]
async fn inc() -> () {
    counter_mo.inc().await.unwrap()
}

#[update]
async fn write(input: candid::Nat) -> () {
    counter_mo.write(input).await.unwrap()
}


-----------------------

/e2e/assets/invalid/invalid.mo:
-----------------------

Some Invalid Code
No way this is valid Motoko, right?

        // Who knows...


-----------------------

/e2e/assets/invalid/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="invalid.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/large_canister/Cargo.toml:
-----------------------

[package]
name = "large"
version = "0.0.0"
edition = "2021"

[lib]
crate-type = ["cdylib"]

[workspace]


-----------------------

/e2e/assets/large_canister/dfx.json:
-----------------------

{
    "canisters": {
        "large": {
            "type": "rust",
            "package": "large",
            "candid": "large.did"
        } 
    }
}


-----------------------

/e2e/assets/large_canister/large.did:
-----------------------

service : {}


-----------------------

/e2e/assets/large_canister/patch.bash:
-----------------------

#!/usr/bin/env bash
head -c $((1024 * 1024 * 8)) /dev/urandom >garbage.bin


-----------------------

/e2e/assets/large_canister/rust-toolchain.toml:
-----------------------

[toolchain]
channel = "stable"
targets = ["wasm32-unknown-unknown"]


-----------------------

/e2e/assets/large_canister/src/lib.rs:
-----------------------

#[used]
#[no_mangle]
pub static LARGE: &[u8] = include_bytes!(concat!(env!("CARGO_MANIFEST_DIR"), "/garbage.bin"));


-----------------------

/e2e/assets/ledger/alice.pem:
-----------------------

-----BEGIN EC PARAMETERS-----
BgUrgQQACg==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHQCAQEEIJuWMc4BX7LWlt0H384ByBfXIlE5TIRYDQ3VsMH0D5QhoAcGBSuBBAAK
oUQDQgAEy7NOO4Y3IKw5bhaqYbWJUdb8jQFIKhS/2gyvtZLNEIABSBQATvpbB3/d
vIBXddEaiMrQBLHAJ9aOZ2WumQFyqw==
-----END EC PRIVATE KEY-----
Identity Principal: fdsgv-62ihb-nbiqv-xgic5-iefsv-3cscz-tmbzv-63qd5-vh43v-dqfrt-pae
Ledger account: 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752

-----------------------

/e2e/assets/ledger/bob.pem:
-----------------------

-----BEGIN EC PARAMETERS-----
BgUrgQQACg==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHQCAQEEIM+pYPWbTIiRTTyFSvG9v99jGQnd8xRKjcteefLbMBhooAcGBSuBBAAK
oUQDQgAEMx1hJK3E1XCb9MU2TfKcHvkTDKR5Y9tIJXj2XzA79CMri6hA4piS9LHR
79AW0le0gmFJ2bW0k8T7baU5BqCBxQ==
-----END EC PRIVATE KEY-----
Identity Principal: pnf55-r7gzn-s3oqn-ah2v7-r6b63-a2ma2-wyzhb-dzbwb-sghid-lzcxh-4ae
Ledger account: 22ca7edac648b814e81d7946e8bacea99280e07c5f51a04ba7a38009d8ad8e89

-----------------------

/e2e/assets/ledger/dfx.json:
-----------------------

{
    "canisters": {
        "dummy_canister": {}
    }
}

-----------------------

/e2e/assets/logs/dfx.json:
-----------------------

{
  "canisters": {
    "e2e_project": {
      "main": "logs.mo",
      "type": "motoko"
    }
  },
  "defaults": {
    "replica": {
      "subnet_type": "system"
    }
  }
}

-----------------------

/e2e/assets/logs/logs.mo:
-----------------------

import Debug "mo:base/Debug";

actor HelloActor {
  public func hello(name : Text) : async () {
    Debug.print("Hello, " # name # "!\n");
  };
};


-----------------------

/e2e/assets/matrix_multiply/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "dot_product": {
      "main": "dot_product.mo"
    },
    "hello_backend": {
      "main": "matrix.mo"
    },
    "transpose": {
      "main": "transpose.mo"
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}

-----------------------

/e2e/assets/matrix_multiply/dot_product.mo:
-----------------------

actor {
    type Vec = [Int];
    var vec : Vec = [];
    
    public func init(a: Vec) : async () {
        vec := a;
    };
    public query func dot_product_with(b: Vec) : async Int {
        assert (vec.size() == b.size());
        var res: Int = 0;
        let len = vec.size();
        var i = 0;
        while (i < len) {
            res := res + vec[i]*b[i];
            i += 1;
        };
        res
    };
};


-----------------------

/e2e/assets/matrix_multiply/matrix.mo:
-----------------------

import A "mo:base/Array";
import M "secret_import";

actor {
    type Matrix = [[Int]];
    
    public func multiply(a: Matrix, b: Matrix) : async Matrix {
        assert (a.size() > 0 and b.size() > 0);
        assert (a[0].size() == b.size());
        let n = a.size();
        let k = b[0].size();
        let bt = await M.T.transpose(b);
        let res : [[var Int]] = A.tabulate<[var Int]>(n, func (_:Nat):[var Int] = A.init<Int>(k, 0));
        var i = 0;
        while (i < n) {
            await M.D.init(a[i]);
            var j = 0;
            while (j < k) {
                res[i][j] := await M.D.dot_product_with(bt[j]);
                j += 1;
            };
            i += 1;
        };
        A.tabulate<[Int]>(
          n,
          func (i:Nat) : [Int] { A.freeze<Int>(res[i]) });
    };
};


-----------------------

/e2e/assets/matrix_multiply/secret_import.mo:
-----------------------

import T_ "canister:transpose";
import D_ "canister:dot_product";
module {
  public let T = T_;
  public let D = D_;
};


-----------------------

/e2e/assets/matrix_multiply/transpose.mo:
-----------------------

import A "mo:base/Array";

actor {
    type Matrix = [[Int]];

    public query func transpose(m: Matrix) : async Matrix {
        assert (m.size() > 0);
        let n_row = m.size();
        let n_col = m[0].size();
        A.tabulate<[Int]>(
          n_col,
          func (j:Nat) : [Int] {
              A.tabulate<Int>(n_row, func (i:Nat) : Int = (m[i][j]));
          });
    };
};


-----------------------

/e2e/assets/memory64/dfx.json:
-----------------------

{
    "version": 1,
    "canisters": {
        "m64": {
            "type": "custom",
            "candid": "empty.did",
            "wasm": "m64.wasm",
            "build": "echo \"generated from (module (memory i64 0 0))\"",
            "shrink": true,
            "optimize": "Oz",
            "gzip": true
        }
    }
}

-----------------------

/e2e/assets/memory64/empty.did:
-----------------------

service : {}


-----------------------

/e2e/assets/memory64/m64.wasm:
-----------------------

/e2e/assets/metadata/custom/custom_with_default_metadata.did:
-----------------------

service : {
  // custom_with_default_metadata
  getCanisterId: () -> (principal) query;
  amInitializer: () -> (bool) query;
}


-----------------------

/e2e/assets/metadata/custom/custom_with_private_candid_service_metadata.did:
-----------------------

service : {
  // custom_with_private_candid_service_metadata
  getCanisterId: () -> (principal) query;
  amInitializer: () -> (bool) query;
}


-----------------------

/e2e/assets/metadata/custom/custom_with_standard_candid_service_metadata.did:
-----------------------

service : {
  // custom_with_standard_candid_service_metadata
  getCanisterId: () -> (principal) query;
  amInitializer: () -> (bool) query;
}


-----------------------

/e2e/assets/metadata/custom/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "custom_with_default_metadata": {
      "type": "custom",
      "candid": "custom_with_default_metadata.did",
      "wasm": "main.wasm",
      "build": "echo anything"
    },
    "custom_with_standard_candid_service_metadata": {
      "type": "custom",
      "candid": "custom_with_standard_candid_service_metadata.did",
      "wasm": "main.wasm",
      "metadata": [
        {
          "name": "candid:service"
        }
      ]
    },
    "custom_with_private_candid_service_metadata": {
      "type": "custom",
      "candid": "custom_with_private_candid_service_metadata.did",
      "wasm": "main.wasm",
      "metadata": [
        {
          "name": "candid:service",
          "visibility": "private"
        }
      ]
    }
  }
}


-----------------------

/e2e/assets/metadata/motoko/main.mo:
-----------------------

actor {
  public query func greet(name : Text) : async Text {
    return "Hello, " # name # "!";
  };

  stable var a : Nat = 0;
  public func inc_a() : async Nat {
    a += 1;
    return a;
  };

  stable var b : Int = 0;
  public func inc_b() : async Int {
    b += 1;
    return b;
  };
};


-----------------------

/e2e/assets/metadata/motoko/not_subtype_numbertype.did:
-----------------------

service : {
  greet: (text) -> (text) query;
  inc_b: () -> (nat);
}


-----------------------

/e2e/assets/metadata/motoko/not_subtype_rename.did:
-----------------------

service : {
  new_method: (text) -> (text) query;
}


-----------------------

/e2e/assets/metadata/motoko/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/metadata/motoko/valid_subtype.did:
-----------------------

service : {
  greet: (text) -> (text) query;
  inc_a: () -> (int);
}


-----------------------

/e2e/assets/metadata/tech_stack/a custom script.sh:
-----------------------

#!/usr/bin/env bash
echo 1.75.0


-----------------------

/e2e/assets/metadata/tech_stack/a.did:
-----------------------

service: {}


-----------------------

/e2e/assets/metadata/tech_stack/dfx.json:
-----------------------

{
    "canisters": {
        "a": {
            "type": "custom",
            "wasm": "a.wasm",
            "candid": "a.did"
        },
        "b": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "cdk": {
                    "ic-cdk": {}
                }
            }
        },
        "c": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "language": {
                    "rust": {
                        "version": "1.75.0"
                    }
                }
            }
        },
        "d": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "language": {
                    "rust": {
                        "version": "$(echo 'rustc 1.75.0 (82e1608df 2023-12-21)' | cut -d ' ' -f 2)"
                    }
                }
            }
        },
        "e": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "lib": {
                    "ic-cdk-timers": {},
                    "ic-stable-structures": {}
                }
            }
        },
        "f": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "cdk": {
                    "ic-cdk": {}
                },
                "language": {
                    "rust": {}
                },
                "lib": {
                    "ic-cdk-timers": {}
                },
                "tool": {
                    "dfx": {}
                },
                "other": {
                    "bitcoin": {}
                }
            }
        },
        "g": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "language": {
                    "rust": {
                        "version": "$(a\\ custom\\ script.sh)"
                    }
                }
            }
        },
        "h": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "language": {
                    "rust": {
                        "version": "$(./a\\ custom\\ script.sh some_args)"
                    }
                }
            }
        },
        "i": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "language": {
                    "rust": {
                        "version": "$(false)"
                    }
                }
            }
        },
        "j": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "language": {
                    "rust": {
                        "version": "$(cat invalid_utf8.txt)"
                    }
                }
            }
        },
        "k": {
            "type": "motoko",
            "main": "main.mo"
        },
        "m": {
            "type": "motoko",
            "main": "main.mo",
            "tech_stack": {
                "other": {
                    "command": {
                        "cwd": "$(pwd)"
                    }
                }
            }
        }
    }
}

-----------------------

/e2e/assets/metadata/tech_stack/main.mo:
-----------------------

actor {
    public query func greet(name : Text) : async Text {
        return "Hello, " # name # "!";
    };
};


-----------------------

/e2e/assets/method_signatures/dfx.json:
-----------------------

{
  "canisters": {
    "hello_backend": {
      "type": "motoko",
      "main": "main.mo"
    }
  }
}


-----------------------

/e2e/assets/method_signatures/main.mo:
-----------------------

import Text "mo:base/Text";

actor {
  public query func returns_string(name: Text) : async Text {
    return "Hello, " # name # "!";
  };

  public query func returns_opt_string(name: ?Text) : async ?Text {
    return switch (name) {
      case null null;
      case (?x) ?("Hello, " # x # "!");
    };
  };

  public query func returns_int(v: Int) : async Int {
    return v;
  };

  public query func returns_int32(v: Int32) : async Int32 {
    return v;
  };

  public query func returns_principal(p: Principal) : async Principal {
    return p;
  };

  public query func returns_strings() : async [Text] {
    return ["Hello, world!", "Hello, Mars!"];
  };

  type ObjectReturnType = {
    foo: Text;
    bar: Int;
  };

  public query func returns_object() : async ObjectReturnType {
    return {foo = "baz"; bar = 42};
  };

  type VariantType = { #foo; #bar : Text; #baz : { a : Int32 }; };
  public query func returns_variant(i: Nat) : async VariantType {
    if (i == 0) {
      return #foo;
    } else if (i == 1) {
      return #bar("a bar");
    } else {
      return #baz({a = 51});
    }
  };

  public query func returns_blob(s: Text): async Blob {
    return Text.encodeUtf8(s);
  };

  public query func returns_tuple(): async (Text, Nat32, Text) {
    return ("the first element", 42, "the third element");
  };

  public query func returns_single_elem_tuple(): async (Text) {
    return ("the only element");
  };
}


-----------------------

/e2e/assets/motoko_management/main.mo:
-----------------------

import Management "ic:aaaaa-aa";

actor {
  public func rand() : async Blob {
    await Management.raw_rand();
  };
};


-----------------------

/e2e/assets/motoko_management/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/motoko_management_recursive/dependency.mo:
-----------------------

import Management "ic:aaaaa-aa";

module Rand {
  public func rand() : async Blob {
    await Management.raw_rand();
  };
};


-----------------------

/e2e/assets/motoko_management_recursive/main.mo:
-----------------------

import Rand "dependency";

actor {
  public func rand() : async Blob {
    await Rand.rand();
  };
};


-----------------------

/e2e/assets/motoko_management_recursive/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/packtool/command-that-fails.bash:
-----------------------

#!/bin/bash

exit 3


-----------------------

/e2e/assets/packtool/configure_packtool.bash:
-----------------------

jq '.defaults.build.packtool="echo --package describe ./vessel/describe/v1.0.1/src --package rate ./vessel/rate/v1.0.0/src"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/packtool/packtool.mo:
-----------------------

import Rate "mo:rate/language";
import Describe "mo:describe/rating";

actor Packtool {

    public query func rate(name: Text) : async Text {
        let rating = Rate.rateLanguage(name);
        let description = Describe.describeRating(rating);
        name # ": " # description;
    }

}


-----------------------

/e2e/assets/packtool/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="packtool.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/packtool/vessel/describe/v1.0.1/src/rating.mo:
-----------------------

module {
    public func describeRating(rating: Nat): Text {
        switch rating {
            case 10 { "So hot right now." };
            case _ { "No comment." };
        }
    }
}


-----------------------

/e2e/assets/packtool/vessel/rate/v1.0.0/src/language.mo:
-----------------------

module {
    public func rateLanguage(name: Text): Nat {
        switch name {
            case "rust" { 10 };
            case _ { 1 };
        }
    };
}


-----------------------

/e2e/assets/playground_backend/.gitignore:
-----------------------

mops/

-----------------------

/e2e/assets/playground_backend/README.md:
-----------------------

# What this asset contains
This asset is here to build a motoko playground locally so that we can test playground interactions without relying on mainnet.
This asset only contains the parts that are necessary to test our integration with it. The other parts (e.g. the frontend) are stripped out to have quicker deployments.
Included are:
- The `wasm-utils` canister, which performs instrumentation on the deployed wasm (inject profiling, strip forbidden function calls like `add_cycles`).
- The `backend` canister, which manages the playground canisters and lends them out to users.

# How to update the `playground_backend` asset
- Go to https://github.com/dfinity/motoko-playground
- Updating the `backend` canister
    - replace `service/pool` with the live version
    - replace `mops.toml` with the live version
- Updating the `wasm-utils` canister
    - We don't build this canister here because compiling it every time in CI takes far too long.
    - Navigate to `service/wasm-utils`
    - Run `./build.sh`
    - Replace `wasm-utils.wasm` in this asset with `service/wasm-utils/target/wasm32-unknown-unknown/release/wasm-utils.wasm`
    - Replace `wasm-utils.did` in this asset with `service/wasm-utils/wasm-utils.did`

-----------------------

/e2e/assets/playground_backend/dfx.json:
-----------------------

{
  "canisters": {
    "wasm-utils": {
      "type": "custom",
      "candid": "wasm-utils.did",
      "wasm": "wasm-utils.wasm",
      "build": [
        "echo 'the wasm-utils canister is prebuilt'"
      ]
    },
    "backend": {
      "dependencies": [
        "wasm-utils"
      ],
      "main": "service/pool/Main.mo",
      "type": "motoko"
    }
  },
  "defaults": {
    "build": {
      "output": "build",
      "packtool": "./mops-sources"
    }
  }
}

-----------------------

/e2e/assets/playground_backend/mops-sources:
-----------------------

#!/usr/bin/env bash

set -e

for i in 0 2 4 8 16 32; do
  if [ $i -gt 0 ]; then
    echo "retrying in $i seconds" 1>&2
    sleep $i
  fi

  if output=$(mops sources); then
    echo "$output"
    exit 0
  fi
  echo "failed with output: $output" 1>&2
done
exit 1


-----------------------

/e2e/assets/playground_backend/mops.toml:
-----------------------

[dependencies]
base = "0.10.4"
splay = "0.1.0"

[toolchain]
moc = "0.10.4"


-----------------------

/e2e/assets/playground_backend/service/pool/IC.mo:
-----------------------

// This is a generated Motoko binding.
module {
  public type canister_id = Principal;
  public type canister_settings = {
    controllers : ?[Principal];
    freezing_threshold : ?Nat;
    memory_allocation : ?Nat;
    compute_allocation : ?Nat;
  };
  public type definite_canister_settings = {
    controllers : [Principal];
    freezing_threshold : Nat;
    memory_allocation : Nat;
    compute_allocation : Nat;
  };
  public type user_id = Principal;
  public type wasm_module = Blob;
  public type Self = actor {
    canister_status : shared { canister_id : canister_id } -> async {
        status : { #stopped; #stopping; #running };
        memory_size : Nat;
        cycles : Nat;
        settings : definite_canister_settings;
        module_hash : ?Blob;
      };
    create_canister : shared { settings : ?canister_settings } -> async {
        canister_id : canister_id;
      };
    delete_canister : shared { canister_id : canister_id } -> async ();
    deposit_cycles : shared { canister_id : canister_id } -> async ();
    install_code : shared {
        arg : Blob;
        wasm_module : wasm_module;
        mode : { #reinstall; #upgrade; #install };
        canister_id : canister_id;
      } -> async ();
    provisional_create_canister_with_cycles : shared {
        settings : ?canister_settings;
        amount : ?Nat;
      } -> async { canister_id : canister_id };
    provisional_top_up_canister : shared {
        canister_id : canister_id;
        amount : Nat;
      } -> async ();
    raw_rand : shared () -> async Blob;
    start_canister : shared { canister_id : canister_id } -> async ();
    stop_canister : shared { canister_id : canister_id } -> async ();
    uninstall_code : shared { canister_id : canister_id } -> async ();
    update_settings : shared {
        canister_id : Principal;
        settings : canister_settings;
      } -> async ();
  };
}


-----------------------

/e2e/assets/playground_backend/service/pool/Logs.mo:
-----------------------

import Map "mo:base/RBTree";
import {compare} "mo:base/Text";
import {toArray} "mo:base/Iter";
import {now = timeNow} "mo:base/Time";
import {toText} "mo:base/Int";
import {get} "mo:base/Option";

module {
    public type Origin = { origin: Text; tags: [Text] };
    public type SharedStatsByOrigin = (Map.Tree<Text,Nat>, Map.Tree<Text,Nat>);
    public class StatsByOrigin() {
        var canisters = Map.RBTree<Text, Nat>(compare);
        var installs = Map.RBTree<Text, Nat>(compare);
        public func share() : SharedStatsByOrigin = (canisters.share(), installs.share());
        public func unshare(x : SharedStatsByOrigin) {
            canisters.unshare(x.0);
            installs.unshare(x.1);
        };
        func addTags(map: Map.RBTree<Text,Nat>, list: [Text]) {
            for (tag in list.vals()) {
                switch (map.get(tag)) {
                case null { map.put(tag, 1) };
                case (?n) { map.put(tag, n + 1) };
                };
            };
        };
        // if to is null, delete the from tag
        func merge_tag_(map: Map.RBTree<Text,Nat>, from: Text, opt_to: ?Text) {
            ignore do ? {
                let n1 = map.remove(from)!;
                let to = opt_to!;
                switch (map.get(to)) {
                case null { map.put(to, n1) };
                case (?n2) { map.put(to, n1 + n2) };
                };
            };
        };
        public func merge_tag(from: Text, to: ?Text) {
            merge_tag_(canisters, from, to);
            merge_tag_(installs, from, to);
        };
        public func addCanister(origin: Origin) {
            addTags(canisters, ["origin:" # origin.origin]);
            addTags(canisters, origin.tags);
        };
        public func addInstall(origin: Origin) {
            addTags(installs, ["origin:" # origin.origin]);
            addTags(installs, origin.tags);
        };
        public func dump() : ([(Text, Nat)], [(Text, Nat)]) {
            (toArray<(Text, Nat)>(canisters.entries()),
             toArray<(Text, Nat)>(installs.entries()),
            )
        };
        public func metrics() : Text {
            var result = "";
            let now = timeNow() / 1_000_000;
            let canister_playground = get(canisters.get("origin:playground"), 0);
            let canister_dfx = get(canisters.get("origin:dfx"), 0);
            let install_playground = get(installs.get("origin:playground"), 0);
            let install_dfx = get(installs.get("origin:dfx"), 0);
            let profiling = get(installs.get("wasm:profiling"), 0);
            let asset = get(installs.get("wasm:asset"), 0);
            let install = get(installs.get("mode:install"), 0);
            let reinstall = get(installs.get("mode:reinstall"), 0);
            let upgrade = get(installs.get("mode:upgrade"), 0);
            result := result
            # encode_single_value("counter", "create_from_playground", canister_playground, "Number of canisters created from playground", now)
            # encode_single_value("counter", "install_from_playground", install_playground, "Number of Wasms installed from playground", now)
            # encode_single_value("counter", "create_from_dfx", canister_dfx, "Number of canisters created from dfx", now)
            # encode_single_value("counter", "install_from_dfx", install_dfx, "Number of Wasms installed from dfx", now)
            # encode_single_value("counter", "profiling", profiling, "Number of Wasms profiled", now)
            # encode_single_value("counter", "asset", asset, "Number of asset Wasms canister installed", now)
            # encode_single_value("counter", "install", install, "Number of Wasms with install mode", now)
            # encode_single_value("counter", "reinstall", reinstall, "Number of Wasms with reinstall mode", now)
            # encode_single_value("counter", "upgrade", upgrade, "Number of Wasms with upgrad mode", now);
            result;
        };
    };
    public func encode_single_value(kind: Text, name: Text, number: Int, desc: Text, time: Int) : Text {
        "# HELP " # name # " " # desc # "\n" #
        "# TYPE " # name # " " # kind # "\n" #
        name # " " # toText(number) # " " # toText(time) # "\n"
    };

    public type Stats = {
        num_of_canisters: Nat;
        num_of_installs: Nat;
        cycles_used: Nat;
        error_out_of_capacity: Nat;
        error_total_wait_time: Nat;
        error_mismatch: Nat;
    };
    public let defaultStats : Stats = {
        num_of_canisters = 0;
        num_of_installs = 0;
        cycles_used = 0;
        error_out_of_capacity = 0;
        error_total_wait_time = 0;
        error_mismatch = 0;
    };
    public type EventType = {
        #getId : Nat;
        #outOfCapacity : Nat;
        #install;
        #mismatch;
    };
    public func updateStats(stats: Stats, event: EventType) : Stats {
        switch (event) {
        case (#getId(cycles)) { {
                 num_of_canisters = stats.num_of_canisters + 1;
                 cycles_used = stats.cycles_used + cycles;
                 num_of_installs = stats.num_of_installs;
                 error_out_of_capacity = stats.error_out_of_capacity;
                 error_total_wait_time = stats.error_total_wait_time;
                 error_mismatch = stats.error_mismatch;
                                } };
        case (#outOfCapacity(time)) { {
                 num_of_canisters = stats.num_of_canisters;
                 cycles_used = stats.cycles_used;
                 num_of_installs = stats.num_of_installs;
                 error_out_of_capacity = stats.error_out_of_capacity + 1;
                 error_total_wait_time = stats.error_total_wait_time + time;                 
                 error_mismatch = stats.error_mismatch;
                                  } };
        case (#install) { {
                 num_of_canisters = stats.num_of_canisters;
                 cycles_used = stats.cycles_used;
                 num_of_installs = stats.num_of_installs + 1;
                 error_out_of_capacity = stats.error_out_of_capacity;
                 error_total_wait_time = stats.error_total_wait_time;
                 error_mismatch = stats.error_mismatch;
                          } };
        case (#mismatch) { {
                 num_of_canisters = stats.num_of_canisters;
                 cycles_used = stats.cycles_used;
                 num_of_installs = stats.num_of_installs;
                 error_out_of_capacity = stats.error_out_of_capacity; 
                 error_total_wait_time = stats.error_total_wait_time;
                 error_mismatch = stats.error_mismatch + 1;
                           } };
        };
    };
}


-----------------------

/e2e/assets/playground_backend/service/pool/Main.mo:
-----------------------

import Cycles "mo:base/ExperimentalCycles";
import InternetComputer "mo:base/ExperimentalInternetComputer";
import Time "mo:base/Time";
import Error "mo:base/Error";
import Option "mo:base/Option";
import Nat "mo:base/Nat";
import Text "mo:base/Text";
import Array "mo:base/Array";
import Buffer "mo:base/Buffer";
import List "mo:base/List";
import Deque "mo:base/Deque";
import Result "mo:base/Result";
import Principal "mo:base/Principal";
import Debug "mo:base/Debug";
import Types "./Types";
import ICType "./IC";
import PoW "./PoW";
import Logs "./Logs";
import Metrics "./Metrics";
import Wasm "canister:wasm-utils";

shared (creator) actor class Self(opt_params : ?Types.InitParams) = this {
    let IC : ICType.Self = actor "aaaaa-aa";
    let params = Option.get(opt_params, Types.defaultParams);
    var pool = Types.CanisterPool(params.max_num_canisters, params.canister_time_to_live, params.max_family_tree_size);
    let nonceCache = PoW.NonceCache(params.nonce_time_to_live);
    var statsByOrigin = Logs.StatsByOrigin();

    stable let controller = creator.caller;
    stable var stats = Logs.defaultStats;
    stable var stablePool : [Types.CanisterInfo] = [];
    stable var stableMetadata : [(Principal, (Int, Bool))] = [];
    stable var stableChildren : [(Principal, [Principal])] = [];
    stable var stableTimers : [Types.CanisterInfo] = [];
    stable var previousParam : ?Types.InitParams = null;
    stable var stableStatsByOrigin : Logs.SharedStatsByOrigin = (#leaf, #leaf);

    system func preupgrade() {
        let (tree, metadata, children, timers) = pool.share();
        stablePool := tree;
        stableMetadata := metadata;
        stableChildren := children;
        stableTimers := timers;
        previousParam := ?params;
        stableStatsByOrigin := statsByOrigin.share();
    };

    system func postupgrade() {
        ignore do ? {
            if (previousParam!.max_num_canisters > params.max_num_canisters) {
                Debug.trap("Cannot reduce canisterPool for upgrade");
            };
        };
        pool.unshare(stablePool, stableMetadata, stableChildren);
        for (info in stableTimers.vals()) {
            updateTimer(info);
        };
        statsByOrigin.unshare(stableStatsByOrigin);
    };

    public query func getInitParams() : async Types.InitParams {
        params;
    };

    public query func getStats() : async (Logs.Stats, [(Text, Nat)], [(Text, Nat)]) {
        let (canister, install) = statsByOrigin.dump();
        (stats, canister, install);
    };

    public query func balance() : async Nat {
        Cycles.balance();
    };

    public func wallet_receive() : async () {
        let amount = Cycles.available();
        ignore Cycles.accept amount;
    };

    private func getExpiredCanisterInfo(origin : Logs.Origin) : async Types.CanisterInfo {
        switch (pool.getExpiredCanisterId()) {
            case (#newId) {
                Cycles.add(params.cycles_per_canister);
                let cid = await IC.create_canister { settings = null };
                let now = Time.now();
                let info = { id = cid.canister_id; timestamp = now };
                pool.add info;
                stats := Logs.updateStats(stats, #getId(params.cycles_per_canister));
                statsByOrigin.addCanister(origin);
                info;
            };
            case (#reuse info) {
                let cid = { canister_id = info.id };
                let status = await IC.canister_status cid;
                let topUpCycles : Nat = if (status.cycles < params.cycles_per_canister) {
                    params.cycles_per_canister - status.cycles;
                } else { 0 };
                if (topUpCycles > 0) {
                    Cycles.add topUpCycles;
                    await IC.deposit_cycles cid;
                };
                if (Option.isSome(status.module_hash)) {
                    await IC.uninstall_code cid;
                };
                switch (status.status) {
                    case (#stopped or #stopping) {
                        await IC.start_canister cid;
                    };
                    case _ {};
                };
                stats := Logs.updateStats(stats, #getId topUpCycles);
                statsByOrigin.addCanister(origin);
                info;
            };
            case (#outOfCapacity time) {
                let second = time / 1_000_000_000;
                stats := Logs.updateStats(stats, #outOfCapacity second);
                throw Error.reject("No available canister id, wait for " # debug_show (second) # " seconds.");
            };
        };
    };
    func validateOrigin(origin: Logs.Origin) : Bool {
        if (origin.origin == "") {
            return false;
        };
        for (tag in origin.tags.vals()) {
            // reject server side tags
            if (tag == "mode:install" or tag == "mode:reinstall" or tag == "mode:upgrade" or tag == "wasm:profiling" or tag == "wasm:asset") {
                return false;
            }
        };
        return true;
    };

    public shared ({ caller }) func getCanisterId(nonce : PoW.Nonce, origin : Logs.Origin) : async Types.CanisterInfo {
        if (not validateOrigin(origin)) {
            throw Error.reject "Please specify a valid origin";
        };
        if (caller != controller and not nonceCache.checkProofOfWork(nonce)) {
            stats := Logs.updateStats(stats, #mismatch);
            throw Error.reject "Proof of work check failed";
        };
        nonceCache.pruneExpired();
        if (nonceCache.contains nonce) {
            stats := Logs.updateStats(stats, #mismatch);
            throw Error.reject "Nonce already used";
        };
        nonceCache.add nonce;
        await getExpiredCanisterInfo(origin);
    };

    type InstallConfig = { profiling: Bool; is_whitelisted: Bool; origin: Logs.Origin };
    public shared ({ caller }) func installCode(info : Types.CanisterInfo, args : Types.InstallArgs, install_config : InstallConfig) : async Types.CanisterInfo {
        if (not validateOrigin(install_config.origin)) {
            throw Error.reject "Please specify a valid origin";
        };
        if (info.timestamp == 0) {
            stats := Logs.updateStats(stats, #mismatch);
            throw Error.reject "Cannot install removed canister";
        };
        if (not pool.find info) {
            stats := Logs.updateStats(stats, #mismatch);
            throw Error.reject "Cannot find canister";
        } else {
            let config = {
                profiling = install_config.profiling;
                remove_cycles_add = true;
                limit_stable_memory_page = ?(16384 : Nat32); // Limit to 1G of stable memory
                backend_canister_id = ?Principal.fromActor(this);
            };
            let wasm = if (caller == controller and install_config.is_whitelisted) {
                args.wasm_module;
            } else if (install_config.is_whitelisted) {
                await Wasm.is_whitelisted(args.wasm_module);
            } else {
                await Wasm.transform(args.wasm_module, config);
            };
            let newArgs = {
                arg = args.arg;
                wasm_module = wasm;
                mode = args.mode;
                canister_id = args.canister_id;
            };
            await IC.install_code newArgs;
            stats := Logs.updateStats(stats, #install);

            // Build tags from install arguments
            let tags = Buffer.fromArray<Text>(install_config.origin.tags);
            if (install_config.profiling) {
                tags.add("wasm:profiling");
            };
            if (install_config.is_whitelisted) {
                tags.add("wasm:asset");
            };
            switch (args.mode) {
            case (#install) { tags.add("mode:install") };
            case (#upgrade) { tags.add("mode:upgrade") };
            case (#reinstall) { tags.add("mode:reinstall") };
            };
            let origin = { origin = install_config.origin.origin; tags = Buffer.toArray(tags) };
            statsByOrigin.addInstall(origin);
            switch (pool.refresh(info, install_config.profiling)) {
                case (?newInfo) {
                     updateTimer(newInfo);
                     newInfo;
                 };
                case null { throw Error.reject "Cannot find canister" };
            };
        };
    };

    func updateTimer(info: Types.CanisterInfo) {
        func job() : async () {
            pool.removeTimer(info.id);
            // It is important that the timer job checks for the timestamp first.
            // This prevents late-runner jobs from deleting newly installed code.
            await removeCode(info);
        };
        pool.updateTimer(info, job);
    };

    public func callForward(info : Types.CanisterInfo, function : Text, args : Blob) : async Blob {
        if (pool.find info) {
            await InternetComputer.call(info.id, function, args);
        } else {
            stats := Logs.updateStats(stats, #mismatch);
            throw Error.reject "Cannot find canister";
        };
    };

    public func removeCode(info : Types.CanisterInfo) : async () {
        if (pool.find info) {
            await IC.uninstall_code { canister_id = info.id };
            ignore pool.retire info;
        } else {
            stats := Logs.updateStats(stats, #mismatch);
        };
    };

    public func GCCanisters() {
        for (id in pool.gcList().vals()) {
            await IC.uninstall_code { canister_id = id };
        };
    };

    public query func getSubtree(parent : Types.CanisterInfo) : async [(Principal, [Types.CanisterInfo])] {
        if (not pool.find(parent)) {
            throw Error.reject "Canister not found";
        };
        // Do not return subtree for non-root parent to save cost
        if (not pool.isRoot(parent.id)) {
            return [];
        };
        var result = List.nil<(Principal, [Types.CanisterInfo])>();
        var queue = Deque.empty<Principal>();
        queue := Deque.pushBack(queue, parent.id);
        label l loop {
            switch (Deque.popFront(queue)) {
                case null break l;
                case (?(id, tail)) {
                    queue := tail;
                    let children = List.map(
                        pool.getChildren(id),
                        func(child : Principal) : Types.CanisterInfo {
                            queue := Deque.pushBack(queue, child);
                            Option.unwrap(pool.info(child));
                        },
                    );
                    result := List.push((id, List.toArray children), result);
                };
            };
        };
        List.toArray(result);
    };

    public query ({ caller }) func dump() : async [Types.CanisterInfo] {
        if (caller != controller) {
            throw Error.reject "Only called by controller";
        };
        pool.share().0;
    };

    public shared ({ caller }) func resetStats() : async () {
        if (caller != controller) {
            throw Error.reject "Only called by controller";
        };
        stats := Logs.defaultStats;
        statsByOrigin := Logs.StatsByOrigin();
    };
    public shared ({ caller }) func mergeTags(from: Text, to: ?Text) : async () {
        if (caller != controller) {
            throw Error.reject "Only called by controller";
        };
        statsByOrigin.merge_tag(from, to);
    };

    // Metrics
    public query func http_request(req : Metrics.HttpRequest) : async Metrics.HttpResponse {
        if (req.url == "/metrics") {
            let body = Metrics.metrics(stats);
            {
                status_code = 200;
                headers = [("Content-Type", "text/plain; version=0.0.4"), ("Content-Length", Nat.toText(body.size()))];
                body = body;
            };
        } else {
            {
                status_code = 404;
                headers = [];
                body = Text.encodeUtf8 "Not supported";
            };
        };
    };

    /*
    * The following methods are wrappers/immitations of the management canister's methods that require controller permissions.
    * In general, the backend is the sole controller of all playground pool canisters. Any canister that attempts to call the
    * management canister will be redirected here instead by the wasm transformation above.
    */
    private func sanitizeInputs(caller : Principal, callee : Principal) : Result.Result<Types.CanisterInfo, Text -> Text> {
        if (not pool.findId caller) {
            return #err(func methodName = "Only a canister managed by the Motoko Playground can call " # methodName);
        };
        switch (pool.info callee) {
            case null {
                #err(func methodName = "Can only call " # methodName # " on canisters in the Motoko Playground");
            };
            case (?info) {
                // Also allow the canister to manage itself, as we don't allow canisters to change settings.
                if (not (caller == callee) and not pool.isParentOf(caller, callee)) {
                    #err(func methodName = "Can only call " # methodName # " on canisters spawned by your own code");
                } else {
                    #ok info;
                };
            };
        };
    };

    public shared ({ caller }) func create_canister({
        settings : ?ICType.canister_settings;
    }) : async { canister_id : ICType.canister_id } {
        if (Option.isSome(settings)) {
            throw Error.reject "Can only call create_canister with null settings";
        };
        if (not pool.findId caller) {
            throw Error.reject "Only a canister managed by the Motoko Playground can call create_canister";
        };
        let info = await getExpiredCanisterInfo({origin="spawned"; tags=[]});
        let result = pool.setChild(caller, info.id);
        if (not result) {
            throw Error.reject("In the Motoko Playground, each top level canister can only spawn " # Nat.toText(params.max_family_tree_size) # " descendants including itself");
        };
        { canister_id = info.id };
    };

    // Disabled to prevent the user from updating the controller list (amongst other settings)
    public shared ({ caller }) func update_settings({
        canister_id : ICType.canister_id;
        settings : ICType.canister_settings;
    }) : async () {
        throw Error.reject "Cannot call update_settings from within Motoko Playground";
    };

    public shared ({ caller }) func install_code({
        arg : Blob;
        wasm_module : ICType.wasm_module;
        mode : { #reinstall; #upgrade; #install };
        canister_id : ICType.canister_id;
    }) : async () {
        switch (sanitizeInputs(caller, canister_id)) {
            case (#ok info) {
                let args = { arg; wasm_module; mode; canister_id };
                let config = { profiling = pool.profiling caller; is_whitelisted = false; origin = {origin = "spawned"; tags = [] } };
                ignore await installCode(info, args, config); // inherit the profiling of the parent
            };
            case (#err makeMsg) throw Error.reject(makeMsg "install_code");
        };
    };

    public shared ({ caller }) func uninstall_code({
        canister_id : ICType.canister_id;
    }) : async () {
        switch (sanitizeInputs(caller, canister_id)) {
            case (#ok _) await IC.uninstall_code { canister_id };
            case (#err makeMsg) throw Error.reject(makeMsg "uninstall_code");
        };
    };

    public shared ({ caller }) func canister_status({
        canister_id : ICType.canister_id;
    }) : async {
        status : { #stopped; #stopping; #running };
        memory_size : Nat;
        cycles : Nat;
        settings : ICType.definite_canister_settings;
        module_hash : ?Blob;
    } {
        switch (sanitizeInputs(caller, canister_id)) {
            case (#ok _) await IC.canister_status { canister_id };
            case (#err makeMsg) {
                if (caller == canister_id) {
                    await IC.canister_status { canister_id };
                } else { throw Error.reject(makeMsg "canister_status") };
            };
        };
    };

    public shared ({ caller }) func stop_canister({
        canister_id : ICType.canister_id;
    }) : async () {
        switch (sanitizeInputs(caller, canister_id)) {
            case (#ok _) await IC.stop_canister { canister_id };
            case (#err makeMsg) throw Error.reject(makeMsg "stop_canister");
        };
    };

    public shared ({ caller }) func start_canister({
        canister_id : ICType.canister_id;
    }) : async () {
        switch (sanitizeInputs(caller, canister_id)) {
            case (#ok _) await IC.start_canister { canister_id };
            case (#err makeMsg) throw Error.reject(makeMsg "start_canister");
        };
    };

    public shared ({ caller }) func delete_canister({
        canister_id : ICType.canister_id;
    }) : async () {
        switch (sanitizeInputs(caller, canister_id)) {
            case (#ok info) await removeCode(info); // retire the canister back into pool instead of deleting
            case (#err makeMsg) throw Error.reject(makeMsg "delete_canister");
        };
    };

    system func inspect({
        msg : {
            #GCCanisters : Any;
            #balance : Any;
            #callForward : Any;
            #dump : Any;
            #getCanisterId : Any;
            #getSubtree : Any;
            #getInitParams : Any;
            #getStats : Any;
            #http_request : Any;
            #installCode : Any;
            #removeCode : Any;
            #resetStats : Any;
            #mergeTags : Any;
            #wallet_receive : Any;

            #create_canister : Any;
            #update_settings : Any;
            #install_code : Any;
            #uninstall_code : Any;
            #canister_status : Any;
            #start_canister : Any;
            #stop_canister : Any;
            #delete_canister : Any;
        };
    }) : Bool {
        switch msg {
            case (#create_canister _) false;
            case (#update_settings _) false;
            case (#install_code _) false;
            case (#uninstall_code _) false;
            case (#canister_status _) false;
            case (#start_canister _) false;
            case (#stop_canister _) false;
            case (#delete_canister _) false;
            case _ true;
        };
    };
};


-----------------------

/e2e/assets/playground_backend/service/pool/Metrics.mo:
-----------------------

import Text "mo:base/Text";
import Time "mo:base/Time";
import Logs "./Logs";

module {
    public type HttpRequest = {
        method: Text;
        url: Text;
        headers: [(Text, Text)];
        body: Blob;
    };
    public type HttpResponse = {
        status_code: Nat16;
        headers: [(Text, Text)];
        body: Blob;
    };
    let encode_single_value = Logs.encode_single_value;
    public func metrics(stats: Logs.Stats) : Blob {
        let now = Time.now() / 1_000_000;
        var result = "";
        result := result # encode_single_value("counter", "canister_count", stats.num_of_canisters, "Number of canisters deployed", now);
        result := result # encode_single_value("counter", "wasm_count", stats.num_of_installs, "Number of Wasm installed", now);
        result := result # encode_single_value("counter", "cycles_used", stats.cycles_used, "Cycles used", now);
        result := result # encode_single_value("counter", "out_of_capacity", stats.error_out_of_capacity, "Number of out of capacity requests", now);
        result := result # encode_single_value("counter", "total_wait_time", stats.error_total_wait_time, "Number of seconds waiting for out of capacity requests", now);
        result := result # encode_single_value("counter", "mismatch", stats.error_mismatch, "Number of mismatch requests including wrong nounce and timestamp", now);
        Text.encodeUtf8(result)
    };
}


-----------------------

/e2e/assets/playground_backend/service/pool/PoW.mo:
-----------------------

import Splay "mo:splay";
import Time "mo:base/Time";
import Text "mo:base/Text";
import Int "mo:base/Int";

module {
    public type Nonce = {
        timestamp: Int;
        nonce: Nat;
    };
    func nonceCompare(x: Nonce, y: Nonce): {#less;#equal;#greater} {
        if (x.timestamp < y.timestamp) { #less }
        else if (x.timestamp == y.timestamp and x.nonce < y.nonce) { #less }
        else if (x.timestamp == y.timestamp and x.nonce == y.nonce) { #equal }
        else { #greater }
    };
    public class NonceCache(TTL: Nat) {
        let known_nonces = Splay.Splay<Nonce>(nonceCompare);
        public func add(nonce: Nonce) {
            known_nonces.insert(nonce);
        };
        public func pruneExpired() {
            let now = Time.now();
            for (info in known_nonces.entries()) {
                if (info.timestamp > now - TTL) { return; };
                known_nonces.remove(info);
            };
        };
        public func contains(nonce: Nonce) : Bool {
            known_nonces.find(nonce)
        };
        public func checkProofOfWork(nonce: Nonce) : Bool {
            let now = Time.now();
            if (nonce.timestamp < now - TTL) return false;
            if (nonce.timestamp > now + TTL) return false;
            let raw = "motoko-playground" # (Int.toText(nonce.timestamp)) # (Int.toText(nonce.nonce));
            let hash = Text.hash(raw);
            if (hash & 0xc0000000 != 0) return false;
            true
        };
    };
}


-----------------------

/e2e/assets/playground_backend/service/pool/Types.mo:
-----------------------

import Principal "mo:base/Principal";
import Splay "mo:splay";
import Time "mo:base/Time";
import Buffer "mo:base/Buffer";
import TrieMap "mo:base/TrieMap";
import TrieSet "mo:base/TrieSet";
import Iter "mo:base/Iter";
import Array "mo:base/Array";
import List "mo:base/List";
import Option "mo:base/Option";
import Int "mo:base/Int";
import Timer "mo:base/Timer";

module {
    public type InitParams = {
        cycles_per_canister: Nat;
        max_num_canisters: Nat;
        canister_time_to_live: Nat;
        nonce_time_to_live: Nat;
        max_family_tree_size: Nat;
    };
    public let defaultParams : InitParams = {
        cycles_per_canister = 550_000_000_000;
        max_num_canisters = 100;
        canister_time_to_live = 1200_000_000_000;
        nonce_time_to_live = 300_000_000_000;
        max_family_tree_size = 5;
    };
    public type InstallArgs = {
        arg : Blob;
        wasm_module : Blob;
        mode : { #reinstall; #upgrade; #install };
        canister_id : Principal;
    };
    public type CanisterInfo = {
        id: Principal;
        timestamp: Int;
    };
    func canisterInfoCompare(x: CanisterInfo, y: CanisterInfo): {#less;#equal;#greater} {
        if (x.timestamp < y.timestamp) { #less }
        else if (x.timestamp == y.timestamp and x.id < y.id) { #less }
        else if (x.timestamp == y.timestamp and x.id == y.id) { #equal }
        else { #greater }
    };

    /*
    * Main data structure of the playground. The splay tree is the source of truth for
    * what canisters live in the playground. Metadata map reflects the state of the tree
    * to allow Map-style lookups on the canister data. Childrens and parents define the
    * controller relationships for dynmically spawned canisters by actor classes.
    */
    public class CanisterPool(size: Nat, ttl: Nat, max_family_tree_size: Nat) {
        var len = 0;
        var tree = Splay.Splay<CanisterInfo>(canisterInfoCompare);
        // Metadata is a replicate of splay tree, which allows lookup without timestamp. Internal use only.
        var metadata = TrieMap.TrieMap<Principal, (Int, Bool)>(Principal.equal, Principal.hash);
        var childrens = TrieMap.TrieMap<Principal, List.List<Principal>>(Principal.equal, Principal.hash);
        var parents = TrieMap.TrieMap<Principal, Principal>(Principal.equal, Principal.hash);
        let timers = TrieMap.TrieMap<Principal, Timer.TimerId>(Principal.equal, Principal.hash);

        public type NewId = { #newId; #reuse:CanisterInfo; #outOfCapacity:Nat };

        public func getExpiredCanisterId() : NewId {
            if (len < size) {
                #newId
            } else {
                switch (tree.entries().next()) {
                    case null { assert false; loop(); };
                    case (?info) {
                        let now = Time.now();
                        let elapsed : Nat = Int.abs(now) - Int.abs(info.timestamp);
                        if (elapsed >= ttl) {
                            // Lazily cleanup pool state before reusing canister
                            tree.remove info;
                            let newInfo = { timestamp = now; id = info.id; };
                            tree.insert newInfo;
                            metadata.put(newInfo.id, (newInfo.timestamp, false));
                            deleteFamilyNode(newInfo.id);
                            #reuse newInfo
                        } else {
                            #outOfCapacity(ttl - elapsed)
                        }
                     };
                };
            };
        };

        public func add(info: CanisterInfo) {
            if (len >= size) {
                assert false;
            };
            len += 1;
            tree.insert info;
            metadata.put(info.id, (info.timestamp, false));
        };

        public func find(info: CanisterInfo) : Bool = tree.find info;
        public func findId(id: Principal) : Bool = Option.isSome(metadata.get id);
        public func profiling(id: Principal) : Bool = Option.getMapped<(Int, Bool), Bool>(metadata.get id, func p = p.1, false);

        public func info(id: Principal) : ?CanisterInfo {
            do ? {
                let (timestamp, _) = metadata.get(id)!;
                { timestamp; id }
            }
        };

        public func refresh(info: CanisterInfo, profiling: Bool) : ?CanisterInfo {
            if (not tree.find info) { return null };
            tree.remove info;
            let newInfo = { timestamp = Time.now(); id = info.id };
            tree.insert newInfo;
            metadata.put(newInfo.id, (newInfo.timestamp, profiling));
            ?newInfo
        };

        public func retire(info: CanisterInfo) : Bool {
            if (not tree.find info) {
                return false;
            };
            let id = info.id;
            tree.remove info;
            tree.insert { timestamp = 0; id };
            metadata.put(id, (0, false));
            deleteFamilyNode id;
            return true;
        };

        public func updateTimer(info: CanisterInfo, job : () -> async ()) {
            let elapsed = Time.now() - info.timestamp;
            let duration = if (elapsed > ttl) { 0 } else { Int.abs(ttl - elapsed) };
            let tid = Timer.setTimer(#nanoseconds duration, job);
            switch (timers.replace(info.id, tid)) {
            case null {};
            case (?old_id) {
                     // The old job can still run when it has expired, but the future
                     // just started to run. To be safe, the job needs to check for timestamp.
                     Timer.cancelTimer(old_id);
                 };
            };
        };
        
        public func removeTimer(cid: Principal) {
            timers.delete cid;
        };
        
        private func notExpired(info: CanisterInfo, now: Int) : Bool = (info.timestamp > now - ttl);

        // Return a list of canister IDs from which to uninstall code
        public func gcList() : Buffer.Buffer<Principal> {
            let now = Time.now();
            let result = Buffer.Buffer<Principal>(len);
            for (info in tree.entries()) {
                if (info.timestamp > 0) {
                    // assumes when timestamp == 0, uninstall_code is already done
                    if (notExpired(info, now)) { return result };
                    result.add(info.id);
                    ignore retire info;
                }
            };
            result
        };

        public func share() : ([CanisterInfo], [(Principal, (Int, Bool))], [(Principal, [Principal])], [CanisterInfo]) {
            let stableInfos = Iter.toArray(tree.entries());
            let stableMetadata = Iter.toArray(metadata.entries());
            let stableChildren = 
                Iter.toArray(
                    Iter.map<(Principal, List.List<Principal>), (Principal, [Principal])>(
                        childrens.entries(),
                        func((parent, children)) = (parent, List.toArray(children))
                    )
                );
            let stableTimers = Iter.toArray(
              Iter.filter<CanisterInfo>(
                tree.entries(),
                func (info) = Option.isSome(timers.get(info.id))
              ));
            (stableInfos, stableMetadata, stableChildren, stableTimers)
        };

        public func unshare(stableInfos: [CanisterInfo], stableMetadata: [(Principal, (Int, Bool))], stableChildrens : [(Principal, [Principal])]) {
            len := stableInfos.size();
            tree.fromArray stableInfos;

            // Ensure that metadata reflects tree
            let profilingMap = TrieMap.fromEntries<Principal, (Int, Bool)>(Iter.fromArray stableMetadata, Principal.equal, Principal.hash);
            Iter.iterate<CanisterInfo>(
                stableInfos.vals(),
                func(info, _) {
                    let profiling = Option.getMapped<(Int, Bool), Bool>(profilingMap.get(info.id), func p = p.1, false);
                    metadata.put(info.id, (info.timestamp, profiling));
                    }
                );

            childrens := 
                TrieMap.fromEntries(
                    Array.map<(Principal, [Principal]), (Principal, List.List<Principal>)>(
                        stableChildrens,
                        func((parent, children)) = (parent, List.fromArray children)
                    ).vals(), 
                    Principal.equal,
                    Principal.hash
                );
            
            let parentsEntries = 
                Array.flatten(
                    Array.map<(Principal, [Principal]), [(Principal, Principal)]>(
                        stableChildrens, 
                        func((parent, children)) = 
                            Array.map<Principal, (Principal, Principal)>(
                                children,
                                func child = (child, parent)
                            )
                    )
                );
            parents := TrieMap.fromEntries(parentsEntries.vals(), Principal.equal, Principal.hash);
        };

        public func getChildren(parent: Principal) : List.List<Principal> {
            switch(childrens.get parent) {
                case null List.nil();
                case (?children) {
                    let now = Time.now();
                    List.filter(children, func(p: Principal) : Bool = notExpired(Option.unwrap(info p), now));
                }
            }
        };

        public func isRoot(node: Principal) : Bool = Option.isNull(parents.get node);

        private func treeSize(node: Principal) : Nat {
            switch (parents.get node) {
                // found root
                case null {
                    countActiveNodes(node)
                };
                case (?parent) {
                    treeSize(parent)
                }
            }
        };

        // Counts number of nodes in the tree rooted at root, excluding expired nodes at time `now
        private func countActiveNodes(root: Principal) : Nat {
            var count = 1;
            let now = Time.now();
            ignore do ? {
                let children = childrens.get(root)!;
                for (child in List.toIter(children)) {
                    if (notExpired((info child)!, now)) {
                        count := count + countActiveNodes(child)
                    }
                };
            };
            count
        };

        public func setChild(parent: Principal, child: Principal) : Bool {
            if (treeSize(parent) >= max_family_tree_size) {
                return false;
            };
            let children = getChildren parent;
            childrens.put(parent, List.push(child, children));
            parents.put(child, parent);
            return true;
        };

        public func isParentOf(parent: Principal, child: Principal) : Bool {
            switch(parents.get child) {
                case null {
                    false
                };
                case (?registerdParent) {
                    Principal.equal(registerdParent, parent)
                };
            };
        };

        private func deleteFamilyNode(id: Principal) {
            // Remove children edges
            ignore do ? {
                List.iterate(childrens.get(id)!, parents.delete);
            };
            childrens.delete id;

            // Remove parent edges
            ignore do ? {
                let parent = parents.get(id)!;
                childrens.put(parent, List.filter<Principal>(childrens.get(parent)!, func child = not Principal.equal(child, id)));
            };
            parents.delete id;
        };
    };
}


-----------------------

/e2e/assets/playground_backend/wasm-utils.did:
-----------------------

type Config = record {
  profiling : bool;
  remove_cycles_add : bool;
  limit_stable_memory_page : opt nat32;
  backend_canister_id : opt principal;
};

service : {
  transform : (blob, Config) -> (blob) query;
  is_whitelisted : (blob) -> (blob) query;
}


-----------------------

/e2e/assets/playground_backend/wasm-utils.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/assets/playground_backend/wasm-utils.wasm

-----------------------

/e2e/assets/post_install/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "postinstall": {
      "main": "main.mo",
      "post_install": "echo hello-file"
    },
    "postinstall_script": {
      "main": "main.mo",
      "post_install": "postinstall.sh",
      "dependencies": ["postinstall"]
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  },
  "networks": {
    "local": {
      "bind": "127.0.0.1:8000"
    }
  }
}


-----------------------

/e2e/assets/post_install/main.mo:
-----------------------

actor {

}


-----------------------

/e2e/assets/post_install/postinstall.sh:
-----------------------

#!/usr/bin/env bash
echo "working directory of post-install script: '$(pwd)'"
echo hello-script


-----------------------

/e2e/assets/prebuilt_custom_canister/custom_with_build_step.did:
-----------------------

service : {
  // custom_with_build_step
  fromCall: () -> (principal);
  isMyself: (principal) -> (bool) query;

  fromQuery: () -> (principal) query;

  getCanisterId: () -> (principal) query;

  amInitializer: () -> (bool) query;
}


-----------------------

/e2e/assets/prebuilt_custom_canister/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "custom_with_build_step": {
      "type": "custom",
      "candid": "custom_with_build_step.did",
      "wasm": "main.wasm",
      "build": "echo just a build step"
    },
    "prebuilt_custom_no_build": {
      "type": "custom",
      "candid": "prebuilt_custom_no_build.did",
      "wasm": "main.wasm"
    },
    "prebuilt_custom_blank_build": {
      "type": "custom",
      "candid": "prebuilt_custom_blank_build.did",
      "wasm": "main.wasm",
      "build": ""
    },
    "prebuilt_custom_empty_build": {
      "type": "custom",
      "candid": "prebuilt_custom_blank_build.did",
      "wasm": "main.wasm",
      "build": []
    },
    "prebuilt_no_metadata_defined": {
      "type": "custom",
      "candid": "custom_with_build_step.did",
      "wasm": "main.wasm",
      "build": "echo just a build step"
    },
    "prebuilt_local_import": {
       "type": "custom",
       "candid": "local_import.did",
       "wasm": "main.wasm",
       "build": [],
       "metadata": [
         { "name": "candid:service" }
       ]
    }
  },
  "networks": {
    "local": {
      "bind": "127.0.0.1:8000"
    }
  }
}


-----------------------

/e2e/assets/prebuilt_custom_canister/local_import.did:
-----------------------

import service "prebuilt_custom_no_build.did";



-----------------------

/e2e/assets/prebuilt_custom_canister/prebuilt_custom_blank_build.did:
-----------------------

service : {
  fromQuery: () -> (principal) query;
}


-----------------------

/e2e/assets/prebuilt_custom_canister/prebuilt_custom_no_build.did:
-----------------------

service : {
  getCanisterId: () -> (principal) query;
}


-----------------------

/e2e/assets/print/dfx.json:
-----------------------

{
  "canisters": {
    "e2e_project": {
      "main": "print.mo",
      "type": "motoko"
    }
  },
  "defaults": {
    "replica": {
      "subnet_type": "system"
    }
  }
}

-----------------------

/e2e/assets/print/print.mo:
-----------------------

import Debug "mo:base/Debug";

actor HelloActor {
  public func hello() : async () {
    Debug.print("Hello, World! from DFINITY \n");
  }
};


-----------------------

/e2e/assets/project-import/project-directory/canister_ids.json:
-----------------------

{
  "sibling": {
    "mainnet": "rwlgt-iiaaa-aaaaa-aaaaa-cai",
    "small01": "rwlgt-iiaaa-aaaaa-aaaaa-cai"
  },
  "normal-canister": {
    "mainnet": "rrkah-fqaaa-aaaaa-aaaaq-cai",
    "small01": "rrkah-fqaaa-aaaaa-aaaaq-cai"
  },
  "ledger": {
    "mainnet": "ryjl3-tyaaa-aaaaa-aaaba-cai",
    "small01": "ryjl3-tyaaa-aaaaa-aaaba-cai"
  },
  "root": {
    "mainnet": "r7inp-6aaaa-aaaaa-aaabq-cai",
    "small01": "r7inp-6aaaa-aaaaa-aaabq-cai"
  },
  "cycles-minting": {
    "mainnet": "rkp4c-7iaaa-aaaaa-aaaca-cai",
    "small01": "rkp4c-7iaaa-aaaaa-aaaca-cai"
  },
  "lifeline": {
    "mainnet": "rno2w-sqaaa-aaaaa-aaacq-cai",
    "small01": "rno2w-sqaaa-aaaaa-aaacq-cai"
  },
  "genesis-token": {
    "mainnet": "renrk-eyaaa-aaaaa-aaada-cai",
    "small01": "renrk-eyaaa-aaaaa-aaada-cai"
  },
  "identity": {
    "mainnet": "rdmx6-jaaaa-aaaaa-aaadq-cai",
    "small01": "rdmx6-jaaaa-aaaaa-aaadq-cai"
  },
  "nns-ui": {
    "mainnet": "qoctq-giaaa-aaaaa-aaaea-cai",
    "small01": "qoctq-giaaa-aaaaa-aaaea-cai"
  }
}

-----------------------

/e2e/assets/project-import/project-directory/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "normal-canister": {
      "type": "custom",
      "candid": "normal-canister-directory/some-subdirectory/the-candid-filename.did",
      "wasm": "../target/wasm32-unknown-unknown/release/governance-canister.wasm",
      "build": "cargo build --target wasm32-unknown-unknown --release -p ic-nns-governance"
    },
    "sibling": {
      "type": "custom",
      "candid": "../sibling-project/canister/canister/the-sibling-candid-definition.did",
      "wasm": "../target/wasm32-unknown-unknown/release/registry-canister.wasm",
      "build": "cargo build --target wasm32-unknown-unknown --release -p registry-canister"
    }
  },
  "networks": {
    "mainnet": {
      "providers": [
        "https://icp0.io"
      ],
      "type": "persistent"
    },
    "small01": {
      "providers": [
        "http://[2a00:fb01:400:42:5000:3dff:feca:9312]:8080"
      ],
      "type": "persistent"
    },
    "local": {
      "bind": "127.0.0.1:8080"
    }
  }
}

-----------------------

/e2e/assets/project-import/project-directory/normal-canister-directory/some-subdirectory/the-candid-filename.did:
-----------------------

type AccountIdentifier = record { hash : vec nat8 };
type Action = variant {
  RegisterKnownNeuron : KnownNeuron;
  ManageNeuron : ManageNeuron;
  ExecuteNnsFunction : ExecuteNnsFunction;
  RewardNodeProvider : RewardNodeProvider;
  SetSnsTokenSwapOpenTimeWindow : SetSnsTokenSwapOpenTimeWindow;
  SetDefaultFollowees : SetDefaultFollowees;
  RewardNodeProviders : RewardNodeProviders;
  ManageNetworkEconomics : NetworkEconomics;
  ApproveGenesisKyc : ApproveGenesisKyc;
  AddOrRemoveNodeProvider : AddOrRemoveNodeProvider;
  Motion : Motion;
};


-----------------------

/e2e/assets/project-import/sibling-project/canister/canister/the-sibling-candid-definition.did:
-----------------------

type AddFirewallRulesPayload = record {
  expected_hash : text;
  scope : FirewallRulesScope;
  positions : vec int32;
  rules : vec FirewallRule;
};
type AddNodeOperatorPayload = record {
  ipv6 : opt text;
  node_operator_principal_id : opt principal;
  node_allowance : nat64;
  rewardable_nodes : vec record { text; nat32 };
  node_provider_principal_id : opt principal;
  dc_id : text;
};


-----------------------

/e2e/assets/recurse/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="recurse.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/recurse/recurse.mo:
-----------------------

actor {
    public func recurse(n : Nat) : async () {
      if (n <= 0) () else await recurse(n - 1);
    };
};


-----------------------

/e2e/assets/remote/actual/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "remote": {
      "main": "remote.mo"
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}


-----------------------

/e2e/assets/remote/actual/remote.mo:
-----------------------

actor {
  var id : Text = "";
    
  public func write(v: Text) : async () {
    id := v;
  };
  public query func read() : async Text {
    id
  };

  public query func which_am_i() : async Text {
    "actual"
  };

  public func write_update_on_actual(v: Text) : async () {
    id := v;
  };
};


-----------------------

/e2e/assets/remote/basic/basic.mo:
-----------------------

import R "canister:remote";

actor {
    public func read_remote() : async Text {
        await R.read()
    };
};


-----------------------

/e2e/assets/remote/basic/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "basic": {
      "main": "basic.mo",
      "dependencies": [ "remote" ]
    },
    "remote": {
      "main": "remote.mo"
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}


-----------------------

/e2e/assets/remote/basic/remote.mo:
-----------------------

actor {
  var id : Text = "";
    
  public func write(v: Text) : async () {
    id := v;
  };
  public query func read() : async Text {
    id
  };

  public query func write_update_on_actual(v: Text) : async () {
    id := v;
  };

  // Subtle: since the code is different, the module hashes are different.
  // This makes it so tests can detect if an upgrade is attempted,
  // since install/deploy skip upgrades if the module hashes are the same.
  public query func which_am_i() : async Text {
    "mock"
  };
};


-----------------------

/e2e/assets/remote/call/actual/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "remote": {
      "main": "remote.mo"
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}


-----------------------

/e2e/assets/remote/call/actual/remote.mo:
-----------------------

actor {
  var id : Text = "";
    
  public func write(v: Text) : async () {
    id := v;
  };
  public query func read() : async Text {
    id
  };

  public query func which_am_i() : async Text {
    "actual"
  };

  public query func make_struct(a: Text, b: Text) : async { a: Text; b: Text; } {
    let result = { a = a; b = b; };
    result
  };

  public func actual_update_mock_query_remote_candid_update(v: Text) : async Text {
        v # " actual actual_update_mock_query_remote_candid_update"
  };
};


-----------------------

/e2e/assets/remote/call/mock/basic.mo:
-----------------------

import R "canister:remote";

actor {
    public func read_remote() : async Text {
        await R.read()
    };
};


-----------------------

/e2e/assets/remote/call/mock/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "basic": {
      "main": "basic.mo",
      "dependencies": [
        "remote"
      ]
    },
    "remote": {
      "main": "remote.mo",
      "remote": {
        "candid": "remote.did"
      }
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}

-----------------------

/e2e/assets/remote/call/mock/remote.did:
-----------------------

service : {
  actual_update_mock_query_remote_candid_update: (text) -> (text);
  make_struct: (text, text) -> (record { a: text; b: text; }) query;
  read: () -> (text) query;
  which_am_i: () -> (text) query;
  write: (text) -> ();
}


-----------------------

/e2e/assets/remote/call/mock/remote.mo:
-----------------------

actor {
  var id : Text = "";
    
  public func write(v: Text) : async () {
    id := v;
  };
  public query func read() : async Text {
    id
  };

  public query func actual_update_mock_query_remote_candid_update(v: Text) : async Text {
    v # " mock actual_update_mock_query_remote_candid_update"
  };

  // Subtle: since the code is different, the module hashes are different.
  // This makes it so tests can detect if an upgrade is attempted,
  // since install/deploy skip upgrades if the module hashes are the same.
  public query func which_am_i() : async Text {
    "mock"
  };
};


-----------------------

/e2e/assets/remote/envvar/dfx.json:
-----------------------

{
    "canisters": {
        "basic": {
            "type": "custom",
            "candid": "main.did",
            "wasm": "main.wasm",
            "build": "bash -c 'echo \"CANISTER_ID_REMOTE: $CANISTER_ID_REMOTE\nCANISTER_CANDID_PATH_REMOTE: $CANISTER_CANDID_PATH_REMOTE\"'",
            "dependencies": [
                "remote"
            ]
        },
        "remote": {
            "main": "remote.mo",
            "remote": {
                "candid": "remotecandid.did",
                "id": {
                    "actuallylocal": "qoctq-giaaa-aaaaa-aaaea-cai"
                }
            }
        }
    }
}

-----------------------

/e2e/assets/remote/envvar/remotecandid.did:
-----------------------

service {}


-----------------------

/e2e/assets/remote/extra/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "extra": {
      "main": "extra.mo",
      "dependencies": [ "remote" ]
    },
    "remote": {
      "main": "remote.mo",
      "remote": {
        "candid": "remote.did"
      }
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}


-----------------------

/e2e/assets/remote/extra/extra.mo:
-----------------------

import R "canister:remote";

actor {
    public func read_remote() : async Text {
        await R.read()
    };

    public func remote_extra() : async Text {
        await R.something_extra()
    };
};


-----------------------

/e2e/assets/remote/extra/remote.did:
-----------------------

service : {
  get_id: () -> (text) query;
  set_id: (text) -> ();
}


-----------------------

/e2e/assets/remote/extra/remote.mo:
-----------------------

actor {
  var id : Text = "";

  //
  public func something_extra() : async Text {
    "extra!"
  };
    
  public func write(v: Text) : async () {
    id := v;
  };
  public query func read() : async Text {
    id
  };

  // Subtle: since the code is different, the module hashes are different.
  // This makes it so tests can detect if an upgrade is attempted,
  // since the module hashes would be different.
  public query func which_am_i() : async Text {
    "mock"
  };
};


-----------------------

/e2e/assets/remote_generate_binding/basic/dfx.json:
-----------------------

{
  "version": 1,
  "canisters": {
    "remote-motoko": {
      "main": "remote.mo",
      "remote": {
        "candid": "remote.did",
        "id": {}
      }
    },
    "remote-rust": {
      "main": "remote.rs",
      "remote": {
        "candid": "remote.did",
        "id": {}
      }
    },
    "remote-typescript": {
      "main": "remote.ts",
      "remote": {
        "candid": "remote.did",
        "id": {}
      }
    },
    "remote-javascript": {
      "main": "remote.js",
      "remote": {
        "candid": "remote.did",
        "id": {}
      }
    }
  },
  "defaults": {
    "build": {
      "output": "canisters/"
    }
  }
}

-----------------------

/e2e/assets/remote_generate_binding/basic/remote.did:
-----------------------

service : {
    "greet": (text) -> (text) query;
}

-----------------------

/e2e/assets/rust_complex/Cargo.toml:
-----------------------

[workspace]
members = ["can1", "can2"]
edition = 2021
resolver = "2"

[workspace.dependencies]
ic-cdk = "0.16"
candid = "0.10"


-----------------------

/e2e/assets/rust_complex/can1/Cargo.toml:
-----------------------

[package]
name = "can1"
version = "0.0.0"

[dependencies]
ic-cdk.workspace = true
candid.workspace = true


-----------------------

/e2e/assets/rust_complex/can1/src/main.rs:
-----------------------

#![no_main]

#[ic_cdk::query]
fn id() -> u32 {
    1
}


-----------------------

/e2e/assets/rust_complex/can2/Cargo.toml:
-----------------------

[package]
name = "can2"
version = "0.0.0"

[dependencies]
ic-cdk.workspace = true
candid.workspace = true

[lib]
crate-type = ["cdylib"]


-----------------------

/e2e/assets/rust_complex/can2/src/bin/can3.rs:
-----------------------

#![no_main]

#[ic_cdk::query]
fn id() -> u32 {
    3
}


-----------------------

/e2e/assets/rust_complex/can2/src/lib.rs:
-----------------------

#[ic_cdk::query]
fn id() -> u32 {
    2
}


-----------------------

/e2e/assets/rust_complex/dfx.json:
-----------------------

{
    "version": 1,
    "canisters": {
        "can1": {
            "type": "rust",
            "package": "can1",
            "candid": "interface.did"
        },
        "can2": {
            "type": "rust",
            "package": "can2",
            "candid": "interface.did"
        },
        "can3": {
            "type": "rust",
            "package": "can2",
            "crate": "can3",
            "candid": "interface.did"
        }
    }
}

-----------------------

/e2e/assets/rust_complex/interface.did:
-----------------------

service : {
    "id" : () -> (nat32) query;
};


-----------------------

/e2e/assets/rust_deps/Cargo.toml:
-----------------------

[workspace]
members = [
    "src/rust_deps",
]


-----------------------

/e2e/assets/rust_deps/dfx.json:
-----------------------

{
  "canisters": {
    "multiply_deps": {
      "main": "src/multiply_deps/main.mo",
      "type": "motoko"
    },
    "rust_deps": {
      "candid": "src/rust_deps/rust_deps.did",
      "package": "rust_deps",
      "type": "rust",
      "dependencies": [
        "multiply_deps"
      ]
    }
  },
  "defaults": {
    "replica": {
      "subnet_type": "verifiedapplication"
    }
  },
  "version": 1
}

-----------------------

/e2e/assets/rust_deps/src/multiply_deps/main.mo:
-----------------------

actor Multiply {

    var cell : Nat = 1;

    public func mul(n:Nat) : async Nat { cell *= n*3; cell };

    public query func read() : async Nat {
        cell
    };
}



-----------------------

/e2e/assets/rust_deps/src/rust_deps/Cargo.toml:
-----------------------

[package]
name = "rust_deps"
version = "0.1.0"
edition = "2018"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
path = "lib.rs"
crate-type = ["cdylib"]

[dependencies]
candid = "0.10"
ic-cdk = "0.16"

[build-dependencies]
ic-cdk-bindgen = "0.1"


-----------------------

/e2e/assets/rust_deps/src/rust_deps/build.rs:
-----------------------

use ic_cdk_bindgen::{Builder, Config};
use std::path::PathBuf;

fn main() {
    let manifest_dir =
        PathBuf::from(std::env::var("CARGO_MANIFEST_DIR").expect("Cannot find manifest dir"));
    let counter = Config::new("multiply_deps");
    let mut builder = Builder::new();
    builder.add(counter);
    builder.build(Some(manifest_dir.join("declarations")));
}


-----------------------

/e2e/assets/rust_deps/src/rust_deps/lib.rs:
-----------------------

use ic_cdk::update;

mod declarations;
use declarations::multiply_deps::multiply_deps;

// Inter-canister call can only be from a update call
#[update]
async fn read() -> candid::Nat {
    multiply_deps.read().await.unwrap().0
}


-----------------------

/e2e/assets/rust_deps/src/rust_deps/rust_deps.did:
-----------------------

service : {
  "read": () -> (nat);
}


-----------------------

/e2e/assets/subnet_type/project_defaults/application/dfx.json:
-----------------------

{
    "defaults": {
        "replica": {
            "subnet_type": "application"
        }
    }
}

-----------------------

/e2e/assets/subnet_type/project_defaults/system/dfx.json:
-----------------------

{
    "defaults": {
        "replica": {
            "subnet_type": "system"
        }
    }
}

-----------------------

/e2e/assets/subnet_type/project_defaults/verified_application/dfx.json:
-----------------------

{
    "defaults": {
        "replica": {
            "subnet_type": "verifiedapplication"
        }
    }
}

-----------------------

/e2e/assets/subnet_type/project_network_settings/application/dfx.json:
-----------------------

{
    "networks": {
        "local": {
            "replica": {
                "subnet_type": "application"
            }
        }
    }
}

-----------------------

/e2e/assets/subnet_type/project_network_settings/system/dfx.json:
-----------------------

{
    "networks": {
        "local": {
            "replica": {
                "subnet_type": "system"
            }
        }
    }
}

-----------------------

/e2e/assets/subnet_type/project_network_settings/verified_application/dfx.json:
-----------------------

{
    "networks": {
        "local": {
            "replica": {
                "subnet_type": "verifiedapplication"
            }
        }
    }
}

-----------------------

/e2e/assets/subnet_type/shared_network_settings/application/networks.json:
-----------------------

{
    "local": {
        "replica": {
            "subnet_type": "application"
        }
    }
}

-----------------------

/e2e/assets/subnet_type/shared_network_settings/system/networks.json:
-----------------------

{
    "local": {
        "replica": {
            "subnet_type": "system"
        }
    }
}

-----------------------

/e2e/assets/subnet_type/shared_network_settings/verified_application/networks.json:
-----------------------

{
    "local": {
        "replica": {
            "subnet_type": "verifiedapplication"
        }
    }
}

-----------------------

/e2e/assets/transitive_deps_canisters/a/main.mo:
-----------------------

actor {
    public func greet(name : Text) : async Text {
        return "Namaste, " # name # "!";
    };
};


-----------------------

/e2e/assets/transitive_deps_canisters/b/main.mo:
-----------------------

actor {
    public func greet(name : Text) : async Text {
        return "Hola, " # name # "!";
    };
};


-----------------------

/e2e/assets/transitive_deps_canisters/c/main.mo:
-----------------------

actor {
    public func greet(name : Text) : async Text {
        return "Hello, " # name # "!";
    };
};


-----------------------

/e2e/assets/transitive_deps_canisters/d/main.mo:
-----------------------

actor {
    public func greet(name : Text) : async Text {
        return "Hello, " # name # "!";
    };
};


-----------------------

/e2e/assets/transitive_deps_canisters/dfx.json:
-----------------------

{
  "canisters": {
    "canister_a": {
      "main": "./a/main.mo",
      "type": "motoko"
    },
    "canister_b": {
      "dependencies": ["canister_a"],
      "main": "./b/main.mo",
      "type": "motoko"
    },
    "canister_c": {
      "dependencies": ["canister_b"],
      "main": "./c/main.mo",
      "type": "motoko"
    },
    "canister_d": {
      "dependencies": ["canister_e"],
      "main": "./d/main.mo",
      "type": "motoko"
    },
    "canister_e": {
      "dependencies": ["canister_d"],
      "main": "./e/main.mo",
      "type": "motoko"
    },
    "canister_f": {
      "dependencies": ["canister_g", "canister_h"],
      "main": "./f/main.mo",
      "type": "motoko"
    },
    "canister_g": {
      "dependencies": ["canister_a"],
      "main": "./g/main.mo",
      "type": "motoko"
    },
    "canister_h": {
      "dependencies": ["canister_a"],
      "main": "./h/main.mo",
      "type": "motoko"
    }
  },
  "defaults": {
    "build": {
      "packtool": ""
    }
  }
}


-----------------------

/e2e/assets/transitive_deps_canisters/e/main.mo:
-----------------------

actor {
    public func greet(name : Text) : async Text {
        return "Hello, " # name # "!";
    };
};


-----------------------

/e2e/assets/transitive_deps_canisters/f/main.mo:
-----------------------

actor {
    public func greet(name : Text) : async Text {
        return "BunÄƒ, " # name # "!";
    };
};


-----------------------

/e2e/assets/transitive_deps_canisters/g/main.mo:
-----------------------

actor {
    public func greet(name : Text) : async Text {
        return "Ciao, " # name # "!";
    };
};


-----------------------

/e2e/assets/transitive_deps_canisters/h/main.mo:
-----------------------

/e2e/assets/upgrade/patch.bash:
-----------------------

jq '.canisters.hello_backend.main="v1.mo"' dfx.json | sponge dfx.json


-----------------------

/e2e/assets/upgrade/v1.mo:
-----------------------

actor {
  stable var state : Int = 0;
  public func inc() : async Int {
    state += 1;
    return state;
  };
  public func f() : async ?Int {
    return ?42;
  };
  public query func read() : async Int { return state; };
}



-----------------------

/e2e/assets/upgrade/v2.mo:
-----------------------

import Int "mo:base/Int";
actor {
  stable var state : Int = 0;
  stable var new_state : Nat = Int.abs(state);
  public func inc() : async Nat {
    new_state += 1;
    return new_state;
  };
  public func f() : async ?Int {
    return ?42;
  };
  public query func read() : async Nat { return new_state; };
}



-----------------------

/e2e/assets/upgrade/v2_bad.mo:
-----------------------

actor {
  stable var state : Nat = 0;
  public func inc() : async Nat {
    state += 1;
    return state;
  };
  public func f() : async ?Int {
    return ?42;
  };
  public query func read() : async Nat { return state; };
}



-----------------------

/e2e/assets/upgrade/v3_bad.mo:
-----------------------

actor {
  stable var state : Int = 0;
  public func inc2() : async Int {
    state += 1;
    return state;
  };
  public func f() : async ?Int {
    return ?42;
  };
  public query func read2() : async Int { return state; };
}



-----------------------

/e2e/assets/upgrade/v4_bad.mo:
-----------------------

actor {
  stable var state : Int = 0;
  public func inc() : async Int {
    state += 1;
    return state;
  };
  public func f() : async ?Text {
    return ?"";
  };
  public query func read() : async Int { return state; };
}



-----------------------

/e2e/assets/upgrade/v5.mo:
-----------------------

actor {
  stable var newState : Int = 0;
  public func inc() : async Int {
    newState += 1;
    return newState;
  };
  public func f() : async ?Int {
    return ?42;
  };
  public query func read() : async Int { return newState; };
}


-----------------------

/e2e/assets/vulnerable_rust_deps/Cargo.toml:
-----------------------

[workspace]
members = [
    "src/hello",
]

-----------------------

/e2e/assets/vulnerable_rust_deps/dfx.json:
-----------------------

{
    "canisters": {
        "hello": {
            "candid": "src/hello/hello.did",
            "package": "hello",
            "type": "rust"
        }
    }
}

-----------------------

/e2e/assets/vulnerable_rust_deps/src/hello/Cargo.toml:
-----------------------

[package]
name = "hello"
version = "0.1.0"
edition = "2018"

[lib]
crate-type = ["cdylib"]

[dependencies]
candid = "0.7.4"
ic-cdk = "0.7"
time = "0.1.43"


-----------------------

/e2e/assets/vulnerable_rust_deps/src/hello/hello.did:
-----------------------

service : {
    "greet": (text) -> (text) query;
}

-----------------------

/e2e/assets/vulnerable_rust_deps/src/hello/src/lib.rs:
-----------------------

use std::time::Duration;

#[ic_cdk::query]
fn greet(name: String) -> String {
    let _dur = Duration::from_secs(1);
    format!("Hello, {}!", name)
}


-----------------------

/e2e/assets/warning/main.mo:
-----------------------

actor {
  func returnsLargeTuple() : (Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat,Nat) = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20);
  func wantsLargeTuple(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20) = ();
  let _ = wantsLargeTuple(returnsLargeTuple());
}



-----------------------

/e2e/assets/warning/patch.bash:
-----------------------

jq '.canisters.e2e_project_backend.main="main.mo"' dfx.json | sponge dfx.json


-----------------------

= How to build the main.wasm

The code for this Wasm is in `/e2e/assets/identity/identity.mo`.

Follow the instructions there; create a new dfx project, overwrite the project's
main.mo with the file above, and then rebuild it.
Take the resulting Wasm and DID and put them here.

- [ ] TODO(#682)


-----------------------

/e2e/assets/wasm/identity/main.did:
-----------------------

service : {
  amInitializer: () -> (bool) query;
  fromCall: () -> (principal);
  fromQuery: () -> (principal) query;
  getCanisterId: () -> (principal) query;
  isMyself: (principal) -> (bool) query;
}


-----------------------

/e2e/assets/wasm/identity/main.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/assets/wasm/identity/main.wasm

-----------------------

/e2e/assets/wasm_memory_persistence/README.md:
-----------------------

`classical-actor.wasm` is built from `actor.mo` with `moc -o classical-actor.wasm actor.mo`.
`enhanced-actor.wasm` is built from `actor.mo` with `moc -o enhanced-actor.wasm --enhanced-orthogonal-persistence actor.mo`.
`actor.did` is obtained by `moc --idl actor.mo`.


-----------------------

/e2e/assets/wasm_memory_persistence/actor.did:
-----------------------

service : {
  getVersion: () -> (nat) query;
}


-----------------------

/e2e/assets/wasm_memory_persistence/actor.mo:
-----------------------

import Prim "mo:prim";

actor {
    stable var version = 0;

    version += 1;
    Prim.debugPrint("Deployed actor version " # debug_show (version));

    public query func getVersion() : async Nat {
        return version;
    };
};


-----------------------

/e2e/assets/wasm_memory_persistence/actor.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/assets/wasm_memory_persistence/actor.wasm

-----------------------

/e2e/assets/wasm_memory_persistence/classical-actor.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/assets/wasm_memory_persistence/classical-actor.wasm

-----------------------

/e2e/assets/wasm_memory_persistence/dfx.json:
-----------------------

{
  "canisters": {
    "test": {
      "type": "custom",
      "candid": "actor.did",
      "wasm": "classical-actor.wasm"
    }
  }
}


-----------------------

/e2e/assets/wasm_memory_persistence/enhanced-actor.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/e2e/assets/wasm_memory_persistence/enhanced-actor.wasm

-----------------------

/e2e/assets/whoami/Main.mo:
-----------------------

actor WhoAmI {
  public shared ({caller}) func whoami() : async Principal {
    return caller;
  };
};


-----------------------

/e2e/assets/whoami/dfx.json:
-----------------------

{
  "canisters": {
    "whoami": {
      "main": "Main.mo"
    }
  }
}


-----------------------

/e2e/tests-dfx/assetscanister.bash:
-----------------------

/e2e/tests-dfx/base.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "provides base library location by default" {
  install_asset base

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install e2e_project_backend

  assert_command dfx canister call --query e2e_project_backend is_digit '("5")'
  assert_eq '(true)'

  assert_command dfx canister call --query e2e_project_backend is_digit '("w")'
  assert_eq '(false)'
}

@test "does not provide base library if there is a packtool" {
  install_asset base
  jq '.defaults.build.packtool="echo"' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  assert_command_fail dfx build
  assert_match 'import error \[M0010\], package "base" not defined'
}


-----------------------

/e2e/tests-dfx/basic-project.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "build + install + call + request-status -- greet_mo" {
  dfx_new hello
  install_asset greet
  dfx_start --artificial-delay 10000
  dfx canister create hello_backend
  dfx build hello_backend
  # INSTALL_REQUEST_ID=$(dfx canister install hello_backend --async)
  # dfx canister request-status $INSTALL_REQUEST_ID
  dfx canister install hello_backend

  assert_command dfx canister call hello_backend greet '("Banzai")'
  assert_eq '("Hello, Banzai!")'

  assert_command dfx canister call hello_backend greet --type raw '4449444c00017103e29883'
  assert_eq '("Hello, â˜ƒ!")'

  assert_command dfx canister call --query hello_backend greet '("Bongalo")'
  assert_eq '("Hello, Bongalo!")'

  # Using call --async and request-status.
  # Call with user Identity as Sender
  assert_command dfx canister call --async hello_backend greet Blueberry
  # At this point $output is the request ID.
  # shellcheck disable=SC2154
  assert_command dfx canister request-status "$stdout" "$(dfx canister id hello_backend)"
  assert_eq '("Hello, Blueberry!")'

  # Call using the wallet's call forwarding
  assert_command dfx canister call --async hello_backend greet Blueberry --wallet=default
  # At this point $output is the request ID.
  # shellcheck disable=SC2154
  assert_command dfx canister request-status "$stdout" "$(dfx identity get-wallet)"
  assert_eq \
'(
  variant {
    17_724 = record {
      153_986_224 = blob "\44\49\44\4c\00\01\71\11\48\65\6c\6c\6f\2c\20\42\6c\75\65\62\65\72\72\79\21";
    }
  },
)'
}

@test "build + install + call + request-status -- counter_mo" {
  dfx_new hello
  install_asset counter
  dfx_start --artificial-delay 10000
  dfx canister create hello_backend
  dfx build hello_backend
  dfx canister install hello_backend

  assert_command dfx canister call hello_backend read
  assert_eq "(0 : nat)"

  assert_command dfx canister call hello_backend inc
  assert_eq "()"

  assert_command dfx canister call hello_backend read
  assert_eq "(1 : nat)"

  dfx canister call hello_backend inc
  assert_command dfx canister call hello_backend read
  assert_eq "(2 : nat)"

  assert_command dfx canister call hello_backend read --output raw
  assert_eq "4449444c00017d02"

  assert_command_fail dfx canister call --query hello_backend inc
  assert_match "Not a query method."


  dfx canister call hello_backend inc
  assert_command dfx canister call --query hello_backend read
  assert_eq "(3 : nat)"

  assert_command dfx canister call hello_backend inc --async
  assert_command dfx canister request-status "$stdout" "$(dfx canister id hello_backend)"

  # Call write.
  assert_command dfx canister call hello_backend write 1337
  assert_eq "()"

  # Write has no return value. But we can _call_ read too.
  # Call with user Identity as Sender
  assert_command dfx canister call hello_backend read --async
  assert_command dfx canister request-status "$stdout" "$(dfx canister id hello_backend)"
  assert_eq "(1_337 : nat)"

  # Call using the wallet's call forwarding
  assert_command dfx canister call hello_backend read --async --wallet=default
  assert_command dfx canister request-status "$stdout" "$(dfx identity get-wallet)"
  assert_eq \
'(
  variant {
    17_724 = record { 153_986_224 = blob "\44\49\44\4c\00\01\7d\b9\0a" }
  },
)'
}

@test "build + install + call -- counter_idl_mo" {
  dfx_new hello
  install_asset counter_idl
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  assert_command dfx canister call hello_backend inc '(42,false,"testzZ",vec{1;2;3},opt record{head=42; tail=opt record{head=+43; tail=null}}, variant { cons=record{ 42; variant { cons=record{43; variant { nil }} } } })'  --output idl
  assert_eq "(43 : int, true, \"uftu{[\", vec { 2 : nat; 3 : nat; 4 : nat;}, opt record { head = 43 : int; tail = opt record { head = 44 : int; tail = null;};}, variant { cons = record { 43 : int; variant { cons = record { 44 : int; variant { nil };} };} })"
}

@test "build + install + call -- matrix_multiply_mo" {
  dfx_new hello
  install_asset matrix_multiply
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  assert_command dfx canister call hello_backend multiply '(vec{vec{1;2};vec{3;4};vec{5;6}},vec{vec{1;2;3};vec{4;5;6}})'
  assert_eq \
"(
  vec {
    vec { 9 : int; 12 : int; 15 : int };
    vec { 19 : int; 26 : int; 33 : int };
    vec { 29 : int; 40 : int; 51 : int };
  },
)"
}

@test "inspect message - motoko" {
  dfx_new hello
  install_asset inspect_message
  dfx_start
  dfx deploy

  assert_command dfx canister call hello_backend always_accepted

  assert_command_fail dfx canister call hello_backend always_rejected
  assert_contains "canister_inspect_message explicitly refused message"
}

@test "inspect message - rust" {
  dfx_new_rust hello

  install_asset inspect_message_rs

  dfx_start
  dfx deploy

  assert_command dfx canister call hello_backend always_accepted

  assert_command_fail dfx canister call hello_backend always_rejected
  assert_contains "Canister rejected the message"
}


-----------------------

/e2e/tests-dfx/bitcoin.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

BITCOIN_CANISTER_ID="g4xu7-jiaaa-aaaan-aaaaq-cai"

setup() {
  standard_setup

  bitcoind -regtest -daemonwait
}

teardown() {
  bitcoin-cli -regtest stop

  dfx_stop
  standard_teardown
}

set_project_default_bitcoin_enabled() {
  jq '.defaults.bitcoin.enabled=true' dfx.json | sponge dfx.json
}

set_shared_local_network_bitcoin_enabled() {
  create_networks_json
  jq '.local.bitcoin.enabled=true' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
}

set_local_network_bitcoin_enabled() {
  jq '.networks.local.bitcoin.enabled=true' dfx.json | sponge dfx.json
}

@test "noop" {
  assert_command bitcoin-cli -regtest createwallet "test"
  ADDRESS="$(bitcoin-cli -regtest getnewaddress)"
  assert_command bitcoin-cli -regtest generatetoaddress 101 "$ADDRESS"
}

@test "dfx restarts replica when ic-btc-adapter restarts" {
  dfx_new_assets hello
  dfx_start --enable-bitcoin

  install_asset greet
  assert_command dfx deploy
  assert_command dfx canister call hello_backend greet '("Alpha")'
  assert_eq '("Hello, Alpha!")'

  REPLICA_PID=$(get_replica_pid)
  BTC_ADAPTER_PID=$(get_btc_adapter_pid)

  echo "replica pid is $REPLICA_PID"
  echo "ic-btc-adapter pid is $BTC_ADAPTER_PID"

  kill -KILL "$BTC_ADAPTER_PID"
  assert_process_exits "$BTC_ADAPTER_PID" 15s
  assert_process_exits "$REPLICA_PID" 15s

  timeout 30s sh -c \
    'until dfx ping; do echo waiting for replica to restart; sleep 1; done' \
    || (echo "replica did not restart" && ps aux && exit 1)
  wait_until_replica_healthy

  # Sometimes initially get an error like:
  #     IC0537: Attempt to execute a message on canister <>> which contains no Wasm module
  # but the condition clears.
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait 1\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting
  # even after the above, still sometimes fails with
  #     IC0208: Certified state is not available yet. Please try again...
  sleep 10
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait 2\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting

  assert_command dfx canister call hello_backend greet '("Omega")'
  assert_eq '("Hello, Omega!")'

  ID=$(dfx canister id hello_frontend)

  timeout 15s sh -c \
    "until curl --fail http://localhost:\$(cat \"$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/webserver-port\")/sample-asset.txt?canisterId=$ID; do echo waiting for pocket-ic proxy to restart; sleep 1; done" \
    || (echo "pocket-ic proxy did not restart" && ps aux && exit 1)

  assert_command curl --fail http://localhost:"$(get_webserver_port)"/sample-asset.txt?canisterId="$ID"
}

@test "dfx start --bitcoin-node <node> implies --enable-bitcoin" {
  dfx_new hello
  dfx_start "--bitcoin-node" "127.0.0.1:18444"

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-btc-adapter-pid"
}

@test "dfx start --enable-bitcoin with no other configuration succeeds" {
  dfx_new hello

  dfx_start --enable-bitcoin

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-btc-adapter-pid"
}

@test "dfx start --enable-bitcoin --background waits until bitcoin canister is installed" {
  dfx_new hello

  dfx_start --enable-bitcoin

  assert_command dfx canister info "$BITCOIN_CANISTER_ID"
  assert_contains "Controllers: 2vxsx-fae"
  assert_contains "Module hash: 0x"
}

@test "can enable bitcoin through default configuration - dfx start" {
  dfx_new hello
  define_project_network
  set_project_default_bitcoin_enabled

  dfx_start

  assert_file_not_empty .dfx/network/local/ic-btc-adapter-pid
}

@test "can enable bitcoin through shared local network - dfx start" {
  dfx_new hello
  set_shared_local_network_bitcoin_enabled

  dfx_start

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-btc-adapter-pid"
}

@test "can enable bitcoin through local network configuration - dfx start" {
  dfx_new hello
  set_local_network_bitcoin_enabled

  dfx_start

  assert_file_not_empty .dfx/network/local/ic-btc-adapter-pid
}

@test "dfx start with both bitcoin and canister http enabled" {
  dfx_new hello

  dfx_start --enable-bitcoin --enable-canister-http

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-btc-adapter-pid"
  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-https-outcalls-adapter-pid"

  install_asset greet
  assert_command dfx deploy
  assert_command dfx canister call hello_backend greet '("Alpha")'
  assert_eq '("Hello, Alpha!")'
}


@test "bitcoin canister has decent amount of cycles" {
  dfx_start --enable-bitcoin
  # The canister is created with default amount of cycles: 100T
  cycles_balance=$( dfx --identity anonymous canister status "$BITCOIN_CANISTER_ID" 2>&1 | grep 'Balance:' | sed 's/[^0-9]//g' )
  assert_command test "$cycles_balance" -gt 99000000000000 # 99T
}

@test "can call bitcoin API of the management canister" {
  install_asset bitcoin
  dfx_start --enable-bitcoin
  dfx identity get-wallet

  # the non-query Bitcoin API can only be called by a canister not an agent
  # we need to proxy the call through the wallet canister

  # bitcoin_get_balance
  assert_command dfx canister call --wallet default aaaaa-aa --candid bitcoin.did bitcoin_get_balance '(
  record {
    network = variant { regtest };
    address = "bcrt1qu58aj62urda83c00eylc6w34yl2s6e5rkzqet7";
    min_confirmations = opt (1 : nat32);
  }
)'
  assert_eq "(0 : nat64)"

  # bitcoin_get_utxos
  assert_command dfx canister call --wallet default aaaaa-aa --candid bitcoin.did bitcoin_get_utxos '(
  record {
    network = variant { regtest };
    filter = opt variant { min_confirmations = 1 : nat32 };
    address = "bcrt1qu58aj62urda83c00eylc6w34yl2s6e5rkzqet7";
  }
)'
  assert_contains "tip_height = 0 : nat32;"

  # bitcoin_get_current_fee_percentiles
  assert_command dfx canister call --wallet default aaaaa-aa --candid bitcoin.did bitcoin_get_current_fee_percentiles '(record { network = variant { regtest } })'

  # bitcoin_send_transaction
  # It's hard to test this without a real transaction, but we can at least check that the call fails.
  # The error message indicates that the argument is in correct format, only the inner transaction is malformed.
  assert_command_fail dfx canister call --wallet default aaaaa-aa --candid bitcoin.did bitcoin_send_transaction '(record { transaction = vec {0:nat8}; network = variant { regtest } })'
  assert_contains "send_transaction failed: MalformedTransaction"
}


-----------------------

/e2e/tests-dfx/build.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  stop_webserver
  dfx_stop

  standard_teardown
}

@test "can build a custom canister with wasm and/or candid from a url" {
  install_asset wasm/identity
  mkdir -p www/wasm
  mv main.wasm www/wasm/
  mv main.did www/wasm
  start_webserver --directory www
  dfx_start

  dfx_new

  jq '.canisters={}' dfx.json | sponge dfx.json

  jq '.canisters.e2e_project.candid="http://localhost:'"$E2E_WEB_SERVER_PORT"'/wasm/main.did"' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project.wasm="http://localhost:'"$E2E_WEB_SERVER_PORT"'/wasm/main.wasm"' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project.type="custom"' dfx.json | sponge dfx.json

  dfx deploy

  ID=$(dfx canister id e2e_project)
  assert_command dfx canister call e2e_project getCanisterId
  assert_match "$ID"
}

@test "report an error if a canister defines both a wasm url and a build step" {
  install_asset wasm/identity
  mkdir -p www/wasm
  mv main.wasm www/wasm/
  mv main.did www/wasm
  start_webserver --directory www
  dfx_start

  dfx_new

  jq '.canisters={}' dfx.json | sponge dfx.json

  jq '.canisters.e2e_project.candid="http://localhost:'"$E2E_WEB_SERVER_PORT"'/wasm/main.did"' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project.wasm="http://localhost:'"$E2E_WEB_SERVER_PORT"'/wasm/main.wasm"' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project.type="custom"' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project.build="echo nope"' dfx.json | sponge dfx.json

  assert_command_fail dfx deploy
  assert_contains "Canister 'e2e_project' defines its wasm field as a URL, and has a build step."
}

@test "build uses default build args" {
  install_asset default_args
  assert_command_fail dfx build --check
  assert_match "unknown option"
  assert_match "compacting-gcX"
}

@test "build uses canister build args" {
  install_asset canister_args
  assert_command_fail dfx build --check
  assert_match "unknown option"
  assert_match "compacting-gcY"
  assert_not_match "compacting-gcX"
}

@test "empty canister build args don't shadow default" {
  install_asset empty_canister_args
  assert_command_fail dfx build --check
  assert_match '"--error-detail" "5"'
  assert_match "unknown option"
  assert_match "compacting-gcX"
}

@test "build fails on invalid motoko" {
  install_asset invalid
  dfx_start
  dfx canister create --all
  assert_command_fail dfx build
  assert_match "syntax error"
}

@test "build --check fails on build error when there are no canister ids" {
  install_asset invalid
  assert_command_fail dfx build --check
  assert_match "syntax error"
}

@test "build --check fails on build error when there are canister ids" {
  dfx_start
  dfx canister create --all
  install_asset invalid
  assert_command_fail dfx build --check
  assert_match "syntax error"
}

@test "build supports relative imports" {
  install_asset import
  dfx_start
  dfx canister create --all
  assert_command dfx build
  dfx canister install --all
  assert_command dfx canister call e2e_project_backend greet World
  assert_match "10World"
}

@test "build supports auto-generated idl for management canister imports in motoko" {
    install_asset motoko_management
    dfx_start
    dfx canister create --all
    assert_command dfx build
    dfx deploy
    assert_command dfx canister call e2e_project_backend rand
}

@test "build supports auto-generated idl for recursive management canister imports in motoko" {
    install_asset motoko_management_recursive
    dfx_start
    dfx canister create --all
    assert_command dfx build
    dfx deploy
    assert_command dfx canister call e2e_project_backend rand
}

@test "build succeeds on default project" {
  dfx_start
  dfx canister create --all
  assert_command dfx build
}

@test "build succeeds if enable optimize" {
  jq '.canisters.e2e_project_backend.optimize="cycles"' dfx.json | sponge dfx.json
  dfx_start
  dfx canister create --all
  assert_command dfx build
}

@test "build custom canister default no shrink" {
  install_asset custom_canister
  install_asset wasm/identity

  dfx_start
  dfx canister create --all
  assert_command dfx build custom -vvv
  assert_not_match "Shrinking Wasm"

  jq '.canisters.custom.shrink=true' dfx.json | sponge dfx.json
  assert_command dfx build custom -vvv
  assert_match "Shrinking Wasm"
}

@test "build custom canister default no optimize" {
  install_asset custom_canister
  install_asset wasm/identity

  dfx_start
  dfx canister create --all
  assert_command dfx build custom -vvv
  assert_not_match "Optimizing"

  jq '.canisters.custom.optimize="size"' dfx.json | sponge dfx.json
  assert_command dfx build custom -vvv
  assert_match "Optimizing Wasm at level"
}

@test "build succeeds if enable gzip" {
  install_asset base
  jq '.canisters.e2e_project_backend.gzip=true' dfx.json | sponge dfx.json
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_file_exists .dfx/local/canisters/e2e_project_backend/e2e_project_backend.wasm.gz
}

@test "build succeeds if specify gzip wasm" {
  install_asset gzip
  install_asset wasm/identity
  dfx_start
  dfx canister create --all
  assert_command dfx build
}

# TODO: Before Tungsten, we need to update this test for code with inter-canister calls.
# Currently due to new canister ids, the wasm binary will be different for inter-canister calls.
@test "build twice produces the same wasm binary" {
  dfx_start
  dfx canister create --all
  assert_command dfx build
  cp .dfx/local/canisters/e2e_project_backend/e2e_project_backend.wasm ./old.wasm
  assert_command dfx build
  assert_command diff .dfx/local/canisters/e2e_project_backend/e2e_project_backend.wasm ./old.wasm
}

@test "build outputs warning" {
  install_asset warning
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_match "warning \[M0145\], this pattern of type"
}

@test "build fails on unknown imports" {
  install_asset import_error
  dfx_start
  dfx canister create --all
  assert_command_fail dfx build
  assert_match 'import error \[M0011\], canister alias "random" not defined'
}

@test "build fails if canister type is not supported" {
  dfx_start
  dfx canister create --all
  jq '.canisters.e2e_project_backend.type="unknown_canister_type"' dfx.json | sponge dfx.json
  assert_command_fail dfx build
  # shellcheck disable=SC2016
  assert_match "canister 'e2e_project_backend' has unknown type 'unknown_canister_type' and there is no installed extension by that name which could define it"

  # If canister type is invalid, `dfx stop` fails
  jq '.canisters.e2e_project_backend.type="motoko"' dfx.json | sponge dfx.json
}

@test "can build a custom canister type" {
  install_asset custom_canister
  install_asset wasm/identity
  dfx_start
  dfx canister create --all
  assert_command dfx build custom
  assert_match "CUSTOM_CANISTER_BUILD_DONE"
  assert_command dfx build custom2
  assert_match "CUSTOM_CANISTER2_BUILD_DONE"
  assert_command dfx build custom3

  dfx canister install --all
  assert_command dfx canister call custom fromQuery
  assert_command dfx canister call custom2 fromQuery

  # dfx sets the candid:service metadata
  dfx canister metadata custom candid:service >installed.did
  assert_command diff main.did installed.did
}

@test "upgrade check writes .old.did under .dfx" {
  install_asset custom_canister
  install_asset wasm/identity

  dfx_start
  dfx deploy

  echo yes | dfx deploy --mode=reinstall custom

  # dfx intentionally leaves this file after creating it for comparison,
  # so that the developer can look at the differences too.
  # This test makes sure that the file is created under the .dfx/ directory,
  # which is where other temporary / build artifacts go.
  assert_file_not_exists ./main.old.did
  assert_file_exists .dfx/local/canisters/custom/constructor.old.did
}

@test "custom canister build script picks local executable first" {
  install_asset custom_canister
  install_asset wasm/identity

  dfx_start
  dfx canister create custom2
  jq '.canisters.custom2.build="ln"' dfx.json | sponge dfx.json
  mv ./build.sh ./ln

  assert_command dfx build custom2
  assert_match CUSTOM_CANISTER2_BUILD_DONE
}

@test "custom canister build script runs in project root" {
  install_asset custom_canister
  install_asset wasm/identity

  dfx_start
  dfx canister create custom2

  cd src/e2e_project_backend
  pwd

  assert_command dfx build custom2
  assert_match CUSTOM_CANISTER2_BUILD_DONE
  assert_match "working directory of build script: '.*/working-dir/e2e_project'"
}

@test "build succeeds with network parameter" {
  dfx_start
  dfx canister create --all --network local
  assert_command dfx build --network local
}

@test "build succeeds with URL as network parameter" {
  dfx_start
  webserver_port=$(get_webserver_port)
  dfx canister create --all --network "http://127.0.0.1:$webserver_port"
  assert_command dfx build --network "http://127.0.0.1:$webserver_port"
}

@test "build succeeds when requested network is configured" {
  dfx_start

  setup_actuallylocal_shared_network

  assert_command dfx canister create --all --network actuallylocal
  assert_command dfx build --network actuallylocal
}

@test "build with wallet succeeds when requested network is configured" {
  dfx_start
  setup_actuallylocal_shared_network
  assert_command dfx_set_wallet

  assert_command dfx canister create --all --network actuallylocal
  assert_command dfx build --network actuallylocal
}

@test "build output for local network is in expected directory" {
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_command ls .dfx/local/canisters/e2e_project_backend/
  assert_command ls .dfx/local/canisters/e2e_project_backend/e2e_project_backend.wasm
}

@test "build with wallet output for non-local network is in expected directory" {
  dfx_start
  setup_actuallylocal_shared_network
  assert_command dfx_set_wallet

  dfx canister create --all --network actuallylocal
  assert_command dfx build --network actuallylocal
  assert_command ls .dfx/actuallylocal/canisters/e2e_project_backend/
  assert_command ls .dfx/actuallylocal/canisters/e2e_project_backend/e2e_project_backend.wasm
}

@test "dfx build does not require a password" {
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_pw.exp"
  assert_command dfx identity use alice

  assert_command timeout 30s dfx build --check
}

@test "dfx build can post-process memory64 Wasm module" {
  install_asset memory64
  assert_command dfx build --check
}


-----------------------

/e2e/tests-dfx/build_granular.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_assets
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "direct dependencies are built" {
  dfx_start
  dfx canister create --all
  #specify build for only assets_canister
  dfx build e2e_project_frontend

  #validate direct dependency built and is callable
  assert_command dfx canister install e2e_project_backend
  assert_command dfx canister call e2e_project_backend greet World
}

@test "transitive dependencies are built" {
  install_asset transitive_deps_canisters
  dfx_start
  dfx canister create --all
  #install of tertiary dependency canister will fail since its not built
  assert_command_fail dfx canister install canister_a
  #specify build for primary canister
  dfx build canister_c

  #validate tertiary transitive dependency is built and callable
  assert_command dfx canister install canister_a
  assert_command dfx canister call canister_a greet World
  assert_match '("Namaste, World!")'
}

@test "unspecified dependencies are not built" {
  dfx_start
  dfx canister create --all
  # only build motoko canister
  dfx build e2e_project_backend
  # validate assets canister wasn't built and can't be installed
  assert_command_fail dfx canister install e2e_project_frontend
  assert_match "The canister must be built before install. Please run \`dfx build\`."
}


@test "manual build of specified canisters succeeds" {
  install_asset assetscanister

  dfx_start
  dfx canister create e2e_project_backend
  dfx build e2e_project_backend
  assert_command dfx canister install e2e_project_backend
  assert_command dfx canister call e2e_project_backend greet World

  assert_command_fail dfx canister install e2e_project_frontend
  assert_match "Cannot find canister id. Please issue 'dfx canister create e2e_project_frontend'."
  dfx canister create e2e_project_frontend
  dfx build e2e_project_frontend
  dfx canister install e2e_project_frontend

  assert_command dfx canister call --query e2e_project_frontend retrieve '("/binary/noise.txt")' --output idl
  # shellcheck disable=SC2154
  assert_eq '(blob "\b8\01\20\80\0a\77\31\32\20\00\78\79\0a\4b\4c\0b\0a\6a\6b")' "$stdout"

  assert_command dfx canister call --query e2e_project_frontend retrieve '("/text-with-newlines.txt")' --output idl
  # shellcheck disable=SC2154
  assert_eq '(blob "cherries\0ait\27s cherry season\0aCHERRIES")' "$stdout"
}

@test "cyclic dependencies are detected" {
  install_asset transitive_deps_canisters
  dfx_start
  dfx canister create --all
  assert_command_fail dfx build canister_e
  assert_match "Circular canister dependencies: canister_e -> canister_d -> canister_e"
}

@test "multiple non-cyclic dependency paths to the same canister are ok" {
  install_asset transitive_deps_canisters
  dfx_start
  dfx canister create --all
  assert_command dfx build canister_f
}

@test "the all flag builds everything" {
  dfx_start
  dfx canister create --all
  assert_command dfx build --all
  assert_command dfx canister install --all
}


@test "the all flags conflicts with canister name" {
  dfx_start
  dfx canister create --all
  assert_command_fail dfx build e2e_project --all
}


-----------------------

/e2e/tests-dfx/build_rust.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "build without cargo-audit installed cannot check for vulnerabilities" {
  assert_command rustup default stable
  assert_command rustup target add wasm32-unknown-unknown
  install_asset vulnerable_rust_deps
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_match "Cannot check for vulnerabilities in rust canisters because cargo-audit is not installed."
}

@test "build with vulnerabilities in rust dependencies emits a warning" {
  assert_command rustup default stable
  assert_command rustup target add wasm32-unknown-unknown
  assert_command cargo install cargo-audit --locked
  assert_command cargo audit --version
  install_asset vulnerable_rust_deps
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_match "Audit found vulnerabilities"
}


-----------------------

/e2e/tests-dfx/call.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "call --output json" {
  install_asset method_signatures

  dfx_start
  dfx deploy

  assert_command dfx canister call hello_backend returns_string '("you")' --output json
  assert_eq '"Hello, you!"'

  assert_command dfx canister call hello_backend returns_opt_string '(null)' --output json
  assert_eq '[]'
  assert_command dfx canister call hello_backend returns_opt_string '(opt "world")' --output json
  assert_eq '[
  "Hello, world!"
]'


  # int is unbounded, so formatted as a string
  assert_command dfx canister call hello_backend returns_int '(67)' --output json
  assert_eq '"67"'
  assert_command dfx canister call hello_backend returns_int '(111222333444555666777888999 : int)' --output json
  assert_eq '"111_222_333_444_555_666_777_888_999"'

  assert_command dfx canister call hello_backend returns_int32 '(67)' --output json
  assert_eq '67'

  assert_command dfx canister call hello_backend returns_principal '(principal "fg7gi-vyaaa-aaaal-qadca-cai")' --output json
  assert_eq '"fg7gi-vyaaa-aaaal-qadca-cai"'

  # variant
  assert_command dfx canister call hello_backend returns_variant '(0)' --output json
  assert_eq '{
  "foo": null
}'
  assert_command dfx canister call hello_backend returns_variant '(1)' --output json
  assert_eq '{
  "bar": "a bar"
}'
  assert_command dfx canister call hello_backend returns_variant '(2)' --output json
  assert_eq '{
  "baz": {
    "a": 51
  }
}'

  assert_command dfx canister call hello_backend returns_strings '()' --output json
  assert_eq '[
  "Hello, world!",
  "Hello, Mars!"
]'

  assert_command dfx canister call hello_backend returns_object '()' --output json
  assert_eq '{
  "bar": "42",
  "foo": "baz"
}'

  assert_command dfx canister call hello_backend returns_blob '("abd")' --output json
  assert_eq '[
  97,
  98,
  100
]'

  assert_command dfx canister call hello_backend returns_tuple '()' --output json
  assert_eq '"the first element"
42
"the third element"'


  assert_command dfx canister call hello_backend returns_single_elem_tuple '()' --output json
  assert_eq '"the only element"'
}

@test "call --candid <path to candid file>" {
  install_asset call

  dfx_start
  dfx deploy
  assert_command dfx canister call hello_backend make_struct '("A", "B")'
  assert_eq '(record { c = "A"; d = "B" })'

  CANISTER_ID=$(dfx canister id hello_backend)
  rm .dfx/local/canister_ids.json

  # if no candid method known, then no field names
  assert_command dfx canister call "$CANISTER_ID" make_struct2 '("A", "B")'
  # shellcheck disable=SC2154
  assert_eq '(record { 99 = "A"; 100 = "B" })' "$stdout"

  # if passing the candid file, field names available
  assert_command dfx canister call --candid full.did "$CANISTER_ID" make_struct2 '("A", "B")'
  assert_eq '(record { c = "A"; d = "B" })'

  # given a canister id, fetch the did file from metadata
  assert_command dfx canister call "$CANISTER_ID" make_struct '("A", "B")'
  assert_eq '(record { c = "A"; d = "B" })'
}

@test "call without argument, using candid assistant" {
  install_asset echo
  dfx_start
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/candid_assist.exp"
}

@test "call subcommand accepts canister identifier as canister name" {
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  assert_command dfx canister call "$(dfx canister id hello_backend)" greet '("Names are difficult")'
  assert_match '("Hello, Names are difficult!")'
}

@test "call subcommand accepts raw argument" {
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  # The encoded raw argument was generated with `didc encode '("raw")'`
  assert_command dfx canister call hello_backend greet '4449444c00017103726177' --type raw
  assert_match '("Hello, raw!")'
}

@test "call subcommand accepts argument from a file" {
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  TMP_NAME_FILE="$(mktemp)"
  printf '("Names can be very long")' > "$TMP_NAME_FILE"
  assert_command dfx canister call --argument-file "$TMP_NAME_FILE" hello_backend greet
  assert_match '("Hello, Names can be very long!")'
  rm "$TMP_NAME_FILE"
}

@test "call subcommand accepts argument from stdin" {
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  TMP_NAME_FILE="$(mktemp)"
  printf '("stdin")' > "$TMP_NAME_FILE"
  assert_command dfx canister call --argument-file - hello_backend greet < "$TMP_NAME_FILE"
  assert_match '("Hello, stdin!")'
  rm "$TMP_NAME_FILE"
}

@test "call random value (pattern)" {
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  assert_command dfx canister call hello_backend greet --random '{ value = Some ["\"DFINITY\""] }'
  assert_match '("Hello, DFINITY!")'
}

@test "error on empty arguments when the method requires some" {
  install_asset greet
  dfx_start
  dfx deploy
  assert_command_fail dfx canister call hello_backend greet
}

@test "call random value (empty)" {
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  assert_command dfx canister call hello_backend greet --random ''
  assert_match '("Hello, .*!")'
}

@test "long call" {
  install_asset recurse
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  assert_command dfx canister call hello_backend recurse 100
}

@test "call with cycles" {
  dfx_start
  dfx deploy
  assert_command_fail dfx canister call hello_backend greet '' --with-cycles 100
  assert_command dfx canister call hello_backend greet '' --with-cycles 100 --wallet "$(dfx identity get-wallet)"
  dfx identity whoami
}

@test "call with cycles with wallet by name or by principal" {
  dfx_start
  dfx deploy
  assert_command_fail dfx canister call hello_backend greet '' --with-cycles 100

  assert_command dfx canister call hello_backend greet '' --with-cycles 100 --wallet "$(dfx identity get-wallet)"
  assert_command dfx canister call hello_backend greet '' --with-cycles 100 --wallet default
}


@test "call by canister id outside of a project" {
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  ID="$(dfx canister id hello_backend)"
  NETWORK="http://localhost:$(get_webserver_port)"
  (
    cd "$E2E_TEMP_DIR"
    mkdir "not-a-project-dir"
    cd "not-a-project-dir"
    assert_command dfx canister call "$ID" greet '("you")' --network "$NETWORK"
    assert_match '("Hello, you!")'
  )
}

@test "call a canister which is deployed then removed from dfx.json" {
  dfx_start
  dfx deploy
  CANISTER_ID=$(dfx canister id hello_backend)
  jq 'del(.canisters.hello_backend)' dfx.json | sponge dfx.json
  assert_command dfx canister call hello_backend greet '("you")'
  assert_match '("Hello, you!")'
  assert_command dfx canister call "$CANISTER_ID" greet '("you")'
  assert_match '("Hello, you!")'
}

@test "inter-canister calls" {
  dfx_new_rust inter
  install_asset inter
  dfx_start
  dfx deploy

  # calling motoko canister from rust canister
  assert_command dfx canister call inter_rs read
  assert_match '(0 : nat)'
  assert_command dfx canister call inter_rs inc
  assert_command dfx canister call inter_rs read
  assert_match '(1 : nat)'
  assert_command dfx canister call inter_rs write '(5)'
  assert_command dfx canister call inter_rs read
  assert_match '(5 : nat)'

  # calling rust canister from motoko canister
  assert_command dfx canister call inter_mo write '(0)'
  assert_command dfx canister call inter_mo read
  assert_match '(0 : nat)'
  assert_command dfx canister call inter_mo inc
  assert_command dfx canister call inter_mo read
  assert_match '(1 : nat)'
  assert_command dfx canister call inter_mo write '(6)'
  assert_command dfx canister call inter_mo read
  assert_match '(6 : nat)'

  # calling rust canister from rust canister, trough motoko canisters
  assert_command dfx canister call inter2_rs write '(0)'
  assert_command dfx canister call inter2_rs read
  assert_match '(0 : nat)'
  assert_command dfx canister call inter2_rs inc
  assert_command dfx canister call inter2_rs read
  assert_match '(1 : nat)'
  assert_command dfx canister call inter2_rs write '(7)'
  assert_command dfx canister call inter2_rs read
  assert_match '(7 : nat)'

  # calling motoko canister from motoko canister, trough rust canisters
  assert_command dfx canister call inter2_mo write '(0)'
  assert_command dfx canister call inter2_mo read
  assert_match '(0 : nat)'
  assert_command dfx canister call inter2_mo inc
  assert_command dfx canister call inter2_mo read
  assert_match '(1 : nat)'
  assert_command dfx canister call inter2_mo write '(8)'
  assert_command dfx canister call inter2_mo read
  assert_match '(8 : nat)'
}


-----------------------

/e2e/tests-dfx/candid_ui.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "Candid UI" {
  dfx_start
  dfx deploy
  ID=$(dfx canister id __Candid_UI)
  PORT=$(get_webserver_port)
  assert_command curl http://localhost:"$PORT"/?canisterId="$ID"
  assert_match "Candid UI"
}


-----------------------

/e2e/tests-dfx/canister_extra.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
    standard_setup
    dfx_new hello
}

teardown() {
    dfx_stop
    standard_teardown
}

@test "canister snapshots" {
    dfx_start
    install_asset counter
    dfx deploy

    assert_command dfx canister call hello_backend inc_read
    assert_contains '(1 : nat)'

    dfx canister stop hello_backend
    assert_command dfx canister snapshot create hello_backend
    assert_match 'Snapshot ID: ([0-9a-f]+)'
    snapshot=${BASH_REMATCH[1]}
    dfx canister start hello_backend

    assert_command dfx canister call hello_backend inc_read
    assert_contains '(2 : nat)'

    dfx canister stop hello_backend
    assert_command dfx canister snapshot load hello_backend "$snapshot"
    dfx canister start hello_backend
    assert_command dfx canister call hello_backend read
    assert_contains '(1 : nat)'

    assert_command dfx canister snapshot list hello_backend
    assert_match "^${snapshot}:"
    assert_command dfx canister snapshot delete hello_backend "$snapshot"
    assert_command dfx canister snapshot list hello_backend
    assert_contains 'No snapshots found in canister hello_backend'

    assert_command_fail dfx canister snapshot create hello_backend
    assert_contains 'Canister hello_backend is running and snapshots should not be taken of running canisters'
}

@test "can query a website" {
  dfx_start

  dfx_new
  install_asset canister_http

  dfx deploy

  assert_command dfx canister call e2e_project_backend get_url '("www.githubstatus.com:443","https://www.githubstatus.com:443")'
  assert_contains "Git Operations"
  assert_contains "API Requests"
}

-----------------------

/e2e/tests-dfx/canister_http_adapter.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop
  standard_teardown
}

set_project_default_canister_http_enabled() {
  jq ".defaults.canister_http.enabled=${1:-true}" dfx.json | sponge dfx.json
}

set_project_local_network_canister_http_enabled() {
  jq ".networks.local.canister_http.enabled=${1:-true}" dfx.json | sponge dfx.json
}

set_shared_local_network_canister_http_enabled() {
  create_networks_json

  jq ".local.canister_http.enabled=${1:-true}" "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
}

set_shared_local_network_canister_http_empty() {
  create_networks_json

  jq '.local.canister_http={}' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
}

@test "canister http feature is enabled by default" {
  dfx_start

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-https-outcalls-adapter-pid"
}

@test "canister http feature is enabled by default with empty json element" {
  set_shared_local_network_canister_http_empty

  dfx_start

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-https-outcalls-adapter-pid"
}

@test "dfx restarts replica when ic-https-outcalls-adapter restarts" {
  dfx_new_assets hello
  dfx_start

  install_asset greet
  assert_command dfx deploy
  assert_command dfx canister call hello_backend greet '("Alpha")'
  assert_eq '("Hello, Alpha!")'

  REPLICA_PID=$(get_replica_pid)
  CANISTER_HTTP_ADAPTER_PID=$(get_canister_http_adapter_pid)

  echo "replica pid is $REPLICA_PID"
  echo "ic-https-outcalls-adapter pid is $CANISTER_HTTP_ADAPTER_PID"

  kill -KILL "$CANISTER_HTTP_ADAPTER_PID"
  assert_process_exits "$CANISTER_HTTP_ADAPTER_PID" 15s
  assert_process_exits "$REPLICA_PID" 15s

  timeout 30s sh -c \
    'until dfx ping; do echo waiting for replica to restart; sleep 1; done' \
    || (echo "replica did not restart" && ps aux && exit 1)
  wait_until_replica_healthy

  # Sometimes initially get an error like:
  #     IC0537: Attempt to execute a message on canister <>> which contains no Wasm module
  # but the condition clears.
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait 1\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting

  # Even so, after that passes, sometimes this happens:
  #     IC0208: Certified state is not available yet. Please try again...
  sleep 10
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait 2\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting

  assert_command dfx canister call hello_backend greet '("Omega")'
  assert_eq '("Hello, Omega!")'

  ID=$(dfx canister id hello_frontend)

  timeout 15s sh -c \
    "until curl --fail http://localhost:\$(cat \"$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/webserver-port\")/sample-asset.txt?canisterId=$ID; do echo waiting for pocket-ic proxy to restart; sleep 1; done" \
    || (echo "pocket-ic proxy did not restart" && ps aux && exit 1)

  assert_command curl --fail http://localhost:"$(get_webserver_port)"/sample-asset.txt?canisterId="$ID"
}

@test "dfx start --enable-canister-http with no other configuration succeeds" {
  dfx_new hello

  dfx_start --enable-canister-http

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-https-outcalls-adapter-pid"
}

@test "can enable http through project default configuration - dfx start" {
  dfx_new hello
  define_project_network
  set_project_default_canister_http_enabled

  dfx_start

  assert_file_not_empty .dfx/network/local/ic-https-outcalls-adapter-pid
}

@test "can disable http through project default configuration - dfx start" {
  dfx_new hello
  define_project_network
  set_project_default_canister_http_enabled false

  dfx_start

  assert_file_empty .dfx/network/local/ic-https-outcalls-adapter-pid
}

@test "can enable http through project local network - dfx start" {
  dfx_new hello
  define_project_network
  set_project_local_network_canister_http_enabled

  dfx_start

  assert_file_not_empty .dfx/network/local/ic-https-outcalls-adapter-pid
}

@test "can disable http through project local network - dfx start" {
  dfx_new hello
  define_project_network
  set_project_local_network_canister_http_enabled false

  dfx_start

  assert_file_empty .dfx/network/local/ic-https-outcalls-adapter-pid
}

@test "can enable http through shared local network - dfx start" {
  dfx_new hello
  set_shared_local_network_canister_http_enabled

  dfx_start

  assert_file_not_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-https-outcalls-adapter-pid"
}

@test "can disable http through shared local network - dfx start" {
  dfx_new hello
  set_shared_local_network_canister_http_enabled false

  dfx_start

  assert_file_empty "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/ic-https-outcalls-adapter-pid"
}

@test "dfx starts http adapter with correct log level - project defaults" {
  dfx_new
  jq '.defaults.canister_http.log_level="warning"' dfx.json | sponge dfx.json
  define_project_network

  assert_command dfx start --background --verbose
  assert_match "log level: Warning"
  assert_command dfx stop

  jq '.defaults.canister_http.log_level="critical"' dfx.json | sponge dfx.json
  assert_command dfx start --background --verbose
  assert_match "log level: Critical"
}

@test "dfx starts http adapter with correct log level - local network" {
  dfx_new
  jq '.networks.local.canister_http.log_level="warning"' dfx.json | sponge dfx.json
  define_project_network

  assert_command dfx start --background --verbose
  assert_match "log level: Warning"
  assert_command dfx stop

  jq '.networks.local.canister_http.log_level="critical"' dfx.json | sponge dfx.json
  assert_command dfx start --background --verbose --clean
  assert_match "log level: Critical"
}

@test "dfx starts http adapter with correct log level - shared network" {
  dfx_new
  create_networks_json
  jq '.local.canister_http.log_level="warning"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  assert_command dfx start --background --verbose
  assert_match "log level: Warning"
  assert_command dfx stop

  jq '.local.canister_http.log_level="critical"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  assert_command dfx start --background --verbose
  assert_match "log level: Critical"
}


-----------------------

/e2e/tests-dfx/canister_logs.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "get canister logs" {
  install_asset logs
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install e2e_project
  dfx canister call e2e_project hello Alice
  dfx canister call e2e_project hello Bob
  sleep 2
  assert_command dfx canister logs e2e_project
  assert_contains "Hello, Alice!"
  assert_contains "Hello, Bob!"
}

dfx_canister_logs_grep_Alice() {
  dfx canister logs e2e_project | grep Alice
}

@test "canister logs output is grep compatible" {
  install_asset logs
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install e2e_project
  dfx canister call e2e_project hello Alice
  dfx canister call e2e_project hello Bob
  sleep 2
  assert_command dfx_canister_logs_grep_Alice
  assert_contains "Alice"
  assert_not_contains "Bob"
}

dfx_canister_logs_tail_n_1() {
  # Extra echo is necessary to simulate file input for tail.
  # shellcheck disable=SC2005
  echo "$(dfx canister logs e2e_project)" | tail -n 1
}

@test "canister logs output is tail compatible" {
  install_asset logs
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install e2e_project
  dfx canister call e2e_project hello Alice
  dfx canister call e2e_project hello Bob
  sleep 2
  assert_command dfx_canister_logs_tail_n_1
  assert_not_contains "Alice"
  assert_contains "Bob"
}

@test "canister logs only visible to allowed viewers." {
  install_asset logs
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install e2e_project
  dfx canister call e2e_project hello Alice
  sleep 2

  assert_command dfx canister logs e2e_project
  assert_contains "Hello, Alice!"

  # Create identity for viewers.
  assert_command dfx identity new --storage-mode plaintext alice
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)

  assert_command_fail dfx canister logs e2e_project --identity alice
  
  assert_command dfx canister update-settings --add-log-viewer="${ALICE_PRINCIPAL}" e2e_project
  assert_command dfx canister status e2e_project
  assert_contains "${ALICE_PRINCIPAL}"

  assert_command dfx canister logs e2e_project --identity alice
  assert_contains "Hello, Alice!"
}


-----------------------

/e2e/tests-dfx/canister_status.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_assets
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "get canister status" {
  dfx_start
  assert_command dfx canister create e2e_project_frontend --no-wallet

  assert_command dfx canister status e2e_project_frontend
  assert_contains "Memory allocation: "
  assert_contains "Balance: "
}

dfx_canister_status_grep_memory_allocation() {
  dfx canister status e2e_project_frontend | grep "Memory allocation"
}

dfx_canister_status_grep_balance() {
  dfx canister status e2e_project_frontend | grep "Balance"
}

@test "canister status output is grep compatible" {
  dfx_start
  assert_command dfx canister create e2e_project_frontend --no-wallet

  assert_command dfx_canister_status_grep_memory_allocation
  assert_contains "Memory allocation: "
  assert_not_contains "Balance: "

  assert_command dfx_canister_status_grep_balance
  assert_not_contains "Memory allocation: "
  assert_contains "Balance: "
}


-----------------------

/e2e/tests-dfx/canister_url.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_assets hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "url subcommand prints valid frontend canister urls on local" {
  dfx_start
  dfx canister create --all
  frontend_id=$(dfx canister id hello_frontend)
  
  assert_command dfx canister url hello_frontend
  assert_match "http://127.0.0.1.+${frontend_id}"
  assert_match "${frontend_id}.localhost"

  assert_command dfx canister url "$frontend_id"
  assert_match "http://127.0.0.1.+${frontend_id}"
  assert_match "${frontend_id}.localhost"
}

@test "url subcommand prints valid backend canister urls on local" {
  dfx_start
  dfx canister create --all
  jq '.__Candid_UI.local="br5f7-7uaaa-aaaaa-qaaca-cai"' .dfx/local/canister_ids.json | sponge .dfx/local/canister_ids.json

  backend_id=$(dfx canister id hello_backend)
  assert_command dfx canister url hello_backend
  assert_match "canisterId=br5f7-7uaaa-aaaaa-qaaca-cai&id=${backend_id}"

  backend_id=$(dfx canister id hello_backend)
  assert_command dfx canister url "$backend_id"
  assert_match "canisterId=br5f7-7uaaa-aaaaa-qaaca-cai&id=${backend_id}"
}

@test "url subcommand prints valid frontend canister urls from a subdirectory" {
  dfx_start
  dfx canister create --all
  frontend_id=$(dfx canister id hello_frontend)

  cd src
  assert_command dfx canister url hello_frontend
  assert_match "http://127.0.0.1.+${frontend_id}"
  assert_match "${frontend_id}.localhost"
}

@test "url subcommand prints valid frontend canister urls on mainnet" {
  dfx_start
  echo "{}" > canister_ids.json
  jq '.hello_frontend.ic = "qsgof-4qaaa-aaaan-qekqq-cai"' canister_ids.json | sponge canister_ids.json
  frontend_id=$(dfx canister id hello_frontend --ic)
  
  assert_command dfx canister url hello_frontend --ic
  assert_match "https://${frontend_id}.icp0.io"

  assert_command dfx canister url "$frontend_id" --ic
  assert_match "https://${frontend_id}.icp0.io"
}

@test "url subcommand prints valid backend canister urls on mainnet" {
  dfx_start
  echo "{}" > canister_ids.json
  jq '.hello_backend.ic = "qvhir-riaaa-aaaan-qekqa-cai"' canister_ids.json | sponge canister_ids.json
  backend_id=$(dfx canister id hello_backend --ic)

  assert_command dfx canister url hello_backend --ic
  assert_contains "https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io/?id=${backend_id}"

  assert_command dfx canister url "$backend_id" --ic
  assert_contains "https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io/?id=${backend_id}"
}

@test "url subcommand prints valid remote canister urls" {
  dfx_start

  jq '.canisters.internet_identity.type = "custom"' dfx.json | sponge dfx.json
  jq '.canisters.internet_identity.candid = ""' dfx.json | sponge dfx.json
  jq '.canisters.internet_identity.wasm = ""' dfx.json | sponge dfx.json
  jq '.canisters.internet_identity.remote.id.ic = "rdmx6-jaaaa-aaaaa-aaadq-cai"' dfx.json | sponge dfx.json
  remote_id=$(dfx canister id internet_identity --ic)

  assert_command dfx canister url internet_identity --ic
  assert_contains "https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io/?id=${remote_id}"

  assert_command dfx canister url "$remote_id" --ic
  assert_contains "https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io/?id=${remote_id}"
}


-----------------------

/e2e/tests-dfx/certificate.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new certificate

  install_asset certificate
  dfx_start

  dfx deploy

  BACKEND="127.0.0.1:$(get_webserver_port)"

  # In github workflows, at the time of this writing, we get:
  #     macos-latest: mitmproxy 7.0.4
  #     ubuntu-latest: mitmproxy 4.x
  if [ "$(mitmdump --version | grep Mitmproxy | cut -d ' ' -f 2 | cut -c 1-2)" = "4." ]; then
    MODIFY_BODY_ARG="--replacements"
  else
    MODIFY_BODY_ARG="--modify-body"
  fi

  # Sometimes, something goes wrong with mitmdump's initialization.
  # It reports that it is listening, and the `nc` call succeeds,
  # but it does not actually respond to requests.
  #
  # This happens whether using a fixed port or a dynamic port.
  # For this reason, we retry initialization until `dfx ping` succeeds.
  #
  # I have seen this process take as many as 9 iterations to succeed,
  # so across a large number of CI runs, it could take even more.
  # The overall CI timeout will limit the maximum time taken.

  while true
  do
    MITM_PORT=$(get_ephemeral_port)
    overwrite_webserver_port "$MITM_PORT"

    mitmdump -p "$MITM_PORT" --mode "reverse:http://$BACKEND"  "$MODIFY_BODY_ARG" '/~s/Hello,/Hullo,' &
    MITMDUMP_PID=$!

    timeout 5 sh -c \
      "until nc -z localhost $MITM_PORT; do echo waiting for mitmdump; sleep 1; done" \
      || (echo "mitmdump did not start on port $MITM_PORT" && exit 1)

    if timeout 10 dfx ping; then
      break
    fi

    kill -9 $MITMDUMP_PID
  done
}

teardown() {
  # Kill child processes of mitmdump. Otherwise they hang around way too long
  pkill -P "$MITMDUMP_PID"
  kill "$MITMDUMP_PID"

  dfx_stop

  standard_teardown
}

@test "mitm attack - update: attack fails because certificate verification fails" {
  assert_command_fail dfx canister call certificate_backend hello_update '("Buckaroo")'
  assert_match 'Certificate verification failed.'
}

@test "mitm attack - query: attack succeeds because there is no certificate to verify" {
  # The wallet does not have a query call forward method (currently calls forward from wallet's update method)
  # So call with users Identity as sender here
  # There may need to be a query version of wallet_call
  declare -x DFX_DISABLE_QUERY_VERIFICATION=1
  assert_command dfx canister call certificate_backend hello_query '("Buckaroo")'
  assert_eq '("Hullo, Buckaroo!")'
}


-----------------------

/e2e/tests-dfx/certified_info.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "get certified-info" {
  dfx_start
  dfx canister create hello_backend
  assert_command dfx canister info "$(dfx canister id hello_backend)"
  WALLET_ID=$(dfx identity get-wallet)
  SELF_ID=$(dfx identity get-principal)
  assert_match \
"Controllers: ($WALLET_ID $SELF_ID|$SELF_ID $WALLET_ID)
Module hash: None"

  dfx build hello_backend
  RESULT="$(openssl dgst -sha256 .dfx/local/canisters/hello_backend/hello_backend.wasm)"
  # shellcheck disable=SC2034
  HASH="0x"
  HASH+=$(echo "${RESULT}" | cut -d' ' -f 2)


  dfx canister install hello_backend
  assert_command dfx canister info "$(dfx canister id hello_backend)"
  assert_match \
"Controllers: ($WALLET_ID $SELF_ID|$SELF_ID $WALLET_ID)
Module hash: $(HASH)"
}


-----------------------

/e2e/tests-dfx/completion.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
  use_test_specific_cache_root # because extensions go in the cache
}

teardown() {
  standard_teardown
}

@test "generate bash completion script using default" {
  assert_command dfx completion
  assert_contains "dfx__ledger__transfer"
  assert_contains "dfx__identity__new"
  assert_contains "dfx__identity__help__whoami"
}

@test "generate bash completion script" {
  assert_command dfx completion bash
  assert_contains "dfx__ledger__transfer"
  assert_contains "dfx__identity__new"
  assert_contains "dfx__identity__help__whoami"
}

@test "generate bash completion script with extensions installed" {
  assert_command dfx extension install nns --version 0.3.1
  assert_command dfx extension install sns --version 0.3.1
  assert_command dfx completion bash
  assert_contains "dfx__ledger__transfer"
  assert_contains "dfx__identity__new"
  assert_contains "dfx__identity__help__whoami"
  assert_contains "dfx__nns__install"
  assert_contains "dfx__nns__help__install"
}

@test "generate zsh completion script" {
  assert_command dfx completion zsh
  assert_contains "_dfx__ledger__help__balance_commands"
  assert_contains "_dfx__canister__install_commands"
}

@test "generate zsh completion script with extensions installed" {
  assert_command dfx extension install nns --version 0.3.1
  assert_command dfx extension install sns --version 0.3.1
  assert_command dfx completion zsh
  assert_contains "_dfx__ledger__help__balance_commands"
  assert_contains "_dfx__canister__install_commands"
  assert_contains "_dfx__nns__install_commands"
  assert_contains "_dfx__nns__help__install_commands"
  assert_contains "_dfx__sns__propose_commands"
}

@test "generate elvish completion script" {
  assert_command dfx completion elvish
  assert_contains "dfx;help;identity;new"
  assert_contains "dfx;canister;create"
}

@test "generate elvish completion script with extensions installed" {
  assert_command dfx extension install nns --version 0.3.1
  assert_command dfx extension install sns --version 0.3.1
  assert_command dfx completion elvish
  assert_contains "dfx;nns;install"
  assert_contains "dfx;help;sns;deploy"
}

@test "generate fish completion script" {
  assert_command dfx completion fish
 assert_contains "Deploys all or a specific canister from the code in your project. By default, all canisters are deployed"
}

@test "generate fish completion script with extensions installed" {
  assert_command dfx extension install nns --version 0.3.1
  assert_command dfx extension install sns --version 0.3.1
  assert_command dfx completion fish
  assert_contains "Install an NNS on the local dfx server"
  assert_contains "Subcommand for preparing dapp canister(s) for 1-proposal SNS creation"
}

@test "generate powershell completion script" {
  assert_command dfx completion powershell
  assert_contains "dfx;deploy"
  assert_contains "dfx;canister;create"
}

@test "generate powershell completion script with extensions installed" {
  assert_command dfx extension install nns --version 0.3.1
  assert_command dfx extension install sns --version 0.3.1
  assert_command dfx completion powershell
  assert_contains "dfx;ledger;transfer"
  assert_contains "dfx;nns;install"
  assert_contains "dfx;help;sns;deploy"
}

@test "completion scripts include built-in project template names" {
  assert_command dfx completion bash
  assert_contains "motoko"
  assert_contains "sveltekit"
  assert_contains "bitcoin"
  assert_contains "frontend-tests"
}


-----------------------

/e2e/tests-dfx/create.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_
load ../utils/cycles-ledger

setup() {
  standard_setup

  dfx_new_assets
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "create with reserved cycles limit" {
    dfx_start

    assert_command_fail dfx canister create e2e_project_backend --reserved-cycles-limit 470000
    assert_contains "Cannot create a canister using a wallet if the reserved_cycles_limit is set. Please create with --no-wallet or use dfx canister update-settings instead."

    assert_command dfx canister create e2e_project_frontend --no-wallet
    assert_command dfx canister status e2e_project_frontend
    assert_contains "Reserved cycles limit: 5_000_000_000_000 Cycles"

    assert_command dfx canister create e2e_project_backend --reserved-cycles-limit 470000 --no-wallet
    assert_command dfx canister status e2e_project_backend
    assert_contains "Reserved cycles limit: 470_000 Cycles"
}

@test "create succeeds on default project" {
  dfx_start
  assert_command dfx canister create --all
}

@test "create succeeds with --specified-id" {
  dfx_start
  assert_command dfx canister create e2e_project_backend --specified-id n5n4y-3aaaa-aaaaa-p777q-cai
  assert_command dfx canister id e2e_project_backend
  assert_match n5n4y-3aaaa-aaaaa-p777q-cai
}

@test "create succeeds when specify large canister ID" {
  [[ "$USE_POCKETIC" ]] && skip "skipped for pocketic: subnet range"
  dfx_start
  # hhn2s-5l777-77777-7777q-cai is the canister ID of (u64::MAX / 2)
  assert_command dfx canister create e2e_project_backend --specified-id hhn2s-5l777-77777-7777q-cai
  assert_command dfx canister id e2e_project_backend
  assert_match hhn2s-5l777-77777-7777q-cai
}

@test "create fails when specify out of range canister ID" {
  dfx_start
  # nojwb-ieaaa-aaaaa-aaaaa-cai is the canister ID of (u64::MAX / 2 + 1)
  assert_command_fail dfx canister create e2e_project_backend --specified-id nojwb-ieaaa-aaaaa-aaaaa-cai

  if [[ "$USE_POCKETIC" ]]; then
    assert_match "The effective canister ID nojwb-ieaaa-aaaaa-aaaaa-cai does not belong to an existing subnet and it is not a mainnet canister ID."
  else
    assert_match "Specified CanisterId nojwb-ieaaa-aaaaa-aaaaa-cai is not hosted by subnet"
  fi
}

@test "create fails if set both --all and --specified-id" {
  dfx_start
  assert_command_fail dfx canister create --all --specified-id xbgkv-fyaaa-aaaaa-aaava-cai
  assert_match "error: the argument '--all' cannot be used with '--specified-id <PRINCIPAL>'"
}

@test "create succeeds when specify canister ID in dfx.json" {
  dfx_start
  jq '.canisters.e2e_project_backend.specified_id="n5n4y-3aaaa-aaaaa-p777q-cai"' dfx.json | sponge dfx.json
  assert_command dfx canister create e2e_project_backend
  assert_command dfx canister id e2e_project_backend
  assert_match n5n4y-3aaaa-aaaaa-p777q-cai
}

@test "create succeeds when specify canister ID both in dfx.json and cli; warning if different; cli value takes effect" {
  dfx_start
  jq '.canisters.e2e_project_backend.specified_id="n5n4y-3aaaa-aaaaa-p777q-cai"' dfx.json | sponge dfx.json
  assert_command dfx canister create e2e_project_backend --specified-id n2m2m-wyaaa-aaaaa-p777a-cai
  assert_contains "WARN: Canister 'e2e_project_backend' has a specified ID in dfx.json: n5n4y-3aaaa-aaaaa-p777q-cai,"
  assert_contains "which is different from the one specified in the command line: n2m2m-wyaaa-aaaaa-p777a-cai."
  assert_contains "The command line value will be used."

  assert_command dfx canister id e2e_project_backend
  assert_match n2m2m-wyaaa-aaaaa-p777a-cai
}

@test "create generates the canister_ids.json" {
  dfx_start
  assert_command dfx canister create --all
  [[ -f .dfx/local/canister_ids.json ]]
}

@test "create without parameters sets wallet and self as controller" {
  dfx_start
  PRINCIPAL=$(dfx identity get-principal)
  WALLET=$(dfx identity get-wallet)
  assert_command dfx canister create --all
  assert_command dfx canister info e2e_project_backend
  assert_match "Controllers: ($PRINCIPAL $WALLET|$WALLET $PRINCIPAL)"
}

@test "create with --no-wallet sets only self as controller" {
  dfx_start
  PRINCIPAL=$(dfx identity get-principal)
  WALLET=$(dfx identity get-wallet)
  assert_command dfx canister create --all --no-wallet
  assert_command dfx canister info e2e_project_backend
  assert_not_match "Controllers: ($PRINCIPAL $WALLET|$WALLET $PRINCIPAL)"
  assert_match "Controllers: $PRINCIPAL"
}

@test "build fails without create" {
  dfx_start
  assert_command_fail dfx build
  assert_match "Cannot find canister id."
}

@test "build fails if all canisters in project are not created" {
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_command_fail dfx build
  assert_match "Cannot find canister id. Please issue 'dfx canister create e2e_project_frontend'"
}

@test "create succeeds with network parameter" {
  dfx_start
  assert_command dfx canister create --all --network local
}

@test "create fails with incorrect network" {
  dfx_start
  assert_command_fail dfx canister create --all --network nosuch
  assert_match "Network not found"
}

@test "create succeeds when requested network is configured" {
  dfx_start

  setup_actuallylocal_shared_network
  assert_command dfx canister create --all --network actuallylocal
}

@test "create with wallet succeeds when requested network is configured" {
  dfx_start
  setup_actuallylocal_shared_network

  assert_command dfx_set_wallet
  assert_command dfx canister create --all --network actuallylocal
}

@test "create fails if selected network exists but has no providers" {
  dfx_start

  jq '.networks.actuallylocal.providers=[]' dfx.json | sponge dfx.json
  assert_command_fail dfx canister create --all --network actuallylocal
  assert_match "Did not find any providers for network 'actuallylocal'"
}

@test "create fails with network parameter when network does not exist" {
  dfx_start
  jq '.networks.actuallylocal.providers=["http://not-real.nowhere.test."]' dfx.json | sponge dfx.json
  assert_command_fail dfx canister create --all --network actuallylocal
  assert_contains "error sending request for url (http://not-real.nowhere.test./api/v2/status)"
}

@test "create accepts --controller <controller> named parameter, with controller by identity name" {
  dfx_start
  dfx identity new --storage-mode plaintext alice
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)


  assert_command dfx canister create --all --controller alice
  assert_command dfx canister info e2e_project_backend
  assert_match "Controllers: $ALICE_PRINCIPAL"

  assert_command_fail dfx deploy
  assert_command dfx deploy --identity alice
}

@test "create accepts --controller <controller> named parameter, with controller by identity principal" {
  dfx_start
  dfx identity new --storage-mode plaintext alice
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)

  assert_command dfx canister create --all --controller "${ALICE_PRINCIPAL}"
  assert_command dfx canister info e2e_project_backend
  assert_not_match "Controllers: ($ALICE_WALLET $ALICE_PRINCIPAL|$ALICE_PRINCIPAL $ALICE_WALLET)"
  assert_match "Controllers: $ALICE_PRINCIPAL"

  assert_command_fail dfx deploy
  assert_command dfx deploy --identity alice
}

@test "create accepts --controller <controller> named parameter, with controller by wallet principal" {
  dfx_start
  dfx identity new --storage-mode plaintext alice
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)

  assert_command dfx canister create --all --controller "${ALICE_WALLET}"
  assert_command dfx canister info e2e_project_backend
  assert_match "Controllers: $ALICE_WALLET"

  assert_command_fail dfx deploy
  assert_command_fail dfx deploy --identity alice
  assert_command dfx deploy --identity alice --wallet alice
}

@test "create accepts --controller <controller> named parameter, with controller by name of selected identity" {
  # there is a different code path if the specified controller happens to be
  # the currently selected identity.
  dfx_start
  dfx identity new --storage-mode plaintext alice
  dfx identity new --storage-mode plaintext bob
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)

  dfx identity use bob

  assert_command dfx canister create --all --controller bob

  dfx identity use alice
  assert_command dfx canister info e2e_project_backend
  assert_match "Controllers: $BOB_PRINCIPAL"

  assert_command_fail dfx deploy
  assert_command dfx deploy --identity bob
}

@test "create single controller accepts --controller <controller> named parameter, with controller by identity name" {
  dfx_start
  dfx identity new --storage-mode plaintext alice
  dfx identity new --storage-mode plaintext bob
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)

  assert_command dfx canister create --controller alice e2e_project_backend
  assert_command dfx canister create --controller bob e2e_project_frontend

  assert_command dfx canister info e2e_project_backend
  assert_match "Controllers: $ALICE_PRINCIPAL"

  assert_command dfx canister info e2e_project_frontend
  assert_match "Controllers: $BOB_PRINCIPAL"

  # check this first, because alice will deploy e2e_project in the next step
  assert_command_fail dfx deploy e2e_project_backend --identity bob
  # this actually deploys e2e_project before failing, because it is a dependency
  assert_command_fail dfx deploy e2e_project_frontend --identity alice

  assert_command dfx deploy e2e_project_backend --identity alice
  assert_command dfx deploy e2e_project_frontend --identity bob
}

@test "create canister with multiple controllers" {
  dfx_start
  dfx identity new --storage-mode plaintext alice
  dfx identity new --storage-mode plaintext bob
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)
  # awk step is to avoid trailing space
  PRINCIPALS_SORTED=$(echo "$ALICE_PRINCIPAL" "$BOB_PRINCIPAL" | tr " " "\n" | sort | tr "\n" " " | awk '{printf "%s %s",$1,$2}' )

  assert_command dfx canister create --all --controller alice --controller bob --identity alice
  assert_command dfx canister info e2e_project_backend
  assert_match "Controllers: ${PRINCIPALS_SORTED}"

  assert_command dfx deploy --identity alice
  assert_command_fail dfx deploy --identity bob

  # The certified assets canister will have added alice as an authorized user, because she was the caller
  # at initialization time.  Bob has to be added separately.  BUT, the canister has to be deployed first
  # in order to call the authorize method.
  assert_command dfx canister call e2e_project_frontend authorize "(principal \"$BOB_PRINCIPAL\")" --identity alice

  assert_command dfx deploy --identity bob
}

@test "reports wallet must be upgraded if attempting to create a canister with multiple controllers through an old wallet" {
  use_wallet_wasm 0.7.2

  dfx_start
  dfx identity new --storage-mode plaintext alice
  dfx identity new --storage-mode plaintext bob

  assert_command_fail dfx canister create --all --controller alice --controller bob --identity alice
  assert_match "The wallet canister must be upgraded: The installed wallet does not support multiple controllers."
  assert_match "To upgrade, run dfx wallet upgrade"

  use_wallet_wasm 0.8.2
  assert_command dfx wallet upgrade --identity alice
  assert_command dfx canister create --all --controller alice --controller bob --identity alice
}

@test "create canister - subnet targetting" {
  # fake cmc setup
  cd ..
  dfx_new fake_cmc
  install_asset fake_cmc
  install_cycles_ledger_canisters
  dfx_start
  assert_command dfx deploy fake-cmc --specified-id "rkp4c-7iaaa-aaaaa-aaaca-cai" # CMC canister id
  cd ../e2e_project

  # use --subnet <principal>
  SUBNET_ID="5kdm2-62fc6-fwnja-hutkz-ycsnm-4z33i-woh43-4cenu-ev7mi-gii6t-4ae" # a random, valid principal
  assert_command dfx canister create e2e_project_backend --subnet "$SUBNET_ID"
  cd ../fake_cmc
  assert_command dfx canister call fake-cmc last_create_canister_args
  assert_contains "subnet = principal \"$SUBNET_ID\";"
  
  # use --subnet-type
  cd ../e2e_project
  assert_command dfx canister create e2e_project_frontend --subnet-type custom_subnet_type
  cd ../fake_cmc
  assert_command dfx canister call fake-cmc last_create_canister_args
  assert_contains 'subnet_type = opt "custom_subnet_type"'
}

@test "create with dfx.json settings" {
  jq '.canisters.e2e_project_backend.initialization_values={
    "compute_allocation": 5,
    "freezing_threshold": "7days",
    "memory_allocation": "2 GiB",
    "reserved_cycles_limit": 1000000000000,
    "wasm_memory_limit": "1 GiB",
    "log_visibility": "public",
  }' dfx.json | sponge dfx.json
  dfx_start
  assert_command dfx deploy e2e_project_backend --no-wallet
  assert_command dfx canister status e2e_project_backend
  assert_contains 'Memory allocation: 2_147_483_648'
  assert_contains 'Compute allocation: 5'
  assert_contains 'Reserved cycles limit: 1_000_000_000_000'
  assert_contains 'Wasm memory limit: 1_073_741_824'
  assert_contains 'Freezing threshold: 604_800'
  assert_contains 'Log visibility: public'
}

@test "create with default settings" {
  dfx_start
  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains 'Memory allocation: 0'
  assert_contains 'Compute allocation: 0'
  assert_contains 'Reserved cycles limit: 5_000_000_000_000'
  assert_contains 'Wasm memory limit: 3_221_225_472'
  assert_contains 'Freezing threshold: 2_592_000'
  assert_contains 'Log visibility: controllers'
}

@test "create with multiple log allowed viewer list in dfx.json" {
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)

  jq '.canisters.e2e_project_backend.initialization_values={
    "compute_allocation": 5,
    "freezing_threshold": "7days",
    "memory_allocation": "2 GiB",
    "reserved_cycles_limit": 1000000000000,
    "wasm_memory_limit": "1 GiB",
    "log_visibility": {
      "allowed_viewers" :
       ['\""$ALICE_PRINCIPAL"\"', '\""$BOB_PRINCIPAL"\"']
    }
  }' dfx.json | sponge dfx.json
  dfx_start
  assert_command dfx deploy e2e_project_backend --no-wallet
  assert_command dfx canister status e2e_project_backend
  assert_contains 'Memory allocation: 2_147_483_648'
  assert_contains 'Compute allocation: 5'
  assert_contains 'Reserved cycles limit: 1_000_000_000_000'
  assert_contains 'Wasm memory limit: 1_073_741_824'
  assert_contains 'Freezing threshold: 604_800'
  assert_contains "${ALICE_PRINCIPAL}"
  assert_contains "${BOB_PRINCIPAL}"
}

@test "create with multiple log allowed viewer list" {
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)

  dfx_start
  assert_command dfx canister create --all --log-viewer "${ALICE_PRINCIPAL}" --log-viewer "${BOB_PRINCIPAL}"  --no-wallet
  assert_command dfx deploy e2e_project_backend --no-wallet
  assert_command dfx canister status e2e_project_backend
  assert_contains "${ALICE_PRINCIPAL}"
  assert_contains "${BOB_PRINCIPAL}"
}

# The following function decodes a canister id in the textual form into its binary form
# and is taken from the [IC Interface Specification](https://internetcomputer.org/docs/current/references/ic-interface-spec#principal).
function textual_decode() {
  echo -n "$1" | tr -d - | tr "[:lower:]" "[:upper:]" |
  fold -w 8 | xargs -n1 printf '%-8s' | tr ' ' = |
  base32 -d | xxd -p | tr -d '\n' | cut -b9- | tr "[:lower:]" "[:upper:]"
}

@test "create targets application subnet in PocketIC" {
  [[ ! "$USE_POCKETIC" ]] && skip "skipped for replica: no support for multiple subnets"
  dfx_start
  # create the backend canister without a wallet canister so that the backend canister is the only canister ever created
  assert_command dfx canister create e2e_project_backend --no-wallet
  # actual canister id
  CANISTER_ID="$(dfx canister id e2e_project_backend)"
  # base64 encode the actual canister id
  CANISTER_ID_BASE64="$(textual_decode "${CANISTER_ID}" | xxd -r -p | base64)"
  # fetch topology from PocketIC server
  TOPOLOGY="$(curl "http://127.0.0.1:$(dfx info replica-port)/instances/0/read/topology")"
  # find application subnet id in the topology
  for subnet_id in $(echo "${TOPOLOGY}" | jq keys[])
  do
    SUBNET_KIND="$(echo "$TOPOLOGY" | jq -r ".${subnet_id}.\"subnet_kind\"")"
    if [ "${SUBNET_KIND}" == "Application" ]
    then
      # find the expected canister id as the beginning of the first canister range of the app subnet
      EXPECTED_CANISTER_ID_BASE64="$(echo "$TOPOLOGY" | jq -r ".${subnet_id}.\"canister_ranges\"[0].\"start\".\"canister_id\"")"
    fi
  done
  # check if the actual canister id matches the expected canister id
  if [ "${CANISTER_ID_BASE64}" != "${EXPECTED_CANISTER_ID_BASE64}" ]
  then
    echo "Canister id ${CANISTER_ID_BASE64} does not match expected canister id ${EXPECTED_CANISTER_ID_BASE64}"
    exit 1
  fi
}


-----------------------

/e2e/tests-dfx/cycles-ledger.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_
load ../utils/cycles-ledger

setup() {
  standard_setup
  install_asset cycles-ledger
  install_shared_asset subnet_type/shared_network_settings/system
  install_cycles_ledger_canisters

  dfx identity new --storage-mode plaintext cycle-giver
  dfx identity new --storage-mode plaintext alice
  echo "Alice principal: $(dfx identity get-principal --identity alice)"
  dfx identity new --storage-mode plaintext bob
  echo "Bob principal: $(dfx identity get-principal --identity bob)"
}

teardown() {
  dfx_stop

  standard_teardown
}

start_and_install_nns() {
  dfx_start_for_nns_install

  dfx extension install nns --version 0.4.3
  dfx nns install --ledger-accounts "$(dfx ledger account-id --identity cycle-giver)"
}

add_cycles_ledger_canisters_to_project() {
  jq -s '.[0] * .[1]' ../dfx.json dfx.json | sponge dfx.json
}

deploy_cycles_ledger() {
  assert_command dfx deploy cycles-ledger --specified-id "um5iw-rqaaa-aaaaq-qaaba-cai" --argument '(variant { Init = record { max_blocks_per_request = 100; index_id = null; } })'
  assert_command dfx deploy depositor --argument "(record {ledger_id = principal \"$(dfx canister id cycles-ledger)\"})" --with-cycles 10000000000000 --specified-id "ul4oc-4iaaa-aaaaq-qaabq-cai"
}

current_time_nanoseconds() {
  echo "$(date +%s)"000000000
}

@test "balance" {
  start_and_install_nns

  ALICE=$(dfx identity get-principal --identity alice)
  ALICE_SUBACCT1="000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT1_CANDID="\00\01\02\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  ALICE_SUBACCT2="9C9B9A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT2_CANDID="\9C\9B\9A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  BOB=$(dfx identity get-principal --identity bob)

  deploy_cycles_ledger

  assert_command dfx cycles balance --identity alice --precise
  assert_eq "0 cycles."

  assert_command dfx cycles balance --identity alice
  assert_eq "0.000 TC (trillion cycles)."

  assert_command dfx cycles balance --identity bob --precise
  assert_eq "0 cycles."

  assert_command dfx cycles balance --identity bob
  assert_eq "0.000 TC (trillion cycles)."


  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 1_700_400_200_150;})" --identity cycle-giver
  assert_eq "(record { balance = 1_700_400_200_150 : nat; block_index = 0 : nat })"

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\"; subaccount = opt blob \"$ALICE_SUBACCT1_CANDID\"};cycles = 3_750_000_000_000;})" --identity cycle-giver
  assert_eq "(record { balance = 3_750_000_000_000 : nat; block_index = 1 : nat })"

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\"; subaccount = opt blob \"$ALICE_SUBACCT2_CANDID\"};cycles = 760_500_000_000;})" --identity cycle-giver
  assert_eq "(record { balance = 760_500_000_000 : nat; block_index = 2 : nat })"

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$BOB\";};cycles = 2_900_000_000_000;})" --identity cycle-giver
  assert_eq "(record { balance = 2_900_000_000_000 : nat; block_index = 3 : nat })"


  assert_command dfx cycles balance --precise --identity alice
  assert_eq "1700400200150 cycles."

  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT1"
  assert_eq "3750000000000 cycles."

  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "760500000000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2900000000000 cycles."


  assert_command dfx cycles balance --identity alice
  assert_eq "1.700 TC (trillion cycles)."

  assert_command dfx cycles balance --identity alice --subaccount "$ALICE_SUBACCT1"
  assert_eq "3.750 TC (trillion cycles)."

  assert_command dfx cycles balance --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "0.760 TC (trillion cycles)."

  assert_command dfx cycles balance --identity bob
  assert_eq "2.900 TC (trillion cycles)."


  # can see cycles balance of other accounts
  assert_command dfx cycles balance --owner "$ALICE" --identity bob
  assert_eq "1.700 TC (trillion cycles)."

  assert_command dfx cycles balance --owner "$ALICE" --subaccount "$ALICE_SUBACCT1" --identity bob
  assert_eq "3.750 TC (trillion cycles)."

  assert_command dfx cycles balance --owner "$BOB" --identity anonymous
  assert_eq "2.900 TC (trillion cycles)."
}

@test "transfer" {
  start_and_install_nns

  ALICE=$(dfx identity get-principal --identity alice)
  ALICE_SUBACCT1="000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT1_CANDID="\00\01\02\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  ALICE_SUBACCT2="9C9B9A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT2_CANDID="\9C\9B\9A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  BOB=$(dfx identity get-principal --identity bob)
  BOB_SUBACCT1="7C7B7A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"

  deploy_cycles_ledger

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 3_000_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\"; subaccount = opt blob \"$ALICE_SUBACCT1_CANDID\"};cycles = 2_000_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\"; subaccount = opt blob \"$ALICE_SUBACCT2_CANDID\"};cycles = 1_000_000_000_000;})" --identity cycle-giver

  # account to account
  assert_command dfx cycles balance --precise --identity alice
  assert_eq "3000000000000 cycles."
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "0 cycles."

  assert_command dfx cycles transfer "$BOB" 100000 --identity alice
  assert_eq "Transfer sent at block index 3"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999899900000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "100000 cycles."

  # account to subaccount
  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999899900000 cycles."
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "0 cycles."

  assert_command dfx cycles transfer "$BOB" 100000 --identity alice --to-subaccount "$BOB_SUBACCT1"
  assert_eq "Transfer sent at block index 4"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999799800000 cycles."

  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "100000 cycles."


  # subaccount to account
  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "1000000000000 cycles."
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "100000 cycles."

  assert_command dfx cycles transfer "$BOB" 700000 --identity alice --from-subaccount "$ALICE_SUBACCT2"
  assert_eq "Transfer sent at block index 5"

  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "999899300000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "800000 cycles."


  # subaccount to subaccount
  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "999899300000 cycles."
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "100000 cycles."

  assert_command dfx cycles transfer "$BOB" 400000 --identity alice --to-subaccount "$BOB_SUBACCT1" --from-subaccount "$ALICE_SUBACCT2"
  assert_eq "Transfer sent at block index 6"

  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "999798900000 cycles."

  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "500000 cycles."
}

@test "transfer deduplication" {
  start_and_install_nns

  ALICE=$(dfx identity get-principal --identity alice)
  BOB=$(dfx identity get-principal --identity bob)

  deploy_cycles_ledger

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 3_000_000_000_000;})" --identity cycle-giver

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "3000000000000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "0 cycles."

  t=$(current_time_nanoseconds)

  assert_command dfx cycles transfer "$BOB" 100000 --created-at-time "$t" --memo 1 --identity alice
  assert_eq "Transfer sent at block index 1"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999899900000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "100000 cycles."

  # same memo and created-at-time: dupe
  assert_command dfx cycles transfer "$BOB" 100000 --created-at-time "$t" --memo 1 --identity alice
  # shellcheck disable=SC2154
  assert_contains "transaction is a duplicate of another transaction in block 1" "$stderr"
  # shellcheck disable=SC2154
  assert_eq "Transfer sent at block index 1" "$stdout"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999899900000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "100000 cycles."

  # different memo and same created-at-time same: not dupe
  assert_command dfx cycles transfer "$BOB" 100000 --created-at-time "$t" --memo 2 --identity alice
  assert_contains "Transfer sent at block index 2"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999799800000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "200000 cycles."

  # same memo and different created-at-time same: not dupe
  assert_command dfx cycles transfer "$BOB" 100000 --created-at-time $((t+1)) --memo 1 --identity alice
  assert_contains "Transfer sent at block index 3"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999699700000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "300000 cycles."
}

@test "approve and transfer_from" {
  start_and_install_nns

  ALICE=$(dfx identity get-principal --identity alice)
  ALICE_SUBACCT1="000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT1_CANDID="\00\01\02\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  ALICE_SUBACCT2="9C9B9A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT2_CANDID="\9C\9B\9A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  BOB=$(dfx identity get-principal --identity bob)
  BOB_SUBACCT1="7C7B7A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"

  deploy_cycles_ledger

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 3_000_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\"; subaccount = opt blob \"$ALICE_SUBACCT1_CANDID\"};cycles = 1_000_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\"; subaccount = opt blob \"$ALICE_SUBACCT2_CANDID\"};cycles = 1_000_000_000_000;})" --identity cycle-giver

  # account to account
  assert_command dfx cycles balance --precise --identity alice
  assert_eq "3000000000000 cycles."
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "0 cycles."

  t=$(current_time_nanoseconds)
  assert_command dfx cycles approve "$BOB" 2000000000 --created-at-time "$t" --memo 123 --identity alice
  assert_eq "Approval sent at block index 3"
  assert_command dfx cycles approve "$BOB" 2000000000 --created-at-time "$t" --memo 123 --identity alice
  assert_contains "Approval is a duplicate of block 3"
  assert_command dfx cycles transfer "$BOB" 100000 --from "$ALICE" --identity bob
  assert_eq "Transfer sent at block index 4"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999799900000 cycles."

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "100000 cycles."

  # account to subaccount
  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999799900000 cycles."
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "0 cycles."

  t=$(current_time_nanoseconds)
  assert_command dfx cycles transfer "$BOB" 100000 --from "$ALICE" --to-subaccount "$BOB_SUBACCT1" --created-at-time "$t" --identity bob
  assert_eq "Transfer sent at block index 5"
  assert_command dfx cycles transfer "$BOB" 100000 --from "$ALICE" --to-subaccount "$BOB_SUBACCT1" --created-at-time "$t" --identity bob
  assert_contains "Transfer is a duplicate of block index 5"

  assert_command dfx cycles balance --precise --identity alice
  assert_eq "2999699800000 cycles."

  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "100000 cycles."

  # subaccount to account
  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT1"
  assert_eq "1000000000000 cycles."
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "100000 cycles."

  assert_command dfx cycles approve "$BOB" 200000000000 --from-subaccount "$ALICE_SUBACCT1" --identity alice
  assert_eq "Approval sent at block index 6"
  assert_command dfx cycles transfer "$BOB" 700000 --from "$ALICE" --from-subaccount "$ALICE_SUBACCT1" --identity bob
  assert_eq "Transfer sent at block index 7"

  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT1"
  assert_eq "999799300000 cycles."
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "800000 cycles."

  # spender subaccount
  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "1000000000000 cycles."

  assert_command dfx cycles approve "$BOB" 200000000000 --spender-subaccount "$BOB_SUBACCT1" --from-subaccount "$ALICE_SUBACCT2" --identity alice
  assert_eq "Approval sent at block index 8"
  assert_command dfx cycles transfer "$BOB" 300000 --from "$ALICE" --from-subaccount "$ALICE_SUBACCT2"  --spender-subaccount "$BOB_SUBACCT1" --identity bob
  assert_eq "Transfer sent at block index 9"

  assert_command dfx cycles balance --precise --identity alice --subaccount "$ALICE_SUBACCT2"
  assert_eq "999799700000 cycles."
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "1100000 cycles."
}

@test "top up canister principal check" {
  start_and_install_nns

  BOB=$(dfx identity get-principal --identity bob)

  deploy_cycles_ledger

  assert_command_fail dfx cycles top-up "$BOB" 600000 --identity alice
  assert_contains "Invalid receiver: $BOB.  Make sure the receiver is a canister."
}

@test "top-up and deposit-cycles" {
  start_and_install_nns

  dfx_new
  add_cycles_ledger_canisters_to_project
  install_cycles_ledger_canisters

  BOB=$(dfx identity get-principal --identity bob)
  BOB_SUBACCT1="7C7B7A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  BOB_SUBACCT1_CANDID="\7C\7B\7A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  BOB_SUBACCT2="6C6B6A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  BOB_SUBACCT2_CANDID="\6C\6B\6A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"

  deploy_cycles_ledger

  assert_command dfx deploy

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$BOB\";};cycles = 2_400_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$BOB\"; subaccount = opt blob \"$BOB_SUBACCT1_CANDID\"};cycles = 2_600_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$BOB\"; subaccount = opt blob \"$BOB_SUBACCT2_CANDID\"};cycles = 2_700_000_000_000;})" --identity cycle-giver

  # shellcheck disable=SC2030
  export DFX_DISABLE_AUTO_WALLET=1

  # account to canister
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2400000000000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_000_000 Cycles"

  assert_command dfx cycles top-up e2e_project_backend 100000 --identity bob
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399899900000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_100_000 Cycles"

  assert_command dfx canister deposit-cycles 100000 e2e_project_backend --identity bob
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399799800000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_200_000 Cycles"

  # subaccount to canister
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "2600000000000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_200_000 Cycles"

  assert_command dfx cycles top-up e2e_project_backend 300000 --identity bob --from-subaccount "$BOB_SUBACCT1"
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "2599899700000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_500_000 Cycles"

  assert_command dfx canister deposit-cycles 300000 e2e_project_backend --identity bob --from-subaccount "$BOB_SUBACCT1"
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT1"
  assert_eq "2599799400000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_800_000 Cycles"

  # subaccount to canister - by canister id
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT2"
  assert_eq "2700000000000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_800_000 Cycles"

  assert_command dfx cycles top-up "$(dfx canister id e2e_project_backend)" 600000 --identity bob --from-subaccount "$BOB_SUBACCT2"
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT2"
  assert_eq "2699899400000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_001_400_000 Cycles"

  assert_command dfx canister deposit-cycles 600000 "$(dfx canister id e2e_project_backend)" --identity bob --from-subaccount "$BOB_SUBACCT2"
  assert_command dfx cycles balance --precise --identity bob --subaccount "$BOB_SUBACCT2"
  assert_eq "2699798800000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_002_000_000 Cycles"

  # deduplication
  t=$(current_time_nanoseconds)
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399799800000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_002_000_000 Cycles"

  assert_command dfx canister deposit-cycles 100000 e2e_project_backend --identity bob --created-at-time "$t"
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399699700000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_002_100_000 Cycles"

  assert_command dfx canister deposit-cycles 100000 e2e_project_backend --identity bob --created-at-time "$t"
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399699700000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_002_100_000 Cycles"
}

@test "top-up deduplication" {
  start_and_install_nns

  dfx_new
  add_cycles_ledger_canisters_to_project
  install_cycles_ledger_canisters

  BOB=$(dfx identity get-principal --identity bob)
  BOB_SUBACCT1="7C7B7A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  BOB_SUBACCT1_CANDID="\7C\7B\7A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"
  BOB_SUBACCT2="6C6B6A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  BOB_SUBACCT2_CANDID="\6C\6B\6A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"

  deploy_cycles_ledger

  assert_command dfx deploy

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$BOB\";};cycles = 2_400_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$BOB\"; subaccount = opt blob \"$BOB_SUBACCT1_CANDID\"};cycles = 2_600_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$BOB\"; subaccount = opt blob \"$BOB_SUBACCT2_CANDID\"};cycles = 2_700_000_000_000;})" --identity cycle-giver

  # account to canister
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2400000000000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_000_000 Cycles"

  t=$(current_time_nanoseconds)
  assert_command dfx cycles top-up "$(dfx canister id e2e_project_backend)" --created-at-time "$t" 100000 --identity bob
  assert_eq "Transfer sent at block index 3"

  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399899900000 cycles."
  assert_command dfx canister status e2e_project_backend
  assert_contains "Balance: 3_100_000_100_000 Cycles"

  # same created-at-time: dupe
  assert_command dfx cycles top-up "$(dfx canister id e2e_project_backend)" --created-at-time "$t" 100000 --identity bob
  # shellcheck disable=SC2154
  assert_contains "transaction is a duplicate of another transaction in block 3" "$stderr"
  # shellcheck disable=SC2154
  assert_contains "Transfer sent at block index 3" "$stdout"
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399899900000 cycles."

  # different created-at-time: not dupe
  assert_command dfx cycles top-up "$(dfx canister id e2e_project_backend)" --created-at-time $((t+1)) 100000 --identity bob
  assert_eq "Transfer sent at block index 4"
  assert_command dfx cycles balance --precise --identity bob
  assert_eq "2399799800000 cycles."
}

@test "howto" {
  start_and_install_nns

  # This is the equivalent of https://www.notion.so/dfinityorg/How-to-install-and-test-the-cycles-ledger-521c9f3c410f4a438514a03e35464299
  ALICE=$(dfx identity get-principal --identity alice)
  BOB=$(dfx identity get-principal --identity bob)

  deploy_cycles_ledger

  assert_command dfx ledger balance --identity cycle-giver
  assert_eq "1000000000.00000000 ICP"

  assert_command dfx canister status depositor
  assert_contains "Balance: 10_000_000_000_000 Cycles"

  dfx canister status depositor

  assert_command dfx cycles balance --identity alice --precise
  assert_eq "0 cycles."

  assert_command dfx cycles balance --identity bob --precise
  assert_eq "0 cycles."


  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 500_000_000;})" --identity cycle-giver
  assert_eq "(record { balance = 500_000_000 : nat; block_index = 0 : nat })"

  assert_command dfx canister status depositor
  assert_contains "Balance: 9_999_500_000_000 Cycles"

  assert_command dfx cycles balance --identity alice --precise
  assert_eq "500000000 cycles."

  assert_command dfx cycles transfer "$BOB" 100000 --identity alice
  assert_eq "Transfer sent at block index 1"

  assert_command dfx cycles balance --identity alice --precise
  assert_eq "399900000 cycles."

  assert_command dfx cycles balance --identity bob --precise
  assert_eq "100000 cycles."

  assert_command dfx cycles top-up depositor 100000 --identity alice
  assert_eq "Transfer sent at block index 2"

  assert_command dfx cycles balance --identity alice --precise
  assert_eq "299800000 cycles."

  assert_command dfx canister status depositor
  assert_contains "Balance: 9_999_500_100_000 Cycles"
}

@test "canister creation" {
  start_and_install_nns
  
  dfx_new temporary
  add_cycles_ledger_canisters_to_project
  install_cycles_ledger_canisters

  ALICE=$(dfx identity get-principal --identity alice)
  ALICE_SUBACCT1="7C7B7A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT1_CANDID="\7C\7B\7A\03\04\05\06\07\08\09\0a\0b\0c\0d\0e\0f\10\11\12\13\14\15\16\17\18\19\1a\1b\1c\1d\1e\1f"

  assert_command deploy_cycles_ledger
  CYCLES_LEDGER_ID=$(dfx canister id cycles-ledger)
  echo "Cycles ledger deployed at id $CYCLES_LEDGER_ID"
  assert_command dfx deploy depositor --argument "(record {ledger_id = principal \"$(dfx canister id cycles-ledger)\"})"
  echo "Cycles depositor deployed at id $(dfx canister id depositor)"
  assert_command dfx ledger fabricate-cycles --canister depositor --t 9999

  assert_command dfx deploy

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 13_400_000_000_000;})" --identity cycle-giver
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\"; subaccount = opt blob \"$ALICE_SUBACCT1_CANDID\"};cycles = 2_600_000_000_000;})" --identity cycle-giver

  cd ..
  dfx_new
  # setup done

  # using dfx canister create
  dfx identity use alice
  # shellcheck disable=SC2030,SC2031
  export DFX_DISABLE_AUTO_WALLET=1
  t=$(current_time_nanoseconds)
  assert_command dfx canister create e2e_project_backend --with-cycles 1T --created-at-time "$t"
  assert_command dfx canister id e2e_project_backend
  E2E_PROJECT_BACKEND_CANISTER_ID=$(dfx canister id e2e_project_backend)
  assert_command dfx cycles balance --precise
  assert_eq "12399900000000 cycles."
  # forget about canister. If --created-at-time is a valid idempotency key we should end up with the same canister id
  rm .dfx/local/canister_ids.json
  assert_command dfx canister create e2e_project_backend --with-cycles 1T --created-at-time "$t"
  assert_command dfx canister id e2e_project_backend
  assert_contains "$E2E_PROJECT_BACKEND_CANISTER_ID"
  assert_command dfx cycles balance --precise
  assert_eq "12399900000000 cycles."
  dfx canister stop e2e_project_backend
  dfx canister delete e2e_project_backend --no-withdrawal

  assert_command dfx canister create e2e_project_backend --with-cycles 0.5T --from-subaccount "$ALICE_SUBACCT1"
  assert_command dfx canister id e2e_project_backend
  assert_command dfx cycles balance --subaccount "$ALICE_SUBACCT1" --precise
  assert_eq "2099900000000 cycles."
  
  # reset deployment status
  rm -r .dfx

  # using dfx deploy
  t=$(current_time_nanoseconds)
  assert_command dfx deploy e2e_project_backend --with-cycles 1T --created-at-time "$t"
  assert_command dfx canister id e2e_project_backend
  E2E_PROJECT_BACKEND_CANISTER_ID=$(dfx canister id e2e_project_backend)
  assert_command dfx cycles balance --precise
  assert_eq "11399800000000 cycles."
  # reset and forget about canister. If --created-at-time is a valid idempotency key we should end up with the same canister id
  dfx canister uninstall-code e2e_project_backend
  rm .dfx/local/canister_ids.json
  assert_command dfx deploy e2e_project_backend --with-cycles 1T --created-at-time "$t" -vv
  assert_command dfx canister id e2e_project_backend
  assert_contains "$E2E_PROJECT_BACKEND_CANISTER_ID"
  assert_command dfx cycles balance --precise
  assert_eq "11399800000000 cycles."
  dfx canister stop e2e_project_backend
  dfx canister delete e2e_project_backend --no-withdrawal
  
  assert_command dfx deploy e2e_project_backend --with-cycles 0.5T --from-subaccount "$ALICE_SUBACCT1"
  assert_command dfx canister id e2e_project_backend
  assert_command dfx cycles balance --subaccount "$ALICE_SUBACCT1" --precise
  assert_eq "1599800000000 cycles."
  dfx canister stop e2e_project_backend
  dfx canister delete e2e_project_backend --no-withdrawal
}

@test "canister deletion" {
  start_and_install_nns

  dfx_new temporary
  add_cycles_ledger_canisters_to_project
  install_cycles_ledger_canisters

  ALICE=$(dfx identity get-principal --identity alice)

  assert_command deploy_cycles_ledger
  CYCLES_LEDGER_ID=$(dfx canister id cycles-ledger)
  echo "Cycles ledger deployed at id $CYCLES_LEDGER_ID"
  assert_command dfx deploy depositor --argument "(record {ledger_id = principal \"$(dfx canister id cycles-ledger)\"})"
  echo "Cycles depositor deployed at id $(dfx canister id depositor)"
  assert_command dfx ledger fabricate-cycles --canister depositor --t 9999
  assert_command dfx deploy
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 22_400_000_000_000;})" --identity cycle-giver

  cd ..
  dfx_new
  # setup done

  dfx identity use alice
  # shellcheck disable=SC2030,SC2031
  export DFX_DISABLE_AUTO_WALLET=1
  assert_command dfx canister create --all

  # delete by name
  assert_command dfx canister stop --all
  assert_command dfx canister delete e2e_project_backend
  assert_contains "Successfully withdrew"

  # delete by id
  assert_command dfx canister create --all
  CANISTER_ID=$(dfx canister id e2e_project_backend)
  rm .dfx/local/canister_ids.json
  assert_command dfx canister stop "${CANISTER_ID}"
  assert_command dfx canister delete "${CANISTER_ID}"
  assert_contains "Successfully withdrew"
}

@test "redeem-faucet-coupon redeems into the cycles ledger" {
  start_and_install_nns

  assert_command deploy_cycles_ledger
  dfx_new hello
  install_asset faucet
  dfx deploy
  dfx ledger fabricate-cycles --canister faucet --t 1000

  dfx identity new --storage-mode plaintext no_wallet_identity
  dfx identity use no_wallet_identity
  SUBACCOUNT="7C7B7A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"

  assert_command dfx cycles balance
  assert_eq "0.000 TC (trillion cycles)."
  assert_command dfx cycles balance --subaccount "$SUBACCOUNT"
  assert_eq "0.000 TC (trillion cycles)."

  assert_command dfx cycles redeem-faucet-coupon --faucet "$(dfx canister id faucet)" 'valid-coupon'
  assert_match "Redeemed coupon 'valid-coupon'"
  assert_command dfx cycles redeem-faucet-coupon --faucet "$(dfx canister id faucet)" 'another-valid-coupon'
  assert_match "Redeemed coupon 'another-valid-coupon'"
  assert_command dfx cycles balance
  assert_eq "20.000 TC (trillion cycles)."

  # with subaccount
  assert_command dfx cycles redeem-faucet-coupon --faucet "$(dfx canister id faucet)" 'another-valid-coupon' --to-subaccount "$SUBACCOUNT"
  assert_match "Redeemed coupon 'another-valid-coupon'"
  assert_command dfx cycles balance --subaccount "$SUBACCOUNT"
  assert_eq "10.000 TC (trillion cycles)."
}

@test "create canister on specific subnet" {
  start_and_install_nns
  
  dfx_new temporary
  add_cycles_ledger_canisters_to_project
  install_cycles_ledger_canisters

  ALICE=$(dfx identity get-principal --identity alice)

  assert_command deploy_cycles_ledger
  CYCLES_LEDGER_ID=$(dfx canister id cycles-ledger)
  echo "Cycles ledger deployed at id $CYCLES_LEDGER_ID"
  assert_command dfx deploy depositor --argument "(record {ledger_id = principal \"$(dfx canister id cycles-ledger)\"})"
  echo "Cycles depositor deployed at id $(dfx canister id depositor)"
  assert_command dfx ledger fabricate-cycles --canister depositor --t 9999

  assert_command dfx deploy

  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 13_400_000_000_000;})" --identity cycle-giver
  cd ..
  dfx_new 

  dfx identity use alice
  # shellcheck disable=SC2030,SC2031
  export DFX_DISABLE_AUTO_WALLET=1
  # setup done

  # use --subnet <principal>
  SUBNET_ID="5kdm2-62fc6-fwnja-hutkz-ycsnm-4z33i-woh43-4cenu-ev7mi-gii6t-4ae" # a random, valid principal
  assert_command_fail dfx canister create e2e_project_backend --subnet "$SUBNET_ID"
  assert_contains "Subnet $SUBNET_ID does not exist"
  
  # use --subnet-type
  assert_command_fail dfx canister create e2e_project_backend --subnet-type custom_subnet_type
  assert_contains "Provided subnet type custom_subnet_type does not exist"
}

@test "automatically choose subnet" {
  [[ "$USE_POCKETIC" ]] && skip "skipped for pocketic: subnet range"
  dfx_start

  REGISTRY="rwlgt-iiaaa-aaaaa-aaaaa-cai"
  CMC="rkp4c-7iaaa-aaaaa-aaaca-cai"
  ALICE=$(dfx identity get-principal --identity alice)
  dfx_new temporary
  install_asset fake_registry
  dfx deploy fake_registry --specified-id "$REGISTRY"
  add_cycles_ledger_canisters_to_project
  install_cycles_ledger_canisters
  assert_command deploy_cycles_ledger
  CYCLES_LEDGER_ID=$(dfx canister id cycles-ledger)
  echo "Cycles ledger deployed at id $CYCLES_LEDGER_ID"
  assert_command dfx deploy depositor --argument "(record {ledger_id = principal \"$(dfx canister id cycles-ledger)\"})"
  echo "Cycles depositor deployed at id $(dfx canister id depositor)"
  assert_command dfx ledger fabricate-cycles --canister depositor --t 9999
  assert_command dfx canister call depositor deposit "(record {to = record{owner = principal \"$ALICE\";};cycles = 99_000_000_000_000;})"
  install_asset fake_cmc
  dfx deploy fake-cmc --specified-id "$CMC"
  cd ..
  # shellcheck disable=SC2030,SC2031
  export DFX_DISABLE_AUTO_WALLET=1
  dfx identity use alice
  dfx_new

  SUBNET1="iqd74-4xnai"
  SUBNET2="2myss-nlbai"

  jq '.canisters.one = { "main": "src/e2e_project_backend/main.mo", "type": "motoko" }' dfx.json | sponge dfx.json
  jq '.canisters.two = { "main": "src/e2e_project_backend/main.mo", "type": "motoko" }' dfx.json | sponge dfx.json
  # setup done


  # no other canisters already exist
  assert_command dfx canister create e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet_selection = null"
  stop_and_delete e2e_project_backend

  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet_selection = null"
  stop_and_delete e2e_project_backend

  # one other canister already exists
  assert_command dfx canister create one --subnet aaaaa-aa
  ONE_ID="$(dfx canister id one)"
  echo "Canister one: $ONE_ID"
  assert_command dfx canister call "$REGISTRY" set_subnet_for_canister "(vec { record {0 = principal \"$ONE_ID\"; 1 = principal \"$SUBNET1\"} })"

  assert_command dfx canister create e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET1\""
  stop_and_delete e2e_project_backend

  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET1\""
  stop_and_delete e2e_project_backend

  # multiple other canisters already exist - all on same subnet
  assert_command dfx canister create two --subnet aaaaa-aa
  TWO_ID="$(dfx canister id two)"
  echo "Canister two: $TWO_ID"
  assert_command dfx canister call "$REGISTRY" set_subnet_for_canister "(vec { record {0 = principal \"$ONE_ID\"; 1 = principal \"$SUBNET1\"}; record { 0 = principal \"$TWO_ID\"; 1 = principal \"$SUBNET1\"} })"

  assert_command dfx canister create e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET1\""
  stop_and_delete e2e_project_backend

  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET1\""
  stop_and_delete e2e_project_backend

  # multiple other canisters already exist - not all on same subnet
  assert_command dfx canister call "$REGISTRY" set_subnet_for_canister "(vec { record {0 = principal \"$ONE_ID\"; 1 = principal \"$SUBNET1\"}; record { 0 = principal \"$TWO_ID\"; 1 = principal \"$SUBNET2\"} })"

  assert_command_fail dfx canister create e2e_project_backend
  assert_contains "Cannot automatically decide which subnet to target."

  assert_command_fail dfx deploy e2e_project_backend
  assert_contains "Cannot automatically decide which subnet to target."
  
  # still can create if a subnet is specified
  assert_command dfx canister create e2e_project_backend --subnet "$SUBNET2"
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET2\""
  stop_and_delete e2e_project_backend

  assert_command dfx deploy e2e_project_backend --subnet "$SUBNET2"
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET2\""
  stop_and_delete e2e_project_backend

  # remote canister exists on different subnet
  THREE_ID="3333u-aiaaa-aaaar-avzbq-cai"
  jq '.canisters.three = { "main": "src/e2e_project_backend/main.mo", "type": "motoko", "remote" : { "candid": "", "id": { "local": "'$THREE_ID'" } } }' dfx.json | sponge dfx.json
  assert_command dfx canister call "$REGISTRY" set_subnet_for_canister "(vec { record {0 = principal \"$ONE_ID\"; 1 = principal \"$SUBNET1\"}; record { 0 = principal \"$TWO_ID\"; 1 = principal \"$SUBNET1\"}; record { 0 = principal \"$THREE_ID\"; 1 = principal \"$SUBNET2\"} })"

  assert_command dfx canister create e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET1\""
  stop_and_delete e2e_project_backend

  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister call "$CMC" last_create_canister_args --query
  assert_contains "subnet = principal \"$SUBNET1\""
  stop_and_delete e2e_project_backend
}

@test "convert icp to cycles" {
  start_and_install_nns

  ALICE=$(dfx identity get-principal --identity alice)
  ALICE_SUBACCT1="000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"
  ALICE_SUBACCT2="6C6B6A030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f"

  deploy_cycles_ledger

  assert_command dfx --identity cycle-giver ledger transfer --memo 1234 --amount 100 "$(dfx ledger account-id --of-principal "$ALICE")"
  assert_command dfx --identity cycle-giver ledger transfer --memo 1234 --amount 100 "$(dfx ledger account-id --of-principal "$ALICE" --subaccount "$ALICE_SUBACCT1")"

  dfx identity use alice
  assert_command dfx ledger balance
  assert_eq "100.00000000 ICP"
  assert_command dfx ledger balance --subaccount "$ALICE_SUBACCT1"
  assert_eq "100.00000000 ICP"
  assert_command dfx cycles balance --precise
  assert_eq "0 cycles."

  dfx canister call rrkah-fqaaa-aaaaa-aaaaq-cai get_proposal_info '(3 : nat64)'

  # base case
  assert_command dfx cycles convert --amount 12.5
  assert_contains "Account was topped up with 1_543_208_750_000_000 cycles!"
  assert_command dfx ledger balance
  assert_eq "87.49990000 ICP"
  assert_command dfx cycles balance --precise
  assert_eq "1543208750000000 cycles."

  # to-subaccount and from-subaccount
  assert_command dfx cycles convert --amount 10 --from-subaccount "$ALICE_SUBACCT1" --to-subaccount "$ALICE_SUBACCT2"
  assert_contains "Account was topped up with 1_234_567_000_000_000 cycles!"
  assert_command dfx ledger balance --subaccount "$ALICE_SUBACCT1"
  assert_eq "89.99990000 ICP"
  assert_command dfx cycles balance --precise --subaccount "$ALICE_SUBACCT2"
  assert_eq "1234567000000000 cycles."

  # deduplication
  t=$(current_time_nanoseconds)
  assert_command dfx cycles convert --amount 10 --created-at-time "$t"
  assert_contains "Transfer sent at block height 12"
  assert_command dfx cycles balance --precise
  assert_eq "2777775750000000 cycles."
  # same created-at-time: dupe
  assert_command dfx cycles convert --amount 10 --created-at-time "$t"
  # shellcheck disable=SC2154
  assert_contains "transaction is a duplicate of another transaction in block 12" "$stderr"
  # shellcheck disable=SC2154
  assert_contains "Transfer sent at block height 12"
}


-----------------------

/e2e/tests-dfx/delete.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "delete by canister id cleans up canister id store" {
  dfx_start
  dfx deploy e2e_project_backend
  id=$(dfx canister id e2e_project_backend)
  dfx canister stop e2e_project_backend
  assert_command dfx canister delete "$id"
  assert_command_fail dfx canister info e2e_project_backend
  assert_contains "Cannot find canister id. Please issue 'dfx canister create e2e_project_backend'."
  assert_command_fail dfx canister status "$id"
  assert_contains "Canister $id not found"
  assert_command dfx deploy
}

@test "delete can be used to delete a canister" {
  dfx_start
  dfx deploy e2e_project_backend
  id=$(dfx canister id e2e_project_backend)
  dfx canister stop e2e_project_backend
  assert_command dfx canister delete e2e_project_backend
  assert_command_fail dfx canister info e2e_project_backend
  assert_contains "Cannot find canister id. Please issue 'dfx canister create e2e_project_backend'."
  assert_command_fail dfx canister status "$id"
  assert_contains "Canister $id not found"
}

@test "delete requires confirmation if the canister is not stopped" {
  dfx_start
  dfx deploy e2e_project_backend
  id=$(dfx canister id e2e_project_backend)
  assert_command_fail timeout -s9 20s dfx canister delete e2e_project_backend
  assert_command dfx canister info e2e_project_backend
  assert_command dfx canister delete e2e_project_backend -y
  assert_command_fail dfx canister info e2e_project_backend
  assert_contains "Cannot find canister id. Please issue 'dfx canister create e2e_project_backend'."
  assert_command_fail dfx canister status "$id"
  assert_contains "Canister $id not found"
}


-----------------------

/e2e/tests-dfx/deploy.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_
load ../utils/cycles-ledger

setup() {
  standard_setup

  dfx_new_assets hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "deploy with reserved cycles limit" {
    dfx_start
    cat dfx.json
    jq '.canisters.hello_backend.initialization_values.reserved_cycles_limit=860000' dfx.json | sponge dfx.json
    assert_command_fail dfx deploy
    assert_contains "Cannot create a canister using a wallet if the reserved_cycles_limit is set. Please create with --no-wallet or use dfx canister update-settings instead."

    assert_command dfx deploy --no-wallet

    assert_command dfx canister status hello_backend
    assert_contains "Reserved cycles limit: 860_000 Cycles"
}

@test "deploy --upgrade-unchanged upgrades even if the .wasm did not change" {
  dfx_start
  assert_command dfx deploy

  assert_command dfx deploy
  assert_match "Module hash.*is already installed"

  assert_command dfx deploy --upgrade-unchanged
  assert_not_match "Module hash.*is already installed"
}

@test "deploy without --no-wallet sets wallet and self as the controllers" {
  dfx_start
  WALLET=$(dfx identity get-wallet)
  PRINCIPAL=$(dfx identity get-principal)
  assert_command dfx deploy hello_backend
  assert_command dfx canister info hello_backend
  assert_match "Controllers: ($WALLET $PRINCIPAL|$PRINCIPAL $WALLET)"
}

@test "deploy --no-wallet sets only self as the controller" {
  dfx_start
  WALLET=$(dfx identity get-wallet)
  PRINCIPAL=$(dfx identity get-principal)
  assert_command dfx deploy hello_backend --no-wallet
  assert_command dfx canister info hello_backend
  assert_not_match "Controllers: ($WALLET $PRINCIPAL|$PRINCIPAL $WALLET)"
  assert_match "Controllers: $PRINCIPAL"
}

@test "deploy from a subdirectory" {
  dfx_new hello
  dfx_start
  install_asset greet

  (
    cd src
    assert_command dfx deploy
    assert_match "Installing code for"
  )

  assert_command dfx canister call hello_backend greet '("Banzai")'
  assert_eq '("Hello, Banzai!")'

  assert_command dfx deploy
  assert_not_match "Installing code for"
  assert_match "is already installed"
}

@test "deploying multiple canisters with arguments fails" {
  assert_command_fail dfx deploy --argument hello
  assert_contains "The init argument can only be set when deploying a single canister."
}

@test "deploy one canister with an argument" {
  dfx_start
  assert_command dfx deploy hello_backend --argument '()'
}

@test "deploy one canister specifying raw argument" {
  dfx_start
  assert_command dfx deploy hello_backend --argument '4449444c0000' --argument-type raw
}

@test "deploy with an argument in a file" {
  dfx_start
  TMPFILE="$(mktemp)"
  echo '()' >"$TMPFILE"
  assert_command dfx deploy hello_backend --argument-file "$TMPFILE"
}

@test "deploying a dependent doesn't require already-installed dependencies to take args" {
  install_asset deploy_deps
  dfx_start
  assert_command dfx deploy dependency --argument '("dfx")'
  touch dependency.mo
  assert_command dfx deploy dependent
  assert_command dfx canister call dependency greet
  assert_match "Hello, dfx!"
}

@test "deploy succeeds if init_arg is defined in dfx.json" {
  install_asset deploy_deps
  dfx_start
  jq '.canisters.dependency.init_arg="(\"dfx\")"' dfx.json | sponge dfx.json
  assert_command dfx deploy dependency
  assert_command dfx canister call dependency greet
  assert_match "Hello, dfx!"

  assert_command dfx deploy dependency --mode reinstall --yes --argument '("icp")'
  assert_contains "Canister 'dependency' has init_arg/init_arg_file in dfx.json: (\"dfx\"),"
  assert_contains "which is different from the one specified in the command line: (\"icp\")."
  assert_contains "The command line value will be used."
  assert_command dfx canister call dependency greet
  assert_match "Hello, icp!"
}

@test "reinstalling a single Motoko canister with imported dependency works" {
  install_asset import_canister
  dfx_start
  assert_command dfx deploy
  assert_command dfx deploy importer --mode reinstall --yes
}

@test "deploy succeeds with --specified-id" {
  dfx_start
  assert_command dfx deploy hello_backend --specified-id n5n4y-3aaaa-aaaaa-p777q-cai
  assert_command dfx canister id hello_backend
  assert_match n5n4y-3aaaa-aaaaa-p777q-cai
}

@test "deploy fails if --specified-id without canister_name" {
  dfx_start
  assert_command_fail dfx deploy --specified-id n5n4y-3aaaa-aaaaa-p777q-cai
  assert_match \
"error: the following required arguments were not provided:
  <CANISTER_NAME>"
}

@test "deploy succeeds when specify canister ID in dfx.json" {
  dfx_start
  jq '.canisters.hello_backend.specified_id="n5n4y-3aaaa-aaaaa-p777q-cai"' dfx.json | sponge dfx.json
  cat dfx.json
  assert_command dfx deploy hello_backend
  assert_command dfx canister id hello_backend
  assert_match n5n4y-3aaaa-aaaaa-p777q-cai
}

@test "deploy succeeds when specify canister ID in dfx.json even if other canisters are malformed" {
  dfx_start
  jq '.canisters.hello_backend.specified_id="n5n4y-3aaaa-aaaaa-p777q-cai"' dfx.json | sponge dfx.json
  # add a malformed canister
  jq '.canisters += {"malformed": {"remote": {"id": {"local": "hptcf-emaaa-aaaaa-qaawq-cai"}}}}' dfx.json | sponge dfx.json
  assert_command dfx deploy hello_backend
  assert_command dfx canister id hello_backend
  assert_match n5n4y-3aaaa-aaaaa-p777q-cai
}

@test "deploy succeeds when specify canister ID both in dfx.json and cli; warning if different; cli value takes effect" {
  dfx_start
  jq '.canisters.hello_backend.specified_id="n5n4y-3aaaa-aaaaa-p777q-cai"' dfx.json | sponge dfx.json
  assert_command dfx deploy hello_backend --specified-id n2m2m-wyaaa-aaaaa-p777a-cai
  assert_contains "WARN: Canister 'hello_backend' has a specified ID in dfx.json: n5n4y-3aaaa-aaaaa-p777q-cai,"
  assert_contains "which is different from the one specified in the command line: n2m2m-wyaaa-aaaaa-p777a-cai."
  assert_contains "The command line value will be used."

  assert_command dfx canister id hello_backend
  assert_match n2m2m-wyaaa-aaaaa-p777a-cai
}

@test "deploy does not require wallet if all canisters are created" {
  dfx_start
  dfx canister create --all --no-wallet
  assert_command dfx deploy
  assert_not_contains "Creating a wallet canister"
  assert_command dfx identity get-wallet
  assert_contains "Creating a wallet canister"
}

@test "can deploy gzip wasm" {
  jq '.canisters.hello_backend.gzip=true' dfx.json | sponge dfx.json
  dfx_start
  assert_command dfx deploy
  BUILD_HASH="0x$(sha256sum .dfx/local/canisters/hello_backend/hello_backend.wasm.gz | cut -d " " -f 1)"
  ONCHAIN_HASH="$(dfx canister info hello_backend | tail -n 1 | cut -d " " -f 3)"
  assert_eq "$BUILD_HASH" "$ONCHAIN_HASH"
}

@test "prints the frontend url after deploy" {
  dfx_new_frontend hello
  dfx_start
  assert_command dfx deploy
  frontend_id=$(dfx canister id hello_frontend)
  assert_match "http://127.0.0.1.+${frontend_id}"
  assert_match "${frontend_id}.localhost"
}

@test "prints the frontend url if 'frontend' section is not present in dfx.json" {
  dfx_new_frontend hello
  jq 'del(.canisters.hello_frontend.frontend)' dfx.json | sponge dfx.json
  dfx_start
  assert_command dfx deploy
  frontend_id=$(dfx canister id hello_frontend)
  assert_match "http://127.0.0.1.+${frontend_id}"
  assert_match "${frontend_id}.localhost"
}

@test "prints the frontend url if the frontend section has been removed after initial deployment" {
  dfx_new_frontend hello
  dfx_start
  assert_command dfx deploy
  frontend_id=$(dfx canister id hello_frontend)
  assert_match "http://127.0.0.1.+${frontend_id}"
  assert_match "${frontend_id}.localhost"
  jq 'del(.canisters.hello_frontend.frontend)' dfx.json | sponge dfx.json
  assert_command dfx deploy
  assert_match "http://127.0.0.1.+${frontend_id}"
  assert_match "${frontend_id}.localhost"
}

@test "subnet targetting" {
  # fake cmc setup
  cd ..
  dfx_new fake_cmc
  install_asset fake_cmc
  install_cycles_ledger_canisters
  dfx_start
  assert_command dfx deploy fake-cmc --specified-id "rkp4c-7iaaa-aaaaa-aaaca-cai" # CMC canister id
  cd ../hello

  # use --subnet <principal>
  SUBNET_ID="5kdm2-62fc6-fwnja-hutkz-ycsnm-4z33i-woh43-4cenu-ev7mi-gii6t-4ae" # a random, valid principal
  assert_command dfx deploy hello_backend --subnet "$SUBNET_ID"
  cd ../fake_cmc
  assert_command dfx canister call fake-cmc last_create_canister_args
  assert_contains "subnet = principal \"$SUBNET_ID\";"
  
  # use --subnet-type
  cd ../hello
  assert_command dfx deploy hello_frontend --subnet-type custom_subnet_type
  cd ../fake_cmc
  assert_command dfx canister call fake-cmc last_create_canister_args
  assert_contains 'subnet_type = opt "custom_subnet_type"'
}

@test "specify install mode" {
  dfx_start
  assert_command dfx deploy --mode install
  assert_command dfx deploy hello_backend --mode reinstall --yes
  assert_command dfx deploy --mode upgrade
  assert_command dfx deploy --mode auto
}

@test "specify upgrade options (skip_pre_upgrade, wasm_memory_persistence)" {
  dfx_start

  # The default mode in deploy is 'auto'.
  # When the canister is not deployed yet, the actual InstallMode is 'install'.
  # In this case, the provided upgrade options are just hint which doesn't take effect.
  assert_command dfx deploy --skip-pre-upgrade

  assert_command dfx deploy --wasm-memory-persistence keep
  assert_command dfx deploy --wasm-memory-persistence replace
  assert_command dfx deploy --skip-pre-upgrade --wasm-memory-persistence keep
  assert_command dfx deploy --mode auto --skip-pre-upgrade
  assert_command dfx deploy --mode upgrade --skip-pre-upgrade

  assert_command_fail dfx deploy --mode install --skip-pre-upgrade
  assert_contains "--skip-pre-upgrade and --wasm-memory-persistence can only be used with mode 'upgrade' or 'auto'."
  assert_command_fail dfx deploy --mode reinstall --wasm-memory-persistence keep
  assert_contains "--skip-pre-upgrade and --wasm-memory-persistence can only be used with mode 'upgrade' or 'auto'."
}


-----------------------

/e2e/tests-dfx/deps.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  stop_webserver
  dfx_stop
  standard_teardown
}

CANISTER_ID_A="yofga-2qaaa-aaaaa-aabsq-cai"
CANISTER_ID_B="yhgn4-myaaa-aaaaa-aabta-cai"
CANISTER_ID_C="yahli-baaaa-aaaaa-aabtq-cai"

# only execute in project root (deps)
setup_onchain() {
  install_asset deps

  # start a webserver to host wasm files
  mkdir www
  start_webserver --directory www

  cd onchain || exit

  jq '.canisters.a.pullable.wasm_url="'"http://localhost:$E2E_WEB_SERVER_PORT/a.wasm"'"' dfx.json | sponge dfx.json
  jq '.canisters.b.pullable.wasm_url="'"http://localhost:$E2E_WEB_SERVER_PORT/b.wasm.gz"'"' dfx.json | sponge dfx.json
  jq '.canisters.c.pullable.wasm_url="'"http://localhost:$E2E_WEB_SERVER_PORT/c.wasm"'"' dfx.json | sponge dfx.json

  dfx canister create a --specified-id "$CANISTER_ID_A"
  dfx canister create b --specified-id "$CANISTER_ID_B"
  dfx canister create c --specified-id "$CANISTER_ID_C"
  dfx build

  dfx canister install a --argument 1
  dfx canister install b
  dfx canister install c --argument "(opt 3)"

  # copy wasm files to web server dir
  cp .dfx/local/canisters/a/a.wasm ../www/a.wasm
  cp .dfx/local/canisters/b/b.wasm.gz ../www/b.wasm.gz
  cp .dfx/local/canisters/c/c.wasm ../www/c.wasm

  cd .. || exit
}

# only execute in project root (deps)
cleanup_onchain() {
  cd onchain || exit
  dfx canister stop a
  dfx canister delete a --no-withdrawal
  dfx canister stop b
  dfx canister delete b --no-withdrawal
  dfx canister stop c
  dfx canister delete c --no-withdrawal
  cd .. || exit
}

@test "dfx build can write required metadata for pullable" {
  dfx_start

  install_asset deps

  cd onchain
  assert_command dfx canister create --all
  assert_command dfx build
  assert_command ic-wasm .dfx/local/canisters/c/c.wasm metadata
  assert_match "icp:public candid:service"
  assert_match "icp:public dfx"

  ic-wasm .dfx/local/canisters/c/c.wasm metadata dfx > c_dfx.json
  assert_command jq -r '.pullable.wasm_url' c_dfx.json
  assert_eq "http://httpbin.org/status/404" "$output"
  assert_command jq -r '.pullable.dependencies | length' c_dfx.json
  assert_eq 1 "$output"
  assert_command jq -r '.pullable.dependencies | first' c_dfx.json
  assert_eq "$CANISTER_ID_A" "$output"
  assert_command jq -r '.pullable.init_guide' c_dfx.json
  assert_eq "An optional natural number, e.g. \"(opt 20)\"." "$output"
}

@test "dfx deps pull can resolve dependencies from on-chain canister metadata" {
  dfx_start

  install_asset deps

  # 1. success path
  ## 1.1. prepare "onchain" canisters
  # a -> []
  # b -> [a]
  # c -> [a]
  # app -> [a, b]

  cd onchain

  dfx canister create a --specified-id "$CANISTER_ID_A"
  dfx canister create b --specified-id "$CANISTER_ID_B"
  dfx canister create c --specified-id "$CANISTER_ID_C"

  dfx deploy a --argument 1
  dfx deploy b
  dfx deploy c

  ## 1.2. pull onchain canisters in "app" project
  cd ../app

  assert_command_fail dfx deps pull --network local # the overall pull fail but succeed to fetch and parse `dfx:deps` recursively
  assert_contains "Fetching dependencies of canister $CANISTER_ID_B...
Fetching dependencies of canister $CANISTER_ID_C...
Fetching dependencies of canister $CANISTER_ID_A...
Found 3 dependencies:
$CANISTER_ID_A
$CANISTER_ID_B
$CANISTER_ID_C"
  assert_occurs 1 "Fetching dependencies of canister $CANISTER_ID_A..." # common dependency onchain_a is pulled only once
  assert_contains "Pulling canister $CANISTER_ID_A...
ERROR: Failed to pull canister $CANISTER_ID_A.
Failed to download from url: http://httpbin.org/status/404."
  assert_contains "Pulling canister $CANISTER_ID_B...
ERROR: Failed to pull canister $CANISTER_ID_B.
Failed to download from url: http://httpbin.org/status/404."
  assert_contains "Pulling canister $CANISTER_ID_C...
ERROR: Failed to pull canister $CANISTER_ID_C.
Failed to download from url: http://httpbin.org/status/404."

  # 3. sad path: if the canister is not present on-chain
  cd ../onchain
  dfx build c
  dfx canister install c --mode=reinstall --yes # reinstall the correct canister c
  dfx canister uninstall-code a

  cd ../app
  assert_command_fail dfx deps pull --network local
  assert_contains "Failed to get dependencies of canister $CANISTER_ID_A."

  cd ../onchain
  dfx canister stop a
  dfx canister delete a --no-withdrawal

  cd ../app
  assert_command_fail dfx deps pull --network local
  assert_contains "Failed to get dependencies of canister $CANISTER_ID_A."
}

@test "dfx deps pull can download wasm and candids to shared cache and generate pulled.json" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  PULLED_DIR="$DFX_CACHE_ROOT/.cache/dfinity/pulled/"
  assert_file_not_exists "$PULLED_DIR/$CANISTER_ID_B/canister.wasm.gz"
  assert_file_not_exists "$PULLED_DIR/$CANISTER_ID_A/canister.wasm"
  assert_file_not_exists "$PULLED_DIR/$CANISTER_ID_C/canister.wasm"
  assert_file_not_exists "$PULLED_DIR/$CANISTER_ID_B/service.did"
  assert_file_not_exists "$PULLED_DIR/$CANISTER_ID_A/service.did"
  assert_file_not_exists "$PULLED_DIR/$CANISTER_ID_C/service.did"

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain

  # pull canisters in app project
  cd app
  assert_file_not_exists "deps/pulled.json"

  assert_command dfx deps pull --network local
  assert_file_exists "$PULLED_DIR/$CANISTER_ID_B/canister.wasm.gz"
  assert_file_exists "$PULLED_DIR/$CANISTER_ID_A/canister.wasm"
  assert_file_exists "$PULLED_DIR/$CANISTER_ID_C/canister.wasm"
  assert_file_exists "$PULLED_DIR/$CANISTER_ID_B/service.did"
  assert_file_exists "$PULLED_DIR/$CANISTER_ID_A/service.did"
  assert_file_exists "$PULLED_DIR/$CANISTER_ID_C/service.did"

  cd deps
  assert_file_exists "pulled.json"
  assert_file_exists "candid/$CANISTER_ID_B.did"
  assert_file_exists "candid/$CANISTER_ID_C.did"
  assert_eq 5 "$(jq -r '.canisters | keys' pulled.json | wc -l | tr -d ' ')" # 3 canisters + 2 lines of '[' and ']'
  assert_command jq -r '.canisters."'"$CANISTER_ID_A"'".init_guide' pulled.json
  assert_eq "A natural number, e.g. 10." "$output"
  assert_command jq -r '.canisters."'"$CANISTER_ID_B"'".name' pulled.json
  assert_eq "dep_b" "$output"
  assert_command jq -r '.canisters."'"$CANISTER_ID_C"'".name' pulled.json
  assert_eq "dep_c" "$output"
  cd ../

  assert_command dfx deps pull --network local -vvv
  assert_contains "The canister wasm was found in the cache." # cache hit

  # hash mismatch is ok
  # the expected hash is written to pulled.json wasm_hash field
  # the hash of downloaded wasm is written to pulled.json wasm_hash_download field
  rm -r "${PULLED_DIR:?}/"
  cd ../onchain
  cp .dfx/local/canisters/c/c.wasm ../www/a.wasm # we will get wasm of canister_c when pulling canister_a
  WASM_HASH_A="$(sha256sum .dfx/local/canisters/a/a.wasm | cut -d " " -f 1)"
  WASM_HASH_DOWNLOAD_A="$(sha256sum .dfx/local/canisters/c/c.wasm | cut -d " " -f 1)"

  cd ../app
  assert_command dfx deps pull --network local
  assert_not_contains "WARN"
  assert_command jq -r '.canisters."'"$CANISTER_ID_A"'".wasm_hash' deps/pulled.json
  assert_match "$WASM_HASH_A" "$output"
  assert_command jq -r '.canisters."'"$CANISTER_ID_A"'".wasm_hash_download' deps/pulled.json
  assert_match "$WASM_HASH_DOWNLOAD_A" "$output"

  # sad path: url server doesn't have the file
  rm -r "${PULLED_DIR:?}/"
  rm ../www/a.wasm

  assert_command_fail dfx deps pull --network local
  assert_contains "Failed to pull canister $CANISTER_ID_A."
  assert_contains "Failed to download from url:"
}

@test "dfx deps pull works when wasm_hash or wasm_hash_url specified" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  install_asset deps

  # start a webserver to host wasm files
  mkdir www
  start_webserver --directory www

  cd onchain

  jq '.canisters.a.pullable.wasm_url="'"http://localhost:$E2E_WEB_SERVER_PORT/a.wasm"'"' dfx.json | sponge dfx.json
  jq '.canisters.b.pullable.wasm_url="'"http://localhost:$E2E_WEB_SERVER_PORT/b.wasm.gz"'"' dfx.json | sponge dfx.json
  jq '.canisters.c.pullable.wasm_url="'"http://localhost:$E2E_WEB_SERVER_PORT/c.wasm"'"' dfx.json | sponge dfx.json

  dfx canister create a --specified-id "$CANISTER_ID_A"
  dfx canister create b --specified-id "$CANISTER_ID_B"
  dfx canister create c --specified-id "$CANISTER_ID_C"
  dfx build

  # copy wasm files to web server dir
  cp .dfx/local/canisters/a/a.wasm ../www/a.wasm
  cp .dfx/local/canisters/b/b.wasm.gz ../www/b.wasm.gz
  cp .dfx/local/canisters/c/c.wasm ../www/c.wasm

  # A: set dfx:wasm_hash
  CUSTOM_HASH_A="$(sha256sum .dfx/local/canisters/a/a.wasm | cut -d " " -f 1)"
  jq '.canisters.a.pullable.wasm_hash="'"$CUSTOM_HASH_A"'"' dfx.json | sponge dfx.json
  # B: set dfx:wasm_hash_url with output of sha256sum
  echo -n "$(sha256sum .dfx/local/canisters/b/b.wasm.gz)" > ../www/b.wasm.gz.sha256
  jq '.canisters.b.pullable.wasm_hash_url="'"http://localhost:$E2E_WEB_SERVER_PORT/b.wasm.gz.sha256"'"' dfx.json | sponge dfx.json
  # C: set dfx:wasm_hash_url with the hash only
  CUSTOM_HASH_C="$(sha256sum .dfx/local/canisters/c/c.wasm | cut -d " " -f 1)"
  echo -n "$CUSTOM_HASH_C" > ../www/c.wasm.sha256
  jq '.canisters.c.pullable.wasm_hash_url="'"http://localhost:$E2E_WEB_SERVER_PORT/c.wasm.sha256"'"' dfx.json | sponge dfx.json

  dfx build

  dfx canister install a --argument 1
  dfx canister install b
  dfx canister install c

  # pull canisters in app project
  cd ../app
  assert_file_not_exists "deps/pulled.json"

  assert_command dfx deps pull --network local -vvv
  assert_contains "Canister $CANISTER_ID_A specified a custom hash:"
  assert_contains "Canister $CANISTER_ID_B specified a custom hash via url:"
  assert_contains "Canister $CANISTER_ID_C specified a custom hash via url:"
 
  # warning: specified both `wasm_hash` and `wasm_hash_url`. Providers should avoid this.
  PULLED_DIR="$DFX_CACHE_ROOT/.cache/dfinity/pulled/"
  rm -r "${PULLED_DIR:?}/"
  cd ../onchain
  jq '.canisters.c.pullable.wasm_hash="'"$CUSTOM_HASH_C"'"' dfx.json | sponge dfx.json
  dfx build
  dfx canister install c --mode=reinstall --yes

  cd ../app
  assert_command dfx deps pull --network local -vvv
  assert_contains "WARN: Canister $CANISTER_ID_C specified both \`wasm_hash\` and \`wasm_hash_url\`. \`wasm_hash\` will be used."

  # hash mismatch is ok
  rm -r "${PULLED_DIR:?}/"
  cd ../onchain
  cp .dfx/local/canisters/a/a.wasm ../www/a.wasm # now the webserver has the onchain version of canister_a which won't match wasm_hash

  cd ../app
  assert_command dfx deps pull --network local -vvv
  assert_contains "Canister $CANISTER_ID_A specified a custom hash:"
}

@test "dfx deps init works" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain

  # pull canisters in app project
  cd app
  assert_command dfx deps pull --network local

  # stop the "mainnet" replica
  dfx_stop

  assert_command dfx deps init
  assert_contains "Canister $CANISTER_ID_C (dep_c) set init argument with \"(null)\"."
  assert_contains "WARN: The following canister(s) require an init argument. Please run \`dfx deps init <NAME/PRINCIPAL>\` to set them individually:
$CANISTER_ID_A"

  assert_command dfx deps init "$CANISTER_ID_A" --argument 11

  # dep_c requires an init argument with top-level opt
  # without --argument, it will try to set "(null)"
  assert_command dfx deps init dep_c
  
  # overwrite the empty argument with a valid one
  assert_command dfx deps init dep_c --argument "(opt 33)"

  # can also set with --argument-file
  echo "(opt 44)" > arg.txt
  assert_command dfx deps init dep_c --argument-file arg.txt

  # The argument is the hex string of '("abc")' which doesn't type check
  # However, passing raw argument will bypass the type check so following command succeed
  assert_command dfx deps init "$CANISTER_ID_A" --argument "4449444c00017103616263" --argument-type raw
  assert_command jq -r '.canisters."'"$CANISTER_ID_A"'".arg_raw' deps/init.json
  assert_eq "4449444c00017103616263" "$output"

  # Canister A has been set, set again without --argument will prompt a info message
  assert_command dfx deps init "$CANISTER_ID_A"
  assert_contains "Canister $CANISTER_ID_A already set init argument."
  
  # error cases
  rm deps/init.json
  ## require init arguments but not provide
  assert_command_fail dfx deps init "$CANISTER_ID_A"
  assert_contains "Canister $CANISTER_ID_A requires an init argument. The following info might be helpful:
init_guide => A natural number, e.g. 10.
candid:args => (nat)"

  ## wrong type
  assert_command_fail dfx deps init "$CANISTER_ID_A" --argument '("abc")'
  assert_contains "Invalid data: Unable to serialize Candid values: type mismatch: \"abc\" cannot be of type nat"

  ## require no init argument but provide
  assert_command_fail dfx deps init dep_b --argument 1
  assert_contains "Canister $CANISTER_ID_B (dep_b) takes no init argument. Please rerun without \`--argument\`"

  ## canister ID not in pulled.json
  assert_command_fail dfx deps init aaaaa-aa
  assert_contains "Could not find aaaaa-aa in pulled.json"
}

@test "dfx deps init can handle init_arg in pullable metadata" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain
  cd onchain
  # Canister A: set init_arg in pullable metadata then redeploy and copy wasm file to web server dir
  jq '.canisters.a.pullable.init_arg="42"' dfx.json | sponge dfx.json
  dfx build a
  dfx canister install a --argument 1 --mode=reinstall --yes
  cp .dfx/local/canisters/a/a.wasm ../www/a.wasm

  # pull canisters in app project
  cd ../app
  assert_command dfx deps pull --network local

  # stop the "mainnet" replica
  dfx_stop

  assert_command dfx deps init
  assert_command jq -r '.canisters."'"$CANISTER_ID_A"'".arg_str' deps/init.json
  assert_match "42" "$output" # This matches the init_arg which was set above

  # Explicitly set with --argument can overwrite
  assert_command dfx deps init "$CANISTER_ID_A" --argument 37
  assert_command jq -r '.canisters."'"$CANISTER_ID_A"'".arg_str' deps/init.json
  assert_match "37" "$output"
}

@test "dfx deps init errors when init_arg in pullable metadata has wrong type" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain
  cd onchain
  # Canister A: set init_arg in pullable metadata then redeploy and copy wasm file to web server dir
  jq '.canisters.a.pullable.init_arg="(\"abc\")"' dfx.json | sponge dfx.json
  dfx build a
  dfx canister install a --argument 1 --mode=reinstall --yes
  cp .dfx/local/canisters/a/a.wasm ../www/a.wasm

  # pull canisters in app project
  cd ../app
  assert_command dfx deps pull --network local

  # stop the "mainnet" replica
  dfx_stop

  assert_command_fail dfx deps init "$CANISTER_ID_A"
  assert_contains "Pulled canister $CANISTER_ID_A provided an invalid \`init_arg\`.
Please try to set an init argument with \`--argument\` option.
The following info might be helpful:
init_guide => A natural number, e.g. 10.
candid:args => (nat)"

  # Consumer set correct init_arg
  assert_command dfx deps init "$CANISTER_ID_A" --argument 10
  assert_command jq -r '.canisters."'"$CANISTER_ID_A"'".arg_str' deps/init.json
  assert_match "10" "$output"
}

@test "dfx deps deploy works" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain

  # pull canisters in app project
  cd app
  assert_command dfx deps pull --network local

  # delete onchain canisters so that the replica has no canisters as a clean local replica
  cd ../
  cleanup_onchain

  cd app
  assert_command dfx deps init # b is set here
  assert_command dfx deps init "$CANISTER_ID_A" --argument 11
  assert_command dfx deps init "$CANISTER_ID_C" --argument "(opt 33)"

  # deploy all
  assert_command dfx deps deploy
  assert_contains "Creating canister: $CANISTER_ID_A
Installing canister: $CANISTER_ID_A"
  assert_contains "Creating canister: $CANISTER_ID_B (dep_b)
Installing canister: $CANISTER_ID_B (dep_b)"
  assert_contains "Creating canister: $CANISTER_ID_C (dep_c)
Installing canister: $CANISTER_ID_C (dep_c)"

  # by name in dfx.json
  assert_command dfx deps deploy dep_b
  assert_contains "Installing canister: $CANISTER_ID_B (dep_b)" # dep_p has been created before, so we only see "Installing ..." here

  # by canister id
  assert_command dfx deps deploy $CANISTER_ID_A
  assert_contains "Installing canister: $CANISTER_ID_A"

  # deployed pull dependencies can be stopped and deleted
  assert_command dfx canister stop dep_b --identity anonymous
  assert_command dfx canister delete dep_b --identity anonymous --no-withdrawal

  assert_command dfx canister stop $CANISTER_ID_A --identity anonymous
  assert_command dfx canister delete $CANISTER_ID_A --identity anonymous --no-withdrawal

  # error cases
  ## set wrong init argument
  assert_command dfx deps init "$CANISTER_ID_A" --argument "4449444c00017103616263" --argument-type raw
  assert_command_fail dfx deps deploy
  assert_contains "Failed to install canister $CANISTER_ID_A"

  ## canister ID not in pulled.json
  assert_command_fail dfx deps deploy aaaaa-aa
  assert_contains "Could not find aaaaa-aa in pulled.json"

  ## no init.json
  rm deps/init.json
  assert_command_fail dfx deps deploy
  assert_contains "Failed to read init.json. Please run \`dfx deps init\`."

  ## forgot to set init argument for some dependencies
  assert_command dfx deps init # b is set here
  assert_command_fail dfx deps deploy "$CANISTER_ID_A"
  assert_contains "Failed to create and install canister $CANISTER_ID_A"
  assert_contains "Failed to find $CANISTER_ID_A entry in init.json. Please run \`dfx deps init $CANISTER_ID_A\`."
}

@test "dfx deps init/deploy works when hash mismatch" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain
  cd onchain
  cp .dfx/local/canisters/c/c.wasm ../www/a.wasm # we will get wasm of canister_c when pulling canister_a

  # pull canisters in app project
  cd ../app
  assert_command dfx deps pull --network local

  # delete onchain canisters so that the replica has no canisters as a clean local replica
  cd ../
  cleanup_onchain

  cd app
  assert_command dfx deps init # b is set here
  assert_command dfx deps init "$CANISTER_ID_A" --argument "(opt 11)" # the downloaded wasm need argument type as canister_c
  assert_command dfx deps init "$CANISTER_ID_C" --argument "(opt 33)"  

  # deploy all
  assert_command dfx deps deploy
}

@test "dfx deps init/deploy abort when pulled.json and cache are invalid" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain

  # pull canisters in app project
  cd app
  assert_command dfx deps pull --network local
  cp deps/pulled.json deps/pulled.json.bak

  # 1. `pulled.json` is not consistent with `dfx.json`

  ## 1.1. missing pull dependency in pulled.json
  jq 'del(.canisters."'"$CANISTER_ID_B"'")' deps/pulled.json.bak > deps/pulled.json
  assert_command_fail dfx deps init
  assert_contains "Failed to find dep_b:$CANISTER_ID_B in pulled.json."
  assert_contains "Please rerun \`dfx deps pull\`."
  assert_command_fail dfx deps deploy
  assert_contains "Failed to find dep_b:$CANISTER_ID_B in pulled.json."
  assert_contains "Please rerun \`dfx deps pull\`."

  ## 1.2. name mismatch in pulled.json and dfx.json
  jq '.canisters."'"$CANISTER_ID_B"'".name="not_dep_b"' deps/pulled.json.bak > deps/pulled.json
  assert_command_fail dfx deps init
  assert_contains "$CANISTER_ID_B is \"dep_b\" in dfx.json, but it has name \"not_dep_b\" in pulled.json."
  assert_command_fail dfx deps deploy
  assert_contains "$CANISTER_ID_B is \"dep_b\" in dfx.json, but it has name \"not_dep_b\" in pulled.json."

  ## 1.3. no name in pulled.json
  jq 'del(.canisters."'"$CANISTER_ID_B"'".name)' deps/pulled.json.bak > deps/pulled.json
  assert_command_fail dfx deps init
  assert_contains "$CANISTER_ID_B is \"dep_b\" in dfx.json, but it doesn't have name in pulled.json."
  assert_command_fail dfx deps deploy
  assert_contains "$CANISTER_ID_B is \"dep_b\" in dfx.json, but it doesn't have name in pulled.json."

  cp deps/pulled.json.bak deps/pulled.json

  # 2. the wasm modules in pulled cache are not consistent with `pulled.json`

  ## 2.1. missing wasm in cache
  WASM_PATH_A="$DFX_CACHE_ROOT/.cache/dfinity/pulled/$CANISTER_ID_A/canister.wasm"
  mv "$WASM_PATH_A" "$WASM_PATH_A.bak"
  assert_command_fail dfx deps init
  assert_contains "failed to read from $WASM_PATH_A"
  assert_command_fail dfx deps deploy
  assert_contains "failed to read from $WASM_PATH_A"
  mv "$WASM_PATH_A.bak" "$WASM_PATH_A"

  ## 2.2. wasm_hash_download is not valid hex string
  jq '.canisters."'"$CANISTER_ID_B"'".wasm_hash_download="xyz"' deps/pulled.json.bak > deps/pulled.json
  assert_command_fail dfx deps init
  assert_contains "In pulled.json, the \`wasm_hash_download\` field of $CANISTER_ID_B is invalid."
  assert_command_fail dfx deps deploy
  assert_contains "In pulled.json, the \`wasm_hash_download\` field of $CANISTER_ID_B is invalid."

  ## 2.3. hash mismatch
  jq '.canisters."'"$CANISTER_ID_A"'".wasm_hash_download="0123456789abcdef"' deps/pulled.json.bak > deps/pulled.json
  assert_command_fail dfx deps init
  assert_contains "The wasm of $CANISTER_ID_A in pulled cache has different hash than in pulled.json:"
  assert_contains "The pulled cache is at \"$WASM_PATH_A\". Its hash is:"
  assert_contains "The hash (wasm_hash_download) in pulled.json is:"
  assert_contains "The pulled cache may be modified manually or the same canister was pulled in different projects."
  assert_command_fail dfx deps deploy
  assert_contains "The wasm of $CANISTER_ID_A in pulled cache has different hash than in pulled.json:"
  assert_contains "The pulled cache is at \"$WASM_PATH_A\". Its hash is:"
  assert_contains "The hash (wasm_hash_download) in pulled.json is:"
  assert_contains "The pulled cache may be modified manually or the same canister was pulled in different projects."
}

@test "dfx deps pulled dependencies work with app canister" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain

  # pull canisters in app project
  cd app
  assert_command dfx deps pull --network local

  # delete onchain canisters so that the replica has no canisters as a clean local replica
  cd ../
  cleanup_onchain

  cd app
  assert_command_fail dfx canister create dep_b
  assert_contains "dep_b is a pull dependency. Please deploy it using \`dfx deps deploy dep_b\`"
  assert_command dfx canister create app

  assert_command dfx canister create --all
  assert_contains "There are pull dependencies defined in dfx.json. Please deploy them using \`dfx deps deploy\`."

  assert_command dfx build app
  assert_command dfx canister install app

  # pulled dependency dep_b hasn't been deployed on local replica
  assert_command_fail dfx canister call app get_b
  assert_contains "Canister $CANISTER_ID_B not found"

  assert_command dfx deps init
  assert_command dfx deps init "$CANISTER_ID_A" --argument 11
  assert_command dfx deps init "$CANISTER_ID_C" --argument "(opt 33)"
  assert_command dfx deps deploy

  assert_command dfx canister call app get_b
  assert_eq "(2 : nat)" "$output"
  assert_command dfx canister call app get_c
  assert_eq "(33 : nat)" "$output" # corresponding to --argument "(opt 33)" above
  assert_command dfx canister call app get_b_times_a
  assert_eq "(22 : nat)" "$output" # 2 * 11
  assert_command dfx canister call app get_c_times_a
  assert_eq "(363 : nat)" "$output" # 33 * 11

  # start a clean local replica
  dfx canister stop app
  dfx canister delete app --no-withdrawal
  assert_command dfx deploy # only deploy app canister
}

@test "dfx deps does nothing in a project has no pull dependencies" {
  dfx_new empty

  # verify the help message
  assert_command dfx deps pull -h
  assert_contains "Pull canisters upon which the project depends. This command connects to the \"ic\" mainnet by default."
  assert_contains "You can still choose other network by setting \`--network\`"

  assert_command dfx deps pull
  assert_contains "There are no pull dependencies defined in dfx.json"
  assert_command dfx deps init
  assert_contains "There are no pull dependencies defined in dfx.json"
  assert_command dfx deps deploy
  assert_contains "There are no pull dependencies defined in dfx.json"

  assert_directory_not_exists "deps"
}

@test "dfx deps pull can set correct pulled.json when the dependency is already in cache" {
  use_test_specific_cache_root # dfx deps pull will download files to cache

  # start a "mainnet" replica which host the onchain canisters
  dfx_start

  setup_onchain

  # pull canisters in app project and the dependencies are cached
  cd app
  assert_command dfx deps pull --network local

  # the second pull should be able to set the correct pulled.json
  assert_command dfx deps pull --network local

  # this command will fail if the pulled.json is not correct
  assert_command dfx deps init
}


-----------------------

/e2e/tests-dfx/describe_local_network.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx start with disabled canister http" {
  create_networks_json
  echo "{}" | jq '.local.canister_http.enabled=false' >"$E2E_NETWORKS_JSON"
  assert_command dfx start --host 127.0.0.1:0 --background --verbose

  assert_match "canister http: disabled \(default: enabled\)"
}

@test "dfx start with a nonstandard subnet type" {
  create_networks_json
  echo "{}" | jq '.local.replica.subnet_type="verifiedapplication"' >"$E2E_NETWORKS_JSON"

  assert_command dfx start --host 127.0.0.1:0 --background --verbose

  assert_match "subnet type: VerifiedApplication \(default: Application\)"
}

@test "dfx start with nonstandard bitcoin node" {
  assert_command dfx start --host 127.0.0.1:0 --background --bitcoin-node 192.168.0.1:18000 --verbose

  assert_match "bitcoin: enabled \(default: disabled\)"
  assert_match "nodes: \[192.168.0.1:18000\] \(default: \[127.0.0.1:18444\]\)"
}

@test "dfx start enabling bitcoin" {
  assert_command dfx start --host 127.0.0.1:0 --background --enable-bitcoin --verbose

  assert_match "bitcoin: enabled \(default: disabled\)"
}

@test "dfx start in a project without a network definition" {
  mkdir some-project
  cd some-project
  echo "{}" >dfx.json

  # we have to pass 0 for port to avoid conflicts
  assert_command dfx start --host 127.0.0.1:0 --background --verbose

  assert_match "There is no project-specific network 'local' defined in .*/some-project/dfx.json."
  assert_match "Using the default configuration for the local shared network"

  assert_match "Local server configuration:"
  assert_match "bind address: 127.0.0.1:0 \(default: 127.0.0.1:4943\)"
  assert_match "bitcoin: disabled"
  assert_match "canister http: enabled"
  assert_match "subnet type: Application"
  assert_match "scope: shared"
}

@test "dfx start outside of a project with default configuration" {
  assert_command dfx start --host 127.0.0.1:0 --background --verbose

  assert_match "There is no project-specific network 'local' because there is no project \(no dfx.json\)."
  assert_match "Using the default configuration for the local shared network"
}

@test "dfx start outside of a project with a shared configuration file" {
  create_networks_json

  assert_command dfx start --background --verbose

  assert_match "There is no project-specific network 'local' because there is no project \(no dfx.json\)."
  assert_match "Using the default configuration for the local shared network"
}


@test "dfx start outside of a project with a shared configuration file that defines the local network" {
  create_networks_json
  echo "{}" | jq '.local.bind="127.0.0.1:0"' >"$E2E_NETWORKS_JSON"

  assert_command dfx start --background --verbose

  assert_match "There is no project-specific network 'local' because there is no project \(no dfx.json\)."
  assert_match "Using shared network 'local' defined in $DFX_CONFIG_ROOT/.config/dfx/networks.json"
}

@test "dfx start describes default project-specific network" {
  # almost default: use a dynamic port
  echo "{}" | jq '.networks.local.bind="127.0.0.1:0"' > dfx.json

  assert_command dfx start --background --verbose

  assert_match "Local server configuration:"
  assert_match "bind address: 127.0.0.1:0 \(default: 127.0.0.1:8000\)"
  assert_match "bitcoin: disabled"
  assert_match "canister http: enabled"
  assert_match "subnet type: Application"
  assert_match "data directory: .*/working-dir/.dfx/network/local"
  assert_match "scope: project"
}

@test "dfx start describes default shared network" {
  # almost default: use a dynamic port
  create_networks_json
  echo "{}" | jq '.local.bind="127.0.0.1:0"' >"$E2E_NETWORKS_JSON"

  assert_command dfx start --background --verbose

  assert_match "Local server configuration:"
  assert_match "bind address: 127.0.0.1:0 \(default: 127.0.0.1:4943\)"
  assert_match "bitcoin: disabled"
  assert_match "canister http: enabled"
  assert_match "subnet type: Application"

  if [ "$(uname)" == "Darwin" ]; then
    assert_match "data directory: .*/home-dir/Library/Application Support/org.dfinity.dfx/network/local"
  elif [ "$(uname)" == "Linux" ]; then
    assert_match "data directory: .*/home-dir/.local/share/dfx/network/local"
  fi

  assert_match "scope: shared"
}

-----------------------

/e2e/tests-dfx/dfx_install.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx cache show does not install the dfx version into the cache" {
  use_test_specific_cache_root
  test -z "$(ls -A "$DFX_CACHE_ROOT")"

  assert_command dfx cache show

  # does not populate the cache with this version
  test ! -e "$(dfx cache show)"

  # it does create the empty versions directory though
  test -d "$DFX_CACHE_ROOT/.cache/dfinity/versions"
  test -z "$(ls -A "$DFX_CACHE_ROOT/.cache/dfinity/versions"``)"
}

@test "non-forced install populates an empty cache" {
  use_test_specific_cache_root
  test ! -e "$(dfx cache show)"/dfx

  dfx_new

  test -f "$(dfx cache show)"/dfx
}

@test "forced install populates an empty cache" {
  use_test_specific_cache_root

  test ! -e "$(dfx cache show)"/dfx

  assert_command dfx cache install

  test -f "$(dfx cache show)"/dfx
}

@test "forced install over an install succeeds" {
  assert_command dfx cache install
  test -f "$(dfx cache show)"/dfx

  assert_command dfx cache install
}

@test "Motoko base library files are not executable" {
  assert_command dfx cache install
  for file in "$(dfx cache show)"/base/*.mo; do
    assert_command_fail test -x "$file"
    assert_command_fail "$file"
    assert_contains "Permission denied"
  done
}


@test "forced install overwrites a cached version" {
  assert_command dfx cache install
  test -f "$(dfx cache show)"/dfx

  # add something extra to the cache
  echo "garbage" >"$(dfx cache show)"/garbage
  test -f "$(dfx cache show)"/garbage

  assert_command dfx cache install

  # dfx cache install should have removed it
  test ! -e "$(dfx cache show)"/garbage

  # and also installed the cache itself
  test -f "$(dfx cache show)"/dfx
}


-----------------------

/e2e/tests-dfx/dotenv.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_assets
}

teardown() {
  dfx_stop

  standard_teardown
}


@test "puts .env in project root" {
  dfx_start
  jq '.canisters["e2e_project_backend"].post_install="echo post install backend"' dfx.json | sponge dfx.json
  jq '.canisters["e2e_project_frontend"].post_install="echo post install frontend"' dfx.json | sponge dfx.json

  mkdir subdir
  mkdir subdir/canister-install-all subdir/canister-install-single
  mkdir subdir/build-all subdir/build-single
  mkdir subdir/deploy-single subdir/deploy-all
  dfx canister create --all
  ( cd subdir/build-single && dfx build e2e_project_frontend )
  ( cd subdir/build-all && dfx build --all )
  ( cd subdir/canister-install-single && dfx canister install e2e_project_backend )
  dfx canister uninstall-code e2e_project_backend
  ( cd subdir/canister-install-all && dfx canister install --all )
  rm -rf .dfx
  ( cd subdir/deploy-single && dfx deploy e2e_project_backend)
  ( cd subdir/deploy-all && dfx deploy )

  assert_command find . -name .env
  assert_eq "./.env"
}

@test "the output_env_file must be contained within project" {
  dfx_start
  mkdir ../outside

  assert_command_fail dfx deploy --output-env-file nonexistent/.env
  assert_contains "failed to canonicalize output_env_file"
  assert_contains "working-dir/e2e_project/nonexistent"
  assert_contains "No such file or directory"
  assert_command_fail dfx deploy --output-env-file /etc/passwd
  assert_contains "The output_env_file must be a relative path, but is /etc/passwd"
  assert_command_fail dfx deploy --output-env-file ../outside/.env
  assert_match "The output_env_file must be within the project root, but is .*/working-dir/e2e_project/../outside/.env"
}

@test "build writes all environment variables to .env" {
  dfx_start
  dfx canister create --all

  assert_dotenv_contains_all_variables_after_command build
}

@test "deploy writes all environment variables to .env" {
  dfx_start
  dfx canister create --all

  assert_dotenv_contains_all_variables_after_command deploy
}

@test "canister install writes all environment variables to .env" {
  dfx_start
  dfx canister create --all
  dfx build e2e_project_frontend
  # set a post-install script so the install will create a .env file
  jq '.canisters.e2e_project_frontend.post_install="echo post install"' dfx.json | sponge dfx.json

  assert_dotenv_contains_all_variables_after_command canister install
}

@test "writing environment variables does not require an asset canister" {
  dfx_new
  dfx_start
  dfx canister create e2e_project_backend
  dfx build e2e_project_backend
  env=$(< .env)
  backend_canister=$(dfx canister id e2e_project_backend)
  assert_contains "DFX_NETWORK='local'" "$env"
  assert_contains "CANISTER_ID_E2E_PROJECT_BACKEND='$backend_canister'" "$env"
}

assert_dotenv_contains_all_variables_after_command() {
  install_asset wasm/identity
  jq '.canisters."nns-cycles-minting".remote.id.local="rkp4c-7iaaa-aaaaa-aaaca-cai"' dfx.json | sponge dfx.json
  jq '.canisters."nns-cycles-minting".type="custom"' dfx.json | sponge dfx.json
  jq '.canisters."nns-cycles-minting".candid="main.did"' dfx.json | sponge dfx.json
  jq '.canisters."nns-cycles-minting".wasm="main.wasm"' dfx.json | sponge dfx.json
  jq '.canisters.lifeline.type="pull"' dfx.json | sponge dfx.json
  jq '.canisters.lifeline.id="rno2w-sqaaa-aaaaa-aaacq-cai"' dfx.json | sponge dfx.json
  mkdir -p deps/candid
  echo "service: {}" >deps/candid/rno2w-sqaaa-aaaaa-aaacq-cai.did

  # .env should also include canisters that are not explicit dependencies
  jq 'del(.canisters.e2e_project_frontend.dependencies)' dfx.json  | sponge dfx.json
  backend_canister=$(dfx canister id e2e_project_backend)
  frontend_canister=$(dfx canister id e2e_project_frontend)

  rm .env || true
  assert_command dfx "$@" e2e_project_frontend

  assert_file_exists .env
  env=$(< .env)
  assert_contains "DFX_NETWORK='local'" "$env"
  assert_contains "CANISTER_ID_E2E_PROJECT_BACKEND='$backend_canister'" "$env"
  assert_contains "CANISTER_ID_E2E_PROJECT_FRONTEND='$frontend_canister'" "$env"
  assert_contains "CANISTER_ID_NNS_CYCLES_MINTING='rkp4c-7iaaa-aaaaa-aaaca-cai'" "$env"
  assert_contains "CANISTER_ID_LIFELINE='rno2w-sqaaa-aaaaa-aaacq-cai'" "$env"

  setup_actuallylocal_project_network
  dfx canister create --all --network actuallylocal
  assert_command dfx build --network actuallylocal
  assert_contains "DFX_NETWORK='actuallylocal'" "$(< .env)"
}

@test "writes environment variables to selected file" {
  dfx_start
  dfx canister create --all

  assert_command dfx build --output-env-file flag.env
  assert_file_exists flag.env
  assert_contains "DFX_NETWORK='local'" "$(< flag.env)"

  jq '.output_env_file="json.env"' dfx.json | sponge dfx.json
  assert_command dfx build
  assert_file_exists json.env
  assert_contains "DFX_NETWORK='local'" "$(< json.env)"

  jq 'del(.output_env_file)' dfx.json | sponge dfx.json
  assert_command dfx build
  assert_file_not_exists .env
}

@test "does not clobber existing .env content" {
  dfx_start
  dfx canister create --all
  echo 'foo=bar' > .env

  assert_command dfx build
  assert_file_exists .env
  env=$(< .env)
  assert_contains "DFX_NETWORK='local'" "$env"
  assert_contains "foo=bar" "$env"

  echo 'baz=quux' >> .env
  assert_command dfx build
  env=$(< .env)
  assert_contains "DFX_NETWORK='local'" "$env"
  assert_contains "foo=bar" "$env"
  assert_contains "baz=quux" "$env"

  # deliberately corrupt the file
  head -n 3 .env | sponge .env
  echo 'baz=quux' >> .env
  assert_command dfx build
  env=$(< .env)
  assert_contains "# END DFX CANISTER ENVIRONMENT VARIABLES" "$env"
  assert_contains "DFX_NETWORK='local'" "$env"
  assert_contains "foo=bar" "$env"
  assert_contains "baz=quux" "$env"
}


-----------------------

/e2e/tests-dfx/error_context.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
  install_asset error_context
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "problems reading wallets.json" {
  dfx_start

  assert_command dfx identity get-wallet

  echo "invalid json" >"$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"

  assert_command_fail dfx identity get-wallet
  assert_match "failed to parse contents of .*/network/local/wallets.json as json"
  assert_match "expected value at line 1 column 1"

  assert_command_fail dfx wallet upgrade
  assert_match "failed to parse contents of .*/network/local/wallets.json as json"
  assert_match "expected value at line 1 column 1"

  echo '{ "identities": {} }' >"$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"

  # maybe you were sudo when you made it
  chmod u=w,go= "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"
  assert_command_fail dfx identity get-wallet
  assert_match "failed to read .*/network/local/wallets.json"
  assert_match "Permission denied"

  assert_command_fail dfx wallet upgrade
  assert_match "failed to read .*/network/local/wallets.json"
  assert_match "Permission denied"

  # can't write it?
  chmod u=r,go= "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command_fail dfx identity get-wallet --identity alice
  assert_match "failed to write to .*/local/wallets.json"
  assert_match "Permission denied"
}

@test "address already in use" {
  dfx_start

  port=$(get_webserver_port)
  address="127.0.0.1:$port"

  # fool dfx start into thinking dfx isn't running
  mv "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/pid" "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/hidden_pid"

  assert_command_fail dfx start  --host "$address"

  # What was the purpose of the address
  assert_match "frontend address"

  # What was the address we were looking for
  assert_match "$address"

  # The underlying cause
  assert_match "Address already in use"

  # Allow dfx stop to stop dfx in teardown.  Otherwise, bats will never exit
  mv "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/hidden_pid" "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/pid"
}

@test "corrupt dfx.json" {
  echo "corrupt" >dfx.json
  assert_command_fail dfx deploy

  # The bare minimum is to mention the file
  assert_match "dfx.json"

  # It's nice to mention the full path to the file
  assert_match "$(pwd)/dfx.json"

  # The underlying cause
  assert_match "expected value at line 1 column 1"
}

@test "packtool missing" {
  dfx_start

  assert_command dfx canister create packtool_missing

  jq '.defaults.build.packtool="not-a-valid-packtool and some parameters"' dfx.json | sponge dfx.json


  assert_command_fail dfx build packtool_missing

  # expect to see the name of the packtool and the parameters
  assert_match '"not-a-valid-packtool" "and" "some" "parameters"'
  # expect to see the underlying cause
  assert_match "No such file or directory"
}

@test "moc missing" {
  use_test_specific_cache_root   # Because this test modifies a file in the cache

  dfx_start

  assert_command dfx canister create m_o_c_missing

  rm -f "$(dfx cache show)/moc"
  assert_command_fail dfx build m_o_c_missing

  # expect to see the name of the binary
  assert_match "moc"

  # expect to see the full path of the binary
  assert_contains "$(dfx cache show)/moc"

  # expect to see the underlying cause
  assert_match "No such file or directory"
}

@test "npm is not installed" {
  dfx_start

  assert_command dfx canister create npm_missing

  # this is how dfx decides to run `npm run build'
  touch package.json

  dfx_path="$(which dfx)"
  # commands needed by assert_command_fail:
  helpers_path="$(which mktemp rm echo | xargs -n 1 dirname | sort | uniq | tr '\n' ':')"
  PATH="$helpers_path" assert_command_fail "$dfx_path" deploy npm_missing

  # expect to see the npm command line
  assert_contains 'program: "npm"'
  assert_match 'args: \[.*"npm".*"run".*"build".*\]'
  # expect to see the name of the canister
  assert_match "npm_missing"
  # expect to see the underlying cause
  assert_match "No such file or directory"
}

@test "missing asset source directory" {
  dfx_start

  assert_command dfx canister create asset_bad_source_path

  assert_command_fail dfx deploy asset_bad_source_path

  # expect to see the bad path
  assert_match "src/does/not/exist"
  # expect to see the name of the canister
  assert_match "asset_bad_source_path"
  # expect to see the underlying cause
  assert_match "No such file or directory"
}

@test "custom bad build step" {
  dfx_start

  assert_command dfx canister create custom_bad_build_step

  assert_command_fail dfx build custom_bad_build_step

  # expect to see what it tried to call
  assert_match "not-the-name-of-an-executable-that-exists"
  # expect to see the name of the canister
  assert_match "custom_bad_build_step"
  # expect to see the underlying cause
  assert_match "The custom tool failed"
}

@test "invalid optimization level" {
  jq '.canisters.bad_optimization_level.optimize="bad_level"' dfx.json | sponge dfx.json
  assert_command_fail dfx_start
  assert_match "expected one of "
}

@test "HTTP 403 has a full diagnosis" {
  dfx_new hello
  install_asset greet
  dfx_start
  assert_command dfx deploy

  # make sure normal status command works
  assert_command dfx canister status hello_backend

  # create a non-controller ID
  assert_command dfx identity new alice --storage-mode plaintext
  assert_command dfx identity use alice

  # calling canister status with different identity provokes HTTP 403
  assert_command_fail dfx canister status hello_backend
  assert_match "not part of the controllers" # this is part of the error explanation
  assert_match "'dfx canister update-settings --add-controller <controller principal to add> <canister id/name or --all> \(--network ic\)'" # this is part of the solution
}

@test "bad wallet canisters get diagnosed" {
  dfx_new hello
  dfx_start
  dfx deploy hello_backend --no-wallet
  id=$(dfx canister id hello_backend)
  dfx identity set-wallet "$id" --force
  assert_command_fail dfx wallet balance
  assert_contains "it did not contain a function that dfx was looking for"
  assert_contains "dfx identity set-wallet <PRINCIPAL> --identity <IDENTITY>"
}


-----------------------

/e2e/tests-dfx/extension.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
  use_test_specific_cache_root
}

teardown() {
  stop_webserver
  dfx_stop

  standard_teardown
}

@test "extension canister type" {
  dfx_start

  install_asset wasm/identity
  CACHE_DIR=$(dfx cache show)
  mkdir -p "$CACHE_DIR"/extensions/embera
  cat > "$CACHE_DIR"/extensions/embera/extension.json <<EOF
  {
      "name": "embera",
      "version": "0.1.0",
      "homepage": "https://github.com/dfinity/dfx-extensions",
      "authors": "DFINITY",
      "summary": "Test extension for e2e purposes.",
      "categories": [],
      "keywords": [],
      "canister_type": {
       "evaluation_order": [ "wasm" ],
       "defaults": {
         "type": "custom",
         "build": [
           "echo the embera build step for canister {{canister_name}} with candid {{canister.candid}} and main file {{canister.main}} and gzip is {{canister.gzip}}",
           "mkdir -p .embera/{{canister_name}}",
           "cp main.wasm {{canister.wasm}}"
         ],
         "gzip": true,
         "wasm": ".embera/{{canister_name}}/{{canister_name}}.wasm",
         "post_install": [
           "echo the embera post-install step for canister {{canister_name}} with candid {{canister.candid}} and main file {{canister.main}} and gzip is {{canister.gzip}}"
         ],
         "metadata": [
           {
             "name": "metadata-from-extension",
             "content": "the content (from extension definition), gzip is {{canister.gzip}}"
           },
           {
             "name": "metadata-in-extension-overwritten-by-dfx-json",
             "content": "the content to be overwritten (from extension definition)"
           },
           {
             "name": "extension-limits-to-local-network",
             "networks": [ "local" ],
             "content": "this only applies to the local network"
           },
           {
             "name": "extension-limits-to-ic-network",
             "networks": [ "ic" ],
             "content": "this only applies to the ic network"
           }
         ],
         "tech_stack": {
           "cdk": {
             "embera": {
               "version": "1.5.2"
             },
             "ic-cdk": {
               "version": "2.14.4"
             }
           },
           "language": {
             "kotlin": {}
           }
         }
       }
      }
  }
EOF
  cat > dfx.json <<EOF
  {
    "canisters": {
      "c1": {
        "type": "embera",
        "candid": "main.did",
        "main": "main-file.embera",
        "metadata": [
          {
            "name": "metadata-in-extension-overwritten-by-dfx-json",
            "content": "the overwritten content (from dfx.json)"
          }
        ]
      },
      "c2": {
        "type": "embera",
        "candid": "main.did",
        "gzip": false,
        "main": "main-file.embera"
      },
      "c3": {
        "type": "embera",
        "candid": "main.did",
        "main": "main-file.embera",
        "metadata": [
          {
            "name": "extension-limits-to-local-network",
            "networks": [ "ic" ],
            "content": "this only applies to the ic network"
          },
          {
            "name": "extension-limits-to-ic-network",
            "networks": [ "local" ],
            "content": "dfx.json configuration applies only to local network"
          }
        ]
      },
      "c4": {
        "type": "embera",
        "candid": "main.did",
        "main": "main-file.embera",
        "tech_stack": {
          "cdk": {
            "ic-cdk": {
              "version": "1.16.6"
            }
          },
          "language": {
            "java": {
              "version": "25.0.6"
            }
          }
        }
      }
    }
  }
EOF

  assert_command dfx extension list
  assert_contains embera
  assert_command dfx deploy
  assert_contains "the embera build step for canister c1 with candid main.did and main file main-file.embera and gzip is true"
  assert_contains "the embera post-install step for canister c1 with candid main.did and main file main-file.embera and gzip is true"
  assert_contains "the embera build step for canister c2 with candid main.did and main file main-file.embera and gzip is false"
  assert_contains "the embera post-install step for canister c2 with candid main.did and main file main-file.embera and gzip is false"

  assert_command dfx canister metadata c1 metadata-in-extension-overwritten-by-dfx-json
  assert_eq "the overwritten content (from dfx.json)"
  assert_command dfx canister metadata c1 metadata-from-extension
  assert_eq "the content (from extension definition), gzip is true"

  assert_command dfx canister metadata c2 metadata-in-extension-overwritten-by-dfx-json
  assert_eq "the content to be overwritten (from extension definition)"
  assert_command dfx canister metadata c2 metadata-from-extension
  assert_eq "the content (from extension definition), gzip is false"

  assert_command dfx canister metadata c3 extension-limits-to-local-network
  assert_eq "this only applies to the local network"

  assert_command dfx canister metadata c3 extension-limits-to-ic-network
  assert_eq "dfx.json configuration applies only to local network"

  assert_command dfx canister metadata c4 dfx
  # shellcheck disable=SC2154
  echo "$stdout" > f.json
  assert_command jq -r '.tech_stack.cdk | keys | sort | join(",")' f.json
  assert_eq "embera,ic-cdk"
  assert_command jq -r '.tech_stack.cdk."ic-cdk".version' f.json
  assert_eq "1.16.6"
  assert_command jq -r '.tech_stack.language | keys | sort | join(",")' f.json
  assert_eq "java,kotlin"
}

@test "extension install with an empty cache does not create a corrupt cache" {
  dfx cache delete
  dfx extension install nns --version 0.2.1
  dfx_start
}

install_extension_from_dfx_extensions_repo() {
  EXTENSION=$1

  assert_command_fail dfx snsx

  assert_command dfx extension list
  assert_match 'No extensions installed'

  assert_command dfx extension install "$EXTENSION" --install-as snsx --version 0.2.1
  assert_contains "Extension 'sns' version 0.2.1 installed successfully, and is available as 'snsx'"

  assert_command dfx extension list
  assert_match 'snsx'

  assert_command dfx --help
  assert_match 'snsx.*Toolkit for'

  assert_command dfx snsx --help

  assert_command dfx extension uninstall snsx
  # TODO: how to capture spinner message?
  # assert_match 'Successfully uninstalled extension'

  assert_command dfx extension list
  assert_match 'No extensions installed'
}

@test "install extension by name from official catalog" {
  install_extension_from_dfx_extensions_repo sns
}

@test "install hosted extension by url" {
  install_extension_from_dfx_extensions_repo https://raw.githubusercontent.com/dfinity/dfx-extensions/main/extensions/sns/extension.json
}

get_extension_architecture() {
  _cputype="$(uname -m)"
  case "$_cputype" in

    x86_64 | x86-64 | x64 | amd64)
      _cputype=x86_64
      ;;

    arm64 | aarch64)
      _cputype=aarch64
      ;;

    *)
      err "unknown CPU type: $_cputype"
      ;;

  esac
  echo "$_cputype"
}

@test "install extension from catalog" {
  start_webserver --directory www

  CATALOG_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-1/catalog.json"
  mkdir -p www/arbitrary-1
  cat > www/arbitrary-1/catalog.json <<EOF
{
  "foo": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/foo/extension.json",
  "bar": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/bar/extension.json"
}
EOF


  EXTENSION_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/foo/extension.json"
  mkdir -p  www/arbitrary-2/foo/downloads

  cat > www/arbitrary-2/foo/extension.json <<EOF
{
  "name": "foo",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "download_url_template": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/downloads/{{tag}}/{{basename}}.{{archive-format}}"
}
EOF
  cat www/arbitrary-2/foo/extension.json

  cat > www/arbitrary-2/foo/dependencies.json <<EOF
{
  "0.1.0": {
    "dfx": {
      "version": ">=0.8.0"
    }
  }
}
EOF

  arch=$(get_extension_architecture)

  if [ "$(uname)" == "Darwin" ]; then
    ARCHIVE_BASENAME="foo-$arch-apple-darwin"
  else
    ARCHIVE_BASENAME="foo-$arch-unknown-linux-gnu"
  fi

  mkdir "$ARCHIVE_BASENAME"
  cp www/arbitrary-2/foo/extension.json "$ARCHIVE_BASENAME"
  tar -czf "$ARCHIVE_BASENAME".tar.gz "$ARCHIVE_BASENAME"
  rm -rf "$ARCHIVE_BASENAME"

  mkdir -p www/arbitrary/downloads/foo-v0.1.0
  mv "$ARCHIVE_BASENAME".tar.gz www/arbitrary/downloads/foo-v0.1.0/

  assert_command dfx extension install foo --catalog-url "$CATALOG_URL"
}

@test "install extension with no subcommands" {
  start_webserver --directory www

  CATALOG_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-1/catalog.json"
  mkdir -p www/arbitrary-1
  cat > www/arbitrary-1/catalog.json <<EOF
{
  "foo": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/foo/extension.json",
  "bar": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/bar/extension.json"
}
EOF


  EXTENSION_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/foo/extension.json"
  mkdir -p  www/arbitrary-2/foo/downloads

  cat > www/arbitrary-2/foo/extension.json <<EOF
{
  "name": "foo",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "download_url_template": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/downloads/{{tag}}/{{basename}}.{{archive-format}}"
}
EOF
  cat www/arbitrary-2/foo/extension.json

  cat > www/arbitrary-2/foo/dependencies.json <<EOF
{
  "0.1.0": {
    "dfx": {
      "version": ">=0.8.0"
    }
  }
}
EOF

  arch=$(get_extension_architecture)

  if [ "$(uname)" == "Darwin" ]; then
    ARCHIVE_BASENAME="foo-$arch-apple-darwin"
  else
    ARCHIVE_BASENAME="foo-$arch-unknown-linux-gnu"
  fi

  mkdir "$ARCHIVE_BASENAME"
  cp www/arbitrary-2/foo/extension.json "$ARCHIVE_BASENAME"
  tar -czf "$ARCHIVE_BASENAME".tar.gz "$ARCHIVE_BASENAME"
  rm -rf "$ARCHIVE_BASENAME"

  mkdir -p www/arbitrary/downloads/foo-v0.1.0
  mv "$ARCHIVE_BASENAME".tar.gz www/arbitrary/downloads/foo-v0.1.0/

  assert_command dfx extension install foo --catalog-url "$CATALOG_URL"
}

@test "install extension with empty subcommands" {
  start_webserver --directory www

  CATALOG_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-1/catalog.json"
  mkdir -p www/arbitrary-1
  cat > www/arbitrary-1/catalog.json <<EOF
{
  "foo": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/foo/extension.json",
  "bar": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/bar/extension.json"
}
EOF


  EXTENSION_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary-2/foo/extension.json"
  mkdir -p  www/arbitrary-2/foo/downloads

  cat > www/arbitrary-2/foo/extension.json <<EOF
{
  "name": "foo",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "subcommands": {},
  "download_url_template": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/downloads/{{tag}}/{{basename}}.{{archive-format}}"
}
EOF
  cat www/arbitrary-2/foo/extension.json

  cat > www/arbitrary-2/foo/dependencies.json <<EOF
{
  "0.1.0": {
    "dfx": {
      "version": ">=0.8.0"
    }
  }
}
EOF

  arch=$(get_extension_architecture)

  if [ "$(uname)" == "Darwin" ]; then
    ARCHIVE_BASENAME="foo-$arch-apple-darwin"
  else
    ARCHIVE_BASENAME="foo-$arch-unknown-linux-gnu"
  fi

  mkdir "$ARCHIVE_BASENAME"
  cp www/arbitrary-2/foo/extension.json "$ARCHIVE_BASENAME"
  tar -czf "$ARCHIVE_BASENAME".tar.gz "$ARCHIVE_BASENAME"
  rm -rf "$ARCHIVE_BASENAME"

  mkdir -p www/arbitrary/downloads/foo-v0.1.0
  mv "$ARCHIVE_BASENAME".tar.gz www/arbitrary/downloads/foo-v0.1.0/

  assert_command dfx extension install foo --catalog-url "$CATALOG_URL"
}


@test "install extension by url from elsewhere" {
  start_webserver --directory www
  EXTENSION_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/extension.json"
  mkdir -p  www/arbitrary/downloads

  cat > www/arbitrary/extension.json <<EOF
{
  "name": "an-extension",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "download_url_template": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/downloads/{{tag}}/{{basename}}.{{archive-format}}"
}
EOF
  cat www/arbitrary/extension.json
  cat > www/arbitrary/an-extension <<EOF
#!/usr/bin/env bash

echo "an extension output"
EOF
  chmod +x www/arbitrary/an-extension

  cat > www/arbitrary/dependencies.json <<EOF
{
  "0.1.0": {
    "dfx": {
      "version": ">=0.8.0"
    }
  }
}
EOF

  arch=$(get_extension_architecture)

  if [ "$(uname)" == "Darwin" ]; then
    ARCHIVE_BASENAME="an-extension-$arch-apple-darwin"
  else
    ARCHIVE_BASENAME="an-extension-$arch-unknown-linux-gnu"
  fi

  mkdir "$ARCHIVE_BASENAME"
  cp www/arbitrary/extension.json "$ARCHIVE_BASENAME"
  cp www/arbitrary/an-extension "$ARCHIVE_BASENAME"
  tar -czf "$ARCHIVE_BASENAME".tar.gz "$ARCHIVE_BASENAME"
  rm -rf "$ARCHIVE_BASENAME"

  mkdir -p www/arbitrary/downloads/an-extension-v0.1.0
  mv "$ARCHIVE_BASENAME".tar.gz www/arbitrary/downloads/an-extension-v0.1.0/

  assert_command dfx extension install "$EXTENSION_URL"
}

@test "install extension with non-platform-specific archive" {
  start_webserver --directory www
  EXTENSION_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/extension.json"
  mkdir -p  www/arbitrary/downloads

  cat > www/arbitrary/extension.json <<EOF
{
  "name": "an-extension",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "download_url_template": "http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/downloads/{{tag}}.{{archive-format}}"
}
EOF
  cat www/arbitrary/extension.json
  cat > www/arbitrary/an-extension <<EOF
#!/usr/bin/env bash

echo "an extension output"
EOF
  chmod +x www/arbitrary/an-extension

  cat > www/arbitrary/dependencies.json <<EOF
{
  "0.1.0": {
    "dfx": {
      "version": ">=0.8.0"
    }
  }
}
EOF


  ARCHIVE_BASENAME="an-extension-v0.1.0"

  mkdir "$ARCHIVE_BASENAME"
  cp www/arbitrary/extension.json "$ARCHIVE_BASENAME"
  cp www/arbitrary/an-extension "$ARCHIVE_BASENAME"
  tar -czf "$ARCHIVE_BASENAME".tar.gz "$ARCHIVE_BASENAME"
  rm -rf "$ARCHIVE_BASENAME"

  mv "$ARCHIVE_BASENAME".tar.gz www/arbitrary/downloads/

  assert_command dfx extension install "$EXTENSION_URL"
}

@test "install is not an error if already installed" {
  assert_command_fail dfx nns --help
  assert_command dfx extension install nns --version 0.4.1
  assert_command dfx extension install nns --version 0.4.1
  # shellcheck disable=SC2154
  assert_eq "WARN: Extension 'nns' version 0.4.1 is already installed" "$stderr"
  assert_command dfx nns --help
}

@test "install is not an error if an older version is already installed and no version was specified" {
  assert_command_fail dfx nns --help
  assert_command dfx extension install nns --version 0.3.1
  assert_command dfx extension install nns
  # shellcheck disable=SC2154
  assert_eq "WARN: Extension 'nns' version 0.3.1 is already installed" "$stderr"
  assert_command dfx nns --help
}

@test "reports error if older version already installed and specific version requested" {
  assert_command_fail dfx nns --help
  assert_command dfx extension install nns --version 0.3.1
  assert_command_fail dfx extension install nns --version 0.4.1
  # shellcheck disable=SC2154
  assert_contains "ERROR: Extension 'nns' is already installed at version 0.3.1" "$stderr"
  # shellcheck disable=SC2154
  assert_contains 'ERROR: To upgrade, run "dfx extension uninstall nns" and then re-run the dfx extension install command' "$stderr"
}

@test "manually create extension" {
  assert_command dfx extension list
  assert_match 'No extensions installed'

  CACHE_DIR=$(dfx cache show)
  mkdir -p "$CACHE_DIR"/extensions/test_extension
  echo '#!/usr/bin/env bash

echo testoutput' > "$CACHE_DIR"/extensions/test_extension/test_extension
  chmod +x "$CACHE_DIR"/extensions/test_extension/test_extension

  assert_command_fail dfx extension list
  assert_match "Error.*Failed to load extension manifest.*failed to read JSON file.*failed to read .*extensions/test_extension/extension.json.*No such file or directory"

  assert_command_fail dfx extension run test_extension
  assert_match "Error.*Failed to load extension manifest.*failed to read JSON file.*failed to read .*extensions/test_extension/extension.json.*No such file or directory"

  assert_command_fail dfx test_extension
  assert_match "Error.*Failed to load extension manifest.*failed to read JSON file.*failed to read .*extensions/test_extension/extension.json.*No such file or directory"

  assert_command_fail dfx --help
  assert_match "Error.*Failed to load extension manifest.*failed to read JSON file.*failed to read .*extensions/test_extension/extension.json.*No such file or directory"

  assert_command_fail dfx test_extension --help
  assert_match "Error.*Failed to load extension manifest.*failed to read JSON file.*failed to read .*extensions/test_extension/extension.json.*No such file or directory"

  echo "{}" > "$CACHE_DIR"/extensions/test_extension/extension.json

  assert_command_fail dfx extension list
  assert_contains "Failed to load extension manifest"
  assert_match "failed to parse contents of .*extensions/test_extension/extension.json as json"
  assert_match "missing field .* at line .* column .*"

  assert_command_fail dfx extension run test_extension
  assert_contains "Failed to load extension manifest"
  assert_match "failed to parse contents of .*extensions/test_extension/extension.json as json.*"
  assert_match "missing field .* at line .* column .*"

  assert_command_fail dfx test_extension
  assert_contains "Failed to load extension manifest"
  assert_match "failed to parse contents of .*extensions/test_extension/extension.json as json.*"
  assert_match "missing field .* at line .* column .*"

  assert_command_fail dfx --help
  assert_contains "Failed to load extension manifest"
  assert_match "failed to parse contents of .*extensions/test_extension/extension.json as json.*"
  assert_match "missing field .* at line .* column .*"

  assert_command_fail dfx test_extension --help
  assert_contains "Failed to load extension manifest"
  assert_match "failed to parse contents of .*extensions/test_extension/extension.json as json.*"
  assert_match "missing field .* at line .* column .*"

  echo '{
  "name": "test_extension",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": []
}' > "$CACHE_DIR"/extensions/test_extension/extension.json

  assert_command dfx --help
  assert_match "test_extension.*Test extension for e2e purposes."

  assert_command dfx test_extension --help
  assert_match "Test extension for e2e purposes..*Usage: dfx test_extension"

  assert_command dfx extension list
  assert_match "test_extension"

  assert_command dfx extension run test_extension
  assert_match "testoutput"

  assert_command dfx test_extension
  assert_match "testoutput"

  assert_command dfx extension uninstall test_extension
  # TODO: how to capture spinner message?
  # assert_match 'Successfully uninstalled extension'

  assert_command dfx extension list
  assert_match 'No extensions installed'
}


@test "run with hyphened parameters" {
  CACHE_DIR=$(dfx cache show)
  mkdir -p "$CACHE_DIR"/extensions/test_extension

  cat > "$CACHE_DIR"/extensions/test_extension/test_extension << "EOF"
#!/usr/bin/env bash

if [ "$2" == "--the-param" ]; then
  echo "pamparam the param is $3"
fi
EOF

  chmod +x "$CACHE_DIR"/extensions/test_extension/test_extension

  cat > "$CACHE_DIR"/extensions/test_extension/extension.json <<EOF
{
  "name": "test_extension",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "subcommands": {
  "abc": {
  "about": "something something",
  "args": {
    "the_param": {
      "about": "this is the param",
      "long": "the-param"
    }
  }
  }
  }
}
EOF

  assert_command dfx test_extension abc --the-param 123
  assert_eq "pamparam the param is 123"
  assert_command dfx extension run test_extension abc --the-param 123
  assert_eq "pamparam the param is 123"
}

@test "extension run uses project root" {
  CACHE_DIR=$(dfx cache show)
  mkdir -p "$CACHE_DIR"/extensions/test_extension

  cat > "$CACHE_DIR"/extensions/test_extension/test_extension << "EOF"
#!/usr/bin/env bash

echo "the current directory is '$(pwd)'"

EOF

  chmod +x "$CACHE_DIR"/extensions/test_extension/test_extension

  cat > "$CACHE_DIR"/extensions/test_extension/extension.json <<EOF
{
  "name": "test_extension",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "subcommands": {
  "abc": {
  "about": "something something",
  "args": {
  }
  }
  }
}
EOF

  mkdir -p project || exit
  cd project || exit
  echo "{}" >dfx.json
  mkdir -p subdir || exit
  cd subdir || exit

  assert_command dfx test_extension abc
  assert_match "the current directory is '.*/working-dir/project'"
}

@test "run with multiple values for the same parameter" {
  CACHE_DIR=$(dfx cache show)
  mkdir -p "$CACHE_DIR"/extensions/test_extension

  cat > "$CACHE_DIR"/extensions/test_extension/test_extension << "EOF"
#!/usr/bin/env bash

echo $@
EOF

  chmod +x "$CACHE_DIR"/extensions/test_extension/test_extension

  cat > "$CACHE_DIR"/extensions/test_extension/extension.json <<EOF
{
  "name": "test_extension",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Test extension for e2e purposes.",
  "categories": [],
  "keywords": [],
  "subcommands": {
  "abc": {
  "about": "something something",
  "args": {
    "the_param": {
      "about": "this is the param",
      "long": "the-param",
      "multiple": true
    },
    "another_param": {
      "about": "this is the param",
      "long": "the-another-param",
      "multiple": true
    }
  }
  }
  }
}
EOF

  assert_command dfx test_extension abc --the-param 123 456 789 --the-another-param 464646
  assert_eq "abc --the-param 123 456 789 --the-another-param 464646 --dfx-cache-path $CACHE_DIR"
  assert_command dfx test_extension abc --the-another-param 464646 --the-param 123 456 789
  assert_eq "abc --the-another-param 464646 --the-param 123 456 789 --dfx-cache-path $CACHE_DIR"
  assert_command dfx extension run test_extension abc --the-param 123 456 789 --the-another-param 464646
  assert_eq "abc --the-param 123 456 789 --the-another-param 464646 --dfx-cache-path $CACHE_DIR"
  assert_command dfx extension run test_extension abc --the-another-param 464646 --the-param 123 456 789
  assert_eq "abc --the-another-param 464646 --the-param 123 456 789 --dfx-cache-path $CACHE_DIR"
}

@test "list available extensions from official catalog" {
  assert_command dfx extension list --available
  assert_contains "sns"
  assert_contains "nns"
}

@test "list available extensions from customized catalog" {
  start_webserver --directory www
  CATALOG_URL_URL="http://localhost:$E2E_WEB_SERVER_PORT/arbitrary/catalog.json"
  mkdir -p  www/arbitrary

    cat > www/arbitrary/catalog.json <<EOF
{
  "nns": "https://raw.githubusercontent.com/dfinity/dfx-extensions/main/extensions/nns/extension.json",
  "sns": "https://raw.githubusercontent.com/dfinity/dfx-extensions/main/extensions/sns/extension.json",
  "test": "https://raw.githubusercontent.com/dfinity/dfx-extensions/main/extensions/sns/extension.json"
}
EOF

  assert_command dfx extension list --catalog-url="$CATALOG_URL_URL"
  assert_contains "sns"
  assert_contains "nns"
  assert_contains "test"
}


-----------------------

/e2e/tests-dfx/fabricate_cycles.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "ledger fabricate-cycles works with default amount" {
  install_asset greet
  dfx_start
  dfx deploy
  # default amount is 10 trillion cycles, which results in an amount like 13_899_071_239_420
  assert_command dfx ledger fabricate-cycles --canister "$(dfx canister id hello_backend)"
  # bash does not accept \d, use [0-9] instead
  assert_match 'updated balance: [0-9]{2}(_[0-9]{3}){4} cycles'
  assert_command dfx ledger fabricate-cycles --all
  assert_match 'updated balance: [0-9]{2}(_[0-9]{3}){4} cycles'
}

@test "ledger fabricate-cycles works with specific amount" {
  install_asset greet
  dfx_start
  dfx deploy
  # adding 100 trillion cycles, which results in an amount like 103_899_071_239_420
  assert_command dfx ledger fabricate-cycles --canister "$(dfx canister id hello_backend)" --cycles 100000000000000
  assert_match 'updated balance: [0-9]{3}(_[0-9]{3}){4} cycles'
  assert_command dfx ledger fabricate-cycles --canister hello_backend --t 100
  assert_match 'updated balance: [0-9]{3}(_[0-9]{3}){4} cycles'
}

@test "ledger fabricate-cycles fails on real IC" {
  install_asset greet
  assert_command_fail dfx ledger fabricate-cycles --all --network ic
  assert_match "Cannot run this on the real IC."
  assert_command_fail dfx ledger fabricate-cycles --all --ic
  assert_match "Cannot run this on the real IC."
}

@test "ledger fabricate-cycles fails with wrong option combinations" {
  install_asset greet
  assert_command_fail dfx ledger fabricate-cycles --all --cycles 1 --icp 1 --network ic
  assert_command_fail dfx ledger fabricate-cycles --all --icp 1 --t 1 --network ic
  assert_command_fail dfx ledger fabricate-cycles --all --t 1 --cycles 1 --network ic
  assert_command_fail dfx ledger fabricate-cycles --all --e8s 1 --amount 1 --network ic
  assert_command_fail dfx ledger fabricate-cycles --all --amount 1 --cycles 1 --network ic
}


-----------------------

/e2e/tests-dfx/frontend.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_frontend
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx deploy shows a url for the frontend and for the candid interface" {
  dfx_start
  PORT=$(get_webserver_port)

  assert_command dfx deploy
  CANDID_UI_ID=$(dfx canister id __Candid_UI)
  APP_ID=$(dfx canister id e2e_project_backend)
  ASSETS_ID=$(dfx canister id e2e_project_frontend)
  assert_match "e2e_project_backend: http://127.0.0.1:$PORT/\?canisterId=$CANDID_UI_ID&id=$APP_ID"
  assert_match "http://127.0.0.1:$PORT/\?canisterId=$ASSETS_ID"

  # the urls are a little nicer if the bind address is localhost:8000 rather than 127.0.0.1:8000
  jq -n '.local.bind="localhost:'"$PORT"'"' >"$E2E_NETWORKS_JSON"

  assert_command dfx deploy
  assert_match "e2e_project_backend: http://$CANDID_UI_ID.localhost:$PORT/\?id=$APP_ID"
  assert_match "e2e_project_frontend: http://$ASSETS_ID.localhost:$PORT/"
}

@test "dfx start serves a frontend with static assets" {
  skip "Need a build of @dfinity/agent that works with HTTP Query"
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  ID=$(dfx canister id e2e_project_frontend)
  PORT=$(get_webserver_port)
  assert_command curl http://localhost:"$PORT"/?canisterId="$ID"
  assert_match "logo.png"
}

@test "dfx start serves a frontend on a port" {
  skip "Need a build of @dfinity/agent that works with HTTP Query"

  dfx_start --host 127.0.0.1:12345

  jq '.networks.local.bind="127.0.0.1:12345"' dfx.json | sponge dfx.json

  dfx canister create --all
  dfx build
  dfx canister install --all

  ID=$(dfx canister id e2e_project_frontend)
  assert_command curl http://localhost:12345/?canisterId="$ID"
  assert_match "<html>"

  assert_command_fail curl http://localhost:8000
  assert_match "Connection refused"
}

@test "dfx routes frontend based on subdomain" {
  dfx_start
  PORT=$(get_webserver_port)
  dfx deploy
  ID=$(dfx canister id e2e_project_frontend)
  assert_command curl "http://$ID.localhost:$PORT/"
  assert_match "<head>"
}

@test "dfx uses .ic-assets.json file provided in src/__project_name__frontend/assets" {
  echo '[{"match": "*", "headers": {"x-key": "x-value"}}]' > src/e2e_project_frontend/assets/.ic-assets.json5

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  ID=$(dfx canister id e2e_project_frontend)
  PORT=$(get_webserver_port)
  assert_command curl -vv http://localhost:"$PORT"/?canisterId="$ID"
  assert_match "< x-key: x-value"
  assert_command curl -vv http://localhost:"$PORT"/favicon.ico?canisterId="$ID"
  assert_match "< x-key: x-value"
}

@test "dfx uses a custom build command if one is provided" {
  jq '.canisters.e2e_project_frontend.source = ["src/e2e_project_frontend/dist2"]' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_frontend.build = ["npm run custom-build --workspace e2e_project_frontend"]' dfx.json | sponge dfx.json
  jq '.scripts["custom-build"] = "npm run build && mkdir -p ./dist2/ && cp -r ./dist/* ./dist2"' src/e2e_project_frontend/package.json | sponge src/e2e_project_frontend/package.json

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  ID=$(dfx canister id e2e_project_frontend)
  PORT=$(get_webserver_port)

  assert_command curl -vv http://localhost:"$PORT"/index.html?canisterId="$ID"
  assert_match "IC Hello Starter"
}


-----------------------

/e2e/tests-dfx/generate.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx generate creates files" {
  dfx_new hello
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  dfx generate

  assert_file_exists "src/declarations/hello_backend/hello_backend.did"
  assert_file_exists "src/declarations/hello_backend/hello_backend.did.js"
  assert_file_exists "src/declarations/hello_backend/hello_backend.did.d.ts"
  assert_file_exists "src/declarations/hello_backend/index.js"
  assert_file_exists "src/declarations/hello_backend/index.d.ts"
}

@test "dfx generate creates only JS files" {
  dfx_new hello
  jq '.canisters.hello_backend.declarations.bindings=["js"]' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  dfx generate

  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did"
  assert_file_exists "src/declarations/hello_backend/hello_backend.did.js"
  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did.d.ts"
  assert_file_exists "src/declarations/hello_backend/index.js"
  assert_file_not_exists "src/declarations/hello_backend/index.d.ts"
}

@test "dfx generate creates only TS files" {
  dfx_new hello
  jq '.canisters.hello_backend.declarations.bindings=["ts"]' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  dfx generate

  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did"
  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did.js"
  assert_file_exists "src/declarations/hello_backend/hello_backend.did.d.ts"
  assert_file_not_exists "src/declarations/hello_backend/index.js"
  assert_file_exists "src/declarations/hello_backend/index.d.ts"
}

@test "dfx generate creates only JS & TS files" {
  dfx_new hello
  jq '.canisters.hello_backend.declarations.bindings=["js", "ts"]' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  dfx generate

  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did"
  assert_file_exists "src/declarations/hello_backend/hello_backend.did.js"
  assert_file_exists "src/declarations/hello_backend/hello_backend.did.d.ts"
  assert_file_exists "src/declarations/hello_backend/index.js"
  assert_file_exists "src/declarations/hello_backend/index.d.ts"
}

@test "dfx generate creates only DID files" {
  dfx_new hello
  jq '.canisters.hello_backend.declarations.bindings=["did"]' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  dfx generate

  assert_file_exists "src/declarations/hello_backend/hello_backend.did"
  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did.js"
  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did.d.ts"
  assert_file_not_exists "src/declarations/hello_backend/index.js"
  assert_file_not_exists "src/declarations/hello_backend/index.d.ts"
}

@test "dfx generate does not create any files" {
  dfx_new hello
  jq '.canisters.hello_backend.declarations.bindings=[]' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install --all

  dfx generate

  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did"
  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did.js"
  assert_file_not_exists "src/declarations/hello_backend/hello_backend.did.d.ts"
  assert_file_not_exists "src/declarations/hello_backend/index.js"
  assert_file_not_exists "src/declarations/hello_backend/index.d.ts"
}

@test "dfx generate succeeds with an encrypted identity without input" {
  dfx_new hello
  dfx_start
  dfx canister create --all

  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_pw.exp"
  assert_command dfx identity use alice

  assert_command timeout 30s dfx generate
}

@test "dfx generate does not require canister IDs for non-Motoko canisters" {
  dfx_new_rust hello
  assert_command dfx generate
}

@test "dfx generate does not require canister IDs for Motoko canisters" {
  dfx_new hello
  assert_command dfx generate
}

@test "dfx generate --network is still valid" {
  # The option has no effect, but is still accepted to not break existing scripts
  dfx_new hello
  assert_command dfx generate --network local

  # Option is not advertised anymore
  assert_command dfx generate --help
  assert_not_contains "--network"
}

@test "dfx generate does not delete source candid file of Rust canister when bindings contains no did" {
  dfx_new_rust hello
  jq '.canisters.hello_backend.declarations.bindings=["js"]' dfx.json | sponge dfx.json
  assert_command dfx generate
  assert_file_exists "src/hello_backend/hello_backend.did"
}


-----------------------

/e2e/tests-dfx/id.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "id subcommand prints valid canister identifier" {
  install_asset id
  dfx_start
  dfx canister create --all
  dfx build
  assert_command dfx canister id e2e_project_backend
  assert_match "$(jq -r .e2e_project_backend.local < .dfx/local/canister_ids.json)"
}

@test "id subcommand does not display warning about plaintext keys" {
  install_asset id
  dfx identity get-principal
  echo "{}" | jq '.e2e_project_backend.ic = "bd3sg-teaaa-aaaaa-qaaba-cai"' >canister_ids.json
  assert_command dfx canister id e2e_project_backend --ic
  assert_eq "bd3sg-teaaa-aaaaa-qaaba-cai"
}

@test "id subcommand works from a subdirectory of the project - ephemeral id" {
  install_asset id
  dfx_start
  dfx canister create --all
  ID=$(dfx canister id e2e_project_backend)
  echo "canister id is $ID"

  (
    cd src
    dfx canister id e2e_project_backend
    assert_command dfx canister id e2e_project_backend
    assert_eq "$ID"
  )
}

@test "id subcommand works from a subdirectory of the project - persistent id" {
  install_asset id

  jq '.networks.local.type="persistent"' dfx.json | sponge dfx.json
  dfx_start
  dfx canister create --all
  ID=$(dfx canister id e2e_project_backend)
  echo "canister id is $ID"
  (
    cd src
    dfx canister id e2e_project_backend
    assert_command dfx canister id e2e_project_backend
    assert_eq "$ID"
  )
}

@test "id subcommand uses default network for remotes only" {
  install_asset id
  install_shared_asset subnet_type/shared_network_settings/application
  # Add a remote canister with a specific ID for one network and a different default for other networks.
  jq '.canisters.external_canister = {
  "build": "",
  "candid": "candid/external_canister.did",
  "remote": {
    "id": {
      "namedremote": "va76m-bqaaa-aaaaa-aaayq-cai",
      "__default": "rkp4c-7iaaa-aaaaa-aaaca-cai"
    }
  },
  "type": "custom",
  "wasm": ""
  }' dfx.json | sponge dfx.json
  # We need to define the networks we are going to use:
  jq '.namedremote= {"type": "persistent", "providers": ["http://namedremote"]}' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  jq '.somethingelse= {"type": "persistent", "providers": ["http://somethingelse"]}' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  cat dfx.json
  cat "$E2E_NETWORKS_JSON"
  # Ok, start:
  dfx_start || true
  dfx canister create --all
  # The local dfx canister ID should not be affected:
  assert_command dfx canister id e2e_project_backend
  assert_match "$(jq -r .e2e_project_backend.local < .dfx/local/canister_ids.json)"
  # Named remotes should be unaffected:
  assert_command dfx canister --network namedremote id external_canister
  assert_match "va76m-bqaaa-aaaaa-aaayq-cai"
  # Other remotes should use the default entry:
  assert_command dfx canister --network somethingelse id external_canister
  assert_match "rkp4c-7iaaa-aaaaa-aaaca-cai"
}


-----------------------

/e2e/tests-dfx/identity.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_assets
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "identity new: name validation" {
  assert_command_fail dfx identity new iden%tity --storage-mode plaintext
  assert_match "Invalid identity name"

  assert_command_fail dfx identity new 'iden tity' --storage-mode plaintext
  assert_match "Invalid identity name"

  assert_command_fail dfx identity new "iden\$tity" --storage-mode plaintext
  assert_match "Invalid identity name"

  assert_command_fail dfx identity new iden\\tity --storage-mode plaintext
  assert_match "Invalid identity name"

  assert_command_fail dfx identity new 'iden\ttity' --storage-mode plaintext
  assert_match "Invalid identity name"

  assert_command_fail dfx identity new iden/tity --storage-mode plaintext
  assert_match "Invalid identity name"

  assert_command dfx identity new i_den.ti-ty --storage-mode plaintext

  assert_command dfx identity new i_den@ti-ty --storage-mode plaintext
}

@test "identity get-principal: the get-principal is the same as sender id" {
  install_asset identity
  dfx_start
  assert_command dfx identity new --storage-mode plaintext jose

  PRINCIPAL_ID=$(dfx identity get-principal --identity jose)

  dfx canister create e2e_project_backend --identity jose
  dfx build e2e_project_backend --identity jose
  dfx canister install e2e_project_backend --identity jose

  assert_command dfx canister call e2e_project_backend amInitializer --identity jose

  SENDER_ID=$(dfx canister call e2e_project_backend fromCall --identity jose)

  if [ "$PRINCIPAL_ID" -ne "$SENDER_ID" ]; then
    echo "IDs did not match: Principal '${PRINCIPAL_ID}' != Sender '${SENDER_ID}'..." | fail
  fi
}

@test "identity get-principal (anonymous): the get-principal is the same as sender id" {
  install_asset identity
  dfx_start
  assert_command dfx identity new --storage-mode plaintext jose

  ANONYMOUS_PRINCIPAL_ID="2vxsx-fae"

  PRINCIPAL_ID=$(dfx identity get-principal --identity anonymous)

  if [ "$PRINCIPAL_ID" -ne "$ANONYMOUS_PRINCIPAL_ID" ]; then
    echo "IDs did not match: Principal '${ANONYMOUS_PRINCIPAL_ID}' != Sender '${PRINCIPAL_ID}'..." | fail
  fi

  dfx canister create e2e_project_backend --identity jose
  dfx build e2e_project_backend --identity jose
  dfx canister install e2e_project_backend --identity jose

  SENDER_ID=$(dfx canister call e2e_project_backend fromCall --identity anonymous)

  if [ "$ANONYMOUS_PRINCIPAL_ID" -ne "$SENDER_ID" ]; then
    echo "IDs did not match: Principal '${ANONYMOUS_PRINCIPAL_ID}' != Sender '${SENDER_ID}'..." | fail
  fi
}

@test "calls and query receive the same principal from dfx" {
  install_asset identity
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_command dfx canister install --all

  ID_CALL=$(dfx canister call e2e_project_backend fromCall)
  ID_QUERY=$(dfx canister call e2e_project_backend fromQuery)
  if [ "$ID_CALL" -ne "$ID_QUERY" ]; then
    echo "IDs did not match: call '${ID_CALL}' != query '${ID_QUERY}'..." | fail
  fi

  ID=$(dfx canister call e2e_project_backend getCanisterId)
  assert_command dfx canister call e2e_project_backend isMyself "$ID"
  assert_eq '(true)'
  assert_command dfx canister call e2e_project_backend isMyself "$ID_CALL"
  assert_eq '(false)'
}

@test "dfx ping does not create a default identity" {
  dfx_start

  assert_file_not_exists "$DFX_CONFIG_ROOT/.config/dfx/identity.json"
  assert_file_not_exists "$DFX_CONFIG_ROOT/.config/dfx/identity/default/identity.pem"

  assert_command dfx ping

  assert_file_not_exists "$DFX_CONFIG_ROOT/.config/dfx/identity.json"
  assert_file_not_exists "$DFX_CONFIG_ROOT/.config/dfx/identity/default/identity.pem"

  # shellcheck disable=SC2154
  assert_not_match 'Creating' "$stderr"
  # shellcheck disable=SC2154
  assert_not_match '(default.*identity|identity.*default)' "$stderr"
  # shellcheck disable=SC2154
  assert_match "ic_api_version" "$stdout"
}

@test "dfx canister: creates the default identity on first run" {
  install_asset identity
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_match 'Creating the "default" identity.' "$stderr"
}

@test "after using a specific identity while creating a canister, that user is the initializer" {
  install_asset identity
  dfx_start
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  dfx canister create --all --identity alice
  assert_command dfx build --identity alice
  assert_command dfx canister install --all --identity alice

  # The user Identity's principal is the initializer
  assert_command dfx canister call e2e_project_backend amInitializer --identity alice
  assert_eq '(true)'

  assert_command dfx canister call e2e_project_backend amInitializer --identity bob
  assert_eq '(false)'

  # these all fail (other identities are not initializer; cannot store assets):
  assert_command_fail dfx canister call e2e_project_frontend store '(record{key="B"; content_type="application/octet-stream"; content_encoding="identity"; content=blob"XWV"})' --identity bob
  assert_command_fail dfx canister call e2e_project_frontend store '(record{key="B"; content_type="application/octet-stream"; content_encoding="identity"; content=blob"XWV"})' --identity default
  assert_command_fail dfx canister call e2e_project_frontend store '(record{key="B"; content_type="application/octet-stream"; content_encoding="identity"; content=blob"XWV"})'
  assert_command_fail dfx canister call e2e_project_frontend retrieve '("B")'

  # but alice, the initializer, can store assets:
  assert_command dfx canister call e2e_project_frontend store '(record{key="B"; content_type="application/octet-stream"; content_encoding="identity"; content=blob"XWV"})' --identity alice
  assert_eq '()'
  assert_command dfx canister call --output idl e2e_project_frontend retrieve '("B")'
  # shellcheck disable=SC2154
  assert_eq '(blob "XWV")' "$stdout"
}

@test "after renaming an identity, the renamed identity is still initializer" {
  install_asset identity
  dfx_start
  assert_command dfx identity new --storage-mode plaintext alice

  dfx canister create --all --identity alice
  assert_command dfx build --identity alice
  assert_command dfx canister install --all --identity alice
  assert_command dfx canister call e2e_project_backend amInitializer --identity alice
  assert_eq '(true)'
  assert_command dfx canister call e2e_project_backend amInitializer
  assert_eq '(false)'

  assert_command dfx identity rename alice bob

  assert_command dfx identity whoami
  assert_eq 'default'
  assert_command dfx canister call e2e_project_backend amInitializer --identity bob
  assert_eq '(true)'

  assert_command dfx canister call e2e_project_frontend store '(record{key="B"; content_type="application/octet-stream"; content_encoding="identity"; content=blob "hello"})' --identity bob
  assert_eq '()'
  assert_command dfx canister call --output idl e2e_project_frontend retrieve '("B")'
  # shellcheck disable=SC2154
  assert_eq '(blob "hello")' "$stdout"
}

@test "using an unencrypted identity on mainnet provokes a warning" {
  assert_command dfx ledger balance --network ic
  assert_match "WARN: The default identity is not stored securely." "$stderr"
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_pw.exp"
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/get_ledger_balance.exp"
  dfx identity new bob --storage-mode plaintext
  assert_command dfx ledger balance --network ic --identity bob
  assert_match "WARN: The bob identity is not stored securely." "$stderr"

  export DFX_WARNING=-mainnet_plaintext_identity
  assert_command dfx ledger balance --network ic --identity bob
  assert_not_contains "not stored securely" "$stderr"

}

@test "can call a canister using an ed25519 identity" {
  install_asset ed25519
  assert_command dfx identity import --storage-mode plaintext ed25519 identity.pem
  dfx_new # This installs replica and other binaries
  dfx identity use ed25519
  install_asset whoami
  dfx_start
  dfx canister create whoami
  dfx build
  dfx canister install whoami
  assert_command dfx canister call whoami whoami
  assert_eq '(principal "2nor3-keehi-duuup-d7jcn-onggn-3atzm-gejtl-5tlzn-k4g6c-nnbf7-7qe")'
  assert_command dfx identity get-principal
  assert_eq "2nor3-keehi-duuup-d7jcn-onggn-3atzm-gejtl-5tlzn-k4g6c-nnbf7-7qe"
}


-----------------------

/e2e/tests-dfx/identity_command.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
  export DFX_CI_MOCK_KEYRING_LOCATION="$MOCK_KEYRING_LOCATION"
}

teardown() {
  standard_teardown
}

##
## dfx identity get-principal
##

@test "identity get-principal: different identities have different principal ids" {
  assert_command dfx identity new jose
  assert_command dfx identity new juana

  PRINCIPAL_ID_JOSE=$(dfx identity get-principal --identity jose)
  PRINCIPAL_ID_JUANA=$(dfx identity get-principal --identity juana)

  if [ "$PRINCIPAL_ID_JOSE" -eq "$PRINCIPAL_ID_JUANA" ]; then
    echo "IDs should not match: Jose '${PRINCIPAL_ID_JOSE}' == Juana '${PRINCIPAL_ID_JUANA}'..." | fail
  fi
}

##
## dfx identity list
##

@test "identity list: shows identities in alpha order" {
  assert_command dfx identity new dan
  assert_command dfx identity new frank
  assert_command dfx identity new alice
  assert_command dfx identity new bob
  assert_command dfx identity list
  assert_match \
'alice
anonymous
bob
dan
default
frank'
  assert_command dfx identity new charlie
  assert_command dfx identity list
  assert_match \
'alice
anonymous
bob
charlie
dan
default
frank'
}

@test "identity list: shows the anonymous identity" {
  assert_command dfx identity list
  # shellcheck disable=SC2154
  assert_match 'anonymous' "$stdout"
}

@test "identity list: shows the default identity" {
  assert_command dfx identity list
  assert_match 'default' "$stdout"
  # shellcheck disable=SC2154
  assert_match 'Creating the "default" identity.' "$stderr"
}

##
## dfx identity new
##

@test "identity new: creates a new keyring identity" {
  assert_command dfx identity new alice
  assert_match 'Created identity: "alice".' "$stderr"
  assert_command cat "$MOCK_KEYRING_LOCATION"
  assert_match "internet_computer_identity_alice"

  # does not change the default identity
  assert_command dfx identity whoami
  assert_eq 'default'
}

@test "identity new --storage-mode plaintext: creates a new identity" {
  assert_command dfx identity new alice --storage-mode plaintext
  assert_match 'Created identity: "alice".' "$stderr"
  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
  assert_match "BEGIN EC PRIVATE KEY"

  # does not change the default identity
  assert_command dfx identity whoami
  assert_eq 'default'
}

@test "identity new: cannot create an identity called anonymous" {
  assert_command_fail dfx identity new anonymous
}

@test "identity new: cannot create an identity that already exists" {
  assert_command dfx identity new bob
  assert_command_fail dfx identity new bob
  assert_match "Identity already exists"
}

@test "identity new: --force re-creates an identity" {
  assert_command dfx identity new alice
  dfx identity use alice
  PRINCIPAL_1="$(dfx identity get-principal)"
  assert_command dfx identity new --force alice
  PRINCIPAL_2="$(dfx identity get-principal)"
  assert_neq "$PRINCIPAL_1" "$PRINCIPAL_2"
}

@test "identity new: --force does not switch to created identity" {
  # Was a bug: https://dfinity.atlassian.net/browse/SDK-911
  assert_command dfx identity new --force alice
  PRINCIPAL_ORIGINAL="$(dfx identity get-principal)"
  assert_command dfx identity use alice
  PRINCIPAL_ALICE="$(dfx identity get-principal)"
  assert_neq "$PRINCIPAL_ORIGINAL" "$PRINCIPAL_ALICE"
}

@test "identity new: create an HSM-backed identity" {
  assert_command dfx identity new --hsm-pkcs11-lib-path /something/else/somewhere.so --hsm-key-id abcd4321 bob
  assert_command jq -r .hsm.pkcs11_lib_path "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.json"
  assert_eq "/something/else/somewhere.so"
  assert_command jq -r .hsm.key_id "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.json"
  assert_eq "abcd4321"
}

@test "identity new: key_id must be hex digits" {
  assert_command_fail dfx identity new --hsm-pkcs11-lib-path xxx --hsm-key-id abcx bob
  assert_match "Key id must contain only hex digits"
}

@test "identity new: key_id must be an even number of digits" {
  assert_command_fail dfx identity new --hsm-pkcs11-lib-path xxx --hsm-key-id fed64 bob
  assert_match "Key id must consist of an even number of hex digits"
}

@test "identity new: key is compatible with openssl" {
  assert_command dfx identity new --storage-mode plaintext bob
  assert_command openssl ec -in "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.pem"
}

##
## dfx identity remove
##

@test "identity remove: can remove an identity that exists" {
  assert_command_fail cat "$MOCK_KEYRING_LOCATION"
  assert_command dfx identity new alice

  assert_command cat "$MOCK_KEYRING_LOCATION"
  assert_match "internet_computer_identity_alice"
  assert_command dfx identity list
  assert_match \
'alice
anonymous
default'

  assert_command dfx identity remove alice
  assert_match 'Removed identity "alice".' "$stderr"
  assert_command cat "$MOCK_KEYRING_LOCATION"
  assert_not_match "internet_computer_identity_alice"

  assert_command dfx identity list
  assert_match 'default'
}

@test "identity remove --storage-mode plaintext: can remove an identity that exists" {
  assert_command_fail head "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
  assert_command dfx identity new alice --storage-mode plaintext

  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
  assert_match "BEGIN EC PRIVATE KEY"
  assert_command dfx identity list
  assert_match \
'alice
anonymous
default'

  assert_command dfx identity remove alice
  assert_match 'Removed identity "alice".' "$stderr"
  assert_command_fail cat "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"

  assert_command dfx identity list
  assert_match 'default'
}

@test "identity remove: reports an error if no such identity" {
  assert_command_fail dfx identity remove charlie
}

@test "identity remove: only remove identities with configured wallet if --drop-wallets is specified" {
  # There's no replica running, and no real wallet.  This is just a valid principal.
  WALLET="rwlgt-iiaaa-aaaaa-aaaaa-cai"
  assert_command dfx identity new alice
  assert_command dfx identity use alice
  assert_command dfx identity set-wallet --force "$WALLET" --network ic
  assert_command dfx identity use default
  assert_command_fail dfx identity remove alice
  # make sure the configured wallet is displayed
  assert_match "identity 'alice' on network 'ic' has wallet $WALLET"
  assert_command dfx identity remove alice --drop-wallets
  assert_match "identity 'alice' on network 'ic' has wallet $WALLET"
}

@test "identity remove: cannot remove the non-default active identity" {
  assert_command dfx identity new alice --storage-mode plaintext
  assert_command dfx identity use alice
  assert_command_fail dfx identity remove alice

  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
  assert_match "BEGIN EC PRIVATE KEY"
  assert_command dfx identity list
  assert_match \
'alice
anonymous
default'
}

@test "identity remove: cannot remove the default identity" {
  # a new one will just get created again
  assert_command_fail dfx identity remove default
  assert_match "Cannot delete the default identity"
}


@test "identity remove: cannot remove the anonymous identity" {
  assert_command_fail dfx identity remove anonymous
}

@test "identity remove: can remove an HSM-backed identity" {
  assert_command dfx identity new --hsm-pkcs11-lib-path /something/else/somewhere.so --hsm-key-id abcd4321 bob
  assert_command jq -r .hsm.pkcs11_lib_path "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.json"
  assert_eq "/something/else/somewhere.so"
  assert_command jq -r .hsm.key_id "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.json"
  assert_eq "abcd4321"
  assert_command ls "$DFX_CONFIG_ROOT/.config/dfx/identity/bob"

  assert_command dfx identity remove bob
  assert_command_fail ls "$DFX_CONFIG_ROOT/.config/dfx/identity/bob"
}

##
## dfx identity rename
##

@test "identity rename: can rename an identity" {
  assert_command dfx identity new alice
  assert_command dfx identity list
  assert_match \
'alice
anonymous
default'
  assert_command cat "$MOCK_KEYRING_LOCATION"
  assert_match "internet_computer_identity_alice"
  KEY="$(dfx identity export alice)"

  assert_command dfx identity rename alice bob
  assert_match 'Renamed identity "alice" to "bob".' "$stderr"

  assert_command dfx identity list
  assert_match \
'anonymous
bob
default'
  assert_command cat "$MOCK_KEYRING_LOCATION"
  assert_match "internet_computer_identity_bob"
  assert_eq "$KEY" "$(dfx identity export bob)"
  assert_command cat "$MOCK_KEYRING_LOCATION"
  assert_not_match "internet_computer_identity_alice"
}

@test "identity rename --storage-mode plaintext: can rename an identity" {
  assert_command dfx identity new alice --storage-mode plaintext
  assert_command dfx identity list
  assert_match \
'alice
anonymous
default'
  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
  assert_match "BEGIN EC PRIVATE KEY"
  x=$(cat "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem")
  KEY="$x"

  assert_command dfx identity rename alice bob
  assert_match 'Renamed identity "alice" to "bob".' "$stderr"

  assert_command dfx identity list
  assert_match \
'anonymous
bob
default'
  assert_command cat "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.pem"
  assert_eq "$KEY" "$(cat "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.pem")"
  assert_match "BEGIN EC PRIVATE KEY"
  assert_command_fail cat "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
}

@test "identity rename: can rename the default identity, which also changes the default" {
  assert_command dfx identity list
  assert_match 'default'
  assert_command dfx identity rename default bob
  assert_command dfx identity list
  assert_match 'bob'
  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.pem"
  assert_match "BEGIN EC PRIVATE KEY"

  assert_command dfx identity whoami
  assert_eq 'bob'
}

@test "identity rename: can rename the selected identity, which also changes the default" {
  assert_command dfx identity new alice --storage-mode plaintext
  assert_command dfx identity use alice
  assert_command dfx identity list
  assert_match \
'alice
anonymous
default'
  assert_command dfx identity rename alice charlie

  assert_command dfx identity list
  assert_match \
'anonymous
charlie
default'

  assert_command dfx identity whoami
  assert_eq 'charlie'

  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/charlie/identity.pem"
  assert_match "BEGIN EC PRIVATE KEY"
  assert_command_fail cat "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
}

@test "identity rename: cannot create an anonymous identity via rename" {
  assert_command dfx identity new alice
  assert_command_fail dfx identity rename alice anonymous
  assert_match "Cannot create an anonymous identity"
}

@test "identity rename: can rename an HSM-backed identity" {
  skip "Need to instantiate identity when renaming so skipping until we have an hsm mock"
  assert_command dfx identity new --hsm-pkcs11-lib-path /something/else/somewhere.so --hsm-key-id abcd4321 bob
  assert_command dfx identity rename bob alice
  assert_command_fail ls "$DFX_CONFIG_ROOT/.config/dfx/identity/bob"

  assert_command jq -r .hsm.pkcs11_lib_path "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.json"
  assert_eq "/something/else/somewhere.so"
  assert_command jq -r .hsm.key_id "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.json"
  assert_eq "abcd4321"
}

##
## dfx identity use
##

@test "identity use: switches to an existing identity" {
  assert_command dfx identity new alice
  assert_command dfx identity whoami
  assert_eq 'default'
  assert_command dfx identity use alice
  assert_command dfx identity whoami
  assert_eq 'alice'

  ## and back
  assert_command dfx identity use default
  assert_command dfx identity whoami
  assert_eq 'default'
}

@test "identity use: cannot use an identity that has not been created yet" {
  assert_command_fail dfx identity use alice
  assert_command dfx identity whoami
  assert_eq 'default'
}

@test "identity use: can switch to the anonymous identity" {
  assert_command dfx identity use anonymous
  assert_command dfx identity whoami
  assert_eq 'anonymous'
  assert_command dfx identity get-principal
  assert_eq '2vxsx-fae'
}

@test "identity use: is overridden by env var DFX_IDENTITY" {
  assert_command dfx identity new dan
  assert_command dfx identity new frank
  assert_command dfx identity new alice
  assert_command dfx identity use dan
  assert_command dfx identity whoami
  assert_eq 'dan'
  DFX_IDENTITY=frank
  export DFX_IDENTITY
  assert_command dfx identity whoami
  assert_eq 'frank'
  assert_command dfx identity whoami --identity alice
  assert_eq 'alice'
}


##
## dfx identity whoami
##

@test "identity whoami: creates the default identity on first run" {
  # Just an example.  All the identity commands do this.
  assert_command dfx identity whoami
  assert_eq 'default' "$stdout"
  assert_match 'Creating the "default" identity.' "$stderr"
  assert_match 'Created the "default" identity.' "$stderr"
}

@test "identity whoami: shows the current identity" {
  assert_command dfx identity whoami
  assert_eq 'default' "$stdout"
  assert_command dfx identity new charlie
  assert_command dfx identity whoami
  assert_eq 'default'
  assert_command dfx identity use charlie
  assert_command dfx identity whoami
  assert_eq 'charlie'
}

## dfx (+other commands) --identity

@test "dfx identity whoami --identity (name): shows the overriding identity" {
  assert_command dfx identity whoami
  assert_eq 'default' "$stdout"
  assert_command dfx identity new charlie
  assert_command dfx identity new alice
  assert_command dfx identity whoami --identity charlie
  assert_eq 'charlie'
  assert_command dfx identity whoami --identity alice
  assert_eq 'alice'
}

@test "dfx (command) --identity does not persistently change the selected identity" {
  assert_command dfx identity whoami
  assert_eq 'default' "$stdout"
  assert_command dfx identity new charlie
  assert_command dfx identity new alice
  assert_command dfx identity use charlie
  assert_command dfx identity whoami
  assert_eq 'charlie'
  assert_command dfx identity whoami --identity alice
  assert_eq 'alice'
  assert_command dfx identity whoami
  assert_eq 'charlie'
}

##
## Identity key migration
##
@test "identity manager copies existing key from $DFX_CONFIG_ROOT/.dfinity/identity/creds.pem" {
  assert_command dfx identity whoami
  assert_command mkdir -p "$DFX_CONFIG_ROOT/.dfinity/identity"
  assert_command mv "$DFX_CONFIG_ROOT/.config/dfx/identity/default/identity.pem" "$DFX_CONFIG_ROOT/.dfinity/identity/creds.pem"
  ORIGINAL_KEY=$(cat "$DFX_CONFIG_ROOT/.dfinity/identity/creds.pem")
  assert_command rmdir "$DFX_CONFIG_ROOT/.config/dfx/identity/default"
  assert_command rmdir "$DFX_CONFIG_ROOT/.config/dfx/identity"
  assert_command rm "$DFX_CONFIG_ROOT/.config/dfx/identity.json"
  assert_command rmdir "$DFX_CONFIG_ROOT/.config/dfx"
  assert_command rmdir "$DFX_CONFIG_ROOT/.config"

  assert_command dfx identity whoami

  assert_match "migrating key from"
  assert_eq "$(cat "$DFX_CONFIG_ROOT"/.config/dfx/identity/default/identity.pem)" "$ORIGINAL_KEY"
}

@test "identity: import" {
  openssl ecparam -name secp256k1 -genkey -out identity.pem
  assert_command dfx identity import alice identity.pem
  assert_match 'Imported identity: "alice".' "$stderr"
  assert_command bash -c "dfx identity export alice > alice.pem"
  assert_command diff identity.pem alice.pem
  assert_eq ""
}

@test "identity: import can only overwrite identity with --force" {
  openssl ecparam -name secp256k1 -genkey -out identity.pem
  openssl ecparam -name secp256k1 -genkey -out identity2.pem
  assert_command dfx identity import alice identity.pem
  assert_match 'Imported identity: "alice".' "$stderr"
  dfx identity use alice
  PRINCIPAL_1="$(dfx identity get-principal)"

  assert_command_fail dfx identity import alice identity2.pem
  assert_match "Identity already exists."
  assert_command dfx identity import --force alice identity2.pem
  assert_match 'Imported identity: "alice".'
  PRINCIPAL_2="$(dfx identity get-principal)"

  assert_neq "$PRINCIPAL_1" "$PRINCIPAL_2"
}

@test "identity: import default" {
  assert_command dfx identity new alice --storage-mode plaintext
  assert_command dfx identity import bob "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
  assert_match 'Imported identity: "bob".' "$stderr"
  assert_command bash -c "dfx identity export bob > bob.pem"
  assert_command diff "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem" "bob.pem"
  assert_eq ""
}

@test "identity: import --storage-mode plaintext" {
  assert_command dfx identity new alice --storage-mode plaintext
  assert_command dfx identity import bob "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem" --storage-mode plaintext
  assert_match 'Imported identity: "bob".' "$stderr"
  assert_command diff "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem" "$DFX_CONFIG_ROOT/.config/dfx/identity/bob/identity.pem"
  assert_eq ""
}

@test "identity: cannot import invalid PEM file" {
  assert_command dfx identity new alice --storage-mode plaintext
  assert_command cp "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem" ./alice.pem
  # Following 3 lines manipulate the pem file so that it will be invalid
  head -n 1 alice.pem > bob.pem
  echo -n 1 >> bob.pem
  tail -n 3 alice.pem > bob.pem
  assert_command_fail dfx identity import bob bob.pem
  assert_match 'Failed to validate PEM content' "$stderr"
}

@test "identity: can import an EC key without an EC PARAMETERS section (as quill generate makes)" {
  cat >private-key-no-ec-parameters.pem <<EOF
-----BEGIN EC PRIVATE KEY-----
MHQCAQEEIE+3ipe2ruuJOmeBAhImUP/jic7Qwk2fXC8BaAmu6VK4oAcGBSuBBAAK
oUQDQgAEBQKn0CLyiA/fQf6L8S07/MDJ9kIJTzZvm2jFo2/yvSToGee+XzP/GCE4
08ZcZFM1EwUsknDBoSd0EF1PzFRmJg==
-----END EC PRIVATE KEY-----
EOF
  assert_command dfx identity import private-key-no-ec-parameters private-key-no-ec-parameters.pem
  assert_command dfx identity get-principal --identity private-key-no-ec-parameters
  assert_eq "j4p4p-o5ogq-4gzev-t3kay-hpm5o-xuwpz-yvrpp-47cc4-qyunt-k76yw-qae"
  echo "{}" >dfx.json # avoid "dfx.json not found, using default."
  assert_command dfx ledger account-id --identity private-key-no-ec-parameters
  assert_eq "3c00cf85d77b9dbf74a2acec1d9a9e73a3fc65f5048c64800b15f3b2c4c8eb11"
}

@test "identity: can export and re-import an identity" {
  assert_command dfx identity new alice
  dfx identity export alice > export.pem
  assert_file_exists export.pem
  assert_command dfx identity import bob export.pem
}

@test "identity: can import a seed phrase" {
  reg="seed phrase for identity 'alice': ([a-z ]+)"
  assert_command dfx identity new alice
  [[ $stderr =~ $reg ]]
  echo "${BASH_REMATCH[1]}" >seed.txt
  principal=$(dfx identity get-principal --identity alice)
  assert_command dfx identity import alice2 --seed-file seed.txt --storage-mode plaintext
  assert_command dfx identity get-principal --identity alice2
  assert_eq "$principal"
  dfx identity export alice2 > export.pem
  assert_command openssl asn1parse -in export.pem
  assert_match ':secp256k1'
}

@test "identity: consistently imports a known seed phrase" {
  echo "display dawn estate night naive stomach receive lock expose boring square boy deposit mistake volume soldier coil rocket match diamond repair opinion action paddle">seed.txt
  assert_command dfx identity import alice --seed-file seed.txt --storage-mode plaintext
  assert_command dfx identity get-principal --identity alice
  assert_eq "qimd7-lqrvx-kdvsm-7zeqn-bgoix-ukjfi-hgmfg-ur2he-odgb2-joms4-nae"
}


-----------------------

/e2e/tests-dfx/identity_encryption.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
  export DFX_CI_USE_PROXY_KEYRING=""
}

teardown() {
  standard_teardown
}

#
# These tests generally only test the case where passwords are set.
# The case without password is tested already in other places, such as identity_command.bash or identity.bash
#

@test "can create and use identity with password" {
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_pw.exp"
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/create_identity_with_password.exp"
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/create_identity_with_invalid_password.exp"
}

@test "wrong password is rejected" {
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_pw.exp"
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/wrong_password_rejected.exp"
}

@test "import and export identity with a password are inverse operations" {
  # key generated using `openssl ecparam -genkey -name secp256k1`
  cat >import.pem <<EOF
-----BEGIN EC PARAMETERS-----
BgUrgQQACg==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHQCAQEEIIPXmSpdZwI5YUwzukz8+GC9fikjMELmdbH4tHcQ9iD2oAcGBSuBBAAK
oUQDQgAEjjBKAxko3RPG8ot7PoeXM7ZHtek2xcbRN/JZVfKKNEnNG4wdnMdpRGyk
37fJkz9WEHR+Wol+nGAuQNnCOIVXdw==
-----END EC PRIVATE KEY-----
EOF
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/import_export_identity_with_password.exp"
  assert_eq "$(cat import.pem)" "$(cat export.pem)"

}

@test "rename identity works on identity with a password" {
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_pw.exp"
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/rename_identity_with_password.exp"
}

@test "remove identity works on identity with a password" {
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_pw.exp"
  assert_command dfx identity remove alice
}

@test "creating/removing identity with --storage-mode password-protected does not attempt to touch keyring" {
  # if any of dfx identity new --storage-mode password-protected or dfx identity remove attempt to touch keyring,
  # then the test will time out because it's waiting for the user to accept/deny access to keyring
  unset DFX_CI_MOCK_KEYRING_LOCATION
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/init_alice_with_storage_mode_pwprotected.exp"
  assert_command dfx identity remove alice
}


-----------------------

/e2e/tests-dfx/import.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

assets="$(dirname "$BATS_TEST_FILENAME")"/../assets

setup() {
    standard_setup

    dfx_new
}

teardown() {
    stop_webserver

    dfx_stop

    standard_teardown
}

test_project_import() {
    DFX_JSON_LOCATION="$1"

    # this test is meant to demonstrate that the various
    assert_command dfx beta project import "$DFX_JSON_LOCATION" --prefix "pfx-" --network-mapping ic=mainnet --all

    jq . dfx.json

    assert_command jq -r '.canisters."pfx-normal-canister".candid' dfx.json
    assert_eq "candid/pfx-normal-canister.did"
    # shellcheck disable=SC2154
    assert_files_eq \
      "${assets}/project-import/project-directory/normal-canister-directory/some-subdirectory/the-candid-filename.did" \
      "candid/pfx-normal-canister.did"

    assert_command jq -r '.canisters."pfx-normal-canister".remote.id.ic' dfx.json
    assert_eq "rrkah-fqaaa-aaaaa-aaaaq-cai"

    assert_command jq -r '.canisters."pfx-sibling".candid' dfx.json
    assert_eq "candid/pfx-sibling.did"
    assert_files_eq \
      "${assets}/project-import/sibling-project/canister/canister/the-sibling-candid-definition.did" \
      "candid/pfx-sibling.did"
}

@test "dfx project import from filesystem" {
    test_project_import "${assets}/project-import/project-directory/dfx.json"
}

@test "dfx project import from url" {
    start_webserver --directory "${assets}/project-import"

    test_project_import "http://localhost:$E2E_WEB_SERVER_PORT/project-directory/dfx.json"
}

test_project_import_specific_canister() {
    LOCATION="$1"

    # this test is meant to demonstrate that the various
    dfx beta project import "$LOCATION" normal-canister

    jq . dfx.json

    assert_command jq -r '.canisters."normal-canister".candid' dfx.json
    assert_eq "candid/normal-canister.did"
    assert_files_eq \
      "${assets}/project-import/project-directory/normal-canister-directory/some-subdirectory/the-candid-filename.did" \
      "candid/normal-canister.did"

    assert_command jq -r '.canisters.sibling.candid' dfx.json
    assert_eq "null"
}

@test "dfx project import specific canister" {
    test_project_import_specific_canister "${assets}/project-import/project-directory/dfx.json"
}

@test "import from url" {
    start_webserver --directory "${assets}/project-import"

    test_project_import_specific_canister "http://localhost:$E2E_WEB_SERVER_PORT/project-directory/dfx.json"
}

@test "project import from filesystem with no canister_ids.json" {
    mkdir www
    cp -R "${assets}/project-import" www/
    rm www/project-import/project-directory/canister_ids.json

    start_webserver --directory "www/project-import"

    assert_command dfx beta project import www/project-import/project-directory/dfx.json --all
}

@test "project import from url with no canister_ids.json" {
    mkdir www
    cp -R "${assets}/project-import" www/
    rm www/project-import/project-directory/canister_ids.json

    start_webserver --directory "www/project-import"

    assert_command dfx beta project import "http://localhost:$E2E_WEB_SERVER_PORT/project-directory/dfx.json" --all
}



-----------------------

/e2e/tests-dfx/info.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "displays the replica port" {
  assert_command_fail dfx info replica-port
  assert_contains "No replica port found"

  dfx_start
  assert_command dfx info replica-port
  if [[ "$USE_POCKETIC" ]]
  then
    assert_eq "$(get_pocketic_port)"
  else
    assert_eq "$(get_replica_port)"
  fi
}

@test "displays the default webserver port for the local shared network" {
  assert_command dfx info webserver-port
  assert_eq "4943"
}

@test "displays the webserver port for a project-specific network" {
  define_project_network
  assert_command dfx info webserver-port
  assert_eq "8000"
}

@test "displays path to networks.json" {
  assert_command dfx info networks-json-path
  assert_eq "$E2E_NETWORKS_JSON"
}

@test "displays the replica revision included in dfx" {
  nix_sources_path="${BATS_TEST_DIRNAME}/../../nix/sources.json"
  expected_rev="$(jq -r '."replica-x86_64-linux".rev' "$nix_sources_path")"

  assert_command dfx info replica-rev
  assert_eq "$expected_rev"
}

@test "displays Candid UI URL" {
  assert_command dfx info candid-ui-url --ic
  # shellcheck disable=SC2154
  assert_eq "https://a4gq6-oaaaa-aaaab-qaa4q-cai.raw.icp0.io/" "$stdout"

  # Before deployment the UI canister does not exist yet
  assert_command_fail dfx info candid-ui-url
  assert_contains "Candid UI not installed on network local."

  dfx_start
  assert_command dfx deploy e2e_project_backend
  assert_command dfx info candid-ui-url  
  assert_eq "http://127.0.0.1:$(dfx info webserver-port)/?canisterId=$(dfx canister id __Candid_UI)"
}

@test "security-policy shows standard/hardened headers with comments" {
  assert_command dfx info security-policy
  assert_contains "Content-Security-Policy" # One of the headers in the standard policy
  assert_contains "X-XSS-Protection" # One of the headers in the standard policy
  assert_contains '"nosniff"' # One of the values in the standard policy
  assert_contains '"same-origin"' # One of the values in the standard policy
  assert_contains "// Notes about the CSP below:" # One of the comment lines in the standard policy, together with `//` to mark it as a comment in json5
  assert_contains "// - We added img-src data: because data: images are used often." # One of the comment lines in the standard policy, together with `//` to mark it as a comment in json5
}

@test "security-policy produces valid json5 headers" {
  dfx_new_frontend
  install_asset assetscanister
  touch src/e2e_project_frontend/assets/thing.json

  dfx_start
  dfx canister create --all

  echo "[
    {
      \"match\": \"**/*\",
      \"headers\": {
        $(dfx info security-policy)
      }
    }
  ]" > src/e2e_project_frontend/assets/.ic-assets.json5
  cat src/e2e_project_frontend/assets/.ic-assets.json5

  # fails if the the above produced invalid json5
  assert_command dfx deploy
}


-----------------------

/e2e/tests-dfx/install.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_assets
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "canister install --upgrade-unchanged upgrades even if the .wasm did not change" {
  dfx_start
  dfx canister create --all
  dfx build

  assert_command dfx canister install --all

  assert_command dfx canister install --all --mode upgrade
  assert_match "Module hash.*is already installed"

  assert_command dfx canister install --all --mode upgrade --upgrade-unchanged
  assert_not_match "Module hash.*is already installed"
}

@test "install fails if no argument is provided" {
  dfx_start
  assert_command_fail dfx canister install
  assert_match "required arguments were not provided"
  assert_match "--all"
}

@test "install succeeds when --all is provided" {
  dfx_start
  dfx canister create --all
  dfx build

  assert_command dfx canister install --all

  assert_match "Installing code for canister e2e_project_backend"
}

@test "install succeeds with network name" {
  dfx_start
  dfx canister create --all
  dfx build

  assert_command dfx canister install --all --network local

  assert_match "Installing code for canister e2e_project_backend"
}

@test "install fails with network name that is not in dfx.json" {
  dfx_start
  dfx canister create --all
  dfx build

  assert_command_fail dfx canister install --all --network nosuch

  assert_match "Network not found.*nosuch"
}

@test "install succeeds with arbitrary wasm" {
  dfx_start
  dfx canister create --all
  wallet="${archive:?}/wallet/0.10.0/wallet.wasm"
  assert_command dfx canister install e2e_project_backend --wasm "$wallet"
  assert_command dfx canister info e2e_project_backend
  assert_match "Module hash: 0x$(sha2sum "$wallet" | head -c 64)"
}

@test "install succeeds with canisterid" {
  dfx_start
  dfx canister create --all
  wallet="${archive:?}/wallet/0.10.0/wallet.wasm"
  CANISTER_ID=$(dfx canister id e2e_project_backend)
  assert_command dfx canister install "$CANISTER_ID" --wasm "$wallet"
  assert_command dfx canister info "$CANISTER_ID"
  assert_match "Module hash: 0x$(sha2sum "$wallet" | head -c 64)"
}

@test "install --all fails with arbitrary wasm" {
  dfx_start
  dfx canister create --all
  assert_command_fail dfx canister install --all --wasm "${archive:?}/wallet/0.10.0/wallet.wasm"
}

@test "install runs post-install tasks" {
  install_asset post_install
  dfx_start

  assert_command dfx canister create --all
  assert_command dfx build

  assert_command dfx canister install postinstall
  assert_match 'hello-file'

  assert_command dfx canister install postinstall_script
  assert_match 'hello-script'

  echo 'return 1' >> postinstall.sh
  assert_command_fail dfx canister install postinstall_script --mode upgrade
  assert_match 'hello-script'
}

@test "post-install tasks run in project root" {
  install_asset post_install
  dfx_start

  assert_command dfx canister create --all
  assert_command dfx build

  cd src/e2e_project_backend

  assert_command dfx canister install postinstall_script
  assert_match 'hello-script'
  assert_match "working directory of post-install script: '.*/working-dir/e2e_project'"
}

@test "post-install tasks receive environment variables" {
  install_asset post_install
  dfx_start
  echo "echo hello \$CANISTER_ID" >> postinstall.sh

  assert_command dfx canister create --all
  assert_command dfx build
  id=$(dfx canister id postinstall_script)

  assert_command dfx canister install --all
  assert_match "hello $id"
  assert_command dfx canister install postinstall_script --mode upgrade
  assert_match "hello $id"

  assert_command dfx deploy
  assert_match "hello $id"
  assert_command dfx deploy postinstall_script
  assert_match "hello $id"
}

@test "post-install tasks discover dependencies" {
  install_asset post_install
  dfx_start
  echo "echo hello \$CANISTER_ID_POSTINSTALL" >> postinstall.sh

  assert_command dfx canister create --all
  assert_command dfx build
  id=$(dfx canister id postinstall)

  assert_command dfx canister install postinstall_script
  assert_match "hello $id"
}

@test "can install gzip wasm" {
  jq '.canisters.e2e_project_backend.gzip=true' dfx.json | sponge dfx.json
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_command dfx canister install --all
  BUILD_HASH="0x$(sha256sum .dfx/local/canisters/e2e_project_backend/e2e_project_backend.wasm.gz | cut -d " " -f 1)"
  ONCHAIN_HASH="$(dfx canister info e2e_project_backend | tail -n 1 | cut -d " " -f 3)"
  assert_eq "$BUILD_HASH" "$ONCHAIN_HASH"
}

@test "can install >2MiB wasm" {
  install_asset large_canister
  dfx_start
  dfx canister create --all
  assert_command dfx build
  assert_command dfx canister install --all
  assert_command dfx canister info large
  HASH="$(sha256sum .dfx/local/canisters/large/large.wasm | head -c 64)"
  assert_match "Module hash: 0x$HASH"
}

@test "can install large wasm with non-empty chunk store" {
  install_asset large_canister
  dfx_start
  dfx canister create --all
  CANISTER_ID=$(dfx canister id large)
  assert_command dfx canister call aaaaa-aa upload_chunk "(
  record {
    chunk = blob \"\\01\\02\";
    canister_id = principal \"$CANISTER_ID\";
  },
)"
  assert_command dfx build
  assert_command dfx canister install --all
  assert_command dfx canister info large
  HASH="$(sha256sum .dfx/local/canisters/large/large.wasm | head -c 64)"
  assert_match "Module hash: 0x$HASH"
}

@test "--mode=auto selects install or upgrade automatically" {
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_command dfx build e2e_project_backend
  assert_command dfx canister install e2e_project_backend --mode auto
  assert_command dfx canister call e2e_project_backend greet dfx
  assert_command dfx canister install e2e_project_backend --mode auto --upgrade-unchanged
  assert_command dfx canister call e2e_project_backend greet dfx
}

@test "-y skips compat check" {
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_command dfx build e2e_project_backend
  assert_command dfx canister install e2e_project_backend
  assert_command timeout -s9 20s dfx canister install e2e_project_backend --mode reinstall -y # if -y does not work, hangs without stdin
}

@test "--no-asset-upgrade skips asset upgrade" {
  dfx_start
  use_asset_wasm 0.12.1
  dfx deploy
  assert_command dfx canister info e2e_project_frontend
  assert_contains db07e7e24f6f8ddf53c33a610713259a7c1eb71c270b819ebd311e2d223267f0
  use_default_asset_wasm
  assert_command dfx canister install e2e_project_frontend --mode upgrade --no-asset-upgrade
  assert_command dfx canister info e2e_project_frontend
  assert_contains db07e7e24f6f8ddf53c33a610713259a7c1eb71c270b819ebd311e2d223267f0
}

@test "installing one canister with an argument succeeds" {
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_command dfx build e2e_project_backend
  assert_command dfx canister install e2e_project_backend --argument '()'
}

@test "installing one canister specifying raw argument succeeds" {
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_command dfx build e2e_project_backend
  assert_command dfx canister install e2e_project_backend --argument '4449444c0000' --argument-type raw
}

@test "installing with an argument in a file succeeds" {
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_command dfx build e2e_project_backend
  TMPFILE="$(mktemp)"
  echo '()' >"$TMPFILE"
  assert_command dfx canister install e2e_project_backend --argument-file "$TMPFILE"
}

@test "installing with an argument on stdin succeeds" {
  dfx_start
  assert_command dfx canister create e2e_project_backend
  assert_command dfx build e2e_project_backend
  TMPFILE="$(mktemp)"
  echo '()' >"$TMPFILE"
  assert_command dfx canister install e2e_project_backend --argument-file - <"$TMPFILE"
}

@test "installing multiple canisters with arguments fails" {
  assert_command_fail dfx canister install --all --argument '()'
  assert_contains "error: the argument '--all' cannot be used with '--argument <ARGUMENT>'"
}

@test "remind to build before install" {
  dfx_start
  dfx canister create --all
  assert_command_fail dfx canister install e2e_project_backend
  assert_contains "The canister must be built before install. Please run \`dfx build\`."
}

@test "install succeeds if init_arg is defined in dfx.json" {
  install_asset deploy_deps
  dfx_start
  jq '.canisters.dependency.init_arg="(\"dfx\")"' dfx.json | sponge dfx.json

  dfx canister create dependency
  dfx build dependency
  assert_command dfx canister install dependency
  assert_command dfx canister call dependency greet
  assert_match "Hello, dfx!"

  assert_command dfx canister install dependency --mode reinstall --yes --argument '("icp")'
  assert_contains "Canister 'dependency' has init_arg/init_arg_file in dfx.json: (\"dfx\"),"
  assert_contains "which is different from the one specified in the command line: (\"icp\")."
  assert_contains "The command line value will be used."
  assert_command dfx canister call dependency greet
  assert_match "Hello, icp!"
}

@test "install succeeds if init_arg_file is defined in dfx.json" {
  install_asset deploy_deps
  dfx_start
  mkdir arg-files
  echo '("dfx")' >> arg-files/args.txt
  jq '.canisters.dependency.init_arg_file="arg-files/args.txt"' dfx.json | sponge dfx.json

  dfx canister create dependency
  dfx build dependency

  # The following commands will be run in this sub-directory, it verifies that the init_arg_file is relative to the dfx.json file
  cd arg-files
  assert_command dfx canister install dependency
  assert_command dfx canister call dependency greet
  assert_match "Hello, dfx!"
}

@test "install fails if both init_arg and init_arg_file are defined in dfx.json" {
  install_asset deploy_deps
  dfx_start
  echo '("dfx")' >> args.txt
  jq '.canisters.dependency.init_arg="(\"dfx\")"' dfx.json | sponge dfx.json
  jq '.canisters.dependency.init_arg_file="args.txt"' dfx.json | sponge dfx.json

  dfx canister create dependency
  dfx build dependency
  assert_command_fail dfx canister install dependency
  assert_contains "At most one of the fields 'init_arg' and 'init_arg_file' should be defined in \`dfx.json\`.
Please remove one of them or leave both undefined."
}

@test "install succeeds when specify canister id and wasm, in dir without dfx.json" {
  dfx_start

  dfx canister create --all
  CANISTER_ID=$(dfx canister id e2e_project_backend)
  dfx build
  rm dfx.json
  assert_command dfx canister install "$CANISTER_ID" --wasm .dfx/local/canisters/e2e_project_backend/e2e_project_backend.wasm
}

@test "repeated install wasm" {
  install_asset custom_canister
  install_asset wasm/identity
  dfx_start
  dfx deploy
  for _ in {1..50}
  do
  echo yes | dfx canister install --mode=reinstall custom
  done
}

@test "specify upgrade options (skip_pre_upgrade, wasm_memory_persistence)" {
  dfx_start
  dfx canister create e2e_project_backend
  dfx build e2e_project_backend

  # The canister is not installed yet and the mode is 'auto'.
  # The actual InstallMode will be 'install'.
  # In this case, the provided upgrade options are just hint which doesn't take effect.
  assert_command dfx canister install e2e_project_backend --mode auto --skip-pre-upgrade

  assert_command dfx canister install e2e_project_backend --mode auto --wasm-memory-persistence keep
  assert_command dfx canister install e2e_project_backend --mode auto --wasm-memory-persistence replace
  assert_command dfx canister install e2e_project_backend --mode auto --skip-pre-upgrade --wasm-memory-persistence keep

  assert_command dfx canister install e2e_project_backend --mode upgrade --skip-pre-upgrade
  assert_command dfx canister install e2e_project_backend --mode upgrade --wasm-memory-persistence keep
  assert_command dfx canister install e2e_project_backend --mode upgrade --wasm-memory-persistence replace
  assert_command dfx canister install e2e_project_backend --mode upgrade --skip-pre-upgrade --wasm-memory-persistence keep

  assert_command_fail dfx canister install e2e_project_backend --mode install --skip-pre-upgrade
  assert_contains "--skip-pre-upgrade and --wasm-memory-persistence can only be used with mode 'upgrade' or 'auto'."
  assert_command_fail dfx canister install e2e_project_backend --mode reinstall --wasm-memory-persistence keep
  assert_contains "--skip-pre-upgrade and --wasm-memory-persistence can only be used with mode 'upgrade' or 'auto'."
}


-----------------------

/e2e/tests-dfx/ledger.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
  install_asset ledger
  install_shared_asset subnet_type/shared_network_settings/system

  dfx identity import --storage-mode plaintext alice alice.pem
  dfx identity import --storage-mode plaintext bob bob.pem

  dfx_start_for_nns_install

  dfx extension install nns --version 0.4.3
  dfx nns install --ledger-accounts 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752 22ca7edac648b814e81d7946e8bacea99280e07c5f51a04ba7a38009d8ad8e89 5a94fe181e9d411c58726cb87cbf2d016241b6c350bc3330e4869ca76e54ecbc
}

teardown() {
  dfx_stop

  standard_teardown
}

current_time_nanoseconds() {
  echo "$(date +%s)"000000000
}

@test "ledger account-id" {
  dfx identity use alice
  assert_command dfx ledger account-id
  assert_match 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752

  assert_command dfx ledger account-id --of-principal fg7gi-vyaaa-aaaal-qadca-cai
  assert_match a014842f64a22e59887162a79c7ca7eb02553250704780ec4d954f12d0ea0b18

  ALICE_PRINCIPAL="$(dfx identity get-principal)"
  assert_command dfx ledger account-id --of-canister qvhpv-4qaaa-aaaaa-aaagq-cai --subaccount-from-principal "${ALICE_PRINCIPAL}"
  # value obtained by running `dfx --identity alice canister call --ic qvhpv-4qaaa-aaaaa-aaagq-cai get_payment_subaccount`
  assert_match 7afe37275178a26c463a6609825748ba3ed3572f7f308917f96f9f7be20e9d01

  # --of-canister accepts both canister alias and canister principal
  assert_command dfx canister create dummy_canister
  assert_command dfx ledger account-id --of-canister "$(dfx canister id dummy_canister)"
  assert_eq "$(dfx ledger account-id --of-canister dummy_canister)"
}

@test "ledger balance & transfer" {
  dfx identity use alice
  assert_command dfx ledger account-id
  assert_eq 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752

  assert_command dfx ledger balance
  assert_eq "1000000000.00000000 ICP"

  assert_command dfx ledger transfer --amount 100 --memo 1 22ca7edac648b814e81d7946e8bacea99280e07c5f51a04ba7a38009d8ad8e89 # to bob
  assert_contains "Transfer sent at block height"

  # The sender(alice) paid transaction fee which is 0.0001 ICP
  assert_command dfx ledger balance
  assert_eq "999999899.99990000 ICP"

  dfx identity use bob
  assert_command dfx ledger account-id
  assert_eq 22ca7edac648b814e81d7946e8bacea99280e07c5f51a04ba7a38009d8ad8e89

  assert_command dfx ledger balance
  assert_eq "1000000100.00000000 ICP"

  assert_command dfx ledger transfer --icp 100 --e8s 1 --memo 2 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752 # to alice
  assert_contains "Transfer sent at block height"

  # The sender(bob) paid transaction fee which is 0.0001 ICP
  # 10100 - 100 - 0.0001 - 0.00000001 = 9999.99989999
  assert_command dfx ledger balance
  assert_eq "999999999.99989999 ICP"

  # Transaction Deduplication
  t=$(current_time_nanoseconds)

  assert_command dfx ledger transfer --icp 1 --memo 1 --created-at-time "$t" 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752
  # shellcheck disable=SC2154
  block_height=$(echo "$stdout" | sed '1q' | sed 's/Transfer sent at block height //')
  # shellcheck disable=SC2154
  assert_eq "Transfer sent at block height $block_height" "$stdout"

  assert_command dfx ledger transfer --icp 1 --memo 1 --created-at-time $((t+1)) 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752
  # shellcheck disable=SC2154
  assert_contains "Transfer sent at block height" "$stdout"
  # shellcheck disable=SC2154
  assert_not_contains "Transfer sent at block height $block_height" "$stdout"

  assert_command dfx ledger transfer --icp 1 --memo 1 --created-at-time "$t" 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752
  # shellcheck disable=SC2154
  assert_eq "transaction is a duplicate of another transaction in block $block_height" "$stderr"
  assert_eq "Transfer sent at block height $block_height" "$stdout"

  assert_command dfx ledger transfer --icp 1 --memo 2 --created-at-time "$t" 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752
  # shellcheck disable=SC2154
  assert_contains "Transfer sent at block height" "$stdout"
  # shellcheck disable=SC2154
  assert_not_contains "Transfer sent at block height $block_height" "$stdout"

}

@test "ledger subaccounts" {
  subacct=000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
  assert_command dfx ledger account-id --identity bob --subaccount "$subacct"
  assert_match 5a94fe181e9d411c58726cb87cbf2d016241b6c350bc3330e4869ca76e54ecbc

  dfx identity use alice
  assert_command dfx ledger balance
  assert_match "1000000000.00000000 ICP"
  assert_command dfx ledger transfer --amount 100 --memo 1 5a94fe181e9d411c58726cb87cbf2d016241b6c350bc3330e4869ca76e54ecbc # to bob+subacct
  assert_match "Transfer sent at block height"
  assert_command dfx ledger balance
  assert_match "999999899.99990000 ICP"

  dfx identity use bob
  assert_command dfx ledger balance
  assert_match "1000000000.00000000 ICP"
  assert_command dfx ledger balance --subaccount "$subacct"
  assert_match "1000000100.00000000 ICP"

  assert_command dfx ledger transfer --amount 100 --memo 2 345f723e9e619934daac6ae0f4be13a7b0ba57d6a608e511a00fd0ded5866752 --from-subaccount "$subacct" # to alice
  assert_match "Transfer sent at block height"
  assert_command dfx ledger balance
  assert_match "1000000000.00000000 ICP"
  assert_command dfx ledger balance --subaccount "$subacct"
  assert_match "999999999.99990000 ICP"
  assert_command dfx ledger balance --identity alice
  assert_match "999999999.99990000 ICP"
}
tc_to_num() {
  if [[ $1 =~ T ]]; then
    echo "${1%%[^0-9]*}000000000000"
  else
    echo "${1%%[^0-9]*}"
  fi
}

@test "ledger top-up" {
  dfx identity use alice
  assert_command dfx ledger balance
  assert_match "1000000000.00000000 ICP"

  wallet=$(dfx identity get-wallet)
  balance=$(tc_to_num "$(dfx wallet balance)")

  assert_command dfx ledger top-up "$wallet" --icp 5
  assert_match "Canister was topped up with 617283500000000 cycles"
  balance_now=$(tc_to_num "$(dfx wallet balance)")

  (( balance_now - balance > 600000000000000 ))

  # Transaction Deduplication
  t=$(current_time_nanoseconds)

  assert_command dfx ledger top-up "$wallet" --icp 5 --created-at-time "$t"

  # shellcheck disable=SC2154
  block_height=$(echo "$stdout" | sed '1q' | sed 's/Transfer sent at block height //')

  # shellcheck disable=SC2154
  assert_match "Transfer sent at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Using transfer at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Canister was topped up with" "$stdout"

  assert_command dfx ledger top-up "$wallet" --icp 5 --created-at-time $((t+1))
  # shellcheck disable=SC2154
  assert_match "Transfer sent at block height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Using transfer at block height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Canister was topped up with" "$stdout"
  # shellcheck disable=SC2154
  assert_not_match "Transfer sent at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_not_match "Using transfer at block height $block_height" "$stdout"

  assert_command dfx ledger top-up "$wallet" --icp 5 --created-at-time "$t"
  # shellcheck disable=SC2154
  assert_contains "transaction is a duplicate of another transaction in block $block_height" "$stderr"
  # shellcheck disable=SC2154
  assert_contains "Transfer sent at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_contains "Using transfer at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_contains "Canister was topped up with" "$stdout"

  # Top up canister by name instead of principal
  dfx_new
  assert_command dfx canister create e2e_project_backend
  assert_command dfx ledger top-up e2e_project_backend --amount 5
  assert_contains "Canister was topped up with 617283500000000 cycles"
}

@test "ledger create-canister" {
  dfx identity use alice
  assert_command dfx ledger create-canister --amount=100 --subnet-type "type1" "$(dfx identity get-principal)"
  assert_match "Transfer sent at block height"
  assert_match "Refunded at block height"
  assert_match "with message: Provided subnet type type1 does not exist"

  SUBNET_ID="5kdm2-62fc6-fwnja-hutkz-ycsnm-4z33i-woh43-4cenu-ev7mi-gii6t-4ae" # a random, valid principal
  assert_command dfx ledger create-canister --amount=100 --subnet "$SUBNET_ID" "$(dfx identity get-principal)"
  assert_match "Transfer sent at block height"
  assert_match "Refunded at block height"
  assert_match "with message: Subnet $SUBNET_ID does not exist"

  # Verify that registry is queried before sending any ICP to CMC
  CANISTER_ID="2vxsx-fae" # anonymous principal
  balance=$(dfx ledger balance)
  assert_command_fail dfx ledger create-canister --amount=100 --next-to "$CANISTER_ID" "$(dfx identity get-principal)"
  # TODO: assert error message once registry is fixed
  assert_eq "$balance" "$(dfx ledger balance)"

  # Verify that creating a canister under a different principal's control properly sets ownership
  CONTROLLER_PRINCIPAL="$(dfx --identity default identity get-principal)"
  assert_command dfx ledger create-canister --amount=100 "$CONTROLLER_PRINCIPAL"
  echo "created with: $stdout"
  created_canister_id=$(echo "$stdout" | sed '3q;d' | sed 's/Canister created with id: //;s/"//g')
  assert_command dfx canister info "$created_canister_id"
  assert_contains "Controllers: $CONTROLLER_PRINCIPAL"
  assert_not_contains "$(dfx identity get-principal)"

  # Transaction Deduplication
  t=$(current_time_nanoseconds)

  assert_command dfx ledger create-canister --amount=100 --created-at-time "$t" "$(dfx identity get-principal)"
  # shellcheck disable=SC2154
  block_height=$(echo "$stdout" | sed '1q' | sed 's/Transfer sent at block height //')
  # shellcheck disable=SC2154
  created_canister_id=$(echo "$stdout" | sed '3q;d' | sed 's/Canister created with id: //')

  # shellcheck disable=SC2154
  assert_match "Transfer sent at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Using transfer at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Canister created with id: $created_canister_id" "$stdout"

  assert_command dfx ledger create-canister --amount=100 --created-at-time $((t+1)) "$(dfx identity get-principal)"
  # shellcheck disable=SC2154
  assert_match "Transfer sent at block height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Using transfer at block height" "$stdout"
  # shellcheck disable=SC2154
  assert_match "Canister created with id:" "$stdout"
  # shellcheck disable=SC2154
  assert_not_match "Transfer sent at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_not_match "Using transfer at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_not_match "Canister created with id: $created_canister_id" "$stdout"

  assert_command dfx ledger create-canister --amount=100 --created-at-time "$t" "$(dfx identity get-principal)"
  # shellcheck disable=SC2154
  assert_contains "transaction is a duplicate of another transaction in block $block_height" "$stderr"
  # shellcheck disable=SC2154
  assert_contains "Transfer sent at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_contains "Using transfer at block height $block_height" "$stdout"
  # shellcheck disable=SC2154
  assert_contains "Canister created with id: $created_canister_id" "$stdout"

}

@test "ledger show-subnet-types" {
  install_asset cmc

  dfx deploy cmc

  CANISTER_ID=$(dfx canister id cmc)

  assert_command dfx ledger show-subnet-types --cycles-minting-canister-id "$CANISTER_ID"
  assert_eq '["type1", "type2"]'
}


-----------------------

/e2e/tests-dfx/metadata.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "custom canister metadata rules" {
  install_asset metadata/custom
  install_asset wasm/identity

  dfx_start
  dfx deploy

  echo "leaves existing metadata in a custom canister with no metadata settings"
  dfx canister metadata --identity anonymous custom_with_default_metadata candid:service >metadata.txt
  diff main.did metadata.txt

  echo "adds candid:service public metadata from candid field if a metadata entry exists"
  dfx canister metadata --identity anonymous custom_with_standard_candid_service_metadata candid:service >metadata.txt
  diff custom_with_standard_candid_service_metadata.did metadata.txt

  echo "adds candid:service metadata from candid field with private visibility per metadata entry"
  assert_command_fail dfx canister metadata --identity anonymous custom_with_private_candid_service_metadata candid:service >metadata.txt
  dfx canister metadata custom_with_private_candid_service_metadata candid:service >metadata.txt
  diff custom_with_private_candid_service_metadata.did metadata.txt
}

@test "rust canister metadata rules" {
  rustup default stable
  rustup target add wasm32-unknown-unknown

  dfx_new_rust

  dfx_start
  dfx deploy

  echo "adds public candid:service metadata to a default rust canister"
  dfx canister metadata --identity anonymous e2e_project_backend candid:service >metadata.txt
  diff src/e2e_project_backend/e2e_project_backend.did metadata.txt

  echo "adds private candid:service metadata if so configured"
  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[0].name="candid:service"|.canisters.e2e_project_backend.metadata[0].visibility="private"' dfx.json | sponge dfx.json
  dfx deploy
  assert_command_fail dfx canister metadata --identity anonymous e2e_project_backend candid:service
  dfx canister metadata e2e_project_backend candid:service >metadata.txt
  diff src/e2e_project_backend/e2e_project_backend.did metadata.txt
}

@test "motoko canister metadata rules" {
  dfx_new
  dfx_start
  install_asset metadata/motoko
  dfx canister create --all

  echo "permits specification of a replacement candid definition, if it is a valid subtype"
  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  assert_command dfx build
  find . -name '*.did'
  jq '.canisters.e2e_project_backend.metadata[0].name="candid:service"|.canisters.e2e_project_backend.metadata[0].path="valid_subtype.did"' dfx.json | sponge dfx.json
  dfx build

  echo "reports an error if a specified candid:service metadata is not a valid subtype for the canister"
  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[0].name="candid:service"|.canisters.e2e_project_backend.metadata[0].path="not_subtype_rename.did"' dfx.json | sponge dfx.json
  assert_command_fail dfx build
  assert_match "Method new_method is only in the expected type"

  echo "reports an error if a specified candid:service metadata is not a valid subtype for the canister"
  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[0].name="candid:service"|.canisters.e2e_project_backend.metadata[0].path="not_subtype_numbertype.did"' dfx.json | sponge dfx.json
  assert_command_fail dfx build
  assert_match "int is not a subtype of nat"


  echo "adds private candid:service metadata if so configured"
  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[0].name="candid:service"|.canisters.e2e_project_backend.metadata[0].visibility="private"' dfx.json | sponge dfx.json
  dfx deploy
  assert_command_fail dfx canister metadata --identity anonymous e2e_project_backend candid:service
  dfx canister metadata e2e_project_backend candid:service >metadata.txt
  diff .dfx/local/canisters/e2e_project_backend/e2e_project_backend.did metadata.txt


  echo "adds public candid:service metadata to a default motoko canister"
  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  dfx deploy
  dfx canister metadata --identity anonymous e2e_project_backend candid:service >metadata.txt
  diff .dfx/local/canisters/e2e_project_backend/e2e_project_backend.did metadata.txt
}

@test "adds arbitrary metadata to a motoko canister" {
  dfx_new
  dfx_start
  install_asset metadata/motoko
  dfx canister create --all

  echo "adds public arbitrary metadata to a default motoko canister"
  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[0].name="arbitrary"|.canisters.e2e_project_backend.metadata[0].path="arbitrary-metadata.txt"' dfx.json | sponge dfx.json
  echo "can be anything" >arbitrary-metadata.txt
  dfx deploy
  dfx canister metadata --identity anonymous e2e_project_backend arbitrary >from-canister.txt
  diff arbitrary-metadata.txt from-canister.txt

  # with private visibility
  jq '.canisters.e2e_project_backend.metadata[0].visibility="private"' dfx.json | sponge dfx.json
  dfx deploy
  assert_command_fail dfx canister metadata --identity anonymous e2e_project_backend arbitrary
  dfx canister metadata e2e_project_backend arbitrary >from-canister.txt
  diff arbitrary-metadata.txt from-canister.txt
}

@test "uses the first metadata definition for name and network" {
  dfx_new
  dfx_start
  install_asset metadata/motoko
  dfx canister create --all

  jq 'del(.canisters.e2e_project_backend.metadata)' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[0].name="multiple"|.canisters.e2e_project_backend.metadata[0].path="empty-networks-matches-nothing.txt"|.canisters.e2e_project_backend.metadata[0].networks=[]' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[1].name="multiple"|.canisters.e2e_project_backend.metadata[1].path="different-network-no-match.txt"|.canisters.e2e_project_backend.metadata[1].networks=["ic"]' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[2].name="multiple"|.canisters.e2e_project_backend.metadata[2].path="first-match-chosen.txt"' dfx.json | sponge dfx.json
  jq '.canisters.e2e_project_backend.metadata[3].name="multiple"|.canisters.e2e_project_backend.metadata[3].path="earlier-match-ignored.txt"' dfx.json | sponge dfx.json
  echo "dfx will install this file" >first-match-chosen.txt
  dfx deploy
  dfx canister metadata --identity anonymous e2e_project_backend multiple >from-canister.txt
  diff first-match-chosen.txt from-canister.txt
}

@test "can add metadata to a compressed canister" {
  dfx_start
  install_asset gzip
  install_asset wasm/identity
  jq '.canisters.gzipped.metadata[0].name="arbitrary"|.canisters.gzipped.metadata[0].content="arbitrary content"' dfx.json | sponge dfx.json

  assert_command dfx deploy
  assert_command dfx canister metadata gzipped arbitrary
  assert_eq "$output" "arbitrary content"
}

@test "existence of build steps do not control custom canister metadata" {
  install_asset prebuilt_custom_canister
  install_asset wasm/identity

  dfx_start
  dfx deploy

  # this canister has a build step, which doesn't matter: dfx leaves the candid metadata
  dfx canister metadata custom_with_build_step candid:service >from_canister.txt
  diff main.did from_canister.txt

  # this canister doesn't have a build step, so dfx leaves the candid metadata as-is
  dfx canister metadata prebuilt_custom_no_build candid:service >from_canister.txt
  diff main.did from_canister.txt

  # this canister has a build step, but it is an empty string, so dfx leaves the candid:service metadata as-is
  dfx canister metadata prebuilt_custom_blank_build candid:service >from_canister.txt
  diff main.did from_canister.txt

  # this canister has a build step, but it is an empty array, so dfx leaves the candid:service metadata as-is
  dfx canister metadata prebuilt_custom_empty_build candid:service >from_canister.txt
  diff main.did from_canister.txt

  # this canister has a local import in did file, the metadata should flatten the definitions
  assert_command dfx canister metadata prebuilt_local_import candid:service
  assert_eq "service : { getCanisterId : () -> (principal) query }"
}

@test "can read canister metadata from replica" {
  dfx_new hello
  dfx_start

  assert_command dfx deploy

  dfx canister metadata hello_backend candid:service >metadata.txt
  assert_command diff .dfx/local/canisters/hello_backend/hello_backend.did ./metadata.txt
}

@test "asset canister provides candid:service metadata" {
  dfx_new_assets hello
  dfx_start

  assert_command dfx deploy
  REPO_ROOT=${BATS_TEST_DIRNAME}/../../

  dfx canister metadata hello_frontend candid:service >candid_service_metadata.txt
  assert_command diff "$REPO_ROOT/src/distributed/assetstorage.did" ./candid_service_metadata.txt
}

# shellcheck disable=SC2154
@test "can generate tech_stack field of the standardized dfx metadata" {
  dfx_new
  install_asset metadata/tech_stack

  dfx_start

  # a doesn't define the tech_stack object, the dfx metadata is not added
  echo -ne '\0asm\x01\0\0\0' > a.wasm
  assert_command dfx deploy a
  assert_command_fail dfx canister metadata a dfx

  # b defines one cdk item
  assert_command dfx deploy b
  assert_command dfx canister metadata b dfx
  echo "$stdout" > b.json
  assert_command jq -r '.tech_stack | keys[]' b.json
  assert_eq "cdk" # only cdk is defined
  assert_command jq -r '.tech_stack.cdk | keys[]' b.json
  assert_eq "ic-cdk"

  # c defines language->rust version
  assert_command dfx deploy c
  assert_command dfx canister metadata c dfx
  echo "$stdout" > c.json
  assert_command jq -r '.tech_stack.language.rust.version' c.json
  assert_eq "1.75.0"

  # d defines language->rust version with value_command
  assert_command dfx deploy d
  assert_command dfx canister metadata d dfx
  echo "$stdout" > d.json
  assert_command jq -r '.tech_stack.language.rust.version' d.json
  assert_eq "1.75.0"

  # e defines multiple lib items
  assert_command dfx deploy e
  assert_command dfx canister metadata e dfx
  echo "$stdout" > e.json
  assert_command jq -r '.tech_stack.lib | keys[]' e.json
  assert_eq "ic-cdk-timers
ic-stable-structures"

  # f defines all 5 categories
  assert_command dfx deploy f
  assert_command dfx canister metadata f dfx
  echo "$stdout" > f.json
  assert_command jq -r '.tech_stack.cdk | keys[]' f.json
  assert_eq "ic-cdk"
  assert_command jq -r '.tech_stack.language | keys[]' f.json
  assert_eq "rust"
  assert_command jq -r '.tech_stack.lib | keys[]' f.json
  assert_eq "ic-cdk-timers"
  assert_command jq -r '.tech_stack.tool | keys[]' f.json
  assert_eq "dfx"
  assert_command jq -r '.tech_stack.other | keys[]' f.json
  assert_eq "bitcoin"

  # g defines a value_command that is a local file without "./" prefix and the file name contains whitespace
  assert_command dfx deploy g
  assert_command dfx canister metadata g dfx
  echo "$stdout" > g.json
  assert_command jq -r '.tech_stack.language.rust.version' g.json
  assert_eq "1.75.0"

  # h defines a value_command that is a local command(prefix "./") contains whitespace
  assert_command dfx deploy h
  assert_command dfx canister metadata h dfx
  echo "$stdout" > h.json
  assert_command jq -r '.tech_stack.language.rust.version' h.json
  assert_eq "1.75.0"

  # i defines a value_command that fails
  assert_command_fail dfx deploy i
  assert_contains "Failed to run the value_command: language->rust->version."

  # j defines a value_command that returns a non-valid string
  echo -e "\xc3\x28" > invalid_utf8.txt
  assert_command_fail dfx deploy j
  assert_contains "The value_command didn't return a valid UTF-8 string: language->rust->version."

  # TODO: remove this when we have motoko extension
  # k is a motoko canister which doesn't define tech_stack
  # tech_stack->language->motoko is added by default
  assert_command dfx deploy k
  assert_command dfx canister metadata k dfx
  echo "$stdout" > k.json
  assert_command jq -r '.tech_stack.language | keys[]' k.json
  assert_eq "motoko"
}

# TODO: remove this when we have rust extension
# shellcheck disable=SC2154
@test "rust canister set default tech_stack" {
  dfx_new_rust
  dfx_start
  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister metadata e2e_project_backend dfx
  echo "$stdout" > e2e_project_backend.json
  assert_command jq -r '.tech_stack.language.rust | keys[]' e2e_project_backend.json
  assert_eq "version"
  assert_command jq -r '.tech_stack.cdk."ic-cdk" | keys[]' e2e_project_backend.json
  assert_eq "version"
}

# shellcheck disable=SC2154
@test "tech stack value generation uses project root as working directory" {
  dfx_new
  install_asset metadata/tech_stack

  dfx_start


  # m exposes other->command->working-directory

  cd src/e2e_project_backend || exit
  assert_command dfx deploy m
  assert_command dfx canister metadata m dfx
  echo "$stdout" > m.json
  assert_command jq -r '.tech_stack.other.command.cwd' m.json
  assert_match ".*/working-dir/e2e_project$"
}


-----------------------

/e2e/tests-dfx/migrate.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "detects the wallet being outdated" {
  use_wallet_wasm 0.7.2
  dfx_start
  WALLET=$(dfx identity get-wallet)
  use_wallet_wasm 0.10.0
  assert_command dfx diagnose
  assert_match "dfx wallet upgrade"
  assert_command_fail dfx canister call "$WALLET" wallet_balance128
  assert_command dfx fix
  assert_command dfx canister call "$WALLET" wallet_balance128
}

@test "detects the wallet being the sole controller" {
  dfx_start
  dfx canister create e2e_project_backend --controller "$(dfx identity get-wallet)" --no-wallet
  dfx build e2e_project_backend
  assert_command dfx diagnose
  assert_match "dfx canister update-settings"
  assert_command_fail dfx canister install e2e_project_backend
  assert_command dfx fix
  assert_command dfx canister install e2e_project_backend
}


-----------------------

/e2e/tests-dfx/mode_reinstall.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new_assets hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "install --mode=reinstall --all fails" {
  dfx_start
  assert_command_fail dfx canister install --mode=reinstall --all

  assert_match "The --mode=reinstall is only valid when specifying a single canister, because reinstallation destroys all data in the canister."
}

@test "install --mode=reinstall fails if no canister is provided" {
  # This fails because clap protects against it.

  dfx_start
  assert_command_fail dfx canister install --mode=reinstall
  assert_match \
"error: the following required arguments were not provided:
  --all"
}

@test "reinstall succeeds when a canister name is provided" {
  dfx_start
  dfx deploy

  # if the pipe is alone with assert_command, $stdout, $stderr etc will not be available,
  # so all the assert_match calls will fail.  http://mywiki.wooledge.org/BashFAQ/024
  echo yes | (
    assert_command dfx canister install --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"
    assert_match "Reinstalling code for canister hello_backend"
  )
}

@test "install --mode=reinstall refused if not approved" {
  dfx_start
  dfx deploy

  echo no | (
    assert_command_fail dfx canister install --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"

    assert_not_match "Installing code for canister"
    assert_contains "Refusing to install canister without approval"
    assert_contains "User declined consent"
  )
}

@test "deploy --mode=reinstall fails if no canister name specified" {
  dfx_start
  assert_command_fail dfx deploy --mode=reinstall

  assert_match "The --mode=reinstall is only valid when deploying a single canister, because reinstallation destroys all data in the canister."
}

@test "deploy --mode=reinstall succeeds when a canister name is provided" {
  dfx_start
  dfx deploy

  # if the pipe is alone with assert_command, $stdout, $stderr etc will not be available,
  # so all the assert_match calls will fail.  http://mywiki.wooledge.org/BashFAQ/024
  echo yes | (
    assert_command dfx deploy --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"
    assert_match "Reinstalling code for canister hello_backend"
  )
}

@test "deploy --mode=reinstall refused if not approved" {
  dfx_start
  dfx deploy

  echo no | (
    assert_command_fail dfx deploy --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"

    assert_not_match "Installing code for canister"
    assert_contains "Refusing to install canister without approval"
    assert_contains "User declined consent"
  )
}

@test "deploy --mode=reinstall does not reinstall dependencies" {
  dfx_start
  install_asset counter
  dfx deploy

  assert_command dfx canister call hello_backend read
  assert_eq "(0 : nat)"

  assert_command dfx canister call hello_backend inc
  assert_eq "()"

  assert_command dfx canister call hello_backend read
  assert_eq "(1 : nat)"

  dfx canister call hello_backend inc
  assert_command dfx canister call hello_backend read
  assert_eq "(2 : nat)"


  # if the pipe is alone with assert_command, $stdout, $stderr etc will not be available,
  # so all the assert_match calls will fail.  http://mywiki.wooledge.org/BashFAQ/024
  echo "yes" | (
    assert_command dfx deploy --mode=reinstall hello_frontend

    assert_match "You are about to reinstall the hello_frontend canister."
    assert_not_match "You are about to reinstall the hello_backend canister."
    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"
    assert_match "Reinstalling code for canister hello_frontend,"
  )

  # the hello_backend canister should not have been upgraded (which would reset the non-stable var)
  assert_command dfx canister call hello_backend read
  assert_eq "(2 : nat)"
}

@test "confirmation dialogue accepts multiple forms of 'yes'" {
  dfx_start
  dfx deploy

  # if the pipe is alone with assert_command, $stdout, $stderr etc will not be available,
  # so all the assert_match calls will fail.  http://mywiki.wooledge.org/BashFAQ/024
  echo yes | (
    assert_command dfx deploy --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"
    assert_match "Reinstalling code for canister hello_backend"
  )
  echo y | (
    assert_command dfx deploy --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"
    assert_match "Reinstalling code for canister hello_backend"
  )
  echo YES | (
    assert_command dfx deploy --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"
    assert_match "Reinstalling code for canister hello_backend"
  )
  echo YeS | (
    assert_command dfx deploy --mode=reinstall hello_backend

    assert_match "YOU WILL LOSE ALL DATA IN THE CANISTER"
    assert_match "Reinstalling code for canister hello_backend"
  )
}


-----------------------

/e2e/tests-dfx/network.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx identity new --storage-mode plaintext test_id
  dfx identity use test_id
  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "create with wallet stores canister ids for default-persistent networks in canister_ids.json" {
  dfx_start
  setup_actuallylocal_shared_network
  dfx_set_wallet
  dfx_set_wallet

  dfx canister create --all --network actuallylocal

  # canister creates writes to a spinner (stderr), not stdout
  assert_command dfx canister id e2e_project_backend --network actuallylocal
  assert_match "$(jq -r .e2e_project_backend.actuallylocal <canister_ids.json)"
}

@test "create with wallet stores canister ids for configured-ephemeral networks in canister_ids.json" {
  dfx_start

  setup_actuallylocal_shared_network
  jq '.actuallylocal.type="ephemeral"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  dfx_set_wallet

  dfx canister create --all --network actuallylocal

  # canister creates writes to a spinner (stderr), not stdout
  assert_command dfx canister id e2e_project_backend --network actuallylocal
  assert_match "$(jq -r .e2e_project_backend.actuallylocal .dfx/actuallylocal/canister_ids.json)"
}

@test "create stores canister ids for default-ephemeral local networks in .dfx/{network}canister_ids.json" {
  dfx_start

  assert_command dfx canister create --all --network local

  # canister creates writes to a spinner (stderr), not stdout
  assert_command dfx canister id e2e_project_backend --network local
  assert_match "$(jq -r .e2e_project_backend.local <.dfx/local/canister_ids.json)"
}

@test "create stores canister ids for configured-persistent local networks in canister_ids.json" {
  dfx_start

  webserver_port=$(get_webserver_port)

  create_networks_json

  jq '.local.bind="127.0.0.1:'"$webserver_port"'"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  jq '.local.type="persistent"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  assert_command dfx canister create --all --network local

  # canister creates writes to a spinner (stderr), not stdout
  assert_command dfx canister id e2e_project_backend --network local
  assert_match "$(jq -r .e2e_project_backend.local <canister_ids.json)"
}

@test "failure message does not include network if for local network" {
  dfx_start
  assert_command_fail dfx build --network local
  assert_match "Cannot find canister id."
  assert_not_contains "--network local"
}

@test "failure message does include network if for non-local network" {
  dfx_start

  setup_actuallylocal_shared_network

  assert_command_fail dfx build --network actuallylocal
  assert_match "Cannot find canister id."
  assert_match "--network actuallylocal"
}

@test "network 'playground' has a default definition" {
  # if network is unknown dfx fails with `Network not found: <network name>`
  assert_command_fail dfx canister id hello_backend --network playground
  assert_contains "Cannot find canister id"
}

@test "equivalent: --network ic and --ic" {
  dfx_start
  dfx identity get-wallet

  assert_command_fail dfx diagnose --network ic
  assert_contains "The test_id identity is not stored securely."
  assert_contains "use it in mainnet-facing commands"
  assert_contains "No wallet found; nothing to do"

  assert_command_fail dfx diagnose --ic
  assert_contains "The test_id identity is not stored securely."
  assert_contains "use it in mainnet-facing commands"
  assert_contains "No wallet found; nothing to do"

  assert_command dfx diagnose
  assert_not_contains "identity is not stored securely"
  assert_eq "No problems found"
}


-----------------------

/e2e/tests-dfx/new.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop
  standard_teardown
}

@test "dfx new - good names" {
  dfx new --no-frontend a_good_name_
  dfx new --no-frontend A
  dfx new --no-frontend b
  dfx new --no-frontend a_
  dfx new --no-frontend a_1
  dfx new --no-frontend a1
  dfx new --no-frontend a1a
}

@test "dfx new - bad names" {
  assert_command_fail dfx new _a_good_name_
  assert_command_fail dfx new __also_good
  assert_command_fail dfx new _1
  assert_command_fail dfx new _a
  assert_command_fail dfx new 1
  assert_command_fail dfx new 1_
  assert_command_fail dfx new -
  assert_command_fail dfx new _
  assert_command_fail dfx new 'ðŸ•¹'
  assert_command_fail dfx new 'ä¸å¥½'
  assert_command_fail dfx new 'a:b'
}

@test "dfx new --help shows possible backend template names" {
  assert_command dfx new --help
  assert_match "\[possible values.*motoko.*\]"
  assert_match "\[possible values.*rust.*\]"
  assert_match "\[possible values.*kybra.*\]"
  assert_match "\[possible values.*azle.*\]"
}

@test "dfx new --type <bad type> shows possible values" {
  assert_command_fail dfx new --type bad_type
  assert_match "\[possible values.*motoko.*\]"
  assert_match "\[possible values.*rust.*\]"
  assert_match "\[possible values.*kybra.*\]"
  assert_match "\[possible values.*azle.*\]"
}

@test "dfx new readmes contain appropriate links" {
  assert_command dfx new --type rust e2e_rust --no-frontend
  assert_command grep "https://docs.rs/ic-cdk" e2e_rust/README.md
  assert_command dfx new --type motoko e2e_motoko --no-frontend
  assert_command grep "https://internetcomputer.org/docs/current/motoko/main/language-manual" e2e_motoko/README.md
}

@test "dfx new emits projects of the correct type" {
  assert_command dfx new --type rust e2e_rust --no-frontend
  assert_command jq -r '.canisters.e2e_rust_backend.type' e2e_rust/dfx.json
  assert_eq "rust"
  assert_command dfx new --type motoko e2e_motoko --no-frontend
  assert_command jq -r '.canisters.e2e_motoko_backend.type' e2e_motoko/dfx.json
  assert_eq "motoko"
}

@test "frontend templates apply successfully" {
  for frontend in sveltekit vue react vanilla simple-assets none; do
    assert_command dfx new e2e_${frontend/-/_} --frontend $frontend
  done
  assert_file_not_exists e2e_none/src/e2e_none_frontend
}

@test "frontend templates pass the frontend tests" {
  dfx_start
  for frontend in sveltekit vue react vanilla; do
    assert_command dfx new e2e_$frontend --frontend $frontend --extras frontend-tests
    pushd e2e_$frontend
    assert_command dfx deploy
    assert_command npm test --workspaces
    popd
  done
}

@test "backend templates" {
  for backend in motoko rust kybra azle; do
    assert_command dfx new e2e_$backend --type $backend --no-frontend
  done
}

@test "interactive template selection" {
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/rust_svelte_with_tests_and_ii.exp"
  assert_file_exists e2e_project/Cargo.toml
  assert_file_exists e2e_project/src/e2e_project_frontend/src/routes/+page.svelte
  assert_file_exists e2e_project/src/e2e_project_frontend/src/setupTests.js
  assert_command jq .canisters.internet_identity e2e_project/dfx.json
}

@test "hyphenated names" {
  dfx_start
  assert_command dfx new e2e-project --type motoko --frontend vanilla --extras frontend-tests
  cd e2e-project
  assert_command jq '.canisters["e2e-project-backend","e2e-project-frontend"]' dfx.json
  assert_command dfx deploy
  assert_command npm test --workspaces
}

@test "variants of node and npm installed or not" {
  which node || skip "node not installed"
  which npm || skip "npm not installed"

  mkdir node-only
  cp "$(which node)" node-only/node

  mkdir dfx-only
  cp "$(which dfx)" dfx-only/dfx

  mkdir node-installable
  cat > node-installable/node <<EOF
#!/bin/bash
echo "Command 'node' not found, but can be installed with:" >&2
echo "apt install npm" >&2
echo "Please ask your administrator."
exit 127
EOF
  chmod +x node-installable/node

  mkdir npm-installable
  cat > npm-installable/npm <<EOF
#!/bin/bash
echo "Command 'npm' not found, but can be installed with:" >&2
echo "apt install npm" >&2
echo "Please ask your administrator."
exit 127
EOF
  chmod +x npm-installable/npm

  DFX_ONLY_DIR=$PWD/dfx-only
  NODE_ONLY_DIR=$PWD/node-only
  NODE_INSTALLABLE_DIR=$PWD/node-installable
  NPM_INSTALLABLE_DIR=$PWD/npm-installable

  # neither node nor npm are installed (no binaries)
  PATH="/usr/bin:/bin:$DFX_ONLY_DIR" \
    assert_command dfx new e2e_project1 --type motoko --frontend sveltekit
  assert_contains "Node could not be found. Skipping installing the frontend example code."
  assert_contains "npm could not be found. Skipping installing the frontend example code."
  assert_contains "You can bypass this check by using the --frontend flag."

  # node is installable, but not installed
  PATH="/usr/bin:/bin:$DFX_ONLY_DIR:$NODE_INSTALLABLE_DIR" \
    assert_command dfx new e2e_project2 --type motoko --frontend sveltekit
  assert_contains "Node could not be found. Skipping installing the frontend example code."
  assert_contains "npm could not be found. Skipping installing the frontend example code."
  assert_contains "You can bypass this check by using the --frontend flag."

  # node is installed, but there is no npm binary
  PATH="/usr/bin:/bin:$DFX_ONLY_DIR:$NODE_ONLY_DIR" \
    assert_command dfx new e2e_project3 --type motoko --frontend sveltekit
  assert_not_contains "Node could not be found"
  assert_contains "npm could not be found. Skipping installing the frontend example code."
  assert_contains "You can bypass this check by using the --frontend flag."

  # node is installed; npm is not, but a stub reports that it is installable
  PATH="/usr/bin:/bin:$DFX_ONLY_DIR:$NODE_ONLY_DIR:$NPM_INSTALLABLE_DIR" \
    assert_command dfx new e2e_project4 --type motoko --frontend sveltekit
  assert_not_contains "Node could not be found"
  assert_contains "npm could not be found. Skipping installing the frontend example code."
  assert_contains "You can bypass this check by using the --frontend flag."
}


-----------------------

/e2e/tests-dfx/packtool.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "build fails if packtool is not configured" {
  install_asset packtool

  dfx_start
  dfx canister create --all
  assert_command_fail dfx build
  assert_match 'import error \[M0010\], package "(rate|describe)" not defined'
}

@test "build succeeds if packtool is configured" {
  install_asset packtool
  # shellcheck disable=SC1091
  source configure_packtool.bash

  dfx_start
  dfx canister create --all
  dfx build
}

@test "project calls dependencies made available by packtool" {
  install_asset packtool
  # shellcheck disable=SC1091
  source configure_packtool.bash

  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install e2e_project_backend

  assert_command dfx canister call e2e_project_backend rate '("rust")'
  assert_eq '("rust: So hot right now.")'

  assert_command dfx canister call e2e_project_backend rate '("php")'
  assert_eq '("php: No comment.")'
}

@test "project calls packtool in project root" {
  install_asset packtool
  # shellcheck disable=SC1091
  source configure_packtool.bash

  dfx_start

  cd vessel || exit
  dfx canister create --all
  dfx build
  dfx canister install e2e_project_backend

  assert_command dfx canister call e2e_project_backend rate '("rust")'
  assert_eq '("rust: So hot right now.")'

  assert_command dfx canister call e2e_project_backend rate '("php")'
  assert_eq '("php: No comment.")'
}

@test "failure to invoke the package tool reports the command line and reason" {
  install_asset packtool
  jq '.defaults.build.packtool="./no-such-command that command cannot be invoked"' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  assert_command_fail dfx build
  assert_match 'Failed to invoke the package tool'
  assert_match 'no-such-command.*that.*command.*cannot.*be.*invoked'
  assert_match 'No such file or directory \(os error 2\)'
}

@test "failure in execution reports the command line and exit code" {
  install_asset packtool
  jq '.defaults.build.packtool="sh ./command-that-fails.bash"' dfx.json | sponge dfx.json

  dfx_start
  dfx canister create --all
  assert_command_fail dfx build
  assert_match 'The command.*failed'
  assert_match 'sh.*command-that-fails.bash'
  assert_match 'exit (code|status): 3'
}


-----------------------

/e2e/tests-dfx/ping.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx ping fails if replica not running" {
  assert_command_fail dfx ping
}


@test "dfx ping succeeds if replica is running" {
  dfx_start
  assert_command dfx ping

  assert_match "\"ic_api_version\""
}

@test "dfx ping succeeds by specific host:post" {
  dfx_start
  webserver_port=$(get_webserver_port)
  assert_command dfx ping http://127.0.0.1:"$webserver_port"

  assert_match "\"ic_api_version\""
}

@test "dfx ping does not require dfx.json" {
  dfx_start
  webserver_port=$(get_webserver_port)

  mkdir "$E2E_TEMP_DIR/not-a-project"
  (
    cd "$E2E_TEMP_DIR/not-a-project"

    assert_command dfx ping http://127.0.0.1:"$webserver_port"
    assert_match "\"ic_api_version\""
  )
}

@test "dfx ping succeeds by network name" {
  dfx_start
  assert_command dfx ping local

  assert_match "\"ic_api_version\""
}

@test "dfx ping succeeds by network name if network bind address is host:port format" {
  dfx_start
  webserver_port=$(get_webserver_port)
  jq '.networks.nnn.bind="127.0.0.1:'"$webserver_port"'"' dfx.json | sponge dfx.json
  assert_command dfx ping nnn

  assert_match "\"ic_api_version\""
}

@test "dfx ping succeeds by arbitrary network name to a nonstandard port" {
  dfx_start --host 127.0.0.1:12345

  # Make dfx use the port from configuration:
  rm "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/webserver-port"

  jq '.networks.arbitrary.providers=["http://127.0.0.1:12345"]' dfx.json | sponge dfx.json

  assert_command dfx ping arbitrary
  assert_match "\"ic_api_version\""

  assert_command_fail dfx ping
  # this port won't match the ephemeral port that the replica picked
  jq '.networks.arbitrary.providers=["127.0.0.1:22113"]' dfx.json | sponge dfx.json
  assert_command_fail dfx ping arbitrary
}

@test "dfx ping can have a URL for network to ping" {
  dfx_start
  webserver_port=$(get_webserver_port)
  assert_command dfx ping "http://127.0.0.1:$webserver_port"
  assert_match "\"ic_api_version\""
}


-----------------------

/e2e/tests-dfx/playground.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
  setup_playground
  dfx_new_assets hello
}

teardown() {
  dfx_stop
  standard_teardown
}

setup_playground() {
  if ! command -v ic-mops &> /dev/null
  then
    npm i -g ic-mops
  fi
  dfx_new hello
  create_networks_json
  install_asset playground_backend
  touch "$HOME/.bashrc" # required by following mops command
  mops toolchain init   # install the pinned moc version defined in mops.toml
  export DFX_MOC_PATH=moc-wrapper # use the moc-wrapper installed by mops
  dfx_start
  dfx deploy backend
  dfx ledger fabricate-cycles --t 9999999 --canister backend
  PLAYGROUND_CANISTER_ID=$(dfx canister id backend)
  export PLAYGROUND_CANISTER_ID
  echo "PLAYGROUND_CANISTER_ID is $PLAYGROUND_CANISTER_ID"
  WEBSERVER_PORT=$(get_webserver_port)
  jq '.playground.bind="127.0.0.1:'"$WEBSERVER_PORT"'"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  jq '.playground.playground.playground_canister="'"$PLAYGROUND_CANISTER_ID"'"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  cd ..
  rm -rf hello
}

@test "--playground aliases to --network playground" {
  assert_command dfx canister create hello_backend --playground -vv
  NETWORK_PLAYGROUND_ID=$(dfx canister id hello_backend --network playground)
  assert_command dfx canister id hello_backend --playground
  assert_match "${NETWORK_PLAYGROUND_ID}"
}

@test "canister lifecycle" {
  assert_command dfx deploy --playground
  assert_command dfx canister --playground call hello_backend greet '("player")'
  assert_match "Hello, player!"

  CANISTER=$(dfx canister --playground id hello_backend)
  assert_command_fail dfx canister --playground stop hello_backend
  assert_match "Canisters borrowed from a playground cannot be stopped."
  assert_command_fail dfx canister stop "${CANISTER}"
  assert_match "The principal you are using to call a management function is not part of the controllers."

  if [ "$(uname)" == "Darwin" ]; then
    sed -i '' 's/Hello/Goodbye/g' src/hello_backend/main.mo
  elif [ "$(uname)" == "Linux" ]; then
    sed -i 's/Hello/Goodbye/g' src/hello_backend/main.mo
  fi
  
  assert_command dfx deploy --playground
  assert_command dfx canister --playground call hello_backend greet '("player")'
  assert_match "Goodbye, player!"

  assert_command dfx canister --playground delete hello_backend
  assert_command_fail dfx canister --playground info hello_backend
  # canister is not actually deleted - the playground would have to do that
  assert_command dfx canister --playground info "$CANISTER"
}

@test "Handle timeout correctly" {
  assert_command dfx canister create hello_backend --playground -vv
  assert_match "Reserved canister 'hello_backend'"
  assert_command dfx canister create hello_backend --playground -vv
  assert_match "hello_backend canister was already created"
  sleep 10
  jq '.playground.playground.timeout_seconds=5' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  assert_command dfx canister create hello_backend --playground -vv
  assert_match "Canister 'hello_backend' has timed out."
  assert_match "Reserved canister 'hello_backend'"
}

# This is important for whitelisting wasm hashes in the playground.
# If the hashes didn't match then the playground would attempt to
# instrument the asset canister during upload which would run into execution limits.
@test "playground-installed asset canister is same wasm as normal asset canister" {
  assert_command dfx deploy --playground
  PLAYGROUND_HASH=$(dfx canister --playground info hello_frontend | grep hash)
  echo "PLAYGROUND_HASH: ${PLAYGROUND_HASH}"
  assert_command dfx deploy
  assert_command bash -c 'dfx canister info hello_frontend | grep hash'
  assert_match "${PLAYGROUND_HASH}"
}


-----------------------

/e2e/tests-dfx/print.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "print_mo" {
  install_asset print
  dfx_start 2>stderr.txt
  dfx canister create --all
  dfx build
  dfx canister install e2e_project
  dfx canister call e2e_project hello
  sleep 2
  run tail -2 stderr.txt
  assert_match "Hello, World! from DFINITY"
}


-----------------------

/e2e/tests-dfx/project_local_network.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx start starts a local network if dfx.json defines one" {
  dfx_new hello
  cat dfx.json
  define_project_network

  dfx_start
  dfx deploy

  assert_directory_not_exists "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY"
  assert_file_exists .dfx/network/local/pid
}

@test "can run more than one project network at the same time" {
  mkdir a
  cd a
  dfx_new hello
  install_asset counter
  define_project_network
  dfx_start
  dfx deploy
  dfx canister call hello_backend inc
  dfx canister call hello_backend inc

  cd ../..

  mkdir b
  cd b
  dfx_new hello
  install_asset counter
  define_project_network
  dfx_start
  dfx deploy
  dfx canister call hello_backend write '(6: nat)'

  cd ../..

  (
    cd a/hello
    assert_command dfx canister call hello_backend read
    assert_eq "(2 : nat)"
  )

  (
    cd b/hello
    assert_command dfx canister call hello_backend read
    assert_eq "(6 : nat)"
  )

  # the above would work even with a shared network.
  # So here's the real trick: they will have the same canister ids, because
  # each project has its own replica.
  HELLO_BACKEND_ID_A="$(cd a/hello ; dfx canister id hello_backend)"
  HELLO_BACKEND_ID_B="$(cd b/hello ; dfx canister id hello_backend)"
  assert_eq "$HELLO_BACKEND_ID_A" "$HELLO_BACKEND_ID_B"

  (cd a/hello ; dfx stop)
  (cd b/hello ; dfx stop)
}

@test "upgrade a wallet in a project-specific network" {
  dfx_new hello
  define_project_network

  dfx_start
  use_wallet_wasm 0.8.2

  dfx identity get-wallet

  assert_command dfx canister info "$(dfx identity get-wallet)"
  assert_match "Module hash: 0x$(wallet_sha 0.8.2)"

  use_wallet_wasm 0.10.0

  dfx wallet upgrade
  assert_command dfx canister info "$(dfx identity get-wallet)"
  assert_match "Module hash: 0x$(wallet_sha 0.10.0)"
}

@test "with a project-specific network, wallet id is stored local to the project" {
  dfx_new hello
  define_project_network

  dfx_start

  assert_file_not_exists .dfx/local/wallets.json

  WALLET_ID="$(dfx identity get-wallet)"

  assert_command jq -r .identities.default.local .dfx/local/wallets.json
  assert_eq "$WALLET_ID"
  assert_file_not_exists "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"
}

@test "with a shared network, wallet id is stored in the shared location" {
  dfx_start

  assert_file_not_exists "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"

  WALLET_ID="$(dfx identity get-wallet)"

  assert_command jq -r .identities.default.local "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"
  assert_eq "$WALLET_ID"
  assert_file_not_exists .dfx/local/wallets.json
}

@test "for project-specific network create stores canister ids for default-ephemeral local networks in .dfx/{network}/canister_ids.json" {
  dfx_new hello
  define_project_network

  dfx_start

  dfx canister create --all --network local

  # canister creates writes to a spinner (stderr), not stdout
  assert_command dfx canister id hello_backend --network local
  assert_match "$(jq -r .hello_backend.local <.dfx/local/canister_ids.json)"
}

@test "for project-specific network, create with wallet stores canister ids for configured-ephemeral networks in canister_ids.json" {
  dfx_new hello
  define_project_network
  dfx_start

  setup_actuallylocal_project_network
  jq '.networks.actuallylocal.type="ephemeral"' dfx.json | sponge dfx.json
  dfx_set_wallet

  dfx canister create --all --network actuallylocal

  # canister creates writes to a spinner (stderr), not stdout
  assert_command dfx canister id hello_backend --network actuallylocal
  assert_match "$(jq -r .hello_backend.actuallylocal <.dfx/actuallylocal/canister_ids.json)"
}


@test "dfx start and stop take into account dfx 0.11.x pid files" {
  dfx_new hello
  define_project_network
  dfx_start

  mv .dfx/network/local/pid .dfx/pid

  assert_command_fail dfx start
  assert_match 'dfx is already running'

  assert_command dfx stop
  assert_file_not_exists .dfx/pid
  assert_not_match "Nothing to do"
  assert_command dfx stop
  assert_match "Nothing to do"
}


-----------------------

/e2e/tests-dfx/provider.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new
}

teardown() {
  [ -f replica.pid ] && kill -9 "$(cat replica.pid)"
  dfx_stop

  standard_teardown
}

# This test is around 15 seconds to run. I don't think it should be faster without raising the
# flakiness (replica start time).
@test "provider flag can be passed in" {
  skip "refactor this test to replace client.bash"
  dfx build

  # Start a replica manually on a specific port.
  "$(dfx cache show)/replica" --config '
  [http_handler]
  write_port_to="port"
  ' &
  echo $! > replica.pid # Use a local file for the replica.
  sleep 5 # Wait for replica to be available.

  x=$(cat port)
  export PORT="$x"
  dfx canister install --all --provider "http://localhost:$PORT"
  dfx canister call e2e_project greet '("Blueberry")' --provider "http://localhost:$PORT"
  assert_command_fail dfx canister call e2e_project greet '("Blueberry")' --provider "http://localhost:$PORT"
  assert_command_fail dfx canister call e2e_project greet '("Blueberry")'
}

@test "uses local bind address if there is no local network" {
  jq 'del(.networks.local)' dfx.json | sponge dfx.json
  dfx_start
}

@test "uses local bind address if there are no networks" {
  jq 'del(.networks)' dfx.json | sponge dfx.json
  dfx_start
}

@test "network as URL creates the expected name" {
  dfx_start
  webserver_port=$(get_webserver_port)
  dfx canister create --all --network "http://127.0.0.1:$webserver_port"
  [ -d ".dfx/http___127_0_0_1_$webserver_port" ]
}

@test "network as URL does not create unexpected names" {
  dfx_start
  webserver_port=$(get_webserver_port)
  dfx canister create --all --network "http://127.0.0.1:$webserver_port"
  dfx build --all --network "http://127.0.0.1:$webserver_port"
  dfx canister install --all --network "http://127.0.0.1:$webserver_port"
  assert_eq 1 "$(find .dfx/http* -maxdepth 0 | wc -l | tr -d ' ')"
}


-----------------------

/e2e/tests-dfx/remote.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "canister call and sign" {
  install_asset remote/call/actual
  dfx_start
  setup_actuallylocal_shared_network

  dfx identity new --storage-mode plaintext alice

  assert_command dfx deploy --network actuallylocal --identity alice
  assert_command dfx canister call remote write '("initial data in the remote canister")' --identity alice --network actuallylocal
  assert_command dfx canister call remote read --identity alice --network actuallylocal
  assert_eq '("initial data in the remote canister")'

  REMOTE_CANISTER_ID=$(jq -r .remote.actuallylocal canister_ids.json)
  echo "Remote canister id: $REMOTE_CANISTER_ID"
  rm canister_ids.json

  install_asset remote/call/mock
  jq ".canisters.remote.remote.id.actuallylocal=\"$REMOTE_CANISTER_ID\"" dfx.json | sponge dfx.json
  setup_actuallylocal_shared_network
  setup_local_shared_network

  # set up: remote method is update, local is query
  # call remote method as update to make a change
  assert_command dfx deploy --network actuallylocal
  assert_command dfx canister call remote which_am_i --network actuallylocal
  assert_eq '("actual")'

  cat dfx.json
  cat canister_ids.json

  #
  # Here the mock doesn't know about the method at all.
  # In order for the candid decoder to know the type, dfx must both:
  #   - look up the canister by (remote) canister id
  #   - use the remote candid definition
  #
  assert_command dfx canister call --query  "$REMOTE_CANISTER_ID" make_struct '("A query by principal", "B query by principal")' --network actuallylocal
  assert_eq '(record { a = "A query by principal"; b = "B query by principal" })'
  assert_command dfx canister call          "$REMOTE_CANISTER_ID" make_struct '("A default by principal", "B default by principal")' --network actuallylocal
  assert_eq '(record { a = "A default by principal"; b = "B default by principal" })'
  assert_command dfx canister call --update "$REMOTE_CANISTER_ID" make_struct '("A update by principal", "B update by principal")' --network actuallylocal
  assert_eq '(record { a = "A update by principal"; b = "B update by principal" })'

  assert_command dfx canister call --query  remote make_struct '("A query by name", "B query by name")' --network actuallylocal
  assert_eq '(record { a = "A query by name"; b = "B query by name" })'
  assert_command dfx canister call          remote make_struct '("A default by name", "B default by name")' --network actuallylocal
  assert_eq '(record { a = "A default by name"; b = "B default by name" })'
  assert_command dfx canister call --update remote make_struct '("A update by name", "B update by name")' --network actuallylocal
  assert_eq '(record { a = "A update by name"; b = "B update by name" })'

  # This also should work when no canister type can be determined / if no info but the bare minimum of remote id and remote candid is given:
  jq 'del(.canisters.remote.main)' dfx.json | sponge dfx.json
  assert_command dfx canister call --query  remote make_struct '("A query by name", "B query by name")' --network actuallylocal
  assert_eq '(record { a = "A query by name"; b = "B query by name" })'

  # We can't check this for sign, because dfx canister send outputs something like this:
  #   To see the content of response, copy-paste the encoded string into cbor.me.
  #   Response: d9d9f7a2667374617475736[snip]2696e636970616c

  #
  # Here:
  #   - the actual method is an update
  #   - the mock method is a query
  #   - in remote candid definition, the method is an update
  # We try to call with --query.
  # We expect dfx to notice that the method is in fact an update, which it knows from the remote candid definition.
  #
  assert_command_fail dfx canister call --query "$REMOTE_CANISTER_ID" actual_update_mock_query_remote_candid_update '("call by principal with --query")' --network actuallylocal
  assert_match 'not a query method'
  assert_command_fail dfx canister call --query remote actual_update_mock_query_remote_candid_update '("call by name with --query")' --network actuallylocal
  assert_match 'not a query method'

  # And the same for dfx canister sign:
  assert_command_fail dfx canister sign --query "$REMOTE_CANISTER_ID" actual_update_mock_query_remote_candid_update '("call by principal with --query")' --network actuallylocal
  assert_match 'not a query method'
  assert_command_fail dfx canister sign --query remote actual_update_mock_query_remote_candid_update '("call by name with --query")' --network actuallylocal
  assert_match 'not a query method'
}

@test "canister create <canister> fails for a remote canister" {
  install_asset remote/actual
  dfx_start
  setup_actuallylocal_shared_network

  dfx identity new --storage-mode plaintext alice

  assert_command dfx deploy --network actuallylocal --identity alice

  REMOTE_CANISTER_ID=$(jq -r .remote.actuallylocal canister_ids.json)
  echo "Remote canister id: $REMOTE_CANISTER_ID"
  rm canister_ids.json

  install_asset remote/basic
  setup_actuallylocal_shared_network
  setup_local_shared_network
  jq ".canisters.remote.remote.id.actuallylocal=\"$REMOTE_CANISTER_ID\"" dfx.json | sponge dfx.json

  assert_command_fail dfx canister create remote --network actuallylocal
  assert_match "Canister 'remote' is a remote canister on network 'actuallylocal', and cannot be created from here."
}

@test "canister install <canister> fails for a remote canister" {
  install_asset remote/actual
  dfx_start
  setup_actuallylocal_shared_network

  dfx identity new --storage-mode plaintext alice

  assert_command dfx deploy --network actuallylocal --identity alice

  REMOTE_CANISTER_ID=$(jq -r .remote.actuallylocal canister_ids.json)
  echo "Remote canister id: $REMOTE_CANISTER_ID"
  rm canister_ids.json

  install_asset remote/basic
  setup_actuallylocal_shared_network
  setup_local_shared_network
  jq ".canisters.remote.remote.id.actuallylocal=\"$REMOTE_CANISTER_ID\"" dfx.json | sponge dfx.json

  assert_command_fail dfx canister install remote --network actuallylocal
  assert_match "Canister 'remote' is a remote canister on network 'actuallylocal', and cannot be installed from here."
}

@test "canister create --all, canister install --all skip remote canisters" {
  install_asset remote/actual
  dfx_start
  setup_actuallylocal_shared_network

  #
  # Set up the "remote" canister, with a different controller in order to
  # demonstrate that we don't try to install/upgrade it as a remote canister.
  #
  dfx identity new --storage-mode plaintext alice

  assert_command dfx deploy --network actuallylocal --identity alice
  assert_command dfx canister call remote write '("this is data in the remote canister")' --identity alice --network actuallylocal

  assert_command dfx canister call remote read --identity alice --network actuallylocal
  assert_eq '("this is data in the remote canister")'

  REMOTE_CANISTER_ID=$(jq -r .remote.actuallylocal canister_ids.json)
  echo "Remote canister id: $REMOTE_CANISTER_ID"
  rm canister_ids.json

  install_asset remote/basic
  setup_actuallylocal_shared_network
  setup_local_shared_network
  jq ".canisters.remote.remote.id.actuallylocal=\"$REMOTE_CANISTER_ID\"" dfx.json | sponge dfx.json

  # Here we want to make sure that create+build+install works with the common flow
  assert_command dfx canister create --all
  assert_command dfx build
  assert_command dfx canister install --all

  assert_command dfx canister call basic read_remote
  assert_eq '("")'
  assert_command dfx canister call remote which_am_i
  assert_eq '("mock")'

  assert_command dfx canister create --all --network actuallylocal
  assert_command dfx build --network actuallylocal -vv
  assert_match "Not building canister 'remote'"
  assert_command dfx canister install --all --network actuallylocal

  assert_command dfx canister call basic read_remote --network actuallylocal
  assert_eq '("this is data in the remote canister")'

  # We can change the value by calling the original canister
  assert_command dfx canister call "${REMOTE_CANISTER_ID}" write '("altered data (by canister id) in the remote canister")' --network actuallylocal
  assert_command dfx canister call basic read_remote --network actuallylocal
  assert_eq '("altered data (by canister id) in the remote canister")'

  # We can also call through the canister name
  assert_command dfx canister call remote write '("altered data (by canister name) in the remote canister")' --network actuallylocal
  assert_command dfx canister call basic read_remote --network actuallylocal
  assert_eq '("altered data (by canister name) in the remote canister")'


  assert_command dfx canister call remote which_am_i --network actuallylocal
  assert_eq '("actual")'

  assert_command jq .basic.actuallylocal canister_ids.json
  assert_eq '"'"$(dfx canister id basic --network actuallylocal)"'"'

  assert_command jq .remote canister_ids.json
  assert_eq "null"

  # Assert frontend declarations are actually created
  dfx generate
  assert_file_exists "src/declarations/remote/remote.did"
  assert_file_exists "src/declarations/remote/remote.did.js"
  assert_file_exists "src/declarations/remote/remote.did.d.ts"
  assert_file_exists "src/declarations/remote/index.js"
  assert_file_exists "src/declarations/remote/index.d.ts"

}

@test "for remote build, checks imports against the candid file rather than the mock" {
  # In this test, a canister with a remote ID also has a candid file specified.
  # When building for the remote network, we expect to use this candid file,
  # and for methods calls that don't match the candid file to fail.

  install_asset remote/actual
  dfx_start
  setup_actuallylocal_shared_network

  #
  # Set up the "remote" canister, with a different controller in order to
  # demonstrate that we don't try to install/upgrade it as a remote canister.
  #
  dfx identity new --storage-mode plaintext alice

  assert_command dfx deploy --network actuallylocal --identity alice

  REMOTE_CANISTER_ID=$(jq -r .remote.actuallylocal canister_ids.json)
  echo "Remote canister id: $REMOTE_CANISTER_ID"
  rm canister_ids.json

  install_asset remote/extra
  jq ".canisters.remote.remote.id.actuallylocal=\"$REMOTE_CANISTER_ID\"" dfx.json | sponge dfx.json
  setup_actuallylocal_shared_network
  setup_local_shared_network

  # We expect the local network deploy to succeed, because it is built using the candid file from
  # the local canister.
  assert_command dfx deploy

  # And we can call the extra method,
  assert_command dfx canister call extra remote_extra
  assert_eq '("extra!")'

  # We expect the remote network deploy to fail, because it is built using the candid file
  # specified for the remote canister.  That candid file doesn't define the extra method
  # that is present in the mock.
  assert_command_fail dfx deploy --network actuallylocal
}

@test "build + install + call -- remote" {
  install_asset remote/actual
  dfx_start
  setup_actuallylocal_shared_network

  #
  # Set up the "remote" canister, with a different controller in order to
  # demonstrate that we don't try to install/upgrade it as a remote canister.
  #
  dfx identity new --storage-mode plaintext alice

  assert_command dfx deploy --network actuallylocal --identity alice
  assert_command dfx canister call remote write '("this is data in the remote canister")' --network actuallylocal --identity alice

  assert_command dfx canister call remote read --network actuallylocal --identity alice
  assert_eq '("this is data in the remote canister")'

  REMOTE_CANISTER_ID=$(jq -r .remote.actuallylocal canister_ids.json)
  echo "Remote canister id: $REMOTE_CANISTER_ID"
  rm canister_ids.json

  install_asset remote/basic
  setup_actuallylocal_shared_network
  setup_local_shared_network
  jq ".canisters.remote.remote.id.actuallylocal=\"$REMOTE_CANISTER_ID\"" dfx.json | sponge dfx.json

  assert_command dfx deploy
  assert_command dfx canister call basic read_remote
  assert_eq '("")'
  assert_command dfx canister call remote which_am_i
  assert_eq '("mock")'

  assert_command dfx deploy --network actuallylocal -vv
  assert_match "Not building canister 'remote'"
  assert_command dfx canister call basic read_remote --network actuallylocal
  assert_eq '("this is data in the remote canister")'

  # We can change the value by calling the original canister
  assert_command dfx canister call "${REMOTE_CANISTER_ID}" write '("data altered by canister id in the remote canister")' --network actuallylocal
  assert_command dfx canister call basic read_remote --network actuallylocal
  assert_eq '("data altered by canister id in the remote canister")'

  # We can also call through the canister name
  assert_command dfx canister call remote write '("data altered by canister name in the remote canister")' --network actuallylocal
  assert_command dfx canister call basic read_remote --network actuallylocal
  assert_eq '("data altered by canister name in the remote canister")'


  assert_command dfx canister call remote which_am_i --network actuallylocal
  assert_eq '("actual")'

  assert_command jq .basic.actuallylocal canister_ids.json
  assert_eq '"'"$(dfx canister id basic --network actuallylocal)"'"'

  assert_command jq .remote canister_ids.json
  assert_eq "null"
}

@test "build step sets remote cid environment variable correctly" {
  install_asset remote/envvar
  install_asset wasm/identity # need to have some .did and .wasm files to point our custom canister to
  dfx_start
  setup_actuallylocal_shared_network

  assert_command dfx deploy --network actuallylocal -vv
  assert_match "CANISTER_ID_REMOTE: qoctq-giaaa-aaaaa-aaaea-cai"
  assert_contains "CANISTER_CANDID_PATH_REMOTE: $(pwd -P)/remotecandid.did"
}


-----------------------

/e2e/tests-dfx/remote_generate_binding.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  standard_teardown
}

@test "remote generate-binding succeeds for --all" {
  install_asset remote_generate_binding/basic

  assert_command dfx remote generate-binding --all

  assert_file_exists remote.mo
  assert_file_exists remote.rs
  assert_file_exists remote.ts
  assert_file_exists remote.js
}

@test "remote generate-binding --overwrite succeeds for --all" {
  install_asset remote_generate_binding/basic
  echo "to_overwrite" > remote.mo
  echo "to_overwrite" > remote.rs

  assert_command dfx remote generate-binding --overwrite --all

  # should generate if not present
  assert_file_exists remote.js
  assert_file_exists remote.ts

  # should overwrite if already present
  assert_neq "to_overwrite" "$(cat remote.mo)"
  assert_neq "to_overwrite" "$(cat remote.rs)"
}

@test "remote generate-binding does not overwrite if file already present" {
  install_asset remote_generate_binding/basic
  echo "to_overwrite" > remote.mo
  echo "to_overwrite" > remote.rs

  assert_command dfx remote generate-binding --all

  # should generate if not present
  assert_file_exists remote.ts
  assert_file_exists remote.js

  # should not overwrite if already present
  assert_eq "to_overwrite" "$(cat remote.mo)"
  assert_eq "to_overwrite" "$(cat remote.rs)"
}

@test "remote generate-binding succeeds for specific rust canister" {
  install_asset remote_generate_binding/basic

  assert_command dfx remote generate-binding remote-rust

  assert_file_exists remote.rs
  assert_file_not_exists remote.mo
  assert_file_not_exists remote.ts
  assert_file_not_exists remote.js
}

@test "remote generate-binding succeeds for specific motoko canister" {
  install_asset remote_generate_binding/basic

  assert_command dfx remote generate-binding remote-motoko

  assert_file_exists remote.mo
  assert_file_not_exists remote.rs
  assert_file_not_exists remote.ts
  assert_file_not_exists remote.js
}

@test "remote generate-binding succeeds for specific javascript canister" {
  install_asset remote_generate_binding/basic

  assert_command dfx remote generate-binding remote-javascript

  assert_file_exists remote.js
  assert_file_not_exists remote.mo
  assert_file_not_exists remote.rs
  assert_file_not_exists remote.ts
}

@test "remote generate-binding succeeds for specific typescript canister" {
  install_asset remote_generate_binding/basic

  assert_command dfx remote generate-binding remote-typescript

  assert_file_exists remote.ts
  assert_file_not_exists remote.mo
  assert_file_not_exists remote.rs
  assert_file_not_exists remote.js
}

@test "remote generate-binding --overwrite succeeds for specific canister" {
  install_asset remote_generate_binding/basic
  echo "to_overwrite" > remote.mo

  # should not overwrite without --overwrite
  assert_command dfx remote generate-binding remote-motoko
  assert_match 'already exists'
  assert_eq "to_overwrite" "$(cat remote.mo)"

  # should overwrite with --overwrite
  assert_command dfx remote generate-binding --overwrite remote-motoko
  assert_neq "to_overwrite" "$(cat remote.mo)"
}

@test "remote generate-binding incomplete command rejected" {
  assert_command_fail dfx remote generate-binding
}


-----------------------

/e2e/tests-dfx/request_status.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "request-status output raw" {
  install_asset greet
  dfx_start --artificial-delay 10000
  dfx canister create hello_backend
  dfx build hello_backend

  dfx canister install hello_backend

  assert_command dfx canister call --async hello_backend greet Bob

  # shellcheck disable=SC2154
  assert_command dfx canister request-status --output raw "$stdout" "$(dfx canister id hello_backend)"
  assert_eq '4449444c0001710b48656c6c6f2c20426f6221'

}


-----------------------

/e2e/tests-dfx/rust.bash:
-----------------------

#!/usr/bin/env bats
# shellcheck disable=SC2030,SC2031

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "rust starter project can build and call" {
  dfx_new_rust hello

  dfx_start
  dfx canister create --all
  assert_command dfx build hello_backend -vvv
  assert_match "Shrinking Wasm"
  assert_command dfx canister install hello_backend
  assert_command dfx canister call hello_backend greet dfinity
  assert_match '("Hello, dfinity!")'

  # dfx sets the candid:service metadata
  dfx canister metadata hello_backend candid:service >installed.did
  assert_command diff src/hello_backend/hello_backend.did installed.did
}

@test "rust canister can resolve dependencies" {
  dfx_new_rust rust_deps
  install_asset rust_deps
  dfx_start
  assert_command dfx deploy
  assert_command dfx canister call multiply_deps read
  assert_match '(1 : nat)'
  assert_command dfx canister call multiply_deps mul '(3)'
  assert_match '(9 : nat)'
  assert_command dfx canister call rust_deps read
  assert_match '(9 : nat)'
}

@test "rust canister can have nonstandard target dir location" {
  dfx_new_rust
  # We used to set CARGO_TARGET_DIR="$(echo -ne '\x81')"
  # But since rust 1.69, `cargo metadata` returns
  #   error: path contains invalid UTF-8 characters
  CARGO_TARGET_DIR="custom-target"
  export CARGO_TARGET_DIR
  dfx_start
  assert_command dfx deploy
  assert_command dfx canister call e2e_project_backend greet dfinity
}

@test "rust canister fails to build with missing lockfile" {
  dfx_new_rust
  rm -f ./Cargo.lock
  dfx_start
  assert_command_fail dfx deploy
  cargo update
  assert_command dfx deploy
}

@test "rust canisters support complex package layout" {
  dfx_new_rust rust_complex
  install_asset rust_complex
  dfx_start
  assert_command dfx deploy
  assert_command dfx canister call can1 id
  assert_contains '(1 : nat32)'
  assert_command dfx canister call can2 id
  assert_contains '(2 : nat32)'
  assert_command dfx canister call can3 id
  assert_contains '(3 : nat32)'
}


-----------------------

/e2e/tests-dfx/schema.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop
  standard_teardown
}

@test "dfx schema prints valid json" {
  assert_command dfx schema --outfile out.json
  # make sure out.json contains exactly one json object
  assert_command jq type out.json
  assert_eq '"object"'
}

@test "dfx schema still works with broken dfx.json" {
  echo '{}' | jq '.broken_key="blahblahblah"' > dfx.json
  assert_command dfx schema
}

@test "dfx schema can display for networks" {
  assert_command dfx schema --for networks --outfile out.json
  # make sure out.json contains exactly one json object
  assert_command jq type out.json
  assert_eq '"object"'
}

@test "dfx schema can display for dfx-metadata" {
  assert_command dfx schema --for dfx-metadata --outfile out.json
  # make sure out.json contains exactly one json object
  assert_command jq type out.json
  assert_eq '"object"'
}


-----------------------

/e2e/tests-dfx/shared_local_network.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx start creates no files in the current directory when run from an empty directory" {
  dfx_start
  assert_command find .
  assert_eq "."
}

@test "dfx start creates only an empty .dfx directory when run from a project" {
  dfx_new hello
  START_DIR_CONTENTS="$(find .)"
  dfx_start
  END_DIR_CONTENTS="$(find . | grep -v '^\./\.dfx$')"
  assert_eq "$START_DIR_CONTENTS" "$END_DIR_CONTENTS"
}

@test "project data is cleared after dfx start --clean from outside the project" {
  mkdir somewhere
  (
    cd somewhere
    dfx_start
  )

  (
    dfx_new hello
    dfx deploy
    dfx canister id hello_backend
  )

  mkdir somewhere_else
  (
    cd somewhere_else
    dfx_stop
    dfx_start --clean
  )

  (
    cd hello
    assert_command_fail dfx canister id hello_backend
  )
}

@test "multiple projects with the same canister names" {
  dfx_start

  mkdir a
  cd a
  dfx_new hello
  install_asset counter
  dfx deploy
  HELLO_BACKEND_A="$(dfx canister id hello_backend)"
  dfx canister call hello_backend inc
  dfx canister call hello_backend inc
  cd ../..

  mkdir b
  cd b
  dfx_new hello
  install_asset counter
  dfx deploy
  HELLO_BACKEND_B="$(dfx canister id hello_backend)"
  dfx canister call hello_backend write '(6: nat)'
  cd ../..

  assert_command dfx canister call "$HELLO_BACKEND_A" read
  assert_eq "(2 : nat)"
  (
    cd a/hello
    assert_command dfx canister call hello_backend read
    assert_eq "(2 : nat)"
  )

  assert_command dfx canister call "$HELLO_BACKEND_B" read
  assert_eq "(6 : nat)"
  (
    cd b/hello
    assert_command dfx canister call hello_backend read
    assert_eq "(6 : nat)"
  )
}


@test "wallet config file is reset after start --clean" {
  dfx_start

  (
    dfx_new hello
    dfx wallet balance
    dfx identity get-wallet
    assert_command dfx diagnose
    assert_eq "No problems found"
  )

  dfx_stop
  dfx_start --clean

  (
    cd hello
    assert_command_fail dfx diagnose
    assert_match "No wallet found; nothing to do"
  )
}

@test "separate projects use the same wallet id for a given identity" {
  dfx_start

  ( dfx_new a )
  ( dfx_new b )
  WALLET_ID_A="$(cd a ; dfx identity get-wallet)"
  WALLET_ID_B="$(cd b ; dfx identity get-wallet)"

  assert_eq "$WALLET_ID_A" "$WALLET_ID_B"
}

@test "dfx identity rename renames wallet for shared local network" {
   dfx_start

   dfx identity new  alice --storage-mode plaintext
   ALICE_WALLET="$(dfx identity get-wallet --identity alice)"

   dfx identity rename alice bob
   BOB_WALLET="$(dfx identity get-wallet --identity bob)"

   assert_eq "$ALICE_WALLET" "$BOB_WALLET"
}


-----------------------

/e2e/tests-dfx/sign_send.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "sign + send" {
  install_asset counter
  dfx_start
  dfx deploy

  assert_command dfx canister sign --query hello_backend read
  assert_eq "Query message generated at [message.json]"

  sleep 10
  echo y | assert_command dfx canister send message.json

  assert_command_fail dfx canister send message.json --status
  assert_eq "Error: Can only check request_status on update calls."

  assert_command_fail dfx canister sign --query hello_backend read
  assert_eq "Error: [message.json] already exists, please specify a different output file name."

  assert_command dfx canister sign --update hello_backend inc --file message-inc.json
  assert_eq "Update and request_status message generated at [message-inc.json]"

  sleep 10
  echo y | assert_command dfx canister send message-inc.json
  assert_command dfx canister send message-inc.json --status
  assert_match "Response:"
}

@test "sign outside of a dfx project" {
  cd "$E2E_TEMP_DIR"
  mkdir not-a-project-dir
  cd not-a-project-dir

  assert_command dfx canister sign --query rwlgt-iiaaa-aaaaa-aaaaa-cai read --network ic
  assert_match "Query message generated at \[message.json\]"
}

@test "sign subcommand accepts argument from a file" {
  install_asset greet
  dfx_start
  dfx deploy
  TMP_NAME_FILE="$(mktemp)"
  printf '("Names can be very long")' > "$TMP_NAME_FILE"

  assert_command dfx canister sign --argument-file "$TMP_NAME_FILE" --query hello_backend greet
  assert_eq "Query message generated at [message.json]"

  assert_command jq -rc .arg message.json
  assert_match "[68,73,68,76,0,1,113,21,78,97,109,101,115,32,99,97,110,32,98,101,32,118,114,121,32,108,111,110,103]"

  rm "$TMP_NAME_FILE"
}

@test "sign subcommand accepts argument from stdin" {
  install_asset greet
  dfx_start
  dfx deploy
  TMP_NAME_FILE="$(mktemp)"
  printf '("stdin")' > "$TMP_NAME_FILE"

  assert_command dfx canister sign --argument-file - --query hello_backend greet < "$TMP_NAME_FILE"
  assert_eq "Query message generated at [message.json]"

  assert_command jq -rc .arg message.json
  assert_match "[68,73,68,76,0,1,113,5,115,116,100,105,110]"

  rm "$TMP_NAME_FILE"
}


-----------------------

/e2e/tests-dfx/start.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "network-id contains settings digest" {
  dfx_start

  NETWORK_ID_PATH="$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/network-id"
  SETTINGS_DIGEST=$(jq -r '.settings_digest' "$NETWORK_ID_PATH")
  NETWORK_ID_BY_SETTINGS_DIGEST_PATH="$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/$SETTINGS_DIGEST/network-id"
  assert_command diff "$NETWORK_ID_PATH" "$NETWORK_ID_BY_SETTINGS_DIGEST_PATH"
}

@test "start and stop with different options" {
  [[ "$USE_POCKETIC" ]] && skip "skipped for pocketic: clean required"
  dfx_start --artificial-delay 101
  dfx_stop

  # notice: no need to --clean
  dfx_start --artificial-delay 102
  dfx_stop
}

@test "project networks still need --clean" {
  dfx_new hello
  define_project_network

  dfx_start --artificial-delay 101
  dfx stop

  assert_command_fail dfx_start --artificial-delay 102
}

@test "stop and start with other options does not disrupt projects" {
  [[ "$USE_POCKETIC" ]] && skip "skipped for pocketic: clean required"
  dfx_start --artificial-delay 101

  dfx_new p1
  assert_command dfx deploy
  CANISTER_ID="$(dfx canister id p1_backend)"

  assert_command dfx stop
  dfx_start --artificial-delay 102
  assert_command dfx stop

  dfx_start --artificial-delay 101

  assert_command dfx canister id p1_backend
  assert_eq "$CANISTER_ID"
}

@test "start and stop outside project" {
  dfx_start

  mkdir subdir
  cd subdir || exit 1
  dfx_new
  assert_command dfx deploy
  CANISTER_ID="$(dfx canister id e2e_project_backend)"
  cd ..
  assert_command dfx canister status "$CANISTER_ID"
  assert_contains "Status: Running"
  assert_command dfx canister stop "$CANISTER_ID"
  assert_command dfx canister status "$CANISTER_ID"
  assert_contains "Status: Stopped"
  assert_command dfx canister start "$CANISTER_ID"
  assert_command dfx canister status "$CANISTER_ID"
  assert_contains "Status: Running"
}

@test "uninstall-code outside of a project" {
  dfx_start

  mkdir subdir
  cd subdir || exit 1
  dfx_new
  assert_command dfx deploy
  CANISTER_ID="$(dfx canister id e2e_project_backend)"
  cd ..
  assert_command dfx canister status "$CANISTER_ID"
  assert_contains "Module hash: 0x"
  assert_command dfx canister uninstall-code "$CANISTER_ID"
  assert_contains "Uninstalling code for canister $CANISTER_ID"
  assert_command dfx canister status "$CANISTER_ID"
  assert_contains "Module hash: None"
}

@test "pocket-ic proxy domain configuration in string form" {
  create_networks_json
  jq '.local.proxy.domain="xyz.domain"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  dfx_start

  domains="$(curl "http://localhost:$(get_pocketic_proxy_config_port)/http_gateway" \
    | jq -c ".[] | select(.port == $(get_webserver_port)) | .domains | sort")"

  assert_eq '["xyz.domain"]' "$domains"
}

@test "pocket-ic proxy domain configuration in vector form" {
  create_networks_json
  jq '.local.proxy.domain=["xyz.domain", "abc.something"]' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  dfx_start

  domains="$(curl "http://localhost:$(get_pocketic_proxy_config_port)/http_gateway" \
    | jq -c ".[] | select(.port == $(get_webserver_port)) | .domains | sort")"

  assert_eq '["abc.something","xyz.domain"]' "$domains"
}

@test "pocket-ic proxy domain configuration from project defaults" {
  dfx_new
  define_project_network

  jq '.defaults.proxy.domain=["xyz.domain", "abc.something"]' dfx.json | sponge dfx.json

  dfx_start

  domains="$(curl "http://localhost:$(get_pocketic_proxy_config_port)/http_gateway" \
    | jq -c ".[] | select(.port == $(get_webserver_port)) | .domains | sort")"

  assert_eq '["abc.something","xyz.domain"]' "$domains"
}

@test "pocket-ic proxy domain configuration from command-line" {
  dfx_start --domain xyz.domain --domain def.somewhere

  domains="$(curl "http://localhost:$(get_pocketic_proxy_config_port)/http_gateway" \
    | jq -c ".[] | select(.port == $(get_webserver_port)) | .domains | sort")"

  assert_eq '["def.somewhere","xyz.domain"]' "$domains"
}

@test "dfx restarts the replica" {
  dfx_new hello
  dfx_start

  install_asset greet
  assert_command dfx deploy
  assert_command dfx canister call hello_backend greet '("Alpha")'
  assert_eq '("Hello, Alpha!")'

  REPLICA_PID=$([[ "$USE_POCKETIC" ]] && get_pocketic_pid || get_replica_pid)

  echo "replica pid is $REPLICA_PID"

  [[ "$USE_POCKETIC" ]] && curl -X DELETE "http://localhost:$(get_pocketic_port)/instances/0"
  kill -KILL "$REPLICA_PID"
  assert_process_exits "$REPLICA_PID" 15s

  timeout 15s sh -c \
    'until dfx ping; do echo waiting for replica to restart; sleep 1; done' \
    || (echo "replica did not restart" && ps aux && exit 1)
  wait_until_replica_healthy

  # Sometimes initially get an error like:
  #     IC0537: Attempt to execute a message on canister <>> which contains no Wasm module
  # but the condition clears.
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting
  # even after the above, still sometimes fails with
  #     IC0208: Certified state is not available yet. Please try again...
  sleep 10
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting

  assert_command dfx canister call hello_backend greet '("Omega")'
  assert_eq '("Hello, Omega!")'
}

@test "dfx restarts pocketic proxy" {
  dfx_new_assets hello
  dfx_start

  install_asset greet
  assert_command dfx deploy
  assert_command dfx canister call hello_backend greet '("Alpha")'
  assert_eq '("Hello, Alpha!")'

  POCKETIC_PROXY_PID=$(get_pocketic_proxy_pid)

  echo "pocket-ic proxy pid is $POCKETIC_PROXY_PID"

  kill -KILL "$POCKETIC_PROXY_PID"
  assert_process_exits "$POCKETIC_PROXY_PID" 15s

  ID=$(dfx canister id hello_frontend)

  timeout 15s sh -c \
    "until curl --fail http://localhost:\$(cat \"$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY\"/webserver-port)/sample-asset.txt?canisterId=$ID; do echo waiting for pocket-ic proxy to restart; sleep 1; done" \
    || (echo "pocket-ic proxy did not restart" && ps aux && exit 1)

  assert_command curl --fail http://localhost:"$(get_webserver_port)"/sample-asset.txt?canisterId="$ID"
}

@test "dfx restarts pocketic proxy when the replica restarts" {
  dfx_new_assets hello
  dfx_start

  install_asset greet
  assert_command dfx deploy
  assert_command dfx canister call hello_backend greet '("Alpha")'
  assert_eq '("Hello, Alpha!")'

  REPLICA_PID=$([[ "$USE_POCKETIC" ]] && get_pocketic_pid || get_replica_pid)
  POCKETIC_PROXY_PID=$(get_pocketic_proxy_pid)

  echo "replica pid is $REPLICA_PID"
  echo "pocket-ic proxy pid is $POCKETIC_PROXY_PID"

  [[ "$USE_POCKETIC" ]] && curl -X DELETE "http://localhost:$(get_pocketic_port)/instances/0"
  kill -KILL "$REPLICA_PID"
  assert_process_exits "$REPLICA_PID" 15s
  assert_process_exits "$POCKETIC_PROXY_PID" 15s

  timeout 15s sh -c \
    'until dfx ping; do echo waiting for replica to restart; sleep 1; done' \
    || (echo "replica did not restart" && ps aux && exit 1)
  wait_until_replica_healthy

  # Sometimes initially get an error like:
  #     IC0537: Attempt to execute a message on canister <>> which contains no Wasm module
  # but the condition clears.
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting
  # even after the above, still sometimes fails with
  #     IC0208: Certified state is not available yet. Please try again...
  sleep 10
  timeout 30s sh -c \
    "until dfx canister call hello_backend greet '(\"wait\")'; do echo waiting for any canister call to succeed; sleep 1; done" \
    || (echo "canister call did not succeed") # but continue, for better error reporting

  assert_command dfx canister call hello_backend greet '("Omega")'
  assert_eq '("Hello, Omega!")'

  ID=$(dfx canister id hello_frontend)

  timeout 15s sh -c \
    "until curl --fail http://localhost:\$(cat \"$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/webserver-port\")/sample-asset.txt?canisterId=$ID; do echo waiting for pocket-ic proxy to restart; sleep 1; done" \
    || (echo "pocket-ic proxy did not restart" && ps aux && exit 1)

  assert_command curl --fail http://localhost:"$(get_webserver_port)"/sample-asset.txt?canisterId="$ID"
}

@test "dfx start honors replica port configuration" {
  create_networks_json
  replica_port=$(get_ephemeral_port)
  jq ".local.replica.port=$replica_port" "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  dfx_start

  assert_command dfx info replica-port
  assert_eq "$replica_port"
}

@test "dfx starts replica with subnet_type application - project defaults" {
  install_asset subnet_type/project_defaults/application
  define_project_network
  jq '.defaults.replica.log_level="info"' dfx.json | sponge dfx.json

  assert_command dfx start --background
  assert_match "subnet_type: Application"
}

@test "dfx starts replica with subnet_type verifiedapplication - project defaults" {
  install_asset subnet_type/project_defaults/verified_application
  define_project_network
  jq '.defaults.replica.log_level="info"' dfx.json | sponge dfx.json

  assert_command dfx start --background
  assert_match "subnet_type: VerifiedApplication"
}

@test "dfx starts replica with subnet_type system - project defaults" {
  install_asset subnet_type/project_defaults/system
  define_project_network
  jq '.defaults.replica.log_level="info"' dfx.json | sponge dfx.json

  assert_command dfx start --background
  assert_match "subnet_type: System"
}

@test "dfx starts replica with subnet_type application - local network" {
  install_asset subnet_type/project_network_settings/application
  define_project_network
  jq '.networks.local.replica.log_level="info"' dfx.json | sponge dfx.json

  assert_command dfx start --background
  assert_match "subnet_type: Application"
}

@test "dfx starts replica with subnet_type verifiedapplication - local network" {
  install_asset subnet_type/project_network_settings/verified_application
  define_project_network
  jq '.networks.local.replica.log_level="info"' dfx.json | sponge dfx.json

  assert_command dfx start --background
  assert_match "subnet_type: VerifiedApplication"
}

@test "dfx starts replica with subnet_type system - local network" {
  install_asset subnet_type/project_network_settings/system
  define_project_network
  jq '.networks.local.replica.log_level="info"' dfx.json | sponge dfx.json

  assert_command dfx start --background
  assert_match "subnet_type: System"
}


@test "dfx starts replica with subnet_type application - shared network" {
  install_shared_asset subnet_type/shared_network_settings/application
  jq '.local.replica.log_level="info"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  assert_command dfx start --background
  assert_match "subnet_type: Application"
}

@test "dfx starts replica with subnet_type verifiedapplication - shared network" {
  install_shared_asset subnet_type/shared_network_settings/verified_application
  jq '.local.replica.log_level="info"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  assert_command dfx start --background
  assert_match "subnet_type: VerifiedApplication"
}

@test "dfx starts replica with subnet_type system - shared network" {
  install_shared_asset subnet_type/shared_network_settings/system
  jq '.local.replica.log_level="info"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  assert_command dfx start --background
  assert_match "subnet_type: System"
}

@test "dfx start detects if dfx is already running - shared network" {
  dfx_new hello
  dfx_start

  assert_command_fail dfx start
  assert_match "dfx is already running"
}

@test "dfx start for shared network warns about default settings specified in dfx.json that do not apply" {
  dfx_new hello

  IGNORED_MESSAGE="Ignoring the 'defaults' field in dfx.json because project settings never apply to the shared network."
  APPLY_SETTINGS_MESSAGE="To apply these settings to the shared network, define them in .*/config-root/.config/dfx/networks.json like so"

  jq 'del(.defaults)' dfx.json | sponge dfx.json
  jq '.defaults.bitcoin.enabled=true' dfx.json | sponge dfx.json
  assert_command dfx start --background
  assert_contains "$IGNORED_MESSAGE"
  assert_match "$APPLY_SETTINGS_MESSAGE"
  assert_contains '"bitcoin": {'
  assert_not_contains '"replica"'
  assert_not_contains '"canister_http"'
  assert_command dfx stop

  jq 'del(.defaults)' dfx.json | sponge dfx.json
  jq '.defaults.replica.log_level="info"' dfx.json | sponge dfx.json
  assert_command dfx start --background
  assert_contains "$IGNORED_MESSAGE"
  assert_match "$APPLY_SETTINGS_MESSAGE"
  assert_not_contains '"bitcoin"'
  assert_contains '"replica": {'
  assert_not_contains '"canister_http"'
  assert_command dfx stop

  jq 'del(.defaults)' dfx.json | sponge dfx.json
  jq '.defaults.canister_http.enabled=false' dfx.json | sponge dfx.json
  assert_command dfx start --background
  assert_contains "$IGNORED_MESSAGE"
  assert_match "$APPLY_SETTINGS_MESSAGE"
  assert_not_contains '"bitcoin"'
  assert_not_contains '"replica"'
  assert_contains '"canister_http": {'
  assert_command dfx stop

  jq 'del(.defaults)' dfx.json | sponge dfx.json
  jq '.defaults.bitcoin.enabled=true' dfx.json | sponge dfx.json
  jq '.defaults.replica.log_level="info"' dfx.json | sponge dfx.json
  jq '.defaults.canister_http.enabled=false' dfx.json | sponge dfx.json
  assert_command dfx start --background
  assert_contains "$IGNORED_MESSAGE"
  assert_match "$APPLY_SETTINGS_MESSAGE"
  assert_contains '"bitcoin": {'
  assert_contains '"replica": {'
  assert_contains '"canister_http": {'
  assert_command dfx stop
}

@test "dfx starts replica with correct log level - project defaults" {
  dfx_new
  jq '.defaults.replica.log_level="warning"' dfx.json | sponge dfx.json
  define_project_network

  assert_command dfx start --background --verbose
  assert_match "log level: Warning"
  assert_command dfx stop

  jq '.defaults.replica.log_level="critical"' dfx.json | sponge dfx.json
  assert_command dfx start --background --verbose --clean
  assert_match "log level: Critical"
}

@test "dfx starts replica with correct log level - local network" {
  dfx_new
  jq '.networks.local.replica.log_level="warning"' dfx.json | sponge dfx.json
  define_project_network

  assert_command dfx start --background --verbose
  assert_match "log level: Warning"
  assert_command dfx stop

  jq '.networks.local.replica.log_level="critical"' dfx.json | sponge dfx.json
  assert_command dfx start --background --verbose --clean
  assert_match "log level: Critical"
}

@test "dfx starts replica with correct log level - shared network" {
  dfx_new
  create_networks_json
  jq '.local.replica.log_level="warning"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"

  assert_command dfx start --background --verbose
  assert_match "log level: Warning"
  assert_command dfx stop

  jq '.local.replica.log_level="critical"' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
  assert_command dfx start --background --verbose --clean
  assert_match "log level: Critical"
}

@test "debug print statements work with default log level" {
  dfx_new
  install_asset print
  dfx_start 2>stderr.txt
  assert_command dfx deploy
  assert_command dfx canister call e2e_project hello
  sleep 2
  run tail -2 stderr.txt
  assert_match "Hello, World! from DFINITY"
}

@test "modifying networks.json does not require --clean on restart" {
  [[ "$USE_POCKETIC" ]] && skip "skipped for pocketic: --force"
  dfx_start
  dfx stop
  assert_command dfx_start
  dfx stop
  jq -n '.local.replica.log_level="warning"' > "$E2E_NETWORKS_JSON"
  assert_command dfx_start
}

@test "project-local networks require --clean if dfx.json was updated" {
  [[ "$USE_POCKETIC" ]] && skip "skipped for pocketic: --force"
  dfx_new
  define_project_network
  dfx_start
  dfx stop
  assert_command dfx_start
  dfx stop
  jq -n '.local.replica.log_level="warning"' > "$E2E_NETWORKS_JSON"
  assert_command dfx_start
  dfx stop
  jq '.networks.local.replica.log_level="warning"' dfx.json | sponge dfx.json
  assert_command_fail dfx_start
  assert_contains  "The network state can't be reused with this configuration. Rerun with \`--clean\`."
  assert_command dfx_start --force
  dfx stop
  assert_command dfx_start --clean
}

@test "flags count as configuration modification and require --clean for a project network" {
  dfx_new
  define_project_network

  dfx start --background
  dfx stop
  assert_command_fail dfx start --artificial-delay 100 --background
  assert_contains "The network state can't be reused with this configuration. Rerun with \`--clean\`."
  assert_command dfx start --artificial-delay 100 --clean --background
  dfx stop
  assert_command dfx start --artificial-delay 100 --background
  dfx stop
  assert_command_fail dfx start --background
  assert_contains "The network state can't be reused with this configuration. Rerun with \`--clean\`."
  assert_command dfx start --force --background
}

@test "dfx start then ctrl-c won't hang and panic but stop actors quickly" {
  assert_command "${BATS_TEST_DIRNAME}/../assets/expect_scripts/ctrl_c_right_after_dfx_start.exp"
}

@test "dfx-started processes can be killed with dfx killall" {
    dfx_start
    dfx killall
    assert_command_fail pgrep dfx replica pocket-ic
}


-----------------------

/e2e/tests-dfx/update_settings.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "set reserved cycles limit" {
    dfx_start
    assert_command dfx deploy hello_backend
    assert_command dfx canister status hello_backend
    assert_contains "Reserved cycles limit: 5_000_000_000_000 Cycles"

    assert_command dfx canister update-settings hello_backend --reserved-cycles-limit 650000
    assert_command dfx canister status hello_backend
    assert_contains "Reserved cycles limit: 650_000 Cycles"
}

@test "set freezing threshold" {
  dfx_start
  assert_command dfx deploy hello_backend

  # trying to set threshold to 1T seconds, which should not work because it's likely a mistake
  assert_command_fail dfx canister update-settings hello_backend --freezing-threshold 100000000000
  assert_match "SECONDS" # error message pointing to the error

  # with manual override it's ok
  assert_command dfx canister update-settings hello_backend --freezing-threshold 100000000000 --confirm-very-long-freezing-threshold

  # to check if threshold is set correctly we have to un-freeze the canister by adding cycles. Fabricating 100T cycles onto it
  assert_command dfx ledger fabricate-cycles --canister hello_backend --t 100
  assert_command dfx canister status hello_backend
  assert_match "Freezing threshold: 100_000_000_000"
}

@test "set wasm memory limit" {
  dfx_new_rust
  install_asset allocate_memory
  dfx_start
  assert_command dfx canister create e2e_project_backend --no-wallet --wasm-memory-limit 2MiB
  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "Wasm memory limit: 2_097_152 Bytes"
  # currently the limit is only checked when the memory grows. uncomment this line when that changes
  # assert_command dfx canister call e2e_project_backend greet_update '("alice")' 
  assert_command dfx canister update-settings e2e_project_backend --wasm-memory-limit 8b
  assert_command dfx canister status e2e_project_backend
  assert_contains "Wasm memory limit: 8 Bytes"
  assert_command dfx canister call e2e_project_backend greet '("alice")' --query
  assert_command dfx canister call e2e_project_backend greet '("alice")' --update
  assert_command_fail dfx canister call e2e_project_backend greet_update '("alice")'
  assert_contains "Canister exceeded its current Wasm memory limit of 8 bytes"
}

@test "set log visibility" {
  dfx_new
  dfx_start
  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister status e2e_project_backend

  # Test against a single canister.
  assert_contains "Log visibility: controllers"
  assert_command dfx canister update-settings e2e_project_backend --log-visibility public
  assert_command dfx canister status e2e_project_backend
  assert_contains "Log visibility: public"
  assert_command dfx canister update-settings e2e_project_backend --log-visibility controllers
  assert_command dfx canister status e2e_project_backend
  assert_contains "Log visibility: controllers"

  # Test --all code path.
  assert_command dfx canister update-settings --log-visibility public --all
  assert_command dfx canister status e2e_project_backend
  assert_contains "Log visibility: public"
  assert_command dfx canister update-settings --log-visibility controllers --all
  assert_command dfx canister status e2e_project_backend
  assert_contains "Log visibility: controllers"
}

@test "update log allowed viewer list" {
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)
  
  dfx_new
  dfx_start
  assert_command dfx deploy e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "Log visibility: controllers"

  # Test against a single canister.
  assert_command dfx canister update-settings --add-log-viewer="${ALICE_PRINCIPAL}" e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "${ALICE_PRINCIPAL}"

  assert_command dfx canister update-settings --remove-log-viewer="${BOB_PRINCIPAL}" e2e_project_backend
  assert_contains "'${BOB_PRINCIPAL}' is not in the allowed list"

  assert_command dfx canister update-settings --add-log-viewer="${BOB_PRINCIPAL}" --remove-log-viewer="${ALICE_PRINCIPAL}" e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "${BOB_PRINCIPAL}"
  assert_not_contains "${ALICE_PRINCIPAL}"

  assert_command dfx canister update-settings --set-log-viewer="${ALICE_PRINCIPAL}" e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "${ALICE_PRINCIPAL}"
  assert_not_contains "${BOB_PRINCIPAL}"

  assert_command dfx canister update-settings --remove-log-viewer="${ALICE_PRINCIPAL}" e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "allowed viewers list is empty"

  assert_command dfx canister update-settings --add-log-viewer="${BOB_PRINCIPAL}" --add-log-viewer="${ALICE_PRINCIPAL}" e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "${ALICE_PRINCIPAL}"
  assert_contains "${BOB_PRINCIPAL}"

  assert_command dfx canister update-settings --remove-log-viewer="${ALICE_PRINCIPAL}" --remove-log-viewer="${BOB_PRINCIPAL}" e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "allowed viewers list is empty"

  assert_command dfx canister update-settings --set-log-viewer="${BOB_PRINCIPAL}" --set-log-viewer="${ALICE_PRINCIPAL}" e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "${ALICE_PRINCIPAL}"
  assert_contains "${BOB_PRINCIPAL}"

  assert_command dfx canister update-settings --log-visibility controllers e2e_project_backend
  assert_command dfx canister status e2e_project_backend
  assert_contains "Log visibility: controllers"

  assert_command_fail dfx canister update-settings --remove-log-viewer="${BOB_PRINCIPAL}" e2e_project_backend
  assert_contains "Removing reviewers is not allowed with 'public' or 'controllers' log visibility."

  # Test --all code path.
  assert_command dfx canister update-settings --add-log-viewer="${ALICE_PRINCIPAL}" --all
  assert_command dfx canister status e2e_project_backend
  assert_contains "${ALICE_PRINCIPAL}"

  assert_command dfx canister update-settings --remove-log-viewer="${ALICE_PRINCIPAL}" --all
  assert_command dfx canister status e2e_project_backend
  assert_contains "allowed viewers list is empty"

  assert_command dfx canister update-settings --set-log-viewer="${ALICE_PRINCIPAL}" --all
  assert_command dfx canister status e2e_project_backend
  assert_contains "${ALICE_PRINCIPAL}"
}

@test "set controller" {
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)

  dfx canister create hello_backend
  dfx build hello_backend
  dfx canister install hello_backend
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller bob --yes
  assert_match "Set controller of \"hello_backend\" to: bob"

  # Bob is controller, Alice cannot reinstall
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall

  # Bob can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob

  assert_command dfx identity use bob
  # Set controller using canister id and principal
  assert_command dfx canister update-settings "$ID" --set-controller "${ALICE_PRINCIPAL}" --yes
  assert_match "Set controller of \"${ID}\" to: ${ALICE_PRINCIPAL}"
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall

  # Set controller using combination of name/id and identity/principal
  assert_command dfx canister update-settings hello_backend --set-controller "${BOB_PRINCIPAL}" --identity alice --yes
  assert_match "Set controller of \"hello_backend\" to: ${BOB_PRINCIPAL}"

  assert_command dfx canister update-settings "${ID}" --set-controller alice --identity bob --yes
  assert_match "Set controller of \"${ID}\" to: alice"

  # Set controller using invalid principal/identity fails
  assert_command_fail dfx canister update-settings hello_backend --set-controller charlie --identity alice --yes
  assert_match "Identity charlie does not exist"

  # Set controller using invalid canister name/id fails
  assert_command_fail dfx canister update-settings hello_assets --set-controller bob --identity alice --yes
  assert_match "Cannot find canister id. Please issue 'dfx canister create hello_assets'."

  # Fails if no consent is given
  echo "no" | assert_command_fail dfx canister update-settings "${ID}" --set-controller "${BOB_PRINCIPAL}" --identity alice
  # But works with typing "yes"
  echo "yes" | assert_command dfx canister update-settings "${ID}" --set-controller "${BOB_PRINCIPAL}" --identity alice
}

@test "set controller with wallet" {
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  BOB_WALLET=$(dfx identity get-wallet --identity bob)

  dfx canister create hello_backend --wallet alice
  dfx build hello_backend
  dfx canister install hello_backend --wallet alice
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller "${BOB_WALLET}" --wallet alice --yes
  assert_match "Set controller of \"hello_backend\" to: ${BOB_WALLET}"

  # Bob is controller, Alice cannot reinstall
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall --wallet alice

  # Bob can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob --wallet bob

  assert_command dfx identity use bob
  # Set controller using canister id and principal
  assert_command dfx canister update-settings "${ID}" --set-controller "${ALICE_WALLET}" --wallet bob --yes
  assert_match "Set controller of \"${ID}\" to: ${ALICE_WALLET}"
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall --wallet bob

  # Set controller using combination of name/id and identity/principal
  assert_command dfx canister update-settings hello_backend --set-controller "${BOB_WALLET}" --identity alice --wallet alice --yes
  assert_match "Set controller of \"hello_backend\" to: ${BOB_WALLET}"

  assert_command dfx canister update-settings "${ID}" --set-controller "${ALICE_WALLET}" --identity bob --wallet bob --yes
  assert_match "Set controller of \"${ID}\" to: ${ALICE_WALLET}"

  # Set controller using invalid principal/identity fails
  assert_command_fail dfx canister update-settings hello_backend --set-controller charlie --identity alice --wallet alice --yes
  assert_match "Identity charlie does not exist"

  # Set controller using invalid canister name/id fails
  assert_command_fail dfx canister update-settings hello_assets --set-controller bob --identity alice --wallet alice --yes
  assert_match "Cannot find canister id. Please issue 'dfx canister create hello_assets'."

  # Fails if no consent is given
  echo "no" | assert_command_fail dfx canister update-settings "${ID}" --set-controller "${BOB_WALLET}" --identity alice --wallet alice
  # But works with typing "yes"
  echo "yes" | assert_command dfx canister update-settings "${ID}" --set-controller "${BOB_WALLET}" --identity alice --wallet alice
}

@test "set controller with wallet 0.7.2" {
  use_wallet_wasm 0.7.2

  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  BOB_WALLET=$(dfx identity get-wallet --identity bob)

  dfx canister create hello_backend --wallet alice
  dfx build hello_backend
  dfx canister install hello_backend --wallet alice
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller "${BOB_WALLET}" --wallet alice --yes
  assert_match "Set controller of \"hello_backend\" to: ${BOB_WALLET}"

  # Bob is controller, Alice cannot reinstall
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall --wallet alice

  # Bob can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob --wallet bob

  assert_command dfx identity use bob
  # Set controller using canister id and principal
  assert_command dfx canister update-settings "${ID}" --set-controller "${ALICE_WALLET}" --wallet bob --yes
  assert_match "Set controller of \"${ID}\" to: ${ALICE_WALLET}"
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall --wallet bob

  # Set controller using combination of name/id and identity/principal
  assert_command dfx canister update-settings hello_backend --set-controller "${BOB_WALLET}" --identity alice --wallet alice --yes
  assert_match "Set controller of \"hello_backend\" to: ${BOB_WALLET}"

  assert_command dfx canister update-settings "${ID}" --set-controller "${ALICE_WALLET}" --identity bob --wallet bob --yes
  assert_match "Set controller of \"${ID}\" to: ${ALICE_WALLET}"

  # Set controller using invalid principal/identity fails
  assert_command_fail dfx canister update-settings hello_backend --set-controller charlie --identity alice --yes
  assert_match "Identity charlie does not exist"

  # Set controller using invalid canister name/id fails
  assert_command_fail dfx canister update-settings hello_assets --set-controller bob --identity alice --yes
  assert_match "Cannot find canister id. Please issue 'dfx canister create hello_assets'."

  # Fails if no consent is given
  echo "no" | assert_command_fail dfx canister update-settings "${ID}" --set-controller "${BOB_WALLET}" --identity alice --wallet alice
  # But works with typing "yes"
  echo "yes" | assert_command dfx canister update-settings "${ID}" --set-controller "${BOB_WALLET}" --identity alice --wallet alice
}

@test "set controller without wallet but using wallet 0.7.2" {
  use_wallet_wasm 0.7.2
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)

  dfx canister create hello_backend
  dfx canister update-settings hello_backend --add-controller "$ALICE_PRINCIPAL" --wallet "$(dfx identity get-wallet)"
  dfx build hello_backend
  dfx canister install hello_backend
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller bob --yes
  assert_match "Set controller of \"hello_backend\" to: bob"

  # Bob is controller, Alice cannot reinstall
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall

  # Bob can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob

  assert_command dfx identity use bob
  # Set controller using canister id and principal
  assert_command dfx canister update-settings "$ID" --set-controller "${ALICE_PRINCIPAL}" --yes
  assert_match "Set controller of \"${ID}\" to: ${ALICE_PRINCIPAL}"
  echo "yes" | assert_command_fail dfx canister install hello_backend -m reinstall

  # Set controller using combination of name/id and identity/principal
  assert_command dfx canister update-settings hello_backend --set-controller "${BOB_PRINCIPAL}" --identity alice --yes
  assert_match "Set controller of \"hello_backend\" to: ${BOB_PRINCIPAL}"

  assert_command dfx canister update-settings "${ID}" --set-controller alice --identity bob --yes
  assert_match "Set controller of \"${ID}\" to: alice"

  # Set controller using invalid principal/identity fails
  assert_command_fail dfx canister update-settings hello_backend --set-controller charlie --identity alice --yes
  assert_match "Identity charlie does not exist"

  # Set controller using invalid canister name/id fails
  assert_command_fail dfx canister update-settings hello_assets --set-controller bob --identity alice --yes
  assert_match "Cannot find canister id. Please issue 'dfx canister create hello_assets'."

  # Fails if no consent is given
  echo "no" | assert_command_fail dfx canister update-settings "${ID}" --set-controller "${BOB_PRINCIPAL}" --identity alice
  # But works with typing "yes"
  echo "yes" | assert_command dfx canister update-settings "${ID}" --set-controller "${BOB_PRINCIPAL}" --identity alice
}


@test "set multiple controllers" {
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)
  # awk step is to avoid trailing space
  PRINCIPALS_SORTED=$(echo "$ALICE_PRINCIPAL" "$BOB_PRINCIPAL" | tr " " "\n" | sort | tr "\n" " " | awk '{printf "%s %s",$1,$2}' )

  dfx canister create hello_backend
  dfx build hello_backend
  dfx canister install hello_backend
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller "${ALICE_PRINCIPAL}" --set-controller "${BOB_PRINCIPAL}"
  assert_match "Set controllers of \"hello_backend\" to: $PRINCIPALS_SORTED"

  # Both can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity alice
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob

  assert_command dfx canister info hello_backend
  assert_match "Controllers: ${PRINCIPALS_SORTED}"
}

@test "set multiple controllers with wallet" {
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  BOB_WALLET=$(dfx identity get-wallet --identity bob)
  # awk step is to avoid trailing space
  WALLETS_SORTED=$(echo "${ALICE_WALLET}" "${BOB_WALLET}" | tr " " "\n" | sort | tr "\n" " " | awk '{printf "%s %s",$1,$2}' )

  dfx canister create hello_backend
  dfx build hello_backend
  dfx canister install hello_backend
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller "${ALICE_WALLET}" --set-controller "${BOB_WALLET}" --wallet "${ALICE_WALLET}"
  assert_match "Set controllers of \"hello_backend\" to: ${WALLETS_SORTED}"

  # Both can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity alice --wallet alice
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob --wallet bob

  assert_command dfx canister info hello_backend
  assert_match "Controllers: ${WALLETS_SORTED}"
}

@test "set multiple controllers even with wallet 0.7.2" {
  use_wallet_wasm 0.7.2
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  BOB_WALLET=$(dfx identity get-wallet --identity bob)
  # awk step is to avoid trailing space
  WALLETS_SORTED=$(echo "${ALICE_WALLET}" "${BOB_WALLET}" | tr " " "\n" | sort | tr "\n" " " | awk '{printf "%s %s",$1,$2}' )

  dfx canister create hello_backend
  dfx build hello_backend
  dfx canister install hello_backend --wallet "${ALICE_WALLET}"
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller "${ALICE_WALLET}" --set-controller "${BOB_WALLET}" --wallet alice
  assert_match "Set controllers of \"hello_backend\" to: $WALLETS_SORTED"

  # Both can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity alice --wallet alice
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob --wallet bob

  assert_command dfx canister info hello_backend
  assert_match "Controllers: ${WALLETS_SORTED}"
}

@test "set multiple controllers without wallet but using wallet 0.7.2" {
  use_wallet_wasm 0.7.2
  # Create two identities
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob

  assert_command dfx identity use alice

  dfx_start
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)
  # awk step is to avoid trailing space
  PRINCIPALS_SORTED=$(echo "$ALICE_PRINCIPAL" "$BOB_PRINCIPAL" | tr " " "\n" | sort | tr "\n" " " | awk '{printf "%s %s",$1,$2}' )

  dfx canister create hello_backend
  dfx canister update-settings hello_backend --add-controller "$ALICE_PRINCIPAL" --wallet alice
  dfx build hello_backend
  dfx canister install hello_backend
  ID=$(dfx canister id hello_backend)

  # Set controller using canister name and identity name
  assert_command dfx canister update-settings hello_backend --set-controller "${ALICE_PRINCIPAL}" --set-controller "${BOB_PRINCIPAL}"
  assert_match "Set controllers of \"hello_backend\" to: $PRINCIPALS_SORTED"

  # Both can reinstall
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity alice
  echo "yes" | assert_command dfx canister install hello_backend -m reinstall --identity bob

  assert_command dfx canister info hello_backend
  assert_match "Controllers: ${PRINCIPALS_SORTED}"
}

@test "add controller to existing canister" {
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob
  assert_command dfx identity new --storage-mode plaintext charlie

  dfx identity use alice
  dfx_start

  ALICE=$(dfx identity get-principal --identity alice)
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  BOB=$(dfx identity get-principal --identity bob)
  CHARLIE=$(dfx identity get-principal --identity charlie)
  SORTED=$(echo "$ALICE" "$ALICE_WALLET" "$BOB" "$CHARLIE" | tr " " "\n" | sort | tr "\n" " " | awk '{printf "%s %s %s %s",$1,$2,$3,$4}' )

  dfx canister create hello_backend
  dfx build hello_backend
  dfx canister install hello_backend

  # make bob a controller
  assert_command dfx canister update-settings hello_backend --add-controller bob
  # check that bob has the authority to make someone else a controller
  assert_command dfx canister update-settings hello_backend --add-controller charlie --identity bob
  assert_command dfx canister info hello_backend
  assert_match "Controllers: $SORTED"
}

@test "add controller to all canisters" {
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob
  assert_command dfx identity new --storage-mode plaintext charlie

  dfx identity use alice
  dfx_start

  ALICE=$(dfx identity get-principal --identity alice)
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  BOB=$(dfx identity get-principal --identity bob)
  CHARLIE=$(dfx identity get-principal --identity charlie)
  SORTED=$(echo "$ALICE" "$ALICE_WALLET" "$BOB" "$CHARLIE" | tr " " "\n" | sort | tr "\n" " " | awk '{printf "%s %s %s %s",$1,$2,$3,$4}' )

  dfx canister create --all
  dfx build --all
  dfx canister install --all

  # make bob a controller
  assert_command dfx canister update-settings --all --add-controller bob
  # check that bob has the authority to make someone else a controller
  assert_command dfx canister update-settings --all --add-controller charlie --identity bob
  assert_command dfx canister info hello_backend
  assert_match "Controllers: $SORTED"
}

@test "update settings by canister id, when canister id is not known to the project" {
  dfx_start
  dfx deploy

  CANISTER_ID=$(dfx canister id hello_backend)

  rm .dfx/local/canister_ids.json
  jq '.canisters={}' dfx.json | sponge dfx.json

  assert_command dfx canister status "$CANISTER_ID"
  assert_match 'Memory allocation: 0'
  assert_match 'Compute allocation: 0'
  assert_match 'Freezing threshold: 2_592_000'

  dfx canister update-settings --memory-allocation 2GB "$CANISTER_ID"
  assert_command dfx canister status "$CANISTER_ID"
  assert_match 'Memory allocation: 2_000_000_000'
  assert_match 'Compute allocation: 0'
  assert_match 'Freezing threshold: 2_592_000'

  # This is just checking that update-settings leaves the previous value
  # (of memory allocation) alone when setting something else

  # Compute allocations are temporarily disabled.
  # See https://dfinity.atlassian.net/browse/RUN-314
  # dfx canister update-settings --compute-allocation 1 "$CANISTER_ID"

  dfx canister update-settings --freezing-threshold 172 "$CANISTER_ID"
  assert_command dfx canister status "$CANISTER_ID"
  assert_match 'Memory allocation: 2_000_000_000'
  # assert_match 'Compute allocation: 4'
  assert_match 'Freezing threshold: 172'
}

@test "remove controller" {
  assert_command dfx identity new --storage-mode plaintext alice
  assert_command dfx identity new --storage-mode plaintext bob
  ALICE_PRINCIPAL=$(dfx identity get-principal --identity alice)
  BOB_PRINCIPAL=$(dfx identity get-principal --identity bob)
  dfx identity use alice

  dfx_start
  dfx deploy
  WALLET_PRINCIPAL=$(dfx identity get-wallet)

  assert_command dfx canister update-settings hello_backend --add-controller "${BOB_PRINCIPAL}"
  assert_command dfx canister info hello_backend
  assert_contains "${BOB_PRINCIPAL}"
  assert_command dfx canister update-settings hello_backend --remove-controller "${BOB_PRINCIPAL}"
  assert_command dfx canister info hello_backend
  assert_not_contains "${BOB_PRINCIPAL}"

  # Cannot remove own controller without extra consent
  echo "no" | assert_command_fail dfx canister update-settings hello_backend --remove-controller "${ALICE_PRINCIPAL}"
  assert_command dfx canister info hello_backend
  assert_contains "${ALICE_PRINCIPAL}"
  echo "yes" | assert_command dfx canister update-settings hello_backend --remove-controller "${ALICE_PRINCIPAL}"
  assert_command dfx canister info hello_backend
  assert_not_contains "${ALICE_PRINCIPAL}"

  # Cannot remove wallet controller without extra consent
  echo "no" | assert_command_fail dfx canister update-settings hello_backend --remove-controller "${WALLET_PRINCIPAL}" --wallet alice
  assert_command dfx canister info hello_backend
  assert_contains "${WALLET_PRINCIPAL}"
  echo "yes" | assert_command dfx canister update-settings hello_backend --remove-controller "${WALLET_PRINCIPAL}" --wallet alice
  assert_command dfx canister info hello_backend
  assert_not_contains "${WALLET_PRINCIPAL}"

}


-----------------------

/e2e/tests-dfx/upgrade_check.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new hello
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "safe upgrade by adding a new stable variable" {
  install_asset upgrade
  dfx_start
  dfx deploy
  dfx canister call hello_backend inc '()'
  jq '.canisters.hello_backend.main="v2.mo"' dfx.json | sponge dfx.json
  dfx deploy
  assert_command dfx canister call hello_backend read '()'
  assert_match "(1 : nat)"
}

@test "changing stable variable from Int to Nat is not allowed" {
  install_asset upgrade
  dfx_start
  dfx deploy
  dfx canister call hello_backend inc '()'
  jq '.canisters.hello_backend.main="v2_bad.mo"' dfx.json | sponge dfx.json
  echo yes | (
  assert_command dfx deploy
  assert_match "Stable interface compatibility check issued an ERROR"
  )
  assert_command dfx canister call hello_backend read '()'
  assert_match "(0 : nat)"
}

@test "changing stable variable from Int to Nat with reinstall is allowed" {
  install_asset upgrade
  dfx_start
  dfx deploy
  dfx canister call hello_backend inc '()'
  jq '.canisters.hello_backend.main="v2_bad.mo"' dfx.json | sponge dfx.json
  dfx build
  echo yes | dfx canister install hello_backend --mode=reinstall
  assert_command dfx canister call hello_backend read '()'
  assert_match "(0 : nat)"
}

@test "warning for changing method name" {
  install_asset upgrade
  dfx_start
  dfx deploy
  dfx canister call hello_backend inc '()'
  jq '.canisters.hello_backend.main="v3_bad.mo"' dfx.json | sponge dfx.json
  echo yes | (
  assert_command dfx deploy
  assert_match "Candid interface compatibility check failed"
  )
  assert_command dfx canister call hello_backend read2 '()'
  assert_match "(1 : int)"
}

@test "warning for using special opt rule" {
  install_asset upgrade
  dfx_start
  dfx deploy
  dfx canister call hello_backend f '()'
  jq '.canisters.hello_backend.main="v4_bad.mo"' dfx.json | sponge dfx.json
  echo yes | (
  assert_command dfx deploy
  assert_match "Candid interface compatibility check failed"
  )
  assert_command dfx canister call hello_backend f '()'
  assert_match "(opt \"\")"
}

@test "warning when dropping stable variable" {
  install_asset upgrade
  dfx_start
  dfx deploy
  dfx canister call hello_backend inc '()'
  jq '.canisters.hello_backend.main="v5.mo"' dfx.json | sponge dfx.json
  echo yes | (
  assert_command dfx deploy
  assert_match "Stable interface compatibility check issued a WARNING"
  )
  assert_command dfx canister call hello_backend read '()'
  assert_match "(0 : int)"
}


-----------------------

/e2e/tests-dfx/usage.bash:
-----------------------

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "dfx help succeeds" {
  dfx --help
}

@test "dfx help contains new command" {
  dfx --help | grep new
}

@test "using an invalid command fails" {
  run dfx blurp
  if [[ $status -eq 0 ]]; then
    echo "$@" >&2
    exit 1
  fi
}

@test "returns the right error if not in a project" {

  assert_command_fail dfx build
  assert_match "Cannot find dfx configuration file in the current working directory. Did you forget to create one?"

  dfx new t --no-frontend
  cd t
  dfx_start
  dfx canister create --all
  assert_command dfx build
}

@test "does not create .dfx just by running dfx even if in a project" {
  echo "{}" >dfx.json
  dfx identity get-principal
  assert_directory_not_exists .dfx
}

@test "does not unconditionally read dfx.json" {
  echo "garbage" >dfx.json
  assert_command dfx identity get-principal
}

@test "--identity and --network are stil accepted as prefix" {
  dfx_new
  install_asset whoami
  dfx_start
  dfx deploy
  dfx identity new alice --storage-mode plaintext
  assert_command dfx --identity alice canister --network local call whoami whoami
  assert_match "$(dfx --identity alice identity get-principal)"
  assert_match "$(dfx identity get-principal --identity alice)"
}



-----------------------

/e2e/tests-dfx/usage_env.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  standard_teardown
}

@test "dfx config root env var stores identity & cache" {
  use_test_specific_cache_root   # Because this test depends on a clean cache state

  #identity
  dfx identity new --storage-mode plaintext alice
  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/alice/identity.pem"
  assert_command head "$DFX_CONFIG_ROOT/.config/dfx/identity/default/identity.pem"

  assert_command_fail head "$HOME/.config/dfx/identity/alice/identity.pem"
  assert_command_fail head "$HOME/.config/dfx/identity/default/identity.pem"

  #cache
  # create a new project to install dfx cache
  assert_command_fail ls "$DFX_CACHE_ROOT/.cache/dfinity/versions"
  dfx new hello --no-frontend
  assert_command ls "$DFX_CACHE_ROOT/.cache/dfinity/versions"
  assert_command_fail ls "$HOME/.cache/dfinity/versions"
  rm -rf hello

  (
    # use subshell to retain $DFX_CONFIG_ROOT for teardown
    # remove configured variable, should use $HOME now
    unset DFX_CACHE_ROOT
    unset DFX_CONFIG_ROOT

    dfx identity new --storage-mode plaintext bob
    assert_command head "$HOME/.config/dfx/identity/bob/identity.pem"
    assert_command head "$HOME/.config/dfx/identity/default/identity.pem"

    #cache
    # create a new project to install dfx cache
    assert_command_fail ls "$HOME/.cache/dfinity/versions"
    dfx new hello --no-frontend
    assert_command ls "$HOME/.cache/dfinity/versions"
    rm -rf hello
  )
}


-----------------------

/e2e/tests-dfx/wallet.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "error reporting for --wallet parameter" {
  assert_command_fail dfx canister call hello_backend greet '' --with-cycles 100 --wallet "abc-def"
  assert_contains "Failed to read principal from id 'abc-def', and did not find a wallet for that identity"
  assert_contains "Text must be in valid Base32 encoding"

  assert_command_fail dfx canister call hello_backend greet '' --with-cycles 100 --wallet "alice"
  assert_contains "Failed to read principal from id 'alice', and did not find a wallet for that identity"

  mkdir -p "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY"
  echo "{}" > "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json"
  assert_command_fail dfx canister call hello_backend greet '' --with-cycles 100 --wallet broken
  assert_contains "Failed to read principal from id 'broken' (Text must be in valid Base32 encoding.), and failed to load the wallet for that identity"
  assert_contains "missing field \`identities\`"
}


@test "deposit cycles inside a project" {
    dfx_start

    dfx_new
    assert_command dfx deploy
    assert_command dfx canister deposit-cycles 47 e2e_project_backend
    assert_contains "Deposited 47 cycles"
}

@test "deposit cycles outside a project" {
    dfx_start

    mkdir subdir
    cd subdir || exit 1
    dfx_new
    assert_command dfx deploy
    CANISTER_ID="$(dfx canister id e2e_project_backend)"
    cd ..
    assert_command dfx canister deposit-cycles 42 "$CANISTER_ID"
    assert_contains "Deposited 42 cycles"
}

@test "DFX_WALLET_WASM environment variable overrides wallet module wasm at installation" {
  dfx_new hello
  dfx_start

  dfx identity new --storage-mode plaintext alice
  dfx identity new --storage-mode plaintext bob

  use_wallet_wasm 0.7.0
  assert_command dfx identity get-wallet --identity alice
  assert_match "Using wasm at path: .*/wallet/0.7.0/wallet.wasm"

  use_wallet_wasm 0.7.2
  assert_command dfx identity get-wallet --identity bob
  assert_match "Using wasm at path: .*/wallet/0.7.2/wallet.wasm"

  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  BOB_WALLET=$(dfx identity get-wallet --identity bob)

  assert_command dfx canister info "$ALICE_WALLET" --identity alice
  assert_match "Module hash: 0xa609400f2576d1d6df72ce868b359fd08e1d68e58454ef17db2361d2f1c242a1"

  assert_command dfx canister info "$BOB_WALLET" --identity bob
  assert_match "Module hash: 0x1404b28b1c66491689b59e184a9de3c2be0dbdd75d952f29113b516742b7f898"
}

@test "DFX_WALLET_WASM environment variable overrides wallet module wasm for upgrade" {
  dfx_new hello
  dfx_start

  use_wallet_wasm 0.7.0-beta.5

  assert_command dfx identity get-wallet
  WALLET_ID=$(dfx identity get-wallet)

  assert_command dfx canister info "$WALLET_ID"
  assert_match "Module hash: 0x3d5b221387875574a9fd75b3165403cf1b301650a602310e9e4229d2f6766dcc"

  use_wallet_wasm 0.7.0
  assert_command dfx wallet upgrade

  assert_command dfx canister info "$WALLET_ID"
  assert_match "Module hash: 0xa609400f2576d1d6df72ce868b359fd08e1d68e58454ef17db2361d2f1c242a1"
}

@test "'dfx identity set-wallet --force' bypasses wallet canister verification" {
  dfx_new_assets hello
  dfx_start
  setup_actuallylocal_shared_network

  # get Canister IDs to install the wasm onto
  dfx canister create hello_backend --network actuallylocal
  ID=$(dfx canister id hello_backend --network actuallylocal)
  dfx canister create hello_frontend --network actuallylocal
  ID_TWO=$(dfx canister id hello_frontend --network actuallylocal)

  # set controller to user
  dfx canister update-settings hello_backend --set-controller "$(dfx identity get-principal)" --network actuallylocal
  dfx canister update-settings hello_frontend --set-controller "$(dfx identity get-principal)" --network actuallylocal

  assert_command_fail dfx identity set-wallet "${ID}" --network actuallylocal
  assert_not_match "Setting wallet for identity"
  assert_command dfx identity set-wallet --force "${ID}" --network actuallylocal
  assert_match "Setting wallet for identity 'default' on network 'actuallylocal' to id '$ID'"
  assert_command jq -r .identities.default.actuallylocal <"$DFX_CONFIG_ROOT"/.config/dfx/identity/default/wallets.json
  assert_eq "$ID"
}

@test "deploy wallet" {
  dfx_new_assets hello
  dfx_start
  setup_actuallylocal_shared_network

  # get Canister IDs to install the wasm onto
  dfx canister create hello_frontend --network actuallylocal
  ID=$(dfx canister id hello_frontend --network actuallylocal)
  dfx deploy hello_backend --network actuallylocal
  ID_TWO=$(dfx canister id hello_backend --network actuallylocal)

  # set controller to user
  dfx canister update-settings hello_backend --set-controller "$(dfx identity get-principal)" --network actuallylocal
  dfx canister update-settings hello_frontend --set-controller "$(dfx identity get-principal)" --network actuallylocal

  # We're testing on a local network so the create command actually creates a wallet
  # Delete this file to force associate wallet created by deploy-wallet to identity
  rm "$DFX_CONFIG_ROOT"/.config/dfx/identity/default/wallets.json

  assert_command dfx identity deploy-wallet "${ID}" --network actuallylocal
  GET_WALLET_RES=$(dfx identity get-wallet --network actuallylocal)
  assert_eq "$ID" "$GET_WALLET_RES"

  # Command should fail on an already-deployed canister
  assert_command_fail dfx identity deploy-wallet "${ID_TWO}" --network actuallylocal
  assert_match "The wallet canister \"${ID_TWO}\"\ already exists for user \"default\" on \"actuallylocal\" network."
}

@test "wallet create wallet" {
  dfx_new
  dfx_start
  WALLET_ID=$(dfx identity get-wallet)
  CREATE_RES=$(dfx canister call "${WALLET_ID}" wallet_create_wallet "(record { cycles = (2000000000000:nat64); settings = record {controller = opt principal \"$(dfx identity get-principal)\";};})")
  CHILD_ID=$(echo "${CREATE_RES}" | tr '\n' ' ' |  cut -d'"' -f 2)
  assert_command dfx canister call "${CHILD_ID}" wallet_balance '()'
}

@test "forward user call through wallet" {
  dfx_new
  install_asset identity
  dfx_start
  WALLET=$(dfx identity get-wallet)
  assert_command dfx canister create --all --wallet default
  assert_command dfx build
  assert_command dfx canister install --all --wallet default

  CALL_RES=$(dfx canister call e2e_project_backend fromCall --wallet default)
  CALLER=$(echo "${CALL_RES}" | cut -d'"' -f 2)
  assert_eq "$CALLER" "$WALLET"

  assert_command dfx canister call "$WALLET" wallet_call \
    "(record { canister = principal \"$(dfx canister id e2e_project_backend)\"; method_name = \"amInitializer\"; args = blob \"DIDL\00\00\"; cycles = (0:nat64)})"
  # shellcheck disable=SC2154
  assert_eq '(variant { 17_724 = record { 153_986_224 = blob "\44\49\44\4c\00\01\7e\01" } })' "$stdout"  # True in DIDL.
}

@test "forward user call through wallet: deploy" {
  dfx_new
  install_asset identity
  dfx_start
  WALLET=$(dfx identity get-wallet)
  assert_command dfx deploy --wallet "$WALLET"
  CALL_RES=$(dfx canister call e2e_project_backend fromCall --wallet default)
  CALLER=$(echo "${CALL_RES}" | cut -d'"' -f 2)
  assert_eq "$CALLER" "$WALLET"

  assert_command dfx canister call e2e_project_backend amInitializer
  assert_command dfx canister call "$WALLET" wallet_call \
    "(record { canister = principal \"$(dfx canister id e2e_project_backend)\"; method_name = \"amInitializer\"; args = blob \"DIDL\00\00\"; cycles = (0:nat64)})"
  # shellcheck disable=SC2154
  assert_eq '(variant { 17_724 = record { 153_986_224 = blob "\44\49\44\4c\00\01\7e\01" } })' "$stdout" # True in DIDL.
}

@test "a 64-bit wallet can still be called in the 128-bit context" {
  use_wallet_wasm 0.8.2
  dfx_new hello
  dfx_start
  WALLET=$(dfx identity get-wallet)
  assert_command dfx wallet balance
  assert_command dfx deploy --wallet default
  assert_command dfx canister call hello_backend greet '("")' --with-cycles 1 --wallet default
  dfx identity new alice --storage-mode plaintext
  ALICE_WALLET=$(dfx identity get-wallet --identity alice)
  dfx wallet send "$ALICE_WALLET" 1
}

@test "dfx canister deposit-cycles succeeds on a canister the caller does not own" {
  dfx_new hello
  dfx_start
  dfx identity new alice --storage-mode plaintext
  dfx deploy --no-wallet hello_backend --identity alice
  dfx identity get-wallet
  assert_command dfx canister deposit-cycles 1 hello_backend --wallet default
}

@test "dfx canister deposit-cycles uses default wallet if no wallet is specified" {
  dfx_new hello
  dfx_start
  dfx deploy
  assert_command dfx canister deposit-cycles 1 hello_backend
}

@test "detects if there is no wallet to upgrade" {
  dfx_new hello
  assert_command_fail dfx wallet upgrade
  assert_match "There is no wallet defined for identity 'default' on network 'local'.  Nothing to do."
}

@test "redeem-faucet-coupon can set a new wallet and top up an existing one" {
  dfx_new hello
  dfx_start
  install_asset faucet
  dfx deploy
  dfx ledger fabricate-cycles --canister faucet --t 1000

  dfx identity new --storage-mode plaintext faucet_testing
  dfx identity use faucet_testing

  # prepare wallet to hand out
  dfx wallet balance # this creates a new wallet with user faucet_testing as controller
  dfx canister call faucet set_wallet_to_hand_out "(principal \"$(dfx identity get-wallet)\")" # register the wallet as the wallet that the faucet will return
  rm "$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY/wallets.json" # forget about the currently configured wallet

  # assert: no wallet configured
  export DFX_DISABLE_AUTO_WALLET=1
  assert_command_fail dfx wallet balance
  assert_match "No wallet configured"

  assert_command dfx wallet redeem-faucet-coupon --faucet "$(dfx canister id faucet)" 'valid-coupon' --yes
  assert_match "Redeemed coupon valid-coupon for a new wallet"
  assert_match "New wallet set."

  # only succeeds if wallet is correctly set
  assert_command dfx wallet balance
  # checking only balance before the dot, rest may fluctuate
  # balance may be 99.??? TC if cycles accounting is done, or 100.000 TC if not
  assert_match "99\.|100\."

  unset DFX_DISABLE_AUTO_WALLET

  assert_command dfx wallet redeem-faucet-coupon --faucet "$(dfx canister id faucet)" 'another-valid-coupon' --yes
  assert_match "Redeemed coupon code another-valid-coupon for 10.000 TC"

  assert_command dfx wallet balance
  # checking only balance before the dot, rest may fluctuate
  # balance may be 109.??? TC if cycles accounting is done, or 110.000 TC if not
  assert_match "109\.|110\."
}


-----------------------

/e2e/tests-dfx/wasm_memory_persistence.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup

  dfx_new test
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "migrate Motoko from classical persistence to classical persistence" {
  install_asset wasm_memory_persistence
  dfx_start
  dfx deploy
  dfx deploy --upgrade-unchanged
  assert_command dfx canister call test getVersion '()'
  assert_match "(2 : nat)"
}

@test "migrate Motoko from classical persistence to enhanced orthogonal persistence" {
  install_asset wasm_memory_persistence
  dfx_start
  dfx deploy
  jq '.canisters.test.wasm="enhanced-actor.wasm"' dfx.json | sponge dfx.json
  dfx deploy
  assert_command dfx canister call test getVersion '()'
  assert_match "(2 : nat)"
}

@test "migrate Motoko from enhanced orthogonal persistence to enhanced orthogonal persistence" {
  install_asset wasm_memory_persistence
  dfx_start
  jq '.canisters.test.wasm="enhanced-actor.wasm"' dfx.json | sponge dfx.json
  dfx deploy
  jq '.canisters.test.wasm="enhanced-actor.wasm"' dfx.json | sponge dfx.json
  dfx deploy --upgrade-unchanged
  assert_command dfx canister call test getVersion '()'
  assert_match "(2 : nat)"
}

@test "failing Motoko downgrade from enhanced orthogonal persistence to classical persistence" {
  install_asset wasm_memory_persistence
  dfx_start
  jq '.canisters.test.wasm="enhanced-actor.wasm"' dfx.json | sponge dfx.json
  dfx deploy
  jq '.canisters.test.wasm="classical-actor.wasm"' dfx.json | sponge dfx.json
  assert_command_fail dfx deploy
  assert_match "The \`wasm_memory_persistence: opt Keep\` upgrade option requires that the new canister module supports enhanced orthogonal persistence."
}

@test "re-install Motoko enhanced orthogonal persistence with classical persistence" {
  install_asset wasm_memory_persistence
  dfx_start
  jq '.canisters.test.wasm="enhanced-actor.wasm"' dfx.json | sponge dfx.json
  dfx deploy
  jq '.canisters.test.wasm="classical-actor.wasm"' dfx.json | sponge dfx.json
  echo yes | dfx canister install test --mode=reinstall
  assert_command dfx canister call test getVersion '()'
  assert_match "(1 : nat)"
}


-----------------------

/e2e/tests-icx-asset/icx-asset.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  # when running e2e tests not in GitHub CI (so e.g. locally), build icx-asset and set environment variable
  if [ -z "$ICX_ASSET" ]; then
    cargo build -p icx-asset
    ICX_ASSET="$(pwd)/target/debug/icx-asset"
  fi

  standard_setup

  dfx_new_assets
  dfx_start

  assert_command dfx deploy
}

teardown() {
  dfx_stop

  standard_teardown
}

icx_asset_sync() {
  IDENTITY="$DFX_CONFIG_ROOT"/.config/dfx/identity/default/identity.pem
  REPLICA_ADDRESS="http://localhost:$(get_webserver_port)"
  CANISTER_ID=$(dfx canister id e2e_project_frontend)
  if [ -z "$1" ]; then
    assert_command "$ICX_ASSET" --pem "$IDENTITY" --replica "$REPLICA_ADDRESS" sync "$CANISTER_ID" src/e2e_project_frontend/assets
  else
    # shellcheck disable=SC2086
    assert_command "$ICX_ASSET" --pem "$IDENTITY" --replica "$REPLICA_ADDRESS" sync "$CANISTER_ID" $1 $2
  fi
}

icx_asset_list() {
  IDENTITY="$DFX_CONFIG_ROOT"/.config/dfx/identity/default/identity.pem
  REPLICA_ADDRESS="http://localhost:$(get_webserver_port)"
  CANISTER_ID=$(dfx canister id e2e_project_frontend)
  assert_command "$ICX_ASSET" --pem "$IDENTITY" --replica "$REPLICA_ADDRESS" ls "$CANISTER_ID"
}

icx_asset_upload() {
  IDENTITY="$DFX_CONFIG_ROOT"/.config/dfx/identity/default/identity.pem
  REPLICA_ADDRESS="http://localhost:$(get_webserver_port)"
  CANISTER_ID=$(dfx canister id e2e_project_frontend)
  # shellcheck disable=SC2086
  assert_command "$ICX_ASSET" --pem "$IDENTITY" --replica "$REPLICA_ADDRESS" upload "$CANISTER_ID" $1 $2
}

@test "lists assets" {
  for i in $(seq 1 400); do
    echo "some easily duplicate text $i" >>src/e2e_project_frontend/assets/notreally.js
  done
  icx_asset_sync

  icx_asset_list

  assert_match "sample-asset.txt.*text/plain.*identity"
  assert_match "notreally.js.*application/javascript.*gzip"
  assert_match "notreally.js.*application/javascript.*identity"
}

@test "creates new files" {
  echo "new file content" >src/e2e_project_frontend/assets/new-asset.txt
  icx_asset_sync

  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/new-asset.txt";accept_encodings=vec{"identity"}})'
}

@test "updates existing files" {
  echo -n "an asset that will change" >src/e2e_project_frontend/assets/asset-to-change.txt
  assert_command dfx deploy

  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/asset-to-change.txt";accept_encodings=vec{"identity"}})'
  # shellcheck disable=SC2154
  assert_match '"an asset that will change"' "$stdout"

  echo -n "an asset that has been changed" >src/e2e_project_frontend/assets/asset-to-change.txt

  icx_asset_sync

  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/asset-to-change.txt";accept_encodings=vec{"identity"}})'
  # shellcheck disable=SC2154
  assert_match '"an asset that has been changed"' "$stdout"
  echo pass
}

@test "deletes removed files" {
  touch src/e2e_project_frontend/assets/will-delete-this.txt
  dfx deploy

  assert_command dfx canister call --query e2e_project_frontend get '(record{key="/will-delete-this.txt";accept_encodings=vec{"identity"}})'
  assert_command dfx canister call --query e2e_project_frontend list  '(record{})'
  assert_match '"/will-delete-this.txt"'

  rm src/e2e_project_frontend/assets/will-delete-this.txt

  icx_asset_sync

  assert_command_fail dfx canister call --query e2e_project_frontend get '(record{key="/will-delete-this.txt";accept_encodings=vec{"identity"}})'
  assert_command dfx canister call --query e2e_project_frontend list  '(record{})'
  assert_not_match '"/will-delete-this.txt"'
}

@test "does not delete removed files if --no-delete is passed" {
  touch src/e2e_project_frontend/assets/will-not-delete-this.txt
  dfx deploy

  assert_command dfx canister call --query e2e_project_frontend get '(record{key="/will-not-delete-this.txt";accept_encodings=vec{"identity"}})'
  assert_command dfx canister call --query e2e_project_frontend list '(record{})'
  assert_match '"/will-not-delete-this.txt"'

  rm src/e2e_project_frontend/assets/will-not-delete-this.txt

  icx_asset_sync src/e2e_project_frontend/assets --no-delete

  assert_command dfx canister call --query e2e_project_frontend get '(record{key="/will-not-delete-this.txt";accept_encodings=vec{"identity"}})'
  assert_command dfx canister call --query e2e_project_frontend list '(record{})'
  assert_match '"/will-not-delete-this.txt"'
  icx_asset_sync src/e2e_project_frontend/assets
  
  assert_command_fail dfx canister call --query e2e_project_frontend get '(record{key="/will-not-delete-this.txt";accept_encodings=vec{"identity"}})'
  assert_command dfx canister call --query e2e_project_frontend list '(record{})'
  assert_not_contains '"/will-not-delete-this.txt"'
}

@test "unsets asset encodings that are removed from project" {

  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --update e2e_project_frontend store '(record{key="/sample-asset.txt"; content_type="text/plain"; content_encoding="arbitrary"; content=blob "content encoded in another way!"})'

  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/sample-asset.txt";accept_encodings=vec{"identity"}})'
  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/sample-asset.txt";accept_encodings=vec{"arbitrary"}})'

  icx_asset_sync

  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/sample-asset.txt";accept_encodings=vec{"identity"}})'
  # shellcheck disable=SC2086
  assert_command_fail dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/sample-asset.txt";accept_encodings=vec{"arbitrary"}})'
}

@test "synchronizes multiple directories" {
  mkdir -p multiple/a
  mkdir -p multiple/b
  echo "x_contents" >multiple/a/x
  echo "y_contents" >multiple/b/y

  icx_asset_sync multiple/a multiple/b
  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/x";accept_encodings=vec{"identity"}})'
  assert_match "x_contents"
  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --query e2e_project_frontend get '(record{key="/y";accept_encodings=vec{"identity"}})'
  assert_match "y_contents"
}

@test "reports errors about assets with the same key from multiple sources" {
  mkdir -p multiple/a
  mkdir -p multiple/b
  echo "a_duplicate_contents" >multiple/a/duplicate
  echo "b_duplicate_contents" >multiple/b/duplicate

  assert_command_fail icx_asset_sync multiple/a multiple/b
  assert_match "Asset with key '/duplicate' defined at .*/e2e_project/multiple/b/duplicate and .*/e2e_project/multiple/a/duplicate"
}

@test "ignores filenames and directories starting with a dot" {
  touch src/e2e_project_frontend/assets/.not-seen
  touch src/e2e_project_frontend/assets/is-seen

  mkdir -p src/e2e_project_frontend/assets/.dir-skipped
  touch src/e2e_project_frontend/assets/.dir-skipped/also-ignored

  mkdir -p src/e2e_project_frontend/assets/dir-not-skipped
  touch src/e2e_project_frontend/assets/dir-not-skipped/not-ignored

  icx_asset_sync

  assert_command dfx canister call --query e2e_project_frontend get '(record{key="/is-seen";accept_encodings=vec{"identity"}})'
  assert_command dfx canister call --query e2e_project_frontend get '(record{key="/dir-not-skipped/not-ignored";accept_encodings=vec{"identity"}})'
  assert_command_fail dfx canister call --query e2e_project_frontend get '(record{key="/.not-seen";accept_encodings=vec{"identity"}})'
  assert_command_fail dfx canister call --query e2e_project_frontend get '(record{key="/.dir-skipped/also-ignored";accept_encodings=vec{"identity"}})'

  assert_command dfx canister call --query e2e_project_frontend list  '(record{})'

  assert_match 'is-seen'
  assert_match 'not-ignored'

  assert_not_match 'not-seen'
  assert_not_match 'also-ignored'
}

@test "does not delete files that are not being uploaded" {
  mkdir some_dir
  echo "some stuff" >some_dir/a.txt
  echo "more things" >some_dir/b.txt

  icx_asset_upload /=some_dir

  icx_asset_list

  assert_match " /a.txt.*text/plain.*identity"
  assert_match " /b.txt.*text/plain.*identity"

  echo "ccc" >c.txt
  icx_asset_upload c.txt

  icx_asset_list

  assert_match " /a.txt.*text/plain.*identity"
  assert_match " /b.txt.*text/plain.*identity"
  assert_match " /c.txt.*text/plain.*identity"
}

@test "deletes asset if necessary in order to change content type" {
  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --update e2e_project_frontend store '(record{key="/sample-asset.txt"; content_type="application/pdf"; content_encoding="identity"; content=blob "whatever contents!"})'
  # shellcheck disable=SC2086
  assert_command dfx canister ${DFX_NO_WALLET:-} call --update e2e_project_frontend store '(record{key="/sample-asset.txt"; content_type="application/pdf"; content_encoding="arbitrary"; content=blob "other contents"})'

  icx_asset_list

  assert_match " /sample-asset.txt.*application/pdf.*identity"
  assert_match " /sample-asset.txt.*application/pdf.*arbitrary"

  echo "just some text" >sample-asset.txt

  # icx-asset upload should delete the asset (and upload its replacement) since the content type is different.
  icx_asset_upload sample-asset.txt

  icx_asset_list

  assert_match " /sample-asset.txt.*text/plain.*identity"
  assert_not_match " /sample-asset.txt.*application/pdf.*arbitrary"
}

@test "uploads multiple files" {
  echo "this is the file content" >uploaded.txt
  echo "this is the file content ttt" >xyz.txt
  mkdir some_dir
  echo "some stuff" >some_dir/a.txt
  echo "more things" >some_dir/b.txt

  icx_asset_upload some_dir/*.txt
  icx_asset_list

  assert_match " /a.txt.*text/plain.*identity"
  assert_match " /b.txt.*text/plain.*identity"
}


@test "uploads multiple files from absolute path" {
  mkdir some_dir
  echo "some stuff" >some_dir/a.txt
  echo "more things" >some_dir/b.txt

  icx_asset_upload "$(realpath some_dir/a.txt)" "$(realpath some_dir/b.txt)"
  icx_asset_list

  assert_match " /a.txt.*text/plain.*identity"
  assert_match " /b.txt.*text/plain.*identity"
}

@test "uploads a file by name" {
  echo "this is the file content" >uploaded.txt

  icx_asset_upload uploaded.txt

  icx_asset_list

  assert_match " /uploaded.txt.*text/plain.*identity"
}

@test "can override asset name" {
  echo "this is the file content" >uploaded.txt

  icx_asset_upload /abcd.txt=uploaded.txt

  icx_asset_list

  assert_match " /abcd.txt.*text/plain.*identity"
}

@test "uploads a directory by name" {
  mkdir some_dir
  echo "some stuff" >some_dir/a.txt
  echo "more things" >some_dir/b.txt

  icx_asset_upload some_dir

  icx_asset_list

  assert_match " /some_dir/a.txt.*text/plain.*identity"
  assert_match " /some_dir/b.txt.*text/plain.*identity"
}

@test "uploads a directory by name as root" {
  mkdir some_dir
  echo "some stuff" >some_dir/a.txt
  echo "more things" >some_dir/b.txt

  icx_asset_upload /=some_dir

  icx_asset_list

  assert_match " /a.txt.*text/plain.*identity"
  assert_match " /b.txt.*text/plain.*identity"
}


-----------------------

/e2e/tests-replica/deploy.bash:
-----------------------

#!/usr/bin/env bats

load ../utils/_

setup() {
  standard_setup
}

teardown() {
  dfx_stop

  standard_teardown
}

@test "deploy from a fresh project" {
  dfx_new hello
  dfx_start
  install_asset greet
  assert_command dfx deploy

  assert_command dfx canister call hello_backend greet '("Banzai")'
  assert_eq '("Hello, Banzai!")'
}

@test "deploy a canister without dependencies" {
  dfx_new_assets hello
  dfx_start
  install_asset greet
  assert_command dfx deploy hello_backend
  assert_match 'Deploying: hello_backend'
  assert_not_match 'hello_frontend'
}

@test "deploy a canister with dependencies" {
  dfx_new_assets hello
  dfx_start
  install_asset greet
  assert_command dfx deploy hello_frontend
  assert_match 'Deploying: hello_backend hello_frontend'
}

@test "deploy a canister with non-circular shared dependencies" {
  install_asset transitive_deps_canisters
  dfx_start
  assert_command dfx deploy canister_f
  assert_match 'Deploying: canister_a canister_f canister_g canister_h'
}

@test "report an error on attempt to deploy a canister with circular dependencies" {
  install_asset transitive_deps_canisters
  dfx_start
  assert_command_fail dfx deploy canister_d
  assert_match 'canister_d -> canister_e -> canister_d'
}

@test "deploy with InstallMode::Install on an empty canister" {
  dfx_new hello
  install_asset greet
  dfx_start
  assert_command dfx canister create --all

  assert_command dfx deploy
  assert_match 'Installing code for canister'
}

@test "dfx deploy supports arguments" {
  dfx_new hello
  install_asset greet_arg
  dfx_start
  assert_command dfx canister create --all

  assert_command dfx deploy hello_backend --argument '("World")'

  assert_command dfx canister call hello_backend greet
  assert_match 'Hello, World'
}

@test "dfx deploy with InstallMode::Install on first invocation, InstallMode::Upgrade on second" {
  dfx_new hello
  install_asset greet
  dfx_start

  # In the normal case, whether for an initial install or a subsequent install,
  # dfx deploy does the right thing, so it doesn't need to retry.
  # Therefore, there is no "attempting (install|upgrade)" message.

  assert_command dfx deploy hello_backend
  assert_match 'Installing code for canister'

  assert_command dfx canister call hello_backend greet '("First")'
  assert_eq '("Hello, First!")'

  assert_command dfx deploy hello_backend --upgrade-unchanged
  assert_match 'Upgrading code for canister'

  assert_command dfx canister call hello_backend greet '("Second")'
  assert_eq '("Hello, Second!")'
}

@test "test canister lifecycle" {
  dfx_new hello
  install_asset greet
  dfx_start
  dfx canister create --all
  dfx build
  dfx canister install hello_backend
  assert_command dfx canister status hello_backend
  assert_match "Status: Running."

  # Stop
  assert_command dfx canister stop hello_backend
  assert_command dfx canister status hello_backend
  assert_match "Status: Stopped."
  assert_command_fail dfx canister call "$(dfx canister id hello_backend)" greet '("Names are difficult")'
  assert_match "is stopped"

  # Start
  assert_command dfx canister start hello_backend
  assert_command dfx canister status hello_backend
  assert_match "Status: Running."

  # Call
  assert_command dfx canister call "$(dfx canister id hello_backend)" greet '("Names are difficult")'
  assert_match '("Hello, Names are difficult!")'

  # Id
  assert_command dfx canister id hello_backend
  assert_match "$(jq -r .hello_backend.local < .dfx/local/canister_ids.json)"
  x="$(dfx canister id hello_backend)"
  local old_id="$x"

  # Delete
  assert_command_fail dfx canister delete hello_backend
  assert_command dfx canister stop hello_backend
  assert_command dfx canister delete hello_backend
  assert_command_fail dfx canister status hello_backend
  assert_match "Cannot find canister id. Please issue 'dfx canister create hello_backend'."

  # Create again
  assert_command dfx canister create hello_backend
  assert_command dfx canister id hello_backend
  assert_neq "$old_id"
}


-----------------------

/e2e/utils/_.bash:
-----------------------

set -eo pipefail
load ../utils/bats-support/load
load ../utils/assertions
load ../utils/webserver

# Takes a name of the asset folder, and copy those files to the current project.
install_asset() {
    ASSET_ROOT=${BATS_TEST_DIRNAME}/../assets/$1/
    cp -R "$ASSET_ROOT"/* .

    # shellcheck source=/dev/null
    if [ -f ./patch.bash ]; then source ./patch.bash; fi
    if [ -f ./Cargo.toml ]; then cargo update; fi
}

install_shared_asset() {
    mkdir -p "$(dirname "$E2E_NETWORKS_JSON")"

    ASSET_ROOT=${BATS_TEST_DIRNAME}/../assets/$1/
    cp -R "$ASSET_ROOT"/* "$(dirname "$E2E_NETWORKS_JSON")"
}

standard_setup() {
    # We want to work from a temporary directory, different for every test.
    x=$(mktemp -d -t dfx-e2e-XXXXXXXX)
    export E2E_TEMP_DIR="$x"

    cache_root="${E2E_CACHE_ROOT:-"$HOME/.e2e-cache-root"}"

    mkdir "$x/working-dir"
    mkdir -p "$cache_root"
    mkdir "$x/config-root"
    mkdir "$x/home-dir"

    cd "$x/working-dir" || exit

    export HOME="$x/home-dir"
    export DFX_CACHE_ROOT="$cache_root"
    export DFX_CONFIG_ROOT="$x/config-root"
    export RUST_BACKTRACE=1
    export MOCK_KEYRING_LOCATION="$HOME/mock_keyring.json"

    if [ "$(uname)" == "Darwin" ]; then
        export E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY="$HOME/Library/Application Support/org.dfinity.dfx/network/local"
    elif [ "$(uname)" == "Linux" ]; then
        export E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY="$HOME/.local/share/dfx/network/local"
    fi
    export E2E_NETWORKS_JSON="$DFX_CONFIG_ROOT/.config/dfx/networks.json"
}

standard_teardown() {
    rm -rf "$E2E_TEMP_DIR" || rm -rf "$E2E_TEMP_DIR"
}

dfx_new_frontend() {
    local project_name=${1:-e2e_project}
    dfx new "${project_name}" --frontend vanilla
    test -d "${project_name}"
    test -f "${project_name}"/dfx.json
    cd "${project_name}"

    echo PWD: "$(pwd)" >&2
}

dfx_new_assets() {
    local project_name=${1:-e2e_project}
    dfx new "${project_name}" --frontend simple-assets
    test -d "${project_name}"
    test -f "${project_name}"/dfx.json
    cd "${project_name}"

    echo PWD: "$(pwd)" >&2
}

dfx_new() {
    local project_name=${1:-e2e_project}
    dfx new "${project_name}" --no-frontend
    test -d "${project_name}"
    test -f "${project_name}/dfx.json"
    cd "${project_name}"

    echo PWD: "$(pwd)" >&2
}

dfx_new_rust() {
    local project_name=${1:-e2e_project}
    rustup default stable
    rustup target add wasm32-unknown-unknown
    dfx new "${project_name}" --type=rust --no-frontend
    test -d "${project_name}"
    test -f "${project_name}/dfx.json"
    test -f "${project_name}/Cargo.toml"
    test -f "${project_name}/Cargo.lock"
    cd "${project_name}"

    echo PWD: "$(pwd)" >&2
}

determine_network_directory() {
    # not perfect: dfx.json can actually exist in a parent
    if [ -f dfx.json ] && [ "$(jq .networks.local dfx.json)" != "null" ]; then
        echo "found dfx.json with local network in $(pwd)"
        data_dir="$(pwd)/.dfx/network/local"
        wallets_json="$(pwd)/.dfx/local/wallets.json"
        dfx_json="$(pwd)/dfx.json"
        export E2E_NETWORK_DATA_DIRECTORY="$data_dir"
        export E2E_NETWORK_WALLETS_JSON="$wallets_json"
        export E2E_ROUTE_NETWORKS_JSON="$dfx_json"
    else
        echo "no dfx.json"
        export E2E_NETWORK_DATA_DIRECTORY="$E2E_SHARED_LOCAL_NETWORK_DATA_DIRECTORY"
        export E2E_NETWORK_WALLETS_JSON="$E2E_NETWORK_DATA_DIRECTORY/wallets.json"
        export E2E_ROUTE_NETWORKS_JSON="$E2E_NETWORKS_JSON"
    fi
}

# Start the replica in the background.
dfx_start() {
    local port dfx_config_root webserver_port

    local args=( "$@" )

    add_default_parameter() {
        local param_name=$1
        local has_param=false
        for arg in "${args[@]}"; do
            if [[ $arg == "$param_name" ]]; then
                has_param=true
                break
            fi
        done
        if ! $has_param; then
            if [[ $# == 1 ]]; then
                args+=( "$param_name" )
            else
                local param_value=$2
                args+=( "$param_name" "$param_value" )
            fi
        fi
    }

    # By default, start on random port for parallel test execution
    add_default_parameter "--host" "127.0.0.1:0"
    if [[ "$USE_POCKETIC" ]]; then
        add_default_parameter "--pocketic"
    fi
    add_default_parameter "--artificial-delay" "100"

    determine_network_directory

    # Bats creates a FD 3 for test output, but child processes inherit it and Bats will
    # wait for it to close. Because `dfx start` leaves child processes running, we need
    # to close this pipe, otherwise Bats will wait indefinitely.

    dfx start --background "${args[@]}" 3>&-

    if [[ "$USE_POCKETIC" ]]; then
        test -f "$E2E_NETWORK_DATA_DIRECTORY/pocket-ic-port"
        port=$(< "$E2E_NETWORK_DATA_DIRECTORY/pocket-ic-port")
    else
        dfx_config_root="$E2E_NETWORK_DATA_DIRECTORY/replica-configuration"
        printf "Configuration Root for DFX: %s\n" "${dfx_config_root}"
        test -f "${dfx_config_root}/replica-1.port"
        port=$(cat "${dfx_config_root}/replica-1.port")
        if [ "$port" == "" ]; then
          port=$(jq -r .local.replica.port "$E2E_NETWORKS_JSON")
        fi
    fi
    webserver_port=$(cat "$E2E_NETWORK_DATA_DIRECTORY/webserver-port")

    printf "Replica Configured Port: %s\n" "${port}"
    printf "Webserver Configured Port: %s\n" "${webserver_port}"

    timeout 5 sh -c \
        "until nc -z localhost ${port}; do echo waiting for replica; sleep 1; done" \
        || (echo "could not connect to replica on port ${port}" && exit 1)
}

# Tries to start dfx on the default port, repeating until it succeeds or times out.
#
# Motivation: dfx nns install works only on port 8080, as URLs are compiled into the wasms.  This means that multiple
# tests MAY compete for the same port.
# - It may be possible in future for the wasms to detect their own URL and recompute signatures accordingly,
#   however until such a time, we have this restriction.
# - It may also be that ic-nns-install, if used on a non-standard port, installs only the core canisters not the UI.
# - However until we have implemented good solutions, all tests on ic-nns-install must run on port 8080.
dfx_start_for_nns_install() {
    # TODO: When nns-dapp supports dynamic ports, this wait can be removed.
    assert_command timeout 300 sh -c \
        "until dfx start --clean --background --host 127.0.0.1:8080 --verbose --artificial-delay 100; do echo waiting for port 8080 to become free; sleep 3; done" \
        || (echo "could not connect to replica on port 8080" && exit 1)
    assert_match "subnet type: System"
    assert_match "127.0.0.1:8080"
}

wait_until_replica_healthy() {
    echo "waiting for replica to become healthy"
    dfx ping --wait-healthy
    echo "replica became healthy"
}

# Stop the replica and verify it is very very stopped.
dfx_stop() {
    # to help tell if other pocket-ic proxy processes are from this test:
    echo "pwd: $(pwd)"
    # A suspicion: "address already is use" errors are due to an extra pocket-ic proxy process.
    echo "pocket-ic processes:"
    pgrep -l pocket-ic || echo "no ps/grep/pocket-ic output"

    dfx stop
    local dfx_root=.dfx/
    rm -rf $dfx_root

    # Verify that processes are killed.
    assert_no_dfx_start_or_replica_processes
}

dfx_set_wallet() {
  export WALLET_CANISTER_ID
  WALLET_CANISTER_ID=$(dfx identity get-wallet)
  assert_command dfx identity set-wallet "${WALLET_CANISTER_ID}" --force --network actuallylocal
  assert_match 'Wallet set successfully.'
}

setup_actuallylocal_project_network() {
    webserver_port=$(get_webserver_port)
    # [ ! -f "$E2E_ROUTE_NETWORKS_JSON" ] && echo "{}" >"$E2E_ROUTE_NETWORKS_JSON"
    jq '.networks.actuallylocal.providers=["http://127.0.0.1:'"$webserver_port"'"]' dfx.json | sponge dfx.json
}

setup_actuallylocal_shared_network() {
    webserver_port=$(get_webserver_port)
    [ ! -f "$E2E_NETWORKS_JSON" ] && echo "{}" >"$E2E_NETWORKS_JSON"
    jq '.actuallylocal.providers=["http://127.0.0.1:'"$webserver_port"'"]' "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
}

setup_local_shared_network() {
    local replica_port
    if [[ "$USE_POCKETIC" ]]; then
        replica_port=$(get_pocketic_port)
    else
        replica_port=$(get_replica_port)
    fi

    [ ! -f "$E2E_NETWORKS_JSON" ] && echo "{}" >"$E2E_NETWORKS_JSON"

    jq ".local.bind=\"127.0.0.1:${replica_port}\"" "$E2E_NETWORKS_JSON" | sponge "$E2E_NETWORKS_JSON"
}

use_wallet_wasm() {
    # shellcheck disable=SC2154
    export DFX_WALLET_WASM="${archive}/wallet/$1/wallet.wasm"
}

use_asset_wasm() {
    # shellcheck disable=SC2154
    export DFX_ASSETS_WASM="${archive}/frontend/$1/assetstorage.wasm.gz"
}

wallet_sha() {
    shasum -a 256 "${archive}/wallet/$1/wallet.wasm" | awk '{ print $1 }'
}

use_default_wallet_wasm() {
    unset DFX_WALLET_WASM
}

use_default_asset_wasm() {
    unset DFX_ASSETS_WASM
}

get_webserver_port() {
  dfx info webserver-port
}
overwrite_webserver_port() {
  echo "$1" >"$E2E_NETWORK_DATA_DIRECTORY/webserver-port"
}

get_replica_pid() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/replica-configuration/replica-pid"
}

get_replica_port() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/replica-configuration/replica-1.port"
}

get_pocketic_pid() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/pocket-ic-pid"
}

get_pocketic_port() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/pocket-ic-port"
}

get_btc_adapter_pid() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/ic-btc-adapter-pid"
}

get_canister_http_adapter_pid() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/ic-https-outcalls-adapter-pid"
}

get_pocketic_proxy_pid() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/pocket-ic-proxy-pid"
}

get_pocketic_proxy_config_port() {
  cat "$E2E_NETWORK_DATA_DIRECTORY/pocket-ic-proxy-port"
}

create_networks_json() {
  mkdir -p "$(dirname "$E2E_NETWORKS_JSON")"
  [ ! -f "$E2E_NETWORKS_JSON" ] && echo "{}" >"$E2E_NETWORKS_JSON"
}

define_project_network() {
    jq .networks.local.bind=\"127.0.0.1:8000\" dfx.json | sponge dfx.json
}

use_test_specific_cache_root() {
    # Use this when a test depends on the initial state of the cache being empty,
    # or if the test corrupts the cache in some way.
    # The effect is to ignore the E2E_CACHE_ROOT environment variable, if set.
    export DFX_CACHE_ROOT="$E2E_TEMP_DIR/cache-root"
    mkdir -p "$DFX_CACHE_ROOT"
}

get_ephemeral_port() {
    local script_dir
    script_dir=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
    python3 "$script_dir/get_ephemeral_port.py"
}

stop_and_delete() {
    assert_command dfx canister stop "$1"
    assert_command dfx canister delete -y --no-withdrawal "$1"
    echo "Canister $1 deleted"
}


-----------------------

/e2e/utils/assertions.bash:
-----------------------

#!

# Asserts that a command line succeeds. Still sets $output to the stdout and stderr
# of the command.
# Arguments:
#   $@ - The command to run.
# Returns:
#   none
assert_command() {
    local stdoutf stderrf statusf
    stderrf="$(mktemp)"
    stdoutf="$(mktemp)"
    statusf="$(mktemp)"
    ( set +e; "$@" 2>"$stderrf" >"$stdoutf"; echo -n "$?" > "$statusf" )
    status=$(< "$statusf"); rm "$statusf"
    stderr=$(< "$stderrf"); rm "$stderrf"
    stdout=$(< "$stdoutf"); rm "$stdoutf"
    output="$(
        if [ "$stderr" ]; then echo "$stderr"; fi 
        if [ "$stdout" ]; then echo "$stdout"; fi
    )"

    if [[ ! $status -eq 0 ]]; then 
        (echo "$*"; echo "status: $status"; echo "$output" | batslib_decorate "Output") \
         | batslib_decorate "Command failed" \
         | fail
    fi
}

# Asserts that a command line fails. Still sets $output to the stdout and stderr
# of the command.
# Arguments:
#   $@ - The command to run.
# Returns:
#   none
assert_command_fail() {
    local stdoutf stderrf statusf
    stderrf="$(mktemp)"
    stdoutf="$(mktemp)"
    statusf="$(mktemp)"
    ( set +e; "$@" 2>"$stderrf" >"$stdoutf"; echo -n "$?" >"$statusf" )
    status=$(< "$statusf"); rm "$statusf"
    stderr=$(< "$stderrf"); rm "$stderrf"
    stdout=$(< "$stdoutf"); rm "$stdoutf"
    output="$(
        if [ "$stderr" ]; then echo "$stderr"; fi;
        if [ "$stdout" ]; then echo "$stdout"; fi;
    )"

    if [[ $status -eq 0 ]]; then
        ( echo "$*"; echo "$output" | batslib_decorate "Output") \
            | batslib_decorate "Command succeeded (should have failed)" \
            | fail
    fi
}

# Asserts that a string contains another string, using regexp.
# Arguments:
#    $1 - The regex to use to match.
#    $2 - The string to match against (output). By default it will use
#         $output.
assert_match() {
    regex="$1"
    if [[ $# -lt 2 ]]; then
        text="$output"
    else
        text="$2"
    fi
    if [[ ! "$text" =~ $regex ]]; then
        batslib_print_kv_single_or_multi 10 "regex" "$regex" "actual" "$text" \
            | batslib_decorate "output does not match" \
            | fail
    fi
}

# Asserts that a string contains another string
# Arguments:
#    $1 - The string to search for.
#    $2 - The string to search in.
assert_contains() {
    search_for="$1"
    if [[ $# -lt 2 ]]; then
        search_in="$output"
    else
        search_in="$2"
    fi
    if [[ ! "$search_in" == *"$search_for"* ]]; then
        batslib_print_kv_single_or_multi 10 "search phrase" "$search_for" "actual output" "$search_in" \
            | batslib_decorate "output does not match" \
            | fail
    fi
}

# Asserts that a string does not contain another string
# Arguments:
#    $1 - The string to search for.
#    $2 - The string to search in.
assert_not_contains() {
    search_for="$1"
    if [[ $# -lt 2 ]]; then
        search_in="$output"
    else
        search_in="$2"
    fi
    if [[ "$search_in" == *"$search_for"* ]]; then
        batslib_print_kv_single_or_multi 10 "search phrase" "$search_for" "actual output" "$search_in" \
            | batslib_decorate "output does match even though it shouldn't" \
            | fail
    fi
}

# Asserts that a string does not contain another string, using regexp.
# Arguments:
#    $1 - The regex to use to match.
#    $2 - The string to match against (output). By default it will use
#         $output.
assert_not_match() {
    regex="$1"
    if [[ $# -lt 2 ]]; then
        text="$output"
    else
        text="$2"
    fi
    if [[ "$text" =~ $regex ]]; then
        batslib_print_kv_single_or_multi 10 "regex" "$regex" "actual" "$text" \
            | batslib_decorate "output matches but is expected not to" \
            | fail
    fi
}

# Asserts that a string occurs a number of times in another string.
# Arguments:
#    $1 - Expected number of occurance.
#    $2 - The string to search for.
#    $3 - The string to search in. By default it will use $output.
assert_occurs() {
    expect="$1"
    search_for="$2"
    if [[ $# -lt 3 ]]; then
        search_in="$output"
    else
        search_in="$3"
    fi

    actual="$( echo "$search_in" | grep -o "$search_for" | wc -l | xargs )"

    if [[ "$expect" -ne "$actual" ]]; then
        batslib_print_kv_single_or_multi 6 "Expect" "$expect" "Actual" "$actual" \
            | batslib_decorate "Occurrences of \"$search_for\" in \"$search_in\" didn't match expectation." \
            | fail
    fi
}

# Asserts a command will timeout. This assertion will fail if the command finishes before
# the timeout period. If the command fails, it will also fail.
# Arguments:
#   $1 - The amount of time (in seconds) to wait for.
#   $@ - The command to run.

# Asserts that two values are equal.
# Arguments:
#    $1 - The expected value.
#    $2 - The actual value.
assert_eq() {
    expected="$1"
    if [[ $# -lt 2 ]]; then
        actual="$output"
    else
        actual="$2"
    fi

    if [[ ! "$actual" == "$expected" ]]; then
        batslib_print_kv_single_or_multi 10 "expected" "$expected" "actual" "$actual" \
            | batslib_decorate "output does not match" \
            | fail
    fi
}

# Asserts that two values are not equal.
# Arguments:
#    $1 - The expected value.
#    $2 - The actual value.
assert_neq() {
    expected="$1"
    if [[ $# -lt 2 ]]; then
        actual="$output"
    else
        actual="$2"
    fi

    if [[ "$actual" = "$expected" ]]; then
        batslib_print_kv_single_or_multi 10 "expected" "$expected" "actual" "$actual" \
            | batslib_decorate "output does not match" \
            | fail
    fi
}


# Asserts that a process exits within a timeframe
# Arguments:
#    $1 - the PID
#    $2 - the timeout
assert_process_exits() {
    pid="$1"
    timeout="$2"

    echo "waiting up to $timeout seconds for process $pid to exit"

    timeout "$timeout" sh -c \
      "while kill -0 $pid; do echo waiting for process $pid to exit; sleep 1; done" \
      || (echo "process $pid did not exit" && ps aux && exit 1)

    echo "process $pid exited"
}

# Asserts that `dfx start` and `replica` are no longer running
assert_no_dfx_start_or_replica_processes() {
    ! ( pgrep "dfx start" )
    if [ -e .dfx/replica-configuration/replica-pid ];
    then
      ! ( kill -0 "$(< .dfx/replica-configuration/replica-pid)" 2>/dev/null )
    fi
}

assert_file_exists() {
    filename="$1"

    if [[ ! -f $filename ]]; then
        echo "$filename does not exist" \
        | batslib_decorate "Missing file" \
        | fail
    fi
}

assert_file_not_empty() {
    filename="$1"

    assert_file_exists "$filename"

    if [[ ! -s $filename ]]; then
        echo "$filename is empty" \
        | batslib_decorate "Empty file" \
        | fail
    fi
}

assert_file_empty() {
    filename="$1"

    assert_file_exists "$filename"

    if [[ -s $filename ]]; then
        echo "$filename is not empty" \
        | batslib_decorate "File not empty" \
        | fail
    fi
}

assert_file_not_exists() {
    filename="$1"

    if [[ -f $filename ]]; then
        head -n 10 "$filename" \
        | batslib_decorate "Expected file to not exist. $filename exists and starts with:" \
        | fail
    fi
}

assert_file_eventually_exists() {
    filename="$1"
    timeout="$2"

    timeout "$timeout" sh -c \
      "until [ -f \"$filename\" ]; do echo waiting for \"$filename\"; sleep 1; done" \
      || (echo "file \"$filename\" was never created" && ls && exit 1)
}

assert_directory_not_exists() {
    directory="$1"
    if [[ -d $directory ]]; then
        ( echo "Contents of $directory:" ; ls -AlR "$directory" ) \
        | batslib_decorate "Expected directory '$directory' to not exist." \
        | fail
    fi
}

# Asserts that the contents of two files are equal.
# Arguments:
#    $1 - The name of the file containing the expected value.
#    $2 - The name of the file containing the actual value.
assert_files_eq() {
    expected="$(cat "$1")"
    actual="$(cat "$2")"

    if [[ ! "$actual" == "$expected" ]]; then
        diff "$1" "$2" \
            | batslib_decorate "contents of $1 do not match contents of $2" \
            | fail
    fi
}


-----------------------

/e2e/utils/cycles-ledger.bash:
-----------------------

CYCLES_LEDGER_VERSION="0.3.0"

build_artifact_url() {
    echo "https://github.com/dfinity/cycles-ledger/releases/download/cycles-ledger-v$CYCLES_LEDGER_VERSION/${1}"
}

downloaded_cycles_ledger_canisters_dir() {
    echo "$DFX_CACHE_ROOT/canisters/cycles-ledger/$CYCLES_LEDGER_VERSION"
}

download_cycles_ledger_canisters() {
    DOWNLOAD_DIR="$DFX_CACHE_ROOT/.download"
    DEST_DIR="$(downloaded_cycles_ledger_canisters_dir)"

    if test -d "$DEST_DIR"; then
        return
    fi

    rm -rf "$DOWNLOAD_DIR"
    mkdir -p "$DOWNLOAD_DIR" "$(dirname "$DEST_DIR")"

    for name in cycles-ledger depositor fake-cmc; do
        for ext in wasm.gz wasm.gz.sha256 did; do
            URL=$(build_artifact_url "${name}.${ext}")
            curl -v -L --fail -o "$DOWNLOAD_DIR/${name}.${ext}" "$URL"
        done
    done

    ( cd "$DOWNLOAD_DIR" && shasum -c cycles-ledger.wasm.gz.sha256 && shasum -c depositor.wasm.gz.sha256 )
    mv "$DOWNLOAD_DIR" "$DEST_DIR"
}

install_cycles_ledger_canisters() {
    download_cycles_ledger_canisters
    cp "$(downloaded_cycles_ledger_canisters_dir)"/* .
}


-----------------------

/e2e/utils/get_ephemeral_port.py:
-----------------------

import socket

with socket.socket() as s:
  s.bind(('', 0))
  print(s.getsockname()[1], end='')


-----------------------

/e2e/utils/webserver.bash:
-----------------------

start_webserver() {
    port="$(get_ephemeral_port)"
    export E2E_WEB_SERVER_PORT="$port"

    python3 -m http.server "$E2E_WEB_SERVER_PORT" "$@" &
    export E2E_WEB_SERVER_PID=$!

    while ! nc -z localhost "$E2E_WEB_SERVER_PORT"; do
      sleep 1
    done
}

stop_webserver() {
    if [ "$E2E_WEB_SERVER_PID" ]; then
        kill "$E2E_WEB_SERVER_PID"
    fi
}


-----------------------

= Nix

== Niv

We use the https://github.com/nmattia/niv[niv] tool to manage references to
external source code. All references to external source code should be specified
in `sources.json`. `niv` can then be used to add, update or drop dependencies.


-----------------------

/nix/sources.json:
-----------------------

{
    "canister_sandbox-x86_64-darwin": {
        "builtin": false,
        "description": "The canister_sandbox binary. It must be updated together with the replica binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1xf645zynh6yhgppdr3z7f45b5476vzcx7b8l3j88ndm1a8qc805",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/canister_sandbox.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/canister_sandbox.gz"
    },
    "canister_sandbox-x86_64-linux": {
        "builtin": false,
        "description": "The canister_sandbox binary. It must be updated together with the replica binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0xdmks8yhrdgmp5wwmxc8xrqf0g9g8n1jpjdn4360fsdjqxk2kq9",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/canister_sandbox.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/canister_sandbox.gz"
    },
    "compiler_sandbox-x86_64-darwin": {
        "builtin": false,
        "description": "The compiler_sandbox binary. It must be updated together with the replica binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1bv9s4nb2j6ni416dpy9bfsx4vmk55aw1x1jy6143kc69wm2hns0",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/compiler_sandbox.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/compiler_sandbox.gz"
    },
    "compiler_sandbox-x86_64-linux": {
        "builtin": false,
        "description": "The compiler_sandbox binary. It must be updated together with the replica binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0jiqi2ij0wcbifz04gk0i60kq6s6bcx3dyrfnhx4mbxnv4m59rvx",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/compiler_sandbox.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/compiler_sandbox.gz"
    },
    "ic-admin-x86_64-darwin": {
        "builtin": false,
        "description": "The ic-admin binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1mbn1xkpf44a4697njrr3gzgajyl13p0dyg0b7mdx57nhzkxm8z1",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-admin.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/ic-admin.gz"
    },
    "ic-admin-x86_64-linux": {
        "builtin": false,
        "description": "The ic-admin binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1337ya9szbz24svhw07r5iwyrn4mcgymm1pzql4fvqz9d35ksa0d",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-admin.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/ic-admin.gz"
    },
    "ic-btc-adapter-x86_64-darwin": {
        "builtin": false,
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "06fjm0z8qqkqd21if9flrsmjdiqizb9xya9knhcm8i4jp99qpzwy",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-btc-adapter.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/ic-btc-adapter.gz"
    },
    "ic-btc-adapter-x86_64-linux": {
        "builtin": false,
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "114a9ia0mic963barlswyhpsl7c821h98i7wyzar80d3c20397n7",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-btc-adapter.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/ic-btc-adapter.gz"
    },
    "ic-btc-canister": {
        "sha256": "1b34jpxkk72h07ls0fspwrgmndmj7xhlivdhn82msvgz8mx69x89",
        "type": "file",
        "url": "https://github.com/dfinity/bitcoin-canister/releases/download/release%2F2023-10-13/ic-btc-canister.wasm.gz",
        "url_template": "https://github.com/dfinity/bitcoin-canister/releases/download/<version>/ic-btc-canister.wasm.gz",
        "version": "release%2F2023-10-13"
    },
    "ic-https-outcalls-adapter-x86_64-darwin": {
        "builtin": false,
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1ki7bz0jbiw00amgxd9gz9b2fyzmxfg6qwnlfrhxr8y1grgsks7a",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-https-outcalls-adapter.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/ic-https-outcalls-adapter.gz"
    },
    "ic-https-outcalls-adapter-x86_64-linux": {
        "builtin": false,
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0qansx4fjyr5ma2vjz4j0ifmqi5xvc69lhkfjnl0snj8lj81wyzl",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-https-outcalls-adapter.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/ic-https-outcalls-adapter.gz"
    },
    "ic-nns-init-x86_64-darwin": {
        "builtin": false,
        "description": "The ic-nns-init binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0cbg099b7pvqz7zr2b75l4s815l5xx9hc5h2ifabb0pg911cylwg",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-nns-init.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/ic-nns-init.gz"
    },
    "ic-nns-init-x86_64-linux": {
        "builtin": false,
        "description": "The ic-nns-init binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "162gxjvhfrj3gpl2rjcmk6np1hvlq0m48y5zbyzfi3s9jx0v5jww",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-nns-init.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/ic-nns-init.gz"
    },
    "ic-starter-x86_64-darwin": {
        "builtin": false,
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1jd3fvrsi2xqd071vciff1rn0f6briz3sq5h7k1rjm47mb5djpxm",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-starter.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/ic-starter.gz"
    },
    "ic-starter-x86_64-linux": {
        "builtin": false,
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0hrwiv8ggba0ymxqmhbg8jzbls0rcd8jsmnksr1xnag9zpac6ihj",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-starter.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/ic-starter.gz"
    },
    "motoko-base": {
        "builtin": false,
        "description": "The Motoko base library",
        "owner": "dfinity",
        "sha256": "02nf25bm21rmyh425arh0srb52bgsyrbqmnmy7r8zd1lfd6jnpfc",
        "type": "tarball",
        "url": "https://github.com/dfinity/motoko/releases/download/0.13.0/motoko-base-library.tar.gz",
        "url_template": "https://github.com/dfinity/motoko/releases/download/<version>/motoko-base-library.tar.gz",
        "version": "0.13.0"
    },
    "motoko-x86_64-darwin": {
        "builtin": false,
        "sha256": "1bknwvhh44ysdr1xg3x856x528b55752yn6x808lwjyi1yysnqcc",
        "type": "file",
        "url": "https://github.com/dfinity/motoko/releases/download/0.13.0/motoko-Darwin-x86_64-0.13.0.tar.gz",
        "url_template": "https://github.com/dfinity/motoko/releases/download/<version>/motoko-Darwin-x86_64-<version>.tar.gz",
        "version": "0.13.0"
    },
    "motoko-x86_64-linux": {
        "builtin": false,
        "sha256": "1njn9cyalgj33imysblbbpxpqp0fvadckk5b8wsw92l67fksp0mf",
        "type": "file",
        "url": "https://github.com/dfinity/motoko/releases/download/0.13.0/motoko-Linux-x86_64-0.13.0.tar.gz",
        "url_template": "https://github.com/dfinity/motoko/releases/download/<version>/motoko-Linux-x86_64-<version>.tar.gz",
        "version": "0.13.0"
    },
    "pocket-ic-x86_64-darwin": {
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0rys3s9506wyj32dsm2ha0awvxchpmps04ax57wbiyq9fkwgfgp0",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/pocket-ic.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/pocket-ic.gz"
    },
    "pocket-ic-x86_64-linux": {
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "06ll418ib57v6pfc7bn8c3gjvjjdvp8kkd558n6xhjmcnl8b0zmx",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/pocket-ic.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/pocket-ic.gz"
    },
    "replica-x86_64-darwin": {
        "builtin": false,
        "description": "The replica binary. It must be updated together with the canister_sandbox binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0cd55b3x7fz1rn1pz3nclkhrgyp3jmmpzdsy3kw9f9l6v0pxzznz",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/replica.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/replica.gz"
    },
    "replica-x86_64-linux": {
        "builtin": false,
        "description": "The replica binary. It must be updated together with the canister_sandbox binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0jlb85yrsbaiixmhfl39f85yd9irb5xilmxrhnapg3kyfrhzm2kb",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/replica.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/replica.gz"
    },
    "sandbox_launcher-x86_64-darwin": {
        "builtin": false,
        "description": "The sandbox_launcher binary. It must be updated together with the replica binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1fxpr28scfgv4lazjl0hqr0pjj15rph92q852sivw97pggv0bq40",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/sandbox_launcher.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/sandbox_launcher.gz"
    },
    "sandbox_launcher-x86_64-linux": {
        "builtin": false,
        "description": "The sandbox_launcher binary. It must be updated together with the replica binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1xr8kzvglzb4gankxvyvbv2wdkr5kska5mwjsdc5kshkrm2bhg1x",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/sandbox_launcher.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/sandbox_launcher.gz"
    },
    "sns-x86_64-darwin": {
        "builtin": false,
        "description": "The sns binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "0l98m2zdlr1vyzs48zg058nn7abqby0v81xqykz9b5dg73y8x5wf",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/sns.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-darwin/sns.gz"
    },
    "sns-x86_64-linux": {
        "builtin": false,
        "description": "The sns binary.",
        "rev": "c43a4880199c00135c8415957851e823b3fb769e",
        "sha256": "1v1v5g5dd7zszb10h9fn3sj55x4y1bbkwb2cghml8ji7y7662ynp",
        "type": "file",
        "url": "https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/sns.gz",
        "url_template": "https://download.dfinity.systems/ic/<rev>/binaries/x86_64-linux/sns.gz"
    }
}


-----------------------

/public/install-dfxvm.sh:
-----------------------

#!/usr/bin/env sh

##
## Borrowed from rustup (https://sh.rustup.rs)
##
## This is just a little script that can be downloaded from the internet to
## install dfx. It just does platform detection, downloads the installer
## and runs it.
##
## You are NOT AUTHORIZED to remove any license agreements or prompts from the following script.
##
set -u

# A newline separated list of boolean flags. See the read_flags function to see how it's parsed.
DFX_BOOL_FLAGS=""

# Make a BOOLEAN flag and its description.
#
# Arguments:
#   $1 - The long name of the boolean. This will be used on the command line. The name of the
#        environment variable will be `flag_NAME` where NAME is this argument, capitalized.
#        The value of this argument is empty string if not specified, and "1" if it is.
#   $2 - A description of the flag. This is not currently used but will be when we have enough
#        flags to implement help.
define_flag_BOOL() {
    local VARNAME
    VARNAME="flag_$(echo "$1" | tr /a-z/ /A-Z)"
    eval "$VARNAME=\${$VARNAME:-}"
    DFX_BOOL_FLAGS="${DFX_BOOL_FLAGS}--${1} $VARNAME $2"
}

# Get the flag name of a line in the flag description.
get_flag_name() {
    echo "$1"
}

# Get the variable name of a line in the flag description.
get_var_name() {
    echo "$2"
}

# Read all the command line flags and set the flag_XXXX environment variables.
#
# Arguments:
#   $* - Flags to parse.
# Side Effects:
#   Environment variables are set according to flags defined and flags.
read_flags() {
    # Set values from command line.
    # shellcheck disable=SC2199
    # https://github.com/koalaman/shellcheck/wiki/SC2199
    while [ -n "$*" ]; do
        local ARG="$1"
        shift

        OLD_IFS="$IFS"
        IFS=$'\n'
        for line in ${DFX_BOOL_FLAGS}; do
            [ "$line" ] || break

            IFS="$OLD_IFS"
            FLAG="$(get_flag_name "$line")"
            VARNAME="$(get_var_name "$line")"

            if [ "$ARG" == "$FLAG" ]; then
                eval "$VARNAME=1"
            fi
        done
    done
}

log() {
    if "$_ansi_escapes_are_valid"; then
        printf "\33[1minfo:\33[0m %s\n" "$1" 1>&2
    else
        printf '%s\n' "$1" 1>&2
    fi
}

say() {
    printf 'dfinity-sdk: %s\n' "$1"
}

warn() {
    if $_ansi_escapes_are_valid; then
        printf "\33[1mwarn:\33[0m %s\n" "$1" 1>&2
    else
        printf '%s\n' "$1" 1>&2
    fi
}

err() {
    say "$1" >&2
    exit 1
}

need_cmd() {
    if ! check_cmd "$1"; then
        err "need '$1' (command not found)"
    fi
}

check_cmd() {
    command -v "$1" >/dev/null 2>&1
}

assert_nz() {
    if [ -z "$1" ]; then err "assert_nz $2"; fi
}

# Run a command that should never fail. If the command fails execution
# will immediately terminate with an error showing the failing
# command.
ensure() {
    if ! "$@"; then err "command failed: $*"; fi
}

# This is just for indicating that commands' results are being
# intentionally ignored. Usually, because it's being executed
# as part of error handling.
ignore() {
    "$@"
}

define_flag_BOOL "insecure" "Allows downloading from insecure URLs, either using HTTP or TLS 1.2 or less."

check_help_for() {
    local _arch
    local _cmd
    local _arg
    _arch="$1"
    shift
    _cmd="$1"
    shift

    local _category
    if "$_cmd" --help | grep -q 'For all options use the manual or "--help all".'; then
        _category="all"
    else
        _category=""
    fi

    case "$_arch" in

        *darwin*)
            if check_cmd sw_vers; then
                case $(sw_vers -productVersion) in
                    10.*)
                        # If we're running on macOS, older than 10.13, then we always
                        # fail to find these options to force fallback
                        if [ "$(sw_vers -productVersion | cut -d. -f2)" -lt 13 ]; then
                            # Older than 10.13
                            echo "Warning: Detected macOS platform older than 10.13"
                            return 1
                        fi
                        ;;

                        # We assume these will be OK for now
                    11.*) ;; # Big Sur
                    12.*) ;; # Monterey
                    13.*) ;; # Ventura
                    14.*) ;; # Sonoma

                    *)
                        # Unknown product version, warn and continue
                        echo "Warning: Detected unknown macOS major version: $(sw_vers -productVersion)"
                        echo "Warning TLS capabilities detection may fail"
                        ;;
                esac
            fi
            ;;

    esac

    for _arg in "$@"; do
        if ! "$_cmd" --help "$_category" | grep -q -- "$_arg"; then
            return 1
        fi
    done

    true # not strictly needed
}

is_zsh() {
    [ -n "${ZSH_VERSION-}" ]
}

# This wraps curl or wget. Try curl first, if not installed,
# use wget instead.
# Arguments:
#   $1 - URL to download.
#   $2 - Path to output the download. Use - to output to stdout.
#   $3 - The architecture, used to determine TLS capabilities.
downloader() {
    # zsh does not split words by default, Required for curl retry arguments below.
    is_zsh && setopt local_options shwordsplit

    local _dld
    local _ciphersuites
    local _err
    local _status
    local _retry
    if check_cmd curl; then
        _dld=curl
    elif check_cmd wget; then
        _dld=wget
    else
        _dld='curl or wget' # to be used in error message of need_cmd
    fi

    if [ "$1" = --check ]; then
        need_cmd "$_dld"
    elif [ "$_dld" = curl ]; then
        check_curl_for_retry_support
        _retry="$RETVAL"
        get_ciphersuites_for_curl
        _ciphersuites="$RETVAL"
        if [ -n "$_ciphersuites" ]; then
            # shellcheck disable=SC2086 # $retry is intentionally split
            _err=$(curl $_retry --proto '=https' --tlsv1.2 --ciphers "$_ciphersuites" --silent --show-error --fail --location "$1" --output "$2" 2>&1)
            _status=$?
        else
            echo "Warning: Not enforcing strong cipher suites for TLS, this is potentially less secure"
            if ! check_help_for "$3" curl --proto --tlsv1.2; then
                echo "Warning: Not enforcing TLS v1.2, this is potentially less secure"
                # shellcheck disable=SC2086 # $retry is intentionally split
                _err=$(curl $_retry --silent --show-error --fail --location "$1" --output "$2" 2>&1)
                _status=$?
            else
                # shellcheck disable=SC2086 # $retry is intentionally split
                _err=$(curl $_retry --proto '=https' --tlsv1.2 --silent --show-error --fail --location "$1" --output "$2" 2>&1)
                _status=$?
            fi
        fi
        if [ -n "$_err" ]; then
            echo "$_err" >&2
            if echo "$_err" | grep -q 404$; then
                err "installer for platform '$3' not found, this may be unsupported"
            fi
        fi
        return $_status
    elif [ "$_dld" = wget ]; then
        if [ "$(wget -V 2>&1 | head -2 | tail -1 | cut -f1 -d" ")" = "BusyBox" ]; then
            echo "Warning: using the BusyBox version of wget.  Not enforcing strong cipher suites for TLS or TLS v1.2, this is potentially less secure"
            _err=$(wget "$1" -O "$2" 2>&1)
            _status=$?
        else
            get_ciphersuites_for_wget
            _ciphersuites="$RETVAL"
            if [ -n "$_ciphersuites" ]; then
                _err=$(wget --https-only --secure-protocol=TLSv1_2 --ciphers "$_ciphersuites" "$1" -O "$2" 2>&1)
                _status=$?
            else
                echo "Warning: Not enforcing strong cipher suites for TLS, this is potentially less secure"
                if ! check_help_for "$3" wget --https-only --secure-protocol; then
                    echo "Warning: Not enforcing TLS v1.2, this is potentially less secure"
                    _err=$(wget "$1" -O "$2" 2>&1)
                    _status=$?
                else
                    _err=$(wget --https-only --secure-protocol=TLSv1_2 "$1" -O "$2" 2>&1)
                    _status=$?
                fi
            fi
        fi
        if [ -n "$_err" ]; then
            echo "$_err" >&2
            if echo "$_err" | grep -q ' 404 Not Found$'; then
                err "installer for platform '$3' not found, this may be unsupported"
            fi
        fi
        return $_status
    else
        err "Unknown downloader" # should not reach here
    fi
}

# Check if curl supports the --retry flag, then pass it to the curl invocation.
check_curl_for_retry_support() {
    local _retry_supported=""
    # "unspecified" is for arch, allows for possibility old OS using macports, homebrew, etc.
    if check_help_for "notspecified" "curl" "--retry"; then
        _retry_supported="--retry 3"
        if check_help_for "notspecified" "curl" "--continue-at"; then
            # "-C -" tells curl to automatically find where to resume the download when retrying.
            _retry_supported="--retry 3 -C -"
        fi
    fi

    RETVAL="$_retry_supported"
}

# Return cipher suite string specified by user, otherwise return strong TLS 1.2-1.3 cipher suites
# if support by local tools is detected. Detection currently supports these curl backends:
# GnuTLS and OpenSSL (possibly also LibreSSL and BoringSSL). Return value can be empty.
get_ciphersuites_for_curl() {
    if [ -n "${RUSTUP_TLS_CIPHERSUITES-}" ]; then
        # user specified custom cipher suites, assume they know what they're doing
        RETVAL="$RUSTUP_TLS_CIPHERSUITES"
        return
    fi

    local _openssl_syntax="no"
    local _gnutls_syntax="no"
    local _backend_supported="yes"
    if curl -V | grep -q ' OpenSSL/'; then
        _openssl_syntax="yes"
    elif curl -V | grep -iq ' LibreSSL/'; then
        _openssl_syntax="yes"
    elif curl -V | grep -iq ' BoringSSL/'; then
        _openssl_syntax="yes"
    elif curl -V | grep -iq ' GnuTLS/'; then
        _gnutls_syntax="yes"
    else
        _backend_supported="no"
    fi

    local _args_supported="no"
    if [ "$_backend_supported" = "yes" ]; then
        # "unspecified" is for arch, allows for possibility old OS using macports, homebrew, etc.
        if check_help_for "notspecified" "curl" "--tlsv1.2" "--ciphers" "--proto"; then
            _args_supported="yes"
        fi
    fi

    local _cs=""
    if [ "$_args_supported" = "yes" ]; then
        if [ "$_openssl_syntax" = "yes" ]; then
            _cs=$(get_strong_ciphersuites_for "openssl")
        elif [ "$_gnutls_syntax" = "yes" ]; then
            _cs=$(get_strong_ciphersuites_for "gnutls")
        fi
    fi

    RETVAL="$_cs"
}

# Return cipher suite string specified by user, otherwise return strong TLS 1.2-1.3 cipher suites
# if support by local tools is detected. Detection currently supports these wget backends:
# GnuTLS and OpenSSL (possibly also LibreSSL and BoringSSL). Return value can be empty.
get_ciphersuites_for_wget() {
    if [ -n "${RUSTUP_TLS_CIPHERSUITES-}" ]; then
        # user specified custom cipher suites, assume they know what they're doing
        RETVAL="$RUSTUP_TLS_CIPHERSUITES"
        return
    fi

    local _cs=""
    if wget -V | grep -q '\-DHAVE_LIBSSL'; then
        # "unspecified" is for arch, allows for possibility old OS using macports, homebrew, etc.
        if check_help_for "notspecified" "wget" "TLSv1_2" "--ciphers" "--https-only" "--secure-protocol"; then
            _cs=$(get_strong_ciphersuites_for "openssl")
        fi
    elif wget -V | grep -q '\-DHAVE_LIBGNUTLS'; then
        # "unspecified" is for arch, allows for possibility old OS using macports, homebrew, etc.
        if check_help_for "notspecified" "wget" "TLSv1_2" "--ciphers" "--https-only" "--secure-protocol"; then
            _cs=$(get_strong_ciphersuites_for "gnutls")
        fi
    fi

    RETVAL="$_cs"
}

# Return strong TLS 1.2-1.3 cipher suites in OpenSSL or GnuTLS syntax. TLS 1.2
# excludes non-ECDHE and non-AEAD cipher suites. DHE is excluded due to bad
# DH params often found on servers (see RFC 7919). Sequence matches or is
# similar to Firefox 68 ESR with weak cipher suites disabled via about:config.
# $1 must be openssl or gnutls.
get_strong_ciphersuites_for() {
    if [ "$1" = "openssl" ]; then
        # OpenSSL is forgiving of unknown values, no problems with TLS 1.3 values on versions that don't support it yet.
        echo "TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256:TLS_AES_256_GCM_SHA384:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384"
    elif [ "$1" = "gnutls" ]; then
        # GnuTLS isn't forgiving of unknown values, so this may require a GnuTLS version that supports TLS 1.3 even if wget doesn't.
        # Begin with SECURE128 (and higher) then remove/add to build cipher suites. Produces same 9 cipher suites as OpenSSL but in slightly different order.
        echo "SECURE128:-VERS-SSL3.0:-VERS-TLS1.0:-VERS-TLS1.1:-VERS-DTLS-ALL:-CIPHER-ALL:-MAC-ALL:-KX-ALL:+AEAD:+ECDHE-ECDSA:+ECDHE-RSA:+AES-128-GCM:+CHACHA20-POLY1305:+AES-256-GCM"
    fi
}

DFXVM_GITHUB_LATEST_RELEASE_ROOT="${DFXVM_GITHUB_LATEST_RELEASE_ROOT:-https://github.com/dfinity/dfxvm/releases/latest/download}"
DFX_VERSION="${DFX_VERSION-}"
DFXVM_INIT_YES="${DFXVM_INIT_YES-}"

# The SHA and the time of the last commit that touched this file.
SCRIPT_COMMIT_DESC="@revision@"

download_and_install() {
    SHASUM="$1"

    get_architecture || return 1
    local _arch="$RETVAL"
    assert_nz "$_arch" "arch"

    local _archive="dfxvm-${_arch}"
    local _tarball_filename="${_archive}.tar.gz"
    local _tarball_url="${DFXVM_GITHUB_LATEST_RELEASE_ROOT}/${_tarball_filename}"
    local _sha256_filename="${_tarball_filename}.sha256"
    local _sha256_url="${_tarball_url}.sha256"

    log "Downloading latest release..."
    ensure downloader "$_tarball_url" "${_tarball_filename}" "$_arch"
    ensure downloader "$_sha256_url" "${_sha256_filename}" "$_arch"

    log "Checking integrity of tarball..."
    ensure "$SHASUM" -c "${_sha256_filename}"

    ensure tar -xzf "${_tarball_filename}" --strip-components=1 "${_archive}/dfxvm"
    ensure chmod u+x dfxvm
    ensure mv dfxvm dfxvm-init

    command="./dfxvm-init"

    if [ -n "${DFX_VERSION}" ]; then
        command="${command} --dfx-version \"${DFX_VERSION}\""
    fi

    if [ -n "${DFXVM_INIT_YES}" ]; then
        command="${command} --yes"
    fi

    eval "${command}"
}

main() {
    _ansi_escapes_are_valid=false
    if [ -t 2 ]; then
        if [ "${TERM+set}" = 'set' ]; then
            case "$TERM" in
                xterm* | rxvt* | urxvt* | linux* | vt*)
                    _ansi_escapes_are_valid=true
                    ;;
            esac
        fi
    fi

    # Read flags.
    read_flags "$@"

    log "Executing dfxvm install script, commit: $SCRIPT_COMMIT_DESC"

    downloader --check
    need_cmd uname
    need_cmd mktemp
    need_cmd chmod
    need_cmd mkdir
    if check_cmd sha256sum; then
        SHASUM=sha256sum
    elif check_cmd shasum; then
        SHASUM=shasum
    else
        err "need 'shasum' or 'sha256sum' (neither command found)"
    fi
    need_cmd rm
    need_cmd tar
    need_cmd gzip
    need_cmd touch

    local _dir

    if ! _dir="$(mktemp -d 2>/dev/null)"; then
        if ! _dir="$(mktemp -d -t dfinity-dfxvm)"; then
            err "failed to create temporary directory"
        fi
    fi

    ensure mkdir -p "${_dir}"

    (
        ensure cd "${_dir}" >/dev/null
        download_and_install "$SHASUM"
    )
    local _subshell_exit_code=$?

    ignore rm "${_dir}"/dfxvm*
    ignore rmdir "${_dir}"

    exit $_subshell_exit_code
}

## output is one of the following, which correspond to part of the release asset filenames:
##    aarch64-apple-darwin
##    x86_64-apple-darwin
##    x86_64-unknown-linux-gnu

get_architecture() {
    local _ostype _cputype _arch
    _ostype="$(uname -s)"
    _cputype="$(uname -m)"

    if [ "$_ostype" = Darwin ] && [ "$_cputype" = i386 ]; then
        # Darwin `uname -m` lies
        if sysctl hw.optional.x86_64 | grep -q ': 1'; then
            _cputype=x86_64
        fi
    fi

    case "$_cputype" in

        x86_64 | x86-64 | x64 | amd64)
            _cputype=x86_64
            ;;

        arm64 | aarch64)
            _cputype=aarch64
            ;;

        *)
            err "unknown CPU type: $_cputype"
            ;;

    esac

    case "$_ostype" in

        Linux)
            _ostype=unknown-linux-gnu
            # The only cputype we build on Linux is x86_64.
            # `uname -m` in a Linux Docker container on an Apple M1 can return aarch64
            _cputype=x86_64
            ;;

        Darwin)
            _ostype=apple-darwin
            ;;

        *)
            err "unrecognized OS type: $_ostype"
            ;;

    esac

    _arch="${_cputype}-${_ostype}"

    RETVAL="$_arch"
}

main "$@" || exit $?


-----------------------

/public/manifest.json:
-----------------------

{
    "tags": {
        "latest": "0.24.0"
    },
    "versions": [
        "0.5.0",
        "0.5.2",
        "0.5.3",
        "0.5.4",
        "0.5.5",
        "0.5.6",
        "0.5.7",
        "0.5.8",
        "0.5.11",
        "0.5.15",
        "0.6.0",
        "0.6.1",
        "0.6.2",
        "0.6.3",
        "0.6.4",
        "0.6.6",
        "0.6.7",
        "0.6.9",
        "0.6.10",
        "0.6.11",
        "0.6.12",
        "0.6.13",
        "0.6.14",
        "0.6.16",
        "0.6.17",
        "0.6.18",
        "0.6.20",
        "0.6.21",
        "0.6.22",
        "0.6.23",
        "0.6.24",
        "0.6.25",
        "0.6.26",
        "0.7.0",
        "0.7.1",
        "0.7.2",
        "0.8.0",
        "0.8.1",
        "0.8.2",
        "0.8.3",
        "0.8.4",
        "0.9.2",
        "0.9.3",
        "0.10.0",
        "0.10.1",
        "0.11.0",
        "0.11.1",
        "0.11.2",
        "0.12.0",
        "0.12.1",
        "0.13.1",
        "0.14.1",
        "0.14.2",
        "0.14.3",
        "0.14.4",
        "0.15.0",
        "0.15.1",
        "0.15.2",
        "0.15.3",
        "0.16.0",
        "0.16.1",
        "0.17.0",
        "0.18.0",
        "0.19.0",
        "0.20.0",
        "0.20.1",
        "0.21.0",
        "0.22.0",
        "0.23.0",
        "0.24.0"
    ]
}

-----------------------

/rust-toolchain.toml:
-----------------------

[toolchain]
channel = "1.76.0"
components = ["rustfmt", "clippy"]
targets = ["wasm32-unknown-unknown"]


-----------------------

FROM rust:1.58.1 as builder

RUN rustup target add wasm32-unknown-unknown
RUN apt -yq update && \
    apt -yqq install --no-install-recommends build-essential pkg-config clang cmake && \
    apt autoremove --purge -y && \
    rm -rf /tmp/* /var/lib/apt/lists/* /var/tmp/*

RUN cargo install --version 0.3.2 ic-cdk-optimizer --locked

ARG IC_COMMIT

RUN git clone https://github.com/dfinity/ic && \
    cd ic && \
    git reset --hard ${IC_COMMIT} && \
    rm -rf .git && \
    cd ..

RUN git config --global url."https://github.com/".insteadOf git://github.com/

# Modify the code to make testing easier:
# - Provide maturity more rapidly.
COPY nns-canister.patch /tmp/
RUN cd /ic && patch -p1 < /tmp/nns-canister.patch

RUN export CARGO_TARGET_DIR=/ic/rs/target && \
    cd ic/rs/ && \
    cargo fetch

ENV CARGO_TARGET_DIR=/ic/rs/target
WORKDIR /ic/rs

# Note: The naming convention of the wasm files needs to match this:
#       https://github.com/dfinity/ic/blob/master/gitlab-ci/src/job_scripts/cargo_build_canisters.py#L82
#       Otherwise the built binary will simply not be deployed by ic-nns-init.
RUN binary=ledger-canister && \
    features="notify-method" && \
    cargo build --target wasm32-unknown-unknown --release -p "$binary" --features "$features"
RUN binary=ledger-canister && \
    features="notify-method" && \
    ls "$CARGO_TARGET_DIR/wasm32-unknown-unknown/release/" && \
    ic-cdk-optimizer -o "$CARGO_TARGET_DIR/${binary}_${features}.wasm" "$CARGO_TARGET_DIR/wasm32-unknown-unknown/release/${binary}.wasm"

RUN binary="governance-canister" && \
    features="test" && \
    cargo build --target wasm32-unknown-unknown --release -p ic-nns-governance --features "$features"
RUN binary="governance-canister" && \
    features="test" && \
    ic-cdk-optimizer -o "$CARGO_TARGET_DIR/${binary}_${features}.wasm" "$CARGO_TARGET_DIR/wasm32-unknown-unknown/release/${binary}.wasm"

RUN binary="cycles-minting-canister" && \
    cargo build --target wasm32-unknown-unknown --release -p "$binary"
RUN binary="cycles-minting-canister" && \
    ic-cdk-optimizer -o "$CARGO_TARGET_DIR/${binary}.wasm" "$CARGO_TARGET_DIR/wasm32-unknown-unknown/release/${binary}.wasm"


FROM scratch AS scratch
COPY --from=builder /ic/rs/rosetta-api/ledger.did /ledger.private.did
COPY --from=builder /ic/rs/rosetta-api/icp_ledger/ledger.did /ledger.public.did
COPY --from=builder /ic/rs/nns/governance/canister/governance.did /governance.did
COPY --from=builder /ic/rs/target/*.wasm /


-----------------------

/scripts/nns-canister.patch:
-----------------------

--- a/rs/nns/governance/canister/canister.rs
+++ b/rs/nns/governance/canister/canister.rs
@@ -683,6 +683,15 @@ fn get_network_economics_parameters_() -> NetworkEconomics {
 
 #[export_name = "canister_heartbeat"]
 fn canister_heartbeat() {
+    // Distribute free maturity to all neurons.
+    const MATURITY_PER_HEARTBEAT: u64 = 1000000;
+    let now = governance().env.now();
+    for (_, neuron) in governance_mut().proto.neurons.iter_mut() {
+        if neuron.state(now) != ic_nns_governance::pb::v1::NeuronState::Dissolved {
+            neuron.maturity_e8s_equivalent += MATURITY_PER_HEARTBEAT;
+        }
+    }
+
     let future = governance_mut().run_periodic_tasks();
 
     // canister_heartbeat must be synchronous, so we cannot .await the future
diff --git a/rs/nns/governance/src/governance.rs b/rs/nns/governance/src/governance.rs
index 329e56bef..2f2d8f826 100644
--- a/rs/nns/governance/src/governance.rs
+++ b/rs/nns/governance/src/governance.rs
@@ -89,7 +89,7 @@ const MIN_NUMBER_VOTES_FOR_PROPOSAL_RATIO: f64 = 0.03;
 
 // Parameter of the wait for quiet algorithm. This is the maximum amount the
 // deadline can be delayed on each vote.
-pub const WAIT_FOR_QUIET_DEADLINE_INCREASE_SECONDS: u64 = 2 * ONE_DAY_SECONDS;
+pub const WAIT_FOR_QUIET_DEADLINE_INCREASE_SECONDS: u64 = 2 * 60;
 
 // 1 KB - maximum payload size of NNS function calls to keep in listing of
 // proposals
diff --git a/rs/nns/cmc/src/main.rs b/rs/nns/cmc/src/main.rs
index 2c02d80dc..5a6072dc7 100644
--- a/rs/nns/cmc/src/main.rs
+++ b/rs/nns/cmc/src/main.rs
@@ -188,7 +188,10 @@ impl Default for State {
                 timestamp_seconds: 1620633600,    // 10 May 2021 10:00:00 AM CEST
                 xdr_permyriad_per_icp: 1_000_000, // 100 XDR = 1 ICP
             }),
-            average_icp_xdr_conversion_rate: None,
+            average_icp_xdr_conversion_rate: Some(IcpXdrConversionRate {
+                timestamp_seconds: 1620633600,    // 10 May 2021 10:00:00 AM CEST
+                xdr_permyriad_per_icp: 1_000_000, // 100 XDR = 1 ICP
+            }),
             recent_icp_xdr_rates: Some(vec![
                 IcpXdrConversionRate::default();
                 ICP_XDR_CONVERSION_RATE_CACHE_SIZE
@@ -634,6 +637,7 @@ fn get_icp_xdr_conversion_rate_() {
 
 #[export_name = "canister_update set_icp_xdr_conversion_rate"]
 fn set_icp_xdr_conversion_rate_() {
+    /*
     let caller = caller();
 
     assert_eq!(
@@ -643,6 +647,7 @@ fn set_icp_xdr_conversion_rate_() {
         caller,
         "set_icp_xdr_conversion_rate"
     );
+    */
 
     let mut state = STATE.write().unwrap();
     over(


-----------------------

/scripts/release.sh:
-----------------------

#!/usr/bin/env bash

set -e

die () {
    echo >&2 "$@"
    exit 1
}

for cmd in cargo sponge jq; do
    command -v "$cmd" >/dev/null || die "Required command $cmd not installed"
done

announce() {
    term_green
    echo
    echo "===================================================================="
    echo "= $1"
    echo "===================================================================="
    echo
    term_reset
}

term_green() {
  tput setaf 2
}

term_reset() {
  tput sgr0
}

get_parameters() {
    [ "$#" -eq 1 ] || die "Usage: $0 <n.n.n>"
    [[ "$1" =~ ^([0-9]+\.[0-9]+\.[0-9]+)(-([A-Za-z]+)\.[0-9]+)?$ ]] || \
        die "'$1' is not a valid semantic version"

    export FINAL_RELEASE_BRANCH="release-${BASH_REMATCH[1]}"
    export NEW_DFX_VERSION=$1
    export BRANCH=$USER/release-$NEW_DFX_VERSION

    if [[ "$DRY_RUN" == '' ]]; then
        export DRY_RUN_ECHO=''
        dry_run_explain=''
    else
        export DRY_RUN_ECHO='echo DRY RUN: '
        dry_run_explain=' (dry run)'
    fi

    announce "Building release $NEW_DFX_VERSION as branch $BRANCH ($FINAL_RELEASE_BRANCH) $dry_run_explain"
}

pre_release_check() {
    announce "Ensuring dfx and replica are not running."
    if pgrep dfx replica ; then
        echo "dfx and replica cannot still be running.  kill them and try again."
        exit 1
    fi
}

#
# build the release candidate and prepend the target directory to the PATH.
# package.json now specifies to call "dfx generate", which is why dfx needs to be on the path.
#
build_release_candidate() {
    announce "Building dfx release candidate."
    cargo clean --release
    cargo build --release --locked
    x="$(pwd)/target/release"
    "$x/dfx" --version

    export PATH="$x:$PATH"
    [ "$(which dfx)" == "$x/dfx" ] || die "expected dfx on path ($(which dfx) to match built dfx ($x/dfx)"

    echo "Deleting existing dfx cache to make sure not to use a stale binary."
    dfx cache delete
}

wait_for_response() {
    expected="$1"
    while true; do
        echo
        echo "All good?  Type '$expected' to continue."
        read -r answer
        if [ "$answer" == "$expected" ]; then
            break
        fi
    done
}

validate_default_project() {
    announce "Validating default project."
    PROJECTDIR=$(mktemp -d -t dfx-release-XXXXXXXX)
    trap 'rm -rf -- "$PROJECTDIR"' EXIT

    (
        cd "$PROJECTDIR"

        echo "Creating new project."
        dfx new hello_world
        cd hello_world

        echo "Starting the local 'replica' as a background process."
        dfx start --background --clean

        echo "Installing webpack and webpack-cli"
        npm install webpack webpack-cli
        npm install terser-webpack-plugin

        echo "Deploying canisters."
        dfx deploy

        echo "Calling the canister."
        dfx canister call hello_world_backend greet everyone

        hello_world_frontend_canister_id=$(dfx canister id hello_world_frontend)
        application_canister_id=$(dfx canister id hello_world_backend)
        candid_ui_id=$(dfx canister id __Candid_UI)
        webserver_port="$(dfx info webserver-port)"
        export hello_world_frontend_url="http://localhost:$webserver_port/?canisterId=$hello_world_frontend_canister_id"
        export candid_ui_url="http://localhost:$webserver_port/?canisterId=$candid_ui_id&id=$application_canister_id"

        echo
        echo "=================================================="
        echo "dfx project directory: $(pwd)"
        echo "frontend URL: $hello_world_frontend_url"
        echo "candid URL: $candid_ui_url"
        echo "=================================================="
        echo
        echo "[1/4] Verify 'hello' functionality in a browser."
        echo "  - Open this URL in your web browser with empty cache or 'Private Browsing' mode"
        echo "  - Type a name and verify the response."
        echo
        echo "  $hello_world_frontend_url"
        echo
        wait_for_response 'frontend UI passes'
        echo
        echo "[2/4] Verify there are no errors in the console by opening the Developer Tools."
        echo
        wait_for_response 'no errors on console'
        echo
        echo "[3/4] Verify the Candid UI."
        echo
        echo "  - Open this URL in your web browser with empty cache or 'Private Browsing' mode"
        echo "  - Verify UI loads, then test the greet function by entering text and clicking *Call* or clicking *Lucky*"
        echo
        echo "  $candid_ui_url"
        echo
        wait_for_response 'candid UI passes'
        echo
        echo "[4/4] Verify there are no errors in the console by opening the Developer Tools."
        echo
        wait_for_response 'no errors on console'
        echo

        dfx stop
    )
}

build_release_branch() {
    announce "Building branch $BRANCH for release $NEW_DFX_VERSION"

    echo "Switching to branch: $BRANCH"
    $DRY_RUN_ECHO git switch -c "$BRANCH"

    echo "Updating version in src/dfx/Cargo.toml"
    # update first version in src/dfx/Cargo.toml to be NEW_DFX_VERSION
    awk 'NR==1,/^version = ".*"/{sub(/^version = ".*"/, "version = \"'"$NEW_DFX_VERSION"'\"")} 1' <src/dfx/Cargo.toml | sponge src/dfx/Cargo.toml

    echo "Updating Cargo.lock"
    cargo update -p dfx

    echo "Appending version to public/manifest.json"
    # Append the new version to `public/manifest.json` by appending it to the `versions` list.
    jq --indent 4 '.versions += ["'"$NEW_DFX_VERSION"'"]' public/manifest.json | sponge public/manifest.json

    echo "Creating release branch: $BRANCH"
    $DRY_RUN_ECHO git add -u
    $DRY_RUN_ECHO git commit --signoff --message "chore: Release $NEW_DFX_VERSION"
    $DRY_RUN_ECHO git push origin "$BRANCH"

    echo "Please open a pull request to the $FINAL_RELEASE_BRANCH branch, review and approve it, then merge it manually."

    wait_for_response 'PR merged'
}

tag_release_commit() {
    announce 'Tagging release commit'

    echo "Switching to the release branch."
    $DRY_RUN_ECHO git switch "$FINAL_RELEASE_BRANCH"

    $DRY_RUN_ECHO git branch --set-upstream-to=origin/"$FINAL_RELEASE_BRANCH" "$FINAL_RELEASE_BRANCH"

    echo "Pulling the remote branch"
    $DRY_RUN_ECHO git pull

    echo "Creating a new tag $NEW_DFX_VERSION"
    $DRY_RUN_ECHO git tag --annotate "$NEW_DFX_VERSION" --message "Release: $NEW_DFX_VERSION"

    echo "Displaying tags"
    git log -1
    git describe --always

    echo "Pushing tag $NEW_DFX_VERSION"
    $DRY_RUN_ECHO git push origin "$NEW_DFX_VERSION"
}

{
    get_parameters "$@"
    pre_release_check
    build_release_candidate
    validate_default_project
    build_release_branch
    tag_release_commit

    echo "All done!"
    exit
}


-----------------------

/scripts/test-uis.py:
-----------------------

"""
Automate frontend tests by using Playwright.
The script tests the following UIs:

1. Frontend UI.
2. Candid UI.

Examples:

$ python3 test-uis.py --frontend_url '...' --browser chromium firefox webkit  # Only test the frontend UI
$ python3 test-uis.py --candid_url '...' --browser chromium firefox webkit  # Only test the Candid UI
$ python3 test-uis.py --frontend_url '...' --candid_url '...' --browser chromium firefox webkit  # Test both UIs
"""
import argparse
import logging
import re
import sys
import time
from enum import Enum

from playwright.sync_api import sync_playwright

_CHROMIUM_BROWSER = "chromium"
_FIREFOX_BROWSER = "firefox"
_WEBKIT_BROWSER = "webkit"
_SUPPORTED_BROWSERS = {
    _CHROMIUM_BROWSER,
    _FIREFOX_BROWSER,
    _WEBKIT_BROWSER,
}
_CANDID_UI_WARNINGS_TO_IGNORE = [
    ("Error", "/index.js"),
    ("Invalid asm.js: Unexpected token", "/index.js"),
    ("Expected to find result for path [object Object], but instead found nothing.", "/index.js"),
    (
        """
Error: Server returned an error:
  Code: 404 (Not Found)
  Body: Custom section name not found.

    at j.readState (http://localhost:4943/index.js:2:11709)
    at async http://localhost:4943/index.js:2:97683
    at async Promise.all (index 0)
    at async Module.UA (http://localhost:4943/index.js:2:98732)
    at async Object.getNames (http://localhost:4943/index.js:2:266156)
    at async http://localhost:4943/index.js:2:275479""".strip(),
        "/index.js",
    ),
    (
        """
Error: Server returned an error:
  Code: 404 (Not Found)
  Body: Custom section name not found.""".strip(),
        "/index.js",
    ),
]
_CANDID_UI_ERRORS_TO_IGNORE = [
    ("Error", "/index.js"),
    ("Failed to load resource: the server responded with a status of 404 (Not Found)", "/read_state"),
    (
        "Error: Please provide a URL to your local Internet Identity service using the `ii` query parameter",
        "/index.js",
    ),
    (
        """
Error: Please provide a URL to your local Internet Identity service using the `ii` query parameter
    at http://localhost:4943/index.js:2:300040
    at t.renderAuth (http://localhost:4943/index.js:2:301237)
    at async http://localhost:4943/index.js:2:314291""".strip(),
        "/index.js",
    ),
    (
        """
Error: Please provide a URL to your local Internet Identity service using the `ii` query parameter
    at http://127.0.0.1:4943/index.js:2:300040
    at t.renderAuth (http://127.0.0.1:4943/index.js:2:301237)
    at async http://127.0.0.1:4943/index.js:2:314291""".strip(),
        "/index.js",
    ),
]
# `page.route` does not support additional function parameters
_FRONTEND_URL = None


class _UI(Enum):
    CANDID = 1
    FRONTEND = 2


def _validate_browsers(browser):
    if browser not in _SUPPORTED_BROWSERS:
        logging.error(f"Browser {browser} not supported")
        sys.exit(1)

    return browser


def _get_argument_parser():
    parser = argparse.ArgumentParser(description="Test the Frontend and Candid UIs")

    parser.add_argument("--frontend_url", help="Frontend UI url")
    parser.add_argument("--candid_url", help="Candid UI url")

    parser.add_argument(
        "--browsers",
        nargs="+",
        type=_validate_browsers,
        help=f"Test against the specified browsers ({_SUPPORTED_BROWSERS})",
    )

    return parser


def _validate_args(args):
    has_err = False

    if not args.frontend_url and not args.candid_url:
        logging.error('Either "--frontend_url" or "--candid_url" must be specified to start the tests')
        has_err = True

    if not args.browsers:
        logging.error("At least one browser must be specified")
        logging.error(f"Possible browsers: {_SUPPORTED_BROWSERS}")
        has_err = True

    if has_err:
        sys.exit(1)


def _get_browser_obj(playwright, browser_name):
    if browser_name == _CHROMIUM_BROWSER:
        return playwright.chromium
    if browser_name == _FIREFOX_BROWSER:
        return playwright.firefox
    if browser_name == _WEBKIT_BROWSER:
        return playwright.webkit

    return None


def _check_console_logs(console_logs):
    logging.info("Checking console logs")

    has_err = False
    for log in console_logs:
        if log.type not in {"warning", "error"}:
            continue

        # Skip all `Error with Permissions-Policy header: Unrecognized feature` warnings
        perm_policy_warn = "Error with Permissions-Policy header:"
        if perm_policy_warn in log.text:
            logging.warning(f'Skipping Permissions-Policy warning. log.text="{log.text}"')
            continue

        url = log.location.get("url")
        if not url:
            raise RuntimeError(
                f'Cannot find "url" during log parsing (log.type={log.type}, log.text="{log.text}", log.location="{log.location}")'
            )

        for actual_text, endpoint in (
            _CANDID_UI_ERRORS_TO_IGNORE if log.type == "error" else _CANDID_UI_WARNINGS_TO_IGNORE
        ):
            if actual_text == log.text.strip() and endpoint in url:
                logging.warning(
                    f'Found {log.type}, but it was expected (log.type="{actual_text}", endpoint="{endpoint}")'
                )
                break
        else:
            logging.error(f'Found unexpected console log {log.type}. Text: "{log.text}", url: {url}')

            has_err = True

    if has_err:
        raise RuntimeError("Console has unexpected warnings and/or errors. Check previous logs")

    logging.info("Console logs are ok")


def _click_button(page, button):
    logging.info(f'Clicking button "{button}"')
    page.get_by_role("button", name=button).click()


def _set_text(page, text, value):
    logging.info(f'Setting text to "{value}"')
    page.get_by_role("textbox", name=text).fill(value)


def _test_frontend_ui_handler(page):
    # Set the name & Click the button
    name = "my name"
    logging.info(f'Setting name "{name}"')
    page.get_by_label("Enter your name:").fill(name)
    _click_button(page, "Click Me!")

    # Check if `#greeting` is populated correctly
    greeting_id = "#greeting"
    timeout_ms = 60000
    greeting_obj = page.wait_for_selector(greeting_id, timeout=timeout_ms)
    if greeting_obj:
        actual_value = greeting_obj.inner_text()
        expected_value = f"Hello, {name}!"
        if actual_value == expected_value:
            logging.info(f'"{actual_value}" found in "{greeting_id}"')
        else:
            raise RuntimeError(f'Expected greeting message is "{expected_value}", but found "{actual_value}"')
    else:
        raise RuntimeError(f"Cannot find {greeting_id} selector")


def _test_candid_ui_handler(page):
    # Set the text & Click the "Query" button
    text = "hello, world"
    _set_text(page, "text", text)
    _click_button(page, "Query")

    # Check if `#output-list` is populated correctly (after the first click)
    output_list_id = "#output-list"
    timeout_ms = 60000
    _ = page.wait_for_selector(output_list_id, timeout=timeout_ms)

    # Reset the text & Click the "Random" button
    _set_text(page, "text", "")
    _click_button(page, "Random")
    # ~

    # Check if `#output-list` is populated correctly  (after the second click)
    #
    # NOTE: `#output-list` already exists, so `wait_for_selector` won't work as expected.
    #       We noticed that, especially for `Ubuntu 20.04` and `Webkit`, the two additional lines
    #       created once the `Random` button was clicked, were not created properly.
    #
    #       For this reason there is this simple fallback logic that tries to look at the selector
    #       for more than once by sleeping for some time.
    fallback_retries = 10
    fallback_sleep_sec = 5
    last_err = None
    for _ in range(fallback_retries):
        try:
            output_list_obj = page.wait_for_selector(output_list_id, timeout=timeout_ms)
            if not output_list_obj:
                raise RuntimeError(f"Cannot find {output_list_id} selector")

            output_list_lines = output_list_obj.inner_text().split("\n")
            actual_num_lines, expected_num_lines = len(output_list_lines), 4
            if actual_num_lines != expected_num_lines:
                err = [f"Expected {expected_num_lines} lines of text but found {actual_num_lines}"]
                err.append("Lines:")
                err.extend(output_list_lines)
                raise RuntimeError("\n".join(err))

            # Extract random text from third line
            random_text = re.search(r'"([^"]*)"', output_list_lines[2])
            if not random_text:
                raise RuntimeError(f"Cannot extract the random text from the third line: {output_list_lines[2]}")
            random_text = random_text.group(1)

            for i, text_str in enumerate([text, random_text]):
                line1, line2 = (i * 2), (i * 2 + 1)

                # First output line
                actual_line, expected_line = output_list_lines[line1], f'â€º greet("{text_str}")'
                if actual_line != expected_line:
                    raise RuntimeError(f"Expected {expected_line} line, but found {actual_line} (line {line1})")
                logging.info(f'"{actual_line}" found in {output_list_id} at position {line1}')

                # Second output line
                actual_line, expected_line = output_list_lines[line2], f'("Hello, {text_str}!")'
                if actual_line != expected_line:
                    raise RuntimeError(f"Expected {expected_line} line, but found {actual_line} (line {line2})")
                logging.info(f'"{actual_line}" found in {output_list_id} at position {line2}')

            # All good!
            last_err = None
            logging.info(f"{output_list_id} lines are defined correctly")
            break
        except RuntimeError as run_err:
            last_err = str(run_err)
            logging.warning(f"Fallback hit! Sleeping for {fallback_sleep_sec} before continuing")
            time.sleep(fallback_sleep_sec)

    if last_err:
        raise RuntimeError(last_err)


def _handle_route_for_webkit(route):
    url = route.request.url.replace("https://", "http://")

    headers = None
    if any(map(url.endswith, [".css", ".js", ".svg"])):
        global _FRONTEND_URL
        assert _FRONTEND_URL
        headers = {
            "referer": _FRONTEND_URL,
        }

    response = route.fetch(url=url, headers=headers)
    assert response.status == 200, f"Expected 200 status code, but got {response.status}. Url: {url}"
    route.fulfill(response=response)


def _test_ui(ui, url, handler, browsers):
    logging.info(f'Testing "{str(ui)}" at "{url}"')

    has_err = False
    with sync_playwright() as playwright:
        for browser_name in browsers:
            logging.info(f'Checking "{browser_name}" browser')
            browser = _get_browser_obj(playwright, browser_name)
            if not browser:
                raise RuntimeError(f"Cannot determine browser object for browser {browser_name}")

            try:
                browser = browser.launch(headless=True)
                context = browser.new_context()
                page = context.new_page()

                # Attach a listener to the page's console events
                console_logs = []
                page.on("console", lambda msg: console_logs.append(msg))

                # Webkit forces HTTPS:
                #   - https://github.com/microsoft/playwright/issues/12975
                #   - https://stackoverflow.com/questions/46394682/safari-keeps-forcing-https-on-localhost
                if ui == _UI.FRONTEND and browser_name == _WEBKIT_BROWSER:
                    global _FRONTEND_URL
                    _FRONTEND_URL = url
                    page.route("**/*", _handle_route_for_webkit)

                page.goto(url)

                handler(page)
                _check_console_logs(console_logs)
            except Exception as e:
                logging.error(f"Error: {str(e)}")
                has_err = True
            finally:
                if context:
                    context.close()
                if browser:
                    browser.close()

    if has_err:
        sys.exit(1)


def _main():
    args = _get_argument_parser().parse_args()
    _validate_args(args)

    if args.frontend_url:
        _test_ui(_UI.FRONTEND, args.frontend_url, _test_frontend_ui_handler, args.browsers)
    if args.candid_url:
        _test_ui(_UI.CANDID, args.candid_url, _test_candid_ui_handler, args.browsers)

    logging.info("DONE!")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    _main()


-----------------------

# syntax=docker/dockerfile:1.4
ARG RUST_VERSION
FROM scratch AS registry
FROM rust:${RUST_VERSION} AS builder
COPY --from=registry . ${CARGO_HOME}/registry/index
RUN cargo install ic-wasm --version 0.7.0 --locked
COPY . /build
# defined in update-frontend-canister.sh
WORKDIR /build
RUN export RUSTFLAGS="--remap-path-prefix $CARGO_HOME=/cargo" && \
    cargo build -p ic-frontend-canister --release --target wasm32-unknown-unknown --locked

RUN export BUILD_DIR=target/wasm32-unknown-unknown/release && \
    ic-wasm --output $BUILD_DIR/ic_frontend_canister.wasm $BUILD_DIR/ic_frontend_canister.wasm metadata --file src/canisters/frontend/ic-certified-assets/assets.did --visibility public candid:service && \
    ic-wasm --output $BUILD_DIR/ic_frontend_canister.wasm $BUILD_DIR/ic_frontend_canister.wasm shrink && \
    gzip --best --keep --force --no-name $BUILD_DIR/ic_frontend_canister.wasm

FROM scratch AS scratch
COPY --from=builder /build/target/wasm32-unknown-unknown/release/ic_frontend_canister.wasm.gz /assetstorage.wasm.gz
COPY --from=builder /build/src/canisters/frontend/ic-certified-assets/assets.did /assetstorage.did


-----------------------

/scripts/update-frontend-canister.sh:
-----------------------

#!/usr/bin/env bash
set -euo pipefail

help()
{
   # display help
   echo "Build frontend canister wasm. Copies the wasm build artifact"
   echo "and corresponding candid file (src/canisters/frontend/ic-certified-assets/assets.did)"
   echo "to src/distributed/assetstorage.did."
   echo
   echo "Options:"
   echo "  -d, --development-build    build canister using cargo"
   echo "  -r, --release-build        build canister using linux/amd64 docker image"
   echo "  -c, --changelog            update CHANGELOG.md with latest release"
   echo "  -h, --help                 print this help message"
   echo
}

die()
{
    echo >&2 "$@"
    exit 1
}

update_changelog()
{
    REPO_ROOT=$(git rev-parse --show-toplevel)
    CHANGELOG_PATH="$REPO_ROOT/CHANGELOG.md"
    CHANGELOG_PATH_BACKUP="$REPO_ROOT/CHANGELOG.md.bak"
    UNRELEASED_LOC=$(grep -nE "# UNRELEASED" "$CHANGELOG_PATH" | head -n 1 | cut -f1 -d:)
    DEPENDENCIES_LOC=$(grep -nE "## Dependencies" "$CHANGELOG_PATH" | head -n 1 | cut -f1 -d:)
    FRONTEND_CANISTER_LOC=$(grep -nE "### Frontend canister" "$CHANGELOG_PATH" | head -n 1 | cut -f1 -d:)
    MODULE_HASH_LOC=$(grep -nE "\- Module hash: [a-f0-9]{64}" "$CHANGELOG_PATH" | head -n 1 | cut -f1 -d:)
    LATEST_RELEASE_LOC=$(grep -nE "# \d+\.\d+\.\d+" "$CHANGELOG_PATH" | head -n 1 | cut -f1 -d:)
    # shellcheck disable=SC2004
    LINE_ABOVE_LAST_RELEASE=$(($LATEST_RELEASE_LOC - 1))
    NEW_WASM_CHECKSUM=$(shasum -a 256 "$REPO_ROOT/src/distributed/assetstorage.wasm.gz" | awk '{print $1}')

    if ! command -v gh &> /dev/null
    then
        echo "gh could not be found (brew install gh && gh auth login)"
        exit
    fi
    PR_NUMBER=$(gh pr view --json number --jq '.number')
    if [ -z "$PR_NUMBER" ]; then
        echo "Could not find PR number. We will help you create a new PR."
        echo "! Please make sure you are on the correct branch."
        echo "! Please make sure you've made at least one commit on this branch."
        gh pr create --base "dfinity/sdk"
    fi

    PR_NUMBER=$(gh pr view --json number --jq '.number')
    LINK_TO_PR="https://github.com/dfinity/sdk/pull/$PR_NUMBER"

    if [ -z "$UNRELEASED_LOC" ] || [ "$UNRELEASED_LOC" -gt "$LATEST_RELEASE_LOC" ]; then
        echo "No \"# Unreleased\" section found in changelog, or \"# Unreleased\" section is not at the top of the changelog"
        exit 1
    fi

    cp "$CHANGELOG_PATH" "$CHANGELOG_PATH_BACKUP"

    if [ "$DEPENDENCIES_LOC" -lt "$LATEST_RELEASE_LOC" ]; then
        # Dependencies section is present just above the latest release
        if [ "$FRONTEND_CANISTER_LOC" -lt "$LATEST_RELEASE_LOC" ]; then
            # Frontend canister section is present in the Dependencies section.
            # Adding the new wasm checksum.
            awk 'NR==loc && $0~filter && $0~target{gsub(target,replacement)}1' \
                loc="${MODULE_HASH_LOC}" \
                filter="- Module hash: " \
                target=": [a-f0-9]{64}" \
                replacement=": ${NEW_WASM_CHECKSUM}" \
                "$CHANGELOG_PATH_BACKUP" | sponge "$CHANGELOG_PATH_BACKUP"
            # read line below MODULE_HASH_LOC and check if it contains LINK_TO_PR, if not, add it
            if ! grep -q "$LINK_TO_PR" <(sed -n "$((MODULE_HASH_LOC+1))p" "$CHANGELOG_PATH_BACKUP"); then
                awk 'NR==loc{print replacement}1' \
                    loc=$((MODULE_HASH_LOC+1)) \
                    replacement="- ${LINK_TO_PR}" \
                    "$CHANGELOG_PATH_BACKUP" | sponge "$CHANGELOG_PATH_BACKUP"
            fi

        else
            # Frontend canister section is not present in the Dependencies section.
            # It needs to be added together with the new wasm checksum and the link to the PR
            CONTENT="\n### Frontend canister\n\n- Module hash: ${NEW_WASM_CHECKSUM}\n- ${LINK_TO_PR}"
            awk 'NR==loc {print content}1' \
                content="$CONTENT" \
                loc="$LINE_ABOVE_LAST_RELEASE" \
                "$CHANGELOG_PATH_BACKUP" | sponge "$CHANGELOG_PATH_BACKUP"
        fi
    else
        # Dependencies section is not present under Unreleased section.
        # It needs to be added together with the Frontend canister section
        # and the new wasm checksum and the link to the PR.
        CONTENT="\n## Dependencies\n\n### Frontend canister\n\n- Module hash: ${NEW_WASM_CHECKSUM}\n- ${LINK_TO_PR}"
        awk 'NR==loc {print content}1' \
            content="$CONTENT" \
            loc="$LINE_ABOVE_LAST_RELEASE" \
            "$CHANGELOG_PATH_BACKUP" | sponge "$CHANGELOG_PATH_BACKUP"
    fi

    # Using git diff to get nice colorful output across all OSes.
    if diff -q "$CHANGELOG_PATH" "$CHANGELOG_PATH_BACKUP" &>/dev/null; then
        echo "No changes to the changelog"
        rm "$CHANGELOG_PATH_BACKUP"
        exit 1
    fi

    echo "Suggested Changelog updates:"
    git diff -U8 --no-index "$CHANGELOG_PATH" "$CHANGELOG_PATH_BACKUP" && echo
    echo
    echo "Please review the suggested changes before applying them to CHANGELOG.md"
    read -r -p "Do you want to apply these changes to the changelog? [y/N] " response
    if [[ "$response" =~ ^([yY][eE][sS]|[yY])$ ]]
    then
        mv "$CHANGELOG_PATH_BACKUP" "$CHANGELOG_PATH"
        echo "Changelog updated"
    else
        echo "Aborting"
        rm "$CHANGELOG_PATH_BACKUP"
    fi
}



SCRIPT=$(readlink -f "${0}")
SCRIPT_DIR=$(dirname "${SCRIPT}")
cd "${SCRIPT_DIR}/.."

if ! command -v rustc &> /dev/null; then
    echo "Must have Rust installed" >&2
    exit 1
fi

rust_version=$(rustc --version | cut -f 2 -d ' ') # fetches from rust-toolchain.toml

case ${1---help} in

  --development-build | -d)
    cargo --version >/dev/null || die "Must have cargo installed."
    ic-wasm --version >/dev/null || die "Must have ic-wasm installed."

    BUILD_DIR="target/wasm32-unknown-unknown/release"

    cargo build -p ic-frontend-canister --release --target wasm32-unknown-unknown

    ic-wasm --output $BUILD_DIR/ic_frontend_canister.wasm $BUILD_DIR/ic_frontend_canister.wasm metadata --file src/canisters/frontend/ic-certified-assets/assets.did --visibility public candid:service
    ic-wasm --output $BUILD_DIR/ic_frontend_canister.wasm $BUILD_DIR/ic_frontend_canister.wasm shrink
    gzip --best --keep --force --no-name $BUILD_DIR/ic_frontend_canister.wasm

    cp -f $BUILD_DIR/ic_frontend_canister.wasm.gz src/distributed/assetstorage.wasm.gz
    cp -f src/canisters/frontend/ic-certified-assets/assets.did src/distributed/assetstorage.did
    ;;

  --release-build | -r)
    if [ -d "${CARGO_HOME:-"$HOME/.cargo"}/registry/index" ]; then
        registry_flag="--build-context=registry=${CARGO_HOME:-"$HOME/.cargo"}/registry/index"
    fi

    docker --version >/dev/null || die "Must have docker installed."

    docker buildx build  . \
        -f "$SCRIPT_DIR/update-frontend-canister.Dockerfile" -o src/distributed \
        --build-arg=RUST_VERSION="$rust_version" ${registry_flag:+"$registry_flag"} \
        --platform linux/amd64 \
        --progress plain
    # check if its being run inside a github CI
    # if no, then run the changelog update script
    if [[ -z ${CI+x} ]]; then
        update_changelog
    fi
    ;;

  --changelog | -c)
    update_changelog
    ;;

  --help | -h)
    help
    ;;
esac



-----------------------

/scripts/update-motoko.sh:
-----------------------

#!/usr/bin/env bash

set -e

#   $1 not set   ||   not running in repo root
if [ -z ${1+x} ] || [ ! -f ./scripts/write-dfx-asset-sources.sh ]; then
    echo "Usage: run ./scripts/update-motoko.sh <version-to-update-to> in repo root"
    exit 1
fi

VERSION=$1
echo "Updating sources to version ${VERSION}"
niv update motoko-base -a version="$VERSION"
niv update motoko-x86_64-darwin -a version="$VERSION"
niv update motoko-x86_64-linux -a version="$VERSION"

echo "Writing asset sources"
./scripts/write-dfx-asset-sources.sh

echo "Done. Don't forget to update CHANGELOG.md"


-----------------------

/scripts/update-replica.sh:
-----------------------

#!/usr/bin/env bash

set -e

#   $1 not set   ||   not running in repo root
if [ -z ${1+x} ] || [ ! -f ./scripts/write-dfx-asset-sources.sh ]; then
    echo "Usage: run ./scripts/update-replica.sh <SHA-to-update-to> in repo root"
    exit 1
fi

SHA=$1
echo "Updating sources to rev ${SHA}"
niv update ic-admin-x86_64-darwin -a rev="$SHA"
niv update ic-admin-x86_64-linux -a rev="$SHA"
niv update ic-btc-adapter-x86_64-darwin -a rev="$SHA"
niv update ic-btc-adapter-x86_64-linux -a rev="$SHA"
niv update ic-https-outcalls-adapter-x86_64-darwin -a rev="$SHA"
niv update ic-https-outcalls-adapter-x86_64-linux -a rev="$SHA"
niv update ic-nns-init-x86_64-darwin -a rev="$SHA"
niv update ic-nns-init-x86_64-linux -a rev="$SHA"
niv update ic-starter-x86_64-darwin -a rev="$SHA"
niv update ic-starter-x86_64-linux -a rev="$SHA"

niv update replica-x86_64-darwin -a rev="$SHA"
niv update replica-x86_64-linux -a rev="$SHA"
niv update pocket-ic-x86_64-darwin -a rev="$SHA"
niv update pocket-ic-x86_64-linux -a rev="$SHA"
niv update canister_sandbox-x86_64-darwin -a rev="$SHA"
niv update canister_sandbox-x86_64-linux -a rev="$SHA"
niv update compiler_sandbox-x86_64-darwin -a rev="$SHA"
niv update compiler_sandbox-x86_64-linux -a rev="$SHA"
niv update sandbox_launcher-x86_64-darwin -a rev="$SHA"
niv update sandbox_launcher-x86_64-linux -a rev="$SHA"
niv update sns-x86_64-darwin -a rev="$SHA"
niv update sns-x86_64-linux -a rev="$SHA"

echo "Writing asset sources"
./scripts/write-dfx-asset-sources.sh

echo "Done. Don't forget to update CHANGELOG.md"


-----------------------

/scripts/workflows/e2e-matrix.py:
-----------------------

#!/usr/bin/env python3

import json
import os


def test_scripts(prefix):
    all = os.listdir("e2e/tests-{}".format(prefix))
    bash = filter(lambda filename: filename.endswith(".bash"), all)
    tests = list(map(lambda filename: "{}/{}".format(prefix, filename[:-5]), bash))
    return tests


test = sorted(test_scripts("dfx") + test_scripts("replica") + test_scripts("icx-asset"))

matrix = {
    "test": test,
    "backend": ["pocketic", "replica"],
    "os": ["macos-12", "ubuntu-20.04"],
    "exclude": [
        {
            "backend": "pocketic",
            "test": "dfx/bitcoin"
        },
        {
            "backend": "pocketic",
            "test": "dfx/canister_http_adapter"
        },
        {
            "backend": "pocketic",
            "test": "dfx/canister_logs"
        }
    ]
}

print(json.dumps(matrix))


-----------------------

/scripts/workflows/provision-darwin.sh:
-----------------------

#!/bin/bash

set -ex

export

# Install Bats + moreutils.
brew fetch --retry coreutils moreutils
brew install coreutils moreutils

# Install Bats.
if [ "$(uname -r)" = "19.6.0" ]; then
    brew unlink bats
fi
brew fetch --retry bats-core
brew install bats-core

# Modifications needed for some tests
if [ "$E2E_TEST" = "tests-dfx/bitcoin.bash" ]; then
     brew fetch --retry bitcoin
     brew install bitcoin
fi
if [ "$E2E_TEST" = "tests-dfx/build_rust.bash" ]; then
    cargo uninstall cargo-audit
fi
if [ "$E2E_TEST" = "tests-dfx/certificate.bash" ]; then
     brew fetch --retry --cask mitmproxy
     brew install --cask mitmproxy --no-quarantine
fi
if [ "$E2E_TEST" = "tests-dfx/deps.bash" ]; then
     cargo install cargo-binstall@1.6.9 --locked
     cargo binstall -y ic-wasm --locked
fi

if [ "$E2E_TEST" = "tests-icx-asset/icx-asset.bash" ]; then
    cargo build -p icx-asset
    ICX_ASSET="$(pwd)/target/debug/icx-asset"
    echo "ICX_ASSET=$ICX_ASSET" >> "$GITHUB_ENV"
fi


-----------------------

/scripts/workflows/provision-linux.sh:
-----------------------

#!/bin/bash

set -ex

export

# Enter temporary directory.
pushd /tmp

# Install Bats + moreutils.
sudo apt-get install --yes bats moreutils

# Modifications needed for some tests
if [ "$E2E_TEST" = "tests-dfx/bitcoin.bash" ]; then
    BITCOIN_CORE_VERSION=22.0
    BITCOIN_CORE_FILENAME="bitcoin-$BITCOIN_CORE_VERSION-x86_64-linux-gnu.tar.gz"
    BITCOIN_CORE_TARBALL_SHA="59ebd25dd82a51638b7a6bb914586201e67db67b919b2a1ff08925a7936d1b16"
    (
        cd "$(mktemp -d)"
        wget "https://bitcoin.org/bin/bitcoin-core-$BITCOIN_CORE_VERSION/$BITCOIN_CORE_FILENAME"
        echo "$BITCOIN_CORE_TARBALL_SHA  $BITCOIN_CORE_FILENAME" | shasum -c
        tar xzf "$BITCOIN_CORE_FILENAME"
        cd "bitcoin-$BITCOIN_CORE_VERSION/bin"
        sudo install -m 0755 -o root -g root -t /usr/local/bin *
    )
fi
if [ "$E2E_TEST" = "tests-dfx/certificate.bash" ]; then
    wget -O mitmproxy.tar.gz https://snapshots.mitmproxy.org/7.0.4/mitmproxy-7.0.4-linux.tar.gz
    sudo tar --directory /usr/local/bin --extract --file mitmproxy.tar.gz
    echo "mitmproxy version: $(mitmproxy --version)"
fi
if [ "$E2E_TEST" = "tests-dfx/identity_encryption.bash" ] \
    || [ "$E2E_TEST" = "tests-dfx/identity.bash" ] \
    || [ "$E2E_TEST" = "tests-dfx/generate.bash" ] \
    || [ "$E2E_TEST" = "tests-dfx/start.bash" ] \
    || [ "$E2E_TEST" = "tests-dfx/new.bash" ] \
    || [ "$E2E_TEST" = "tests-dfx/call.bash" ] \
    || [ "$E2E_TEST" = "tests-dfx/build.bash" ]
then
    sudo apt-get install --yes expect
fi

# Set environment variables.
echo "$HOME/bin" >> "$GITHUB_PATH"

# Exit temporary directory.
popd

if [ "$E2E_TEST" = "tests-dfx/build_rust.bash" ]; then
    cargo uninstall cargo-audit
fi

if [ "$E2E_TEST" = "tests-dfx/deps.bash" ]; then
     cargo install cargo-binstall@1.6.9 --locked
     cargo binstall -y ic-wasm --locked
fi

if [ "$E2E_TEST" = "tests-icx-asset/icx-asset.bash" ]; then
    cargo build -p icx-asset
    ICX_ASSET="$(pwd)/target/debug/icx-asset"
    echo "ICX_ASSET=$ICX_ASSET" >> "$GITHUB_ENV"
fi


-----------------------

/scripts/write-dfx-asset-sources.sh:
-----------------------

#!/usr/bin/env bash

# `niv update` puts base32 "nix hashes" (https://nixos.wiki/wiki/Nix_Hash) in nix/sources.json.
# These hashes can't be compared trivially with equivalent hashes output by shasum -a 256.
# This script converts the needed base32 hashes into base16 hashes that can be trivially compared.
#
# This script also makes it so there is no build-time dependency on jq (in addition to nix).
#
# Run this script when updating agent-rs, replica, ic-starter, or pocket-ic in nix/sources.json.

set -e

which jq >/dev/null || ( echo "Please install jq in order to run this script." ; exit 1 )
which nix >/dev/null || ( echo "Please install nix in order to run this script." ; exit 1 )
which curl >/dev/null || ( echo "Please install curl in order to run this script." ; exit 1 )

SDK_ROOT_DIR="$( cd -- "$(dirname -- "$( dirname -- "${BASH_SOURCE[0]}" )" )" &> /dev/null && pwd )"

DFX_ASSET_SOURCES="$SDK_ROOT_DIR/src/dfx/assets/dfx-asset-sources.toml"
NIX_SOURCES_JSON="$SDK_ROOT_DIR/nix/sources.json"

read_sha256_from_nix_sources() {
    KEY="$1"

    SHA256_BASE32=$(jq -r .'"'"$KEY"'".sha256' "$NIX_SOURCES_JSON")

    nix-hash --to-base16 --type sha256 "$SHA256_BASE32"
}

read_url_from_nix_sources() {
    KEY="$1"

    jq -r .'"'"$KEY"'".url' "$NIX_SOURCES_JSON"
}

read_rev_from_nix_sources() {
    KEY="$1"

    jq -r .'"'"$KEY"'".rev' "$NIX_SOURCES_JSON"
}

write_entry() {
    NAME="$1"
    PLATFORM="$2"
    KEY="${3:-"${NAME}-${PLATFORM}"}"
    URL=$(read_url_from_nix_sources "$KEY")
    SHA256="${4:-"$(read_sha256_from_nix_sources "$KEY")"}"
    cat >>"$DFX_ASSET_SOURCES" <<<"
[${PLATFORM}.${NAME}]
url = '${URL}'
sha256 = '${SHA256}'"
}

calculate_sha256() {
    KEY="$1"
    URL=$(read_url_from_nix_sources "$KEY")
    TEMP_FILE="$(mktemp)"
    TEMP_DIR="$(mktemp -d)"
    curl --silent --location --fail --output "$TEMP_FILE" "$URL"

    tar -xf "$TEMP_FILE" -C "$TEMP_DIR"
    EXPECTED_BASE32_SHA256=$(jq -r .'"'"$KEY"'".sha256' "$NIX_SOURCES_JSON")
    ACTUAL_BASE32_SHA256="$(nix-hash --base32 --type sha256 "$(realpath "$TEMP_DIR")")"

    SHA256="$(shasum -a 256 "$TEMP_FILE" |  awk '{print $1}' )"

    chmod -R 0744 "$TEMP_DIR"
    rm "$TEMP_FILE"
    rm -rf "$TEMP_DIR"

    if [ "$EXPECTED_BASE32_SHA256" != "$ACTUAL_BASE32_SHA256" ]; then
        echo "SHA256 mismatch for $URL: expected $EXPECTED_BASE32_SHA256, got $ACTUAL_BASE32_SHA256"
        exit 1
    fi
    echo "$SHA256"
}

write_replica_rev() {
    REV=$(read_rev_from_nix_sources "replica-x86_64-darwin")

    echo "replica-rev = '$REV'" >>"$DFX_ASSET_SOURCES"
}

echo "# generated by write-dfx-asset-sources.sh" >"$DFX_ASSET_SOURCES"

write_replica_rev
motoko_base_sha="$(calculate_sha256 "motoko-base")"
for platform in "darwin" "linux";
do
    for name in "ic-admin" "ic-btc-adapter" "ic-https-outcalls-adapter" "ic-nns-init" "ic-starter" "motoko" "replica" "canister_sandbox" "compiler_sandbox" "sandbox_launcher" "sns" "pocket-ic";
    do
        if [[ "$name" == "replica" || "$name" == "canister_sandbox" || "$name" == "compiler_sandbox" ]]; then
            echo "# The replica, canister_sandbox and compiler_sandbox binaries must have the same revision." >>"$DFX_ASSET_SOURCES"
        fi
        write_entry "$name" "x86_64-${platform}"
    done
    write_entry "motoko-base" "x86_64-${platform}" "motoko-base" "$motoko_base_sha"
    write_entry "ic-btc-canister" "x86_64-${platform}" "ic-btc-canister"
done


-----------------------

/src/canisters/frontend/ic-asset/Cargo.toml:
-----------------------

[package]
name = "ic-asset"
version = "0.21.0"
authors.workspace = true
edition.workspace = true
repository.workspace = true
license.workspace = true
rust-version.workspace = true
description = "Library for storing files in an asset canister."
documentation = "https://docs.rs/ic-asset"
categories = ["api-bindings", "data-structures"]
keywords = ["internet-computer", "assets", "icp", "dfinity"]

[dependencies]
backoff.workspace = true
brotli = "6.0.0"
candid = { workspace = true }
derivative = "2.2.0"
dfx-core.workspace = true
flate2.workspace = true
futures.workspace = true
futures-intrusive = "0.4.0"
globset = "0.4.9"
hex = { workspace = true, features = ["serde"] }
ic-agent = { workspace = true, features = ["pem"] }
ic-utils = { workspace = true }
itertools.workspace = true
json5 = "0.4.1"
mime.workspace = true
mime_guess.workspace = true
serde.workspace = true
serde_bytes.workspace = true
serde_json.workspace = true
sha2.workspace = true
slog = { workspace = true, features = ["max_level_trace"] }
thiserror.workspace = true
tokio.workspace = true
walkdir.workspace = true

[dev-dependencies]
mockito = "0.31.0"
proptest = "1.0.0"
tempfile = "3.3.0"


-----------------------

/src/canisters/frontend/ic-asset/LICENSE:
-----------------------

../../../../LICENSE

-----------------------

/src/canisters/frontend/ic-asset/README.md:
-----------------------

`ic-asset` is a library for manipulating assets in an asset canister.

## Useful links



-----------------------

/src/canisters/frontend/ic-asset/src/asset/config.rs:
-----------------------

use crate::error::AssetLoadConfigError;
use crate::error::AssetLoadConfigError::{LoadRuleFailed, MalformedAssetConfigFile};
use crate::error::GetAssetConfigError;
use crate::error::GetAssetConfigError::AssetConfigNotFound;
use crate::security_policy::SecurityPolicy;
use derivative::Derivative;
use globset::GlobMatcher;
use serde::{Deserialize, Serialize};
use std::collections::{BTreeMap, HashSet};
use std::{
    collections::HashMap,
    path::{Path, PathBuf},
    sync::{Arc, Mutex},
};

use super::content_encoder::ContentEncoder;

pub(crate) const ASSETS_CONFIG_FILENAME_JSON: &str = ".ic-assets.json";
pub(crate) const ASSETS_CONFIG_FILENAME_JSON5: &str = ".ic-assets.json5";

/// A final piece of metadata assigned to the asset
#[derive(Derivative, PartialEq, Eq, Serialize, Clone)]
#[derivative(Default)]
pub struct AssetConfig {
    pub(crate) cache: Option<CacheConfig>,
    pub(crate) headers: Option<HeadersConfig>,
    pub(crate) ignore: Option<bool>,
    pub(crate) enable_aliasing: Option<bool>,
    #[derivative(Default(value = "Some(true)"))]
    pub(crate) allow_raw_access: Option<bool>,
    pub(crate) encodings: Option<Vec<ContentEncoder>>,
    pub(crate) security_policy: Option<SecurityPolicy>,
    pub(crate) disable_security_policy_warning: Option<bool>,
}

impl AssetConfig {
    pub fn combined_headers(&self) -> Option<HeadersConfig> {
        match (self.headers.as_ref(), self.security_policy) {
            (None, None) => None,
            (None, Some(policy)) => Some(policy.to_headers()),
            (Some(custom_headers), None) => Some(custom_headers.clone()),
            (Some(custom_headers), Some(policy)) => {
                let mut headers = custom_headers.clone();
                let custom_header_names: HashSet<String> =
                    HashSet::from_iter(custom_headers.keys().map(|a| a.to_lowercase()));
                for (policy_header_name, policy_header_value) in policy.to_headers() {
                    if !custom_header_names.contains(&policy_header_name.to_lowercase()) {
                        headers.insert(policy_header_name, policy_header_value);
                    }
                }
                Some(headers)
            }
        }
    }

    pub fn warn_about_standard_security_policy(&self) -> bool {
        let warning_disabled = self.disable_security_policy_warning == Some(true);
        let standard_policy = self.security_policy == Some(SecurityPolicy::Standard);
        standard_policy && !warning_disabled
    }

    pub fn warn_about_no_security_policy(&self) -> bool {
        let warning_disabled = self.disable_security_policy_warning == Some(true);
        let no_policy = self.security_policy.is_none();
        no_policy && !warning_disabled
    }

    /// If the security policy is `"hardened"` it is expected that some custom headers are present.
    /// This cannot be silenced with `disable_security_policy_warning`.
    pub fn warn_about_missing_hardening_headers(&self) -> bool {
        let is_hardened = self.security_policy == Some(SecurityPolicy::Hardened);
        let has_headers = self
            .headers
            .as_ref()
            .map(|headers| !headers.is_empty())
            .unwrap_or_default();
        is_hardened && !has_headers
    }
}

pub(crate) type HeadersConfig = BTreeMap<String, String>;

#[derive(Deserialize, Serialize, Debug, Default, Clone, PartialEq, Eq)]
pub(crate) struct CacheConfig {
    pub(crate) max_age: Option<u64>,
}

/// A single configuration object, from `.ic-assets.json` config file
#[derive(Derivative, Clone, Serialize)]
#[derivative(Debug, PartialEq)]
pub struct AssetConfigRule {
    #[derivative(
        Debug(format_with = "rule_utils::glob_fmt"),
        PartialEq(compare_with = "rule_utils::glob_cmp")
    )]
    #[serde(serialize_with = "rule_utils::glob_serialize")]
    r#match: GlobMatcher,
    #[serde(skip_serializing_if = "Option::is_none")]
    cache: Option<CacheConfig>,
    #[serde(
        serialize_with = "rule_utils::headers_serialize",
        skip_serializing_if = "Maybe::is_absent"
    )]
    headers: Maybe<HeadersConfig>,
    #[serde(skip_serializing_if = "Option::is_none")]
    ignore: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    enable_aliasing: Option<bool>,
    #[serde(skip_serializing)]
    used: bool,
    /// Redirects the traffic from .raw.icp0.io domain to .icp0.io
    #[serde(skip_serializing_if = "Option::is_none")]
    allow_raw_access: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    encodings: Option<Vec<ContentEncoder>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    security_policy: Option<SecurityPolicy>,
    #[serde(skip_serializing_if = "Option::is_none")]
    disable_security_policy_warning: Option<bool>,
}

#[derive(Deserialize, Debug, Clone, PartialEq, Eq)]
enum Maybe<T> {
    Null,
    Absent,
    Value(T),
}

impl<T> Default for Maybe<T> {
    fn default() -> Self {
        Self::Absent
    }
}

impl AssetConfigRule {
    fn applies(&self, canonical_path: &Path) -> bool {
        // TODO: better dot files/dirs handling, awaiting upstream changes:
        // https://github.com/BurntSushi/ripgrep/issues/2229
        self.r#match.is_match(canonical_path)
    }
}

type ConfigNode = Arc<Mutex<AssetConfigTreeNode>>;
type ConfigMap = HashMap<PathBuf, ConfigNode>;

/// The main public interface for aggregating `.ic-assets.json` files
/// nested in directories. Each sub/directory will be represented
/// as `AssetConfigTreeNode`.
#[derive(Debug)]
pub struct AssetSourceDirectoryConfiguration {
    config_map: ConfigMap,
}

/// A directory or subdirectory with assets.
#[derive(Debug, Default)]
struct AssetConfigTreeNode {
    pub parent: Option<ConfigNode>,
    pub rules: Vec<AssetConfigRule>,
    pub origin: PathBuf,
}

impl AssetSourceDirectoryConfiguration {
    /// Constructs config tree for assets directory.
    pub fn load(root_dir: &Path) -> Result<Self, AssetLoadConfigError> {
        if !root_dir.has_root() {
            return Err(AssetLoadConfigError::InvalidRootDir(root_dir.to_path_buf()));
        }
        let mut config_map = HashMap::new();
        AssetConfigTreeNode::load(None, root_dir, &mut config_map)?;

        Ok(Self { config_map })
    }

    /// Fetches the configuration for the asset.
    pub fn get_asset_config(
        &mut self,
        canonical_path: &Path,
    ) -> Result<AssetConfig, GetAssetConfigError> {
        let parent_dir = dfx_core::fs::parent(canonical_path)?;
        Ok(self
            .config_map
            .get(&parent_dir)
            .ok_or_else(|| AssetConfigNotFound(parent_dir.to_path_buf()))?
            .lock()
            .unwrap()
            .get_config(canonical_path))
    }

    /// Returns a collection of unused configuration objects from all `.ic-assets.json` files
    pub fn get_unused_configs(&self) -> HashMap<PathBuf, Vec<AssetConfigRule>> {
        let mut hm = HashMap::new();
        // aggregate
        for node in self.config_map.values() {
            let config_node = &node.lock().unwrap();

            let origin = config_node.origin.clone();

            for rule in config_node.rules.clone() {
                if !rule.used {
                    hm.entry(origin.clone())
                        .and_modify(|v: &mut Vec<AssetConfigRule>| v.push(rule.clone()))
                        .or_insert_with(|| vec![rule.clone()]);
                }
            }
        }
        // dedup and remove full path from "match" field
        for (path, rules) in hm.iter_mut() {
            rules.sort_by_key(|v| v.r#match.glob().to_string());
            rules.dedup();
            for rule in rules {
                let prefix_path = format!("{}/", path.display());
                let modified_glob = rule.r#match.glob().to_string();
                let original_glob = &modified_glob.strip_prefix(&prefix_path);
                if let Some(og) = original_glob {
                    let original_glob = globset::Glob::new(og).unwrap().compile_matcher();
                    rule.r#match = original_glob;
                }
            }
        }
        hm
    }
}

impl AssetConfigTreeNode {
    /// Constructs config tree for assets directory in a recursive fashion.
    fn load(
        parent: Option<ConfigNode>,
        dir: &Path,
        configs: &mut ConfigMap,
    ) -> Result<(), AssetLoadConfigError> {
        let config_path = match (
            dir.join(ASSETS_CONFIG_FILENAME_JSON).exists(),
            dir.join(ASSETS_CONFIG_FILENAME_JSON5).exists(),
        ) {
            (true, true) => {
                return Err(AssetLoadConfigError::MultipleConfigurationFiles(
                    dir.to_path_buf(),
                ));
            }
            (true, false) => Some(dir.join(ASSETS_CONFIG_FILENAME_JSON)),
            (false, true) => Some(dir.join(ASSETS_CONFIG_FILENAME_JSON5)),
            (false, false) => None,
        };
        let mut rules = vec![];
        if let Some(config_path) = config_path {
            let content = dfx_core::fs::read_to_string(&config_path)?;

            let interim_rules: Vec<rule_utils::InterimAssetConfigRule> = json5::from_str(&content)
                .map_err(|e| MalformedAssetConfigFile(config_path.to_path_buf(), e))?;
            for interim_rule in interim_rules {
                let rule = AssetConfigRule::from_interim(interim_rule, dir)
                    .map_err(|e| LoadRuleFailed(config_path.to_path_buf(), e))?;
                rules.push(rule);
            }
        }

        let parent_ref = match parent {
            Some(p) if rules.is_empty() => p,
            _ => Arc::new(Mutex::new(Self {
                parent,
                rules,
                origin: dir.to_path_buf(),
            })),
        };

        configs.insert(dir.to_path_buf(), parent_ref.clone());
        for f in dfx_core::fs::read_dir(dir)?
            .filter_map(|x| x.ok())
            .filter(|x| x.file_type().map_or_else(|_e| false, |ft| ft.is_dir()))
        {
            Self::load(Some(parent_ref.clone()), &f.path(), configs)?;
        }
        Ok(())
    }

    /// Fetches asset config in a recursive fashion.
    /// Marks config rules as *used*, whenever the rule' glob patter matched queried file.
    fn get_config(&mut self, canonical_path: &Path) -> AssetConfig {
        let base_config = match &self.parent {
            Some(parent) => parent.clone().lock().unwrap().get_config(canonical_path),
            None => AssetConfig::default(),
        };
        self.rules
            .iter_mut()
            .filter(|rule| rule.applies(canonical_path))
            .fold(base_config, |acc, x| {
                x.used = true;
                acc.merge(x)
            })
    }
}

impl AssetConfig {
    fn merge(mut self, other: &AssetConfigRule) -> Self {
        if let Some(c) = &other.cache {
            self.cache = Some(c.to_owned());
        };
        match (self.headers.as_mut(), &other.headers) {
            (Some(sh), Maybe::Value(oh)) => sh.extend(oh.to_owned()),
            (None, Maybe::Value(oh)) => self.headers = Some(oh.to_owned()),
            (_, Maybe::Null) => self.headers = None,
            (_, Maybe::Absent) => (),
        };

        if other.ignore.is_some() {
            self.ignore = other.ignore;
        }

        if other.enable_aliasing.is_some() {
            self.enable_aliasing = other.enable_aliasing;
        }

        if other.allow_raw_access.is_some() {
            self.allow_raw_access = other.allow_raw_access;
        }

        if other.encodings.is_some() {
            self.encodings = other.encodings.clone();
        }

        if other.security_policy.is_some() {
            self.security_policy = other.security_policy;
        }

        if other.disable_security_policy_warning.is_some() {
            self.disable_security_policy_warning = other.disable_security_policy_warning;
        }
        self
    }
}

/// This module contains various utilities needed for serialization/deserialization
/// and pretty-printing of the `AssetConfigRule` data structure.
mod rule_utils {
    use super::{AssetConfig, AssetConfigRule, CacheConfig, HeadersConfig, Maybe, SecurityPolicy};
    use crate::asset::content_encoder::ContentEncoder;
    use crate::error::LoadRuleError;
    use globset::{Glob, GlobMatcher};
    use itertools::Itertools;
    use serde::{Deserialize, Serializer};
    use serde_json::Value;
    use std::collections::BTreeMap;
    use std::fmt;
    use std::path::Path;

    pub(super) fn glob_cmp(a: &GlobMatcher, b: &GlobMatcher) -> bool {
        a.glob() == b.glob()
    }

    pub(super) fn glob_fmt(
        field: &GlobMatcher,
        formatter: &mut fmt::Formatter,
    ) -> Result<(), fmt::Error> {
        formatter.write_str(field.glob().glob())?;
        Ok(())
    }

    pub(super) fn glob_serialize<S>(bytes: &GlobMatcher, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_str(&bytes.glob().to_string())
    }

    pub(super) fn headers_serialize<S>(
        headers: &super::Maybe<HeadersConfig>,
        serializer: S,
    ) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        use serde::ser::SerializeMap;
        match headers {
            super::Maybe::Null => serializer.serialize_map(Some(0))?.end(),
            super::Maybe::Value(hm) => {
                let mut map = serializer.serialize_map(Some(hm.len()))?;
                for (k, v) in hm {
                    map.serialize_entry(k, &v)?;
                }
                map.end()
            }
            super::Maybe::Absent => unreachable!(), // this option is already skipped via `skip_serialization_with`
        }
    }

    fn headers_deserialize<'de, D>(deserializer: D) -> Result<Maybe<HeadersConfig>, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        match serde_json::value::Value::deserialize(deserializer)? {
            Value::Object(v) => Ok(Maybe::Value(
                v.into_iter()
                    .map(|(k, v)| (k, v.to_string().trim_matches('"').to_string()))
                    .collect::<BTreeMap<String, String>>(),
            )),
            Value::Null => Ok(Maybe::Null),
            _ => Err(serde::de::Error::custom(
                "wrong data format for field `headers` (only map or null are allowed)",
            )),
        }
    }

    impl<T> Maybe<T> {
        pub(super) fn is_absent(&self) -> bool {
            matches!(*self, Self::Absent)
        }
    }

    #[derive(Deserialize)]
    #[serde(deny_unknown_fields)]
    pub(super) struct InterimAssetConfigRule {
        r#match: String,
        cache: Option<CacheConfig>,
        #[serde(default, deserialize_with = "headers_deserialize")]
        headers: Maybe<HeadersConfig>,
        ignore: Option<bool>,
        enable_aliasing: Option<bool>,
        allow_raw_access: Option<bool>,
        encodings: Option<Vec<ContentEncoder>>,
        security_policy: Option<SecurityPolicy>,
        disable_security_policy_warning: Option<bool>,
    }

    impl AssetConfigRule {
        pub(super) fn from_interim(
            InterimAssetConfigRule {
                r#match,
                cache,
                headers,
                ignore,
                enable_aliasing,
                allow_raw_access,
                encodings,
                security_policy,
                disable_security_policy_warning,
            }: InterimAssetConfigRule,
            config_file_parent_dir: &Path,
        ) -> Result<Self, LoadRuleError> {
            let glob = config_file_parent_dir.join(&r#match);
            let glob = glob.to_str().ok_or_else(|| {
                LoadRuleError::FormGlobPatternFailed(
                    config_file_parent_dir.to_path_buf(),
                    r#match.clone(),
                )
            })?;
            let matcher = Glob::new(glob)
                .map_err(|e| LoadRuleError::InvalidGlobPattern(r#match, e))?
                .compile_matcher();

            Ok(Self {
                r#match: matcher,
                cache,
                headers,
                ignore,
                used: false,
                enable_aliasing,
                allow_raw_access,
                encodings,
                security_policy,
                disable_security_policy_warning,
            })
        }
    }

    impl std::fmt::Display for AssetConfig {
        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
            let mut s = String::new();

            if self.cache.is_some() || self.headers.is_some() {
                s.push('(');
                if self.cache.as_ref().map_or(false, |v| v.max_age.is_some()) {
                    s.push_str("with cache");
                }
                if let Some(ref headers) = self.headers {
                    if !headers.is_empty() {
                        if s.len() > 1 {
                            s.push_str(" and ");
                        } else {
                            s.push_str("with ");
                        }
                        s.push_str(headers.len().to_string().as_str());
                        if headers.len() == 1 {
                            s.push_str(" header");
                        } else {
                            s.push_str(" headers");
                        }
                    }
                }
                if let Some(encodings) = self.encodings.as_ref() {
                    s.push_str(&format!(", {} encodings", encodings.len()));
                }
                if let Some(policy) = self.security_policy {
                    s.push_str(&format!(" and security policy '{policy}'"));
                }
                s.push(')');
            }

            write!(f, "{}", s)
        }
    }

    impl std::fmt::Debug for AssetConfig {
        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
            let mut s = String::new();

            if let Some(ref cache) = self.cache {
                if let Some(ref max_age) = cache.max_age {
                    s.push_str(&format!("  - HTTP cache max-age: {}\n", max_age));
                }
            }
            if let Some(allow_raw_access) = self.allow_raw_access {
                s.push_str(&format!(
                    "  - enable raw access: {}\n",
                    if allow_raw_access {
                        "enabled"
                    } else {
                        "disabled"
                    }
                ));
            }
            if let Some(policy) = self.security_policy {
                s.push_str(&format!("  - Security policy: {policy}"));
            }
            if let Some(aliasing) = self.enable_aliasing {
                s.push_str(&format!(
                    "  - URL path aliasing: {}\n",
                    if aliasing { "enabled" } else { "disabled" }
                ));
            }
            if let Some(ref headers) = self.headers {
                for (key, value) in headers {
                    s.push_str(&format!(
                        "  - HTTP Response header: {key}: {value}\n",
                        key = key,
                        value = value
                    ));
                }
            }
            if let Some(encodings) = self.encodings.as_ref() {
                s.push_str(&format!(
                    "  - encodings: {}",
                    encodings.iter().map(|enc| enc.to_string()).join(",")
                ));
            }
            if let Some(disable_warning) = self.disable_security_policy_warning {
                s.push_str(&format!(
                    "  - disable standard security policy warning: {disable_warning}"
                ));
            }
            write!(f, "{}", s)
        }
    }
}

#[cfg(test)]
mod with_tempdir {

    use super::*;
    #[cfg(target_family = "unix")]
    use std::error::Error;
    use std::io::Write;
    #[cfg(target_family = "unix")]
    use std::os::unix::prelude::PermissionsExt;
    use std::{collections::BTreeMap, fs::File};
    use tempfile::{Builder, TempDir};

    fn create_temporary_assets_directory(
        config_files: Option<HashMap<String, String>>,
        assets_count: usize,
    ) -> TempDir {
        let assets_dir = Builder::new()
            .prefix("assets")
            .rand_bytes(5)
            .tempdir()
            .unwrap();

        let _subdirs = ["css", "js", "nested/deep"]
            .map(|d| assets_dir.as_ref().join(d))
            .map(std::fs::create_dir_all);

        [
            "index.html",
            "js/index.js",
            "js/index.map.js",
            "css/main.css",
            "css/stylish.css",
            "nested/the-thing.txt",
            "nested/deep/the-next-thing.toml",
        ]
        .iter()
        .map(|path| assets_dir.path().join(path))
        .take(assets_count)
        .for_each(|path| {
            File::create(path).unwrap();
        });

        let new_empty_config = |directory: &str| (directory.to_string(), "[]".to_string());
        let mut h = HashMap::from([
            new_empty_config(""),
            new_empty_config("css"),
            new_empty_config("js"),
            new_empty_config("nested"),
            new_empty_config("nested/deep"),
        ]);
        if let Some(cf) = config_files {
            h.extend(cf);
        }
        h.into_iter().for_each(|(dir, content)| {
            let path = assets_dir
                .path()
                .join(dir)
                .join(ASSETS_CONFIG_FILENAME_JSON);
            let mut file = File::create(path).unwrap();
            write!(file, "{}", content).unwrap();
        });

        assets_dir
    }

    #[test]
    fn match_only_nested_files() {
        let cfg = HashMap::from([(
            "nested".to_string(),
            r#"[{"match": "*", "cache": {"max_age": 333}}]"#.to_string(),
        )]);
        let assets_temp_dir = create_temporary_assets_directory(Some(cfg), 7);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();

        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        for f in ["nested/the-thing.txt", "nested/deep/the-next-thing.toml"] {
            assert_eq!(
                assets_config
                    .get_asset_config(assets_dir.join(f).as_path())
                    .unwrap(),
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(333) }),
                    ..Default::default()
                }
            );
        }
        for f in [
            "index.html",
            "js/index.js",
            "js/index.map.js",
            "css/main.css",
            "css/stylish.css",
        ] {
            assert_eq!(
                assets_config
                    .get_asset_config(assets_dir.join(f).as_path())
                    .unwrap(),
                AssetConfig::default()
            );
        }
    }

    #[test]
    fn overriding_cache_rules() {
        let cfg = Some(HashMap::from([
            (
                "nested".to_string(),
                r#"[{"match": "*", "cache": {"max_age": 111}}]"#.to_string(),
            ),
            (
                "".to_string(),
                r#"[{"match": "*", "cache": {"max_age": 333}}]"#.to_string(),
            ),
        ]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 7);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();

        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        for f in ["nested/the-thing.txt", "nested/deep/the-next-thing.toml"] {
            assert_eq!(
                assets_config
                    .get_asset_config(assets_dir.join(f).as_path())
                    .unwrap(),
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(111) }),
                    ..Default::default()
                }
            );
        }
        for f in [
            "index.html",
            "js/index.js",
            "js/index.map.js",
            "css/main.css",
            "css/stylish.css",
        ] {
            assert_eq!(
                assets_config
                    .get_asset_config(assets_dir.join(f).as_path())
                    .unwrap(),
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(333) }),
                    ..Default::default()
                }
            );
        }
    }

    #[test]
    fn overriding_headers() {
        use serde_json::Value::*;
        let cfg = Some(HashMap::from([(
            "".to_string(),
            r#"
    [
      {
        "match": "index.html",
        "cache": {
          "max_age": 22
        },
        "headers": {
          "Content-Security-Policy": "add",
          "x-frame-options": "NONE",
          "x-content-type-options": "nosniff"
        }
      },
      {
        "match": "*",
        "headers": {
          "Content-Security-Policy": "delete"
        }
      },
      {
        "match": "*",
        "headers": {
          "Some-Other-Policy": "add"
        }
      },
      {
        "match": "*",
        "cache": {
          "max_age": 88
        },
        "headers": {
          "x-xss-protection": 1,
          "x-frame-options": "SAMEORIGIN"
        }
      }
    ]
    "#
            .to_string(),
        )]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 1);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        let parsed_asset_config = assets_config
            .get_asset_config(assets_dir.join("index.html").as_path())
            .unwrap();
        let expected_asset_config = AssetConfig {
            cache: Some(CacheConfig { max_age: Some(88) }),
            headers: Some(BTreeMap::from([
                ("x-content-type-options".to_string(), "nosniff".to_string()),
                ("x-frame-options".to_string(), "SAMEORIGIN".to_string()),
                ("Some-Other-Policy".to_string(), "add".to_string()),
                ("Content-Security-Policy".to_string(), "delete".to_string()),
                (
                    "x-xss-protection".to_string(),
                    Number(serde_json::Number::from(1)).to_string(),
                ),
            ])),
            ..Default::default()
        };

        assert_eq!(parsed_asset_config.cache, expected_asset_config.cache);
        assert_eq!(
            parsed_asset_config
                .headers
                .unwrap()
                .iter()
                // keys are sorted
                .collect::<BTreeMap<_, _>>(),
            expected_asset_config
                .headers
                .unwrap()
                .iter()
                .collect::<BTreeMap<_, _>>(),
        );
    }

    #[test]
    fn overriding_encodings() {
        let cfg = Some(HashMap::from([
            (
                "".to_string(),
                r#"[{"match": "**/*.txt", "encodings": []},{"match": "**/*.unknown", "encodings": ["gzip"]}]"#.to_string(),
            ),
        ]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 7);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();

        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        // override default (.unknown defaults to empty list)
        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("file.unknown").as_path())
                .unwrap(),
            AssetConfig {
                encodings: Some(Vec::from([ContentEncoder::Gzip])),
                ..Default::default()
            }
        );
        // override default with empty list (.txt defaults to gzip)
        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("text.txt").as_path())
                .unwrap(),
            AssetConfig {
                encodings: Some(Vec::from([])),
                ..Default::default()
            }
        );
    }

    #[test]
    fn prioritization() {
        // 1. the most deeply nested config file takes precedens over the one in parent dir
        // 2. order of rules withing file matters - last rule in config file takes precedens over the first one
        let cfg = Some(HashMap::from([
            (
                "".to_string(),
                r#"[
        {"match": "**/*", "cache": {"max_age": 999}},
        {"match": "nested/**/*", "cache": {"max_age": 900}},
        {"match": "nested/deep/*", "cache": {"max_age": 800}},
        {"match": "nested/**/*.toml","cache": {"max_age": 700}}
    ]"#
                .to_string(),
            ),
            (
                "nested".to_string(),
                r#"[
        {"match": "the-thing.txt", "cache": {"max_age": 600}},
        {"match": "*.txt", "cache": {"max_age": 500}},
        {"match": "*", "cache": {"max_age": 400}}
    ]"#
                .to_string(),
            ),
            (
                "nested/deep".to_string(),
                r#"[
        {"match": "**/*", "cache": {"max_age": 300}},
        {"match": "*", "cache": {"max_age": 200}},
        {"match": "*.toml", "cache": {"max_age": 100}}
    ]"#
                .to_string(),
            ),
        ]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 7);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();

        let mut assets_config = dbg!(AssetSourceDirectoryConfiguration::load(&assets_dir)).unwrap();
        for f in [
            "index.html",
            "js/index.js",
            "js/index.map.js",
            "css/main.css",
            "css/stylish.css",
        ] {
            assert_eq!(
                assets_config
                    .get_asset_config(assets_dir.join(f).as_path())
                    .unwrap(),
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(999) }),
                    ..Default::default()
                }
            );
        }

        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("nested/the-thing.txt").as_path())
                .unwrap(),
            AssetConfig {
                cache: Some(CacheConfig { max_age: Some(400) }),
                ..Default::default()
            },
        );
        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("nested/deep/the-next-thing.toml").as_path())
                .unwrap(),
            AssetConfig {
                cache: Some(CacheConfig { max_age: Some(100) }),
                ..Default::default()
            },
        );
    }

    #[test]
    fn json5_config_file_with_comments() {
        let cfg = Some(HashMap::from([(
            "".to_string(),
            r#"[
// comment
  {
    "match": "*",
    /*
    look at this beatiful key below, not wrapped in quotes
*/  cache: { max_age: 999 } }
]"#
            .to_string(),
        )]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("index.html").as_path())
                .unwrap(),
            AssetConfig {
                cache: Some(CacheConfig { max_age: Some(999) }),
                ..Default::default()
            },
        );
    }

    #[test]
    fn no_content_config_file() {
        let cfg = Some(HashMap::from([
            ("".to_string(), "".to_string()),
            ("css".to_string(), "".to_string()),
            ("js".to_string(), "".to_string()),
            ("nested".to_string(), "".to_string()),
            ("nested/deep".to_string(), "".to_string()),
        ]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir);
        assert_eq!(
            assets_config.err().unwrap().to_string(),
            format!(
                "Malformed JSON asset config file '{}':  {}",
                assets_dir
                    .join(ASSETS_CONFIG_FILENAME_JSON)
                    .to_str()
                    .unwrap(),
                "--> 1:1\n  |\n1 | \n  | ^---\n  |\n  = expected array, boolean, null, number, object, or string"
            )
        );
    }

    #[test]
    fn invalid_json_config_file() {
        let cfg = Some(HashMap::from([("".to_string(), "[[[{{{".to_string())]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir);
        assert_eq!(
            assets_config.err().unwrap().to_string(),
            format!(
                "Malformed JSON asset config file '{}':  {}",
                assets_dir
                    .join(ASSETS_CONFIG_FILENAME_JSON)
                    .to_str()
                    .unwrap(),
                "--> 1:5\n  |\n1 | [[[{{{\n  |     ^---\n  |\n  = expected identifier or string"
            )
        );
    }

    #[test]
    fn invalid_glob_pattern() {
        let cfg = Some(HashMap::from([(
            "".to_string(),
            r#"[
        {"match": "{{{\\\", "cache": {"max_age": 900}},
    ]"#
            .to_string(),
        )]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir);
        assert_eq!(
            assets_config.err().unwrap().to_string(),
            format!(
                "Malformed JSON asset config file '{}':  {}",
                assets_dir
                    .join(ASSETS_CONFIG_FILENAME_JSON)
                    .to_str()
                    .unwrap(),
                "--> 2:19\n  |\n2 |         {\"match\": \"{{{\\\\\\\", \"cache\": {\"max_age\": 900}},\n  |                   ^---\n  |\n  = expected boolean or null"
            )
        );
    }

    #[test]
    fn invalid_asset_path() {
        let cfg = Some(HashMap::new());
        let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("doesnt.exists").as_path())
                .unwrap(),
            AssetConfig::default()
        );
    }

    #[cfg(target_family = "unix")]
    #[test]
    fn no_read_permission() {
        let cfg = Some(HashMap::from([(
            "".to_string(),
            r#"[
        {"match": "*", "cache": {"max_age": 20}}
    ]"#
            .to_string(),
        )]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 1);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        std::fs::set_permissions(
            assets_dir.join(ASSETS_CONFIG_FILENAME_JSON).as_path(),
            std::fs::Permissions::from_mode(0o000),
        )
        .unwrap();

        let assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir);
        assert_eq!(
            assets_config.as_ref().err().unwrap().to_string(),
            format!(
                "failed to read {} as string",
                assets_dir
                    .join(ASSETS_CONFIG_FILENAME_JSON)
                    .as_path()
                    .to_str()
                    .unwrap()
            )
        );
        assert_eq!(
            assets_config.err().unwrap().source().unwrap().to_string(),
            "Permission denied (os error 13)"
        );
    }

    #[test]
    fn allow_raw_access_flag() {
        let cfg = Some(HashMap::from([(
            "".to_string(),
            r#"[
  {
    "match": "*",
    "allow_raw_access": true
  }
]"#
            .to_string(),
        )]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("index.html").as_path())
                .unwrap(),
            AssetConfig {
                allow_raw_access: Some(true),
                ..Default::default()
            },
        );
    }

    #[test]
    fn default_value_for_allow_raw_access_flag() {
        let cfg = Some(HashMap::from([("".to_string(), "[]".to_string())]));
        let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
        assert_eq!(
            assets_config
                .get_asset_config(assets_dir.join("index.html").as_path())
                .unwrap(),
            AssetConfig {
                allow_raw_access: Some(true),
                ..Default::default()
            },
        );
    }

    #[test]
    fn the_order_does_not_matter() {
        let cfg = Some(HashMap::from([(
            "".to_string(),
            r#"[
                {
                    "match": "**/deep/**/*",
                    "allow_raw_access": false,
                    "cache": {
                      "max_age": 22
                    },
                    "enable_aliasing": true,
                    "ignore": true
                },
                {
                    "match": "**/*",
                    "headers": {
                        "X-Frame-Options": "DENY"
                    }
                }
            ]
"#
            .to_string(),
        )]));
        let cfg2 = Some(HashMap::from([(
            "".to_string(),
            r#"[
                {
                    "match": "**/*",
                    "headers": {
                        "X-Frame-Options": "DENY"
                    }
                },
                {
                    "match": "**/deep/**/*",
                    "allow_raw_access": false,
                    "cache": {
                      "max_age": 22
                    },
                    "enable_aliasing": true,
                    "ignore": true
                }
            ]
            "#
            .to_string(),
        )]));

        let x = {
            let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
            let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
            let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
            assets_config
                .get_asset_config(assets_dir.join("nested/deep/the-next-thing.toml").as_path())
                .unwrap()
        };
        let y = {
            let assets_temp_dir = create_temporary_assets_directory(cfg2, 0);
            let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
            let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
            assets_config
                .get_asset_config(assets_dir.join("nested/deep/the-next-thing.toml").as_path())
                .unwrap()
        };

        dbg!(&x, &y);
        assert_eq!(x.allow_raw_access, Some(false));
        assert_eq!(y.allow_raw_access, Some(false));
        assert_eq!(x.enable_aliasing, Some(true));
        assert_eq!(y.enable_aliasing, Some(true));
        assert_eq!(x.ignore, Some(true));
        assert_eq!(y.ignore, Some(true));
        assert_eq!(x.cache.clone().unwrap().max_age, Some(22));
        assert_eq!(y.cache.clone().unwrap().max_age, Some(22));

        // same as above but with different values
        let cfg = Some(HashMap::from([(
            "".to_string(),
            r#"[
                {
                    "match": "**/deep/**/*",
                    "allow_raw_access": true,
                    "enable_aliasing": false,
                    "ignore": false,
                    "headers": {
                        "X-Frame-Options": "ALLOW"
                    }
                },
                {
                    "match": "**/*",
                    "cache": {
                      "max_age": 22
                    }
                }
            ]
"#
            .to_string(),
        )]));
        let cfg2 = Some(HashMap::from([(
            "".to_string(),
            r#"[
                {
                    "match": "**/*",
                    "cache": {
                      "max_age": 22
                    }
                },
                {
                    "match": "**/deep/**/*",
                    "allow_raw_access": true,
                    "enable_aliasing": false,
                    "ignore": false,
                    "headers": {
                        "X-Frame-Options": "ALLOW"
                    }
                }
            ]
            "#
            .to_string(),
        )]));

        let x = {
            let assets_temp_dir = create_temporary_assets_directory(cfg, 0);
            let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
            let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
            assets_config
                .get_asset_config(assets_dir.join("nested/deep/the-next-thing.toml").as_path())
                .unwrap()
        };
        let y = {
            let assets_temp_dir = create_temporary_assets_directory(cfg2, 0);
            let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
            let mut assets_config = AssetSourceDirectoryConfiguration::load(&assets_dir).unwrap();
            assets_config
                .get_asset_config(assets_dir.join("nested/deep/the-next-thing.toml").as_path())
                .unwrap()
        };

        dbg!(&x, &y);
        assert_eq!(x.allow_raw_access, Some(true));
        assert_eq!(y.allow_raw_access, Some(true));
        assert_eq!(x.enable_aliasing, Some(false));
        assert_eq!(y.enable_aliasing, Some(false));
        assert_eq!(x.ignore, Some(false));
        assert_eq!(y.ignore, Some(false));
        assert_eq!(x.cache.clone().unwrap().max_age, Some(22));
        assert_eq!(y.cache.clone().unwrap().max_age, Some(22));
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/asset/content.rs:
-----------------------

use crate::asset::content_encoder::ContentEncoder;
use brotli::CompressorWriter;
use dfx_core::error::fs::ReadFileError;
use flate2::write::GzEncoder;
use flate2::Compression;
use mime::Mime;
use sha2::{Digest, Sha256};
use std::io::Write;
use std::path::Path;

#[derive(Clone)]
pub(crate) struct Content {
    pub data: Vec<u8>,
    pub media_type: Mime,
}

impl Content {
    pub fn load(path: &Path) -> Result<Content, ReadFileError> {
        let data = dfx_core::fs::read(path)?;

        // todo: check contents if mime_guess fails https://github.com/dfinity/sdk/issues/1594
        let media_type = mime_guess::from_path(path)
            .first()
            .unwrap_or(mime::APPLICATION_OCTET_STREAM);

        Ok(Content { data, media_type })
    }

    pub fn encode(&self, encoder: &ContentEncoder) -> Result<Content, std::io::Error> {
        match encoder {
            ContentEncoder::Gzip => self.to_gzip(),
            ContentEncoder::Brotli => self.to_brotli(),
            ContentEncoder::Identity => Ok(self.clone()),
        }
    }

    pub fn to_gzip(&self) -> Result<Content, std::io::Error> {
        let mut e = GzEncoder::new(Vec::new(), Compression::default());
        e.write_all(&self.data)?;
        let data = e.finish()?;
        Ok(Content {
            data,
            media_type: self.media_type.clone(),
        })
    }

    pub fn to_brotli(&self) -> Result<Content, std::io::Error> {
        let mut compressed_data = Vec::new();
        {
            let mut compressor = CompressorWriter::new(&mut compressed_data, 4096, 11, 22);
            compressor.write_all(&self.data)?;
            compressor.flush()?;
        }
        Ok(Content {
            data: compressed_data,
            media_type: self.media_type.clone(),
        })
    }

    pub fn sha256(&self) -> Vec<u8> {
        Sha256::digest(&self.data).to_vec()
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/asset/content_encoder.rs:
-----------------------

use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, Copy, Serialize, Deserialize, Eq, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum ContentEncoder {
    Gzip,
    #[serde(alias = "br")]
    Brotli,
    Identity,
}

impl std::fmt::Display for ContentEncoder {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match &self {
            ContentEncoder::Gzip => f.write_str("gzip"),
            ContentEncoder::Brotli => f.write_str("br"),
            ContentEncoder::Identity => f.write_str("identity"),
        }
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/asset/mod.rs:
-----------------------

pub(crate) mod config;
pub(crate) mod content;
pub(crate) mod content_encoder;


-----------------------

/src/canisters/frontend/ic-asset/src/batch_upload/mod.rs:
-----------------------

pub(crate) mod operations;
pub(crate) mod plumbing;
pub(crate) mod retryable;
pub(crate) mod semaphores;


-----------------------

/src/canisters/frontend/ic-asset/src/batch_upload/operations.rs:
-----------------------

use crate::batch_upload::plumbing::ProjectAsset;
use crate::canister_api::types::asset::{
    AssetDetails, AssetProperties, SetAssetPropertiesArguments,
};
use crate::canister_api::types::batch_upload::common::{
    CreateAssetArguments, DeleteAssetArguments, SetAssetContentArguments,
    UnsetAssetContentArguments,
};
use crate::canister_api::types::batch_upload::v1::{BatchOperationKind, CommitBatchArguments};
use candid::Nat;
use std::collections::HashMap;

pub(crate) const BATCH_UPLOAD_API_VERSION: u16 = 1;

pub(crate) fn assemble_batch_operations(
    project_assets: &HashMap<String, ProjectAsset>,
    canister_assets: HashMap<String, AssetDetails>,
    asset_deletion_reason: AssetDeletionReason,
    canister_asset_properties: HashMap<String, AssetProperties>,
) -> Vec<BatchOperationKind> {
    let mut canister_assets = canister_assets;

    let mut operations = vec![];

    delete_assets(
        &mut operations,
        project_assets,
        &mut canister_assets,
        asset_deletion_reason,
    );
    create_new_assets(&mut operations, project_assets, &canister_assets);
    unset_obsolete_encodings(&mut operations, project_assets, &canister_assets);
    set_encodings(&mut operations, project_assets);
    update_properties(&mut operations, project_assets, &canister_asset_properties);

    operations
}

pub(crate) fn assemble_commit_batch_arguments(
    project_assets: HashMap<String, ProjectAsset>,
    canister_assets: HashMap<String, AssetDetails>,
    asset_deletion_reason: AssetDeletionReason,
    canister_asset_properties: HashMap<String, AssetProperties>,
    batch_id: Nat,
) -> CommitBatchArguments {
    let operations = assemble_batch_operations(
        &project_assets,
        canister_assets,
        asset_deletion_reason,
        canister_asset_properties,
    );
    CommitBatchArguments {
        operations,
        batch_id,
    }
}

pub(crate) enum AssetDeletionReason {
    Obsolete,
    Incompatible,
}

pub(crate) fn delete_assets(
    operations: &mut Vec<BatchOperationKind>,
    project_assets: &HashMap<String, ProjectAsset>,
    canister_assets: &mut HashMap<String, AssetDetails>,
    reason: AssetDeletionReason,
) {
    let mut deleted_canister_assets = vec![];
    for (key, canister_asset) in canister_assets.iter() {
        let project_asset = project_assets.get(key);
        match reason {
            AssetDeletionReason::Obsolete => {
                if project_asset
                    .filter(|&x| x.media_type.to_string() == canister_asset.content_type)
                    .is_none()
                {
                    operations.push(BatchOperationKind::DeleteAsset(DeleteAssetArguments {
                        key: key.clone(),
                    }));
                    deleted_canister_assets.push(key.clone());
                }
            }
            AssetDeletionReason::Incompatible => {
                if let Some(project_asset) = project_assets.get(key) {
                    if project_asset.media_type.to_string() != canister_asset.content_type {
                        operations.push(BatchOperationKind::DeleteAsset(DeleteAssetArguments {
                            key: key.clone(),
                        }));
                        deleted_canister_assets.push(key.clone());
                    }
                }
            }
        }
    }
    for k in deleted_canister_assets {
        canister_assets.remove(&k);
    }
}

pub(crate) fn create_new_assets(
    operations: &mut Vec<BatchOperationKind>,
    project_assets: &HashMap<String, ProjectAsset>,
    canister_assets: &HashMap<String, AssetDetails>,
) {
    for (key, project_asset) in project_assets {
        if !canister_assets.contains_key(key) {
            let max_age = project_asset
                .asset_descriptor
                .config
                .cache
                .as_ref()
                .and_then(|c| c.max_age);

            let headers = project_asset.asset_descriptor.config.combined_headers();
            let enable_aliasing = project_asset.asset_descriptor.config.enable_aliasing;
            let allow_raw_access = project_asset.asset_descriptor.config.allow_raw_access;

            operations.push(BatchOperationKind::CreateAsset(CreateAssetArguments {
                key: key.clone(),
                content_type: project_asset.media_type.to_string(),
                max_age,
                headers,
                enable_aliasing,
                allow_raw_access,
            }));
        }
    }
}

pub(crate) fn unset_obsolete_encodings(
    operations: &mut Vec<BatchOperationKind>,
    project_assets: &HashMap<String, ProjectAsset>,
    canister_assets: &HashMap<String, AssetDetails>,
) {
    for (key, details) in canister_assets {
        // delete_obsolete_assets handles the case where key is not found in project_assets
        if let Some(project_asset) = project_assets.get(key) {
            for encoding_details in &details.encodings {
                let project_contains_encoding = project_asset
                    .encodings
                    .contains_key(&encoding_details.content_encoding);
                if !project_contains_encoding {
                    operations.push(BatchOperationKind::UnsetAssetContent(
                        UnsetAssetContentArguments {
                            key: key.clone(),
                            content_encoding: encoding_details.content_encoding.clone(),
                        },
                    ));
                }
            }
        }
    }
}

pub(crate) fn set_encodings(
    operations: &mut Vec<BatchOperationKind>,
    project_assets: &HashMap<String, ProjectAsset>,
) {
    for (key, project_asset) in project_assets {
        for (content_encoding, v) in &project_asset.encodings {
            if v.already_in_place {
                continue;
            }

            operations.push(BatchOperationKind::SetAssetContent(
                SetAssetContentArguments {
                    key: key.clone(),
                    content_encoding: content_encoding.clone(),
                    chunk_ids: v.chunk_ids.clone(),
                    sha256: Some(v.sha256.clone()),
                },
            ));
        }
    }
}

pub(crate) fn update_properties(
    operations: &mut Vec<BatchOperationKind>,
    project_assets: &HashMap<String, ProjectAsset>,
    canister_asset_properties: &HashMap<String, AssetProperties>,
) {
    for (key, project_asset) in project_assets {
        let project_asset_properties = project_asset.asset_descriptor.config.clone();
        // skip if the asset is not already in the canister, because
        // properties are going to be created during create_new_assets call
        if let Some(canister_asset_properties) = canister_asset_properties.get(key) {
            let set_asset_props = SetAssetPropertiesArguments {
                key: key.clone(),
                max_age: {
                    let project_asset_max_age = project_asset_properties
                        .cache
                        .as_ref()
                        .and_then(|v| v.max_age);
                    if project_asset_max_age != canister_asset_properties.max_age {
                        Some(project_asset_max_age)
                    } else {
                        None
                    }
                },
                headers: {
                    let project_asset_headers =
                        project_asset_properties.combined_headers().map(|hm| {
                            let mut vec = Vec::from_iter(hm.into_iter());
                            vec.sort();
                            vec
                        });
                    let canister_asset_headers =
                        canister_asset_properties.headers.as_ref().map(|hm| {
                            // collect into a vec and sort it
                            let mut vec = Vec::from_iter(hm.clone().into_iter());
                            vec.sort();
                            vec
                        });
                    if project_asset_headers != canister_asset_headers {
                        Some(project_asset_headers)
                    } else {
                        None
                    }
                },
                is_aliased: {
                    if project_asset_properties.enable_aliasing
                        != canister_asset_properties.is_aliased
                    {
                        Some(project_asset_properties.enable_aliasing)
                    } else {
                        None
                    }
                },
                allow_raw_access: {
                    if project_asset_properties.allow_raw_access
                        != canister_asset_properties.allow_raw_access
                    {
                        Some(project_asset_properties.allow_raw_access)
                    } else {
                        None
                    }
                },
            };
            // check if the properties are the same and skip if they are to save saves cycles
            if set_asset_props.allow_raw_access.is_some()
                || set_asset_props.max_age.is_some()
                || set_asset_props.headers.is_some()
                || set_asset_props.is_aliased.is_some()
            {
                operations.push(BatchOperationKind::SetAssetProperties(set_asset_props));
            }
        }
    }
}

#[cfg(test)]
mod test_update_properties {
    use super::update_properties;
    use crate::asset::config::{AssetConfig, CacheConfig};
    use crate::batch_upload::plumbing::{AssetDescriptor, ProjectAsset};
    use crate::canister_api::types::asset::{AssetProperties, SetAssetPropertiesArguments};
    use crate::canister_api::types::batch_upload::v1::BatchOperationKind;
    use std::collections::{BTreeMap, HashMap};
    use std::path::PathBuf;

    fn dummy_project_asset(key: &str, asset_props: AssetConfig) -> ProjectAsset {
        ProjectAsset {
            media_type: mime::TEXT_PLAIN,
            encodings: HashMap::new(),
            asset_descriptor: AssetDescriptor {
                key: key.to_string(),
                source: PathBuf::from(""),
                config: asset_props,
            },
        }
    }

    #[test]
    fn basic_test() {
        let mut project_assets = HashMap::new();
        let mut canister_asset_properties = HashMap::new();
        project_assets.insert(
            "key1".to_string(),
            dummy_project_asset(
                "key1",
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(100) }),
                    headers: Some(BTreeMap::from([("key".to_string(), "value".to_string())])),
                    enable_aliasing: Some(false),
                    allow_raw_access: Some(false),
                    ..Default::default()
                },
            ),
        );
        project_assets.insert(
            "key2".to_string(),
            dummy_project_asset(
                "key2",
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(100) }),
                    headers: Some(BTreeMap::new()),
                    enable_aliasing: Some(true),
                    allow_raw_access: Some(true),
                    ..Default::default()
                },
            ),
        );
        canister_asset_properties.insert(
            "key1".to_string(),
            AssetProperties {
                max_age: Some(1),
                headers: Some(HashMap::new()),
                is_aliased: Some(true),
                allow_raw_access: Some(true),
            },
        );
        let mut operations = vec![];
        update_properties(&mut operations, &project_assets, &canister_asset_properties);
        assert_eq!(operations.len(), 1);
        assert_eq!(
            operations[0],
            BatchOperationKind::SetAssetProperties(SetAssetPropertiesArguments {
                key: "key1".to_string(),
                max_age: Some(Some(100)),
                headers: Some(Some(vec![("key".to_string(), "value".to_string())])),
                is_aliased: Some(Some(false)),
                allow_raw_access: Some(Some(false)),
            })
        );
    }

    #[test]
    fn update_no_properties() {
        let mut project_assets = HashMap::new();
        let mut canister_asset_properties = HashMap::new();
        project_assets.insert(
            "key1".to_string(),
            dummy_project_asset(
                "key1",
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(100) }),
                    headers: Some(BTreeMap::new()),
                    enable_aliasing: Some(true),
                    allow_raw_access: Some(true),
                    ..Default::default()
                },
            ),
        );
        project_assets.insert(
            "key2".to_string(),
            dummy_project_asset(
                "key2",
                AssetConfig {
                    cache: Some(CacheConfig { max_age: Some(100) }),
                    headers: Some(BTreeMap::new()),
                    enable_aliasing: Some(true),
                    allow_raw_access: Some(true),
                    ..Default::default()
                },
            ),
        );
        canister_asset_properties.insert(
            "key1".to_string(),
            AssetProperties {
                max_age: Some(100),
                headers: Some(HashMap::new()),
                is_aliased: Some(true),
                allow_raw_access: Some(true),
            },
        );
        canister_asset_properties.insert(
            "key3".to_string(),
            AssetProperties {
                max_age: Some(100),
                headers: Some(HashMap::new()),
                is_aliased: Some(true),
                allow_raw_access: Some(true),
            },
        );
        let mut operations = vec![];
        update_properties(&mut operations, &project_assets, &canister_asset_properties);
        assert_eq!(operations.len(), 0);
    }

    #[test]
    fn update_with_nones() {
        let mut project_assets = HashMap::new();
        let mut canister_asset_properties = HashMap::new();
        project_assets.insert(
            "key1".to_string(),
            dummy_project_asset(
                "key1",
                AssetConfig {
                    cache: None,
                    headers: None,
                    enable_aliasing: None,
                    allow_raw_access: None,
                    ..Default::default()
                },
            ),
        );
        canister_asset_properties.insert(
            "key1".to_string(),
            AssetProperties {
                max_age: Some(100),
                headers: Some(HashMap::from([("key".to_string(), "value".to_string())])),
                is_aliased: Some(true),
                allow_raw_access: Some(true),
            },
        );
        let mut operations = vec![];
        update_properties(&mut operations, &project_assets, &canister_asset_properties);
        assert_eq!(operations.len(), 1);
        assert_eq!(
            operations[0],
            BatchOperationKind::SetAssetProperties(SetAssetPropertiesArguments {
                key: "key1".to_string(),
                max_age: Some(None),
                headers: Some(None),
                is_aliased: Some(None),
                allow_raw_access: Some(None),
            })
        );
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/batch_upload/plumbing.rs:
-----------------------

use crate::asset::config::AssetConfig;
use crate::asset::content::Content;
use crate::asset::content_encoder::ContentEncoder;
use crate::batch_upload::semaphores::Semaphores;
use crate::canister_api::methods::chunk::create_chunk;
use crate::canister_api::types::asset::AssetDetails;
use crate::error::CreateChunkError;
use crate::error::CreateEncodingError;
use crate::error::CreateEncodingError::EncodeContentFailed;
use crate::error::CreateProjectAssetError;
use candid::Nat;
use futures::future::try_join_all;
use futures::TryFutureExt;
use ic_utils::Canister;
use mime::Mime;
use slog::{debug, info, Logger};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;

const CONTENT_ENCODING_IDENTITY: &str = "identity";

// The most mb any one file is considered to have for purposes of limiting data loaded at once.
// Any file counts as at least 1 mb.
const MAX_COST_SINGLE_FILE_MB: usize = 45;

const MAX_CHUNK_SIZE: usize = 1_900_000;

#[derive(Clone, Debug)]
pub(crate) struct AssetDescriptor {
    pub(crate) source: PathBuf,
    pub(crate) key: String,
    pub(crate) config: AssetConfig,
}

pub(crate) struct ProjectAssetEncoding {
    pub(crate) chunk_ids: Vec<Nat>,
    pub(crate) sha256: Vec<u8>,
    pub(crate) already_in_place: bool,
}

pub(crate) struct ProjectAsset {
    pub(crate) asset_descriptor: AssetDescriptor,
    pub(crate) media_type: Mime,
    pub(crate) encodings: HashMap<String, ProjectAssetEncoding>,
}

pub(crate) struct ChunkUploader<'agent> {
    canister: Canister<'agent>,
    batch_id: Nat,
    chunks: Arc<AtomicUsize>,
    bytes: Arc<AtomicUsize>,
}
impl<'agent> ChunkUploader<'agent> {
    pub(crate) fn new(canister: Canister<'agent>, batch_id: Nat) -> Self {
        Self {
            canister,
            batch_id,
            chunks: Arc::new(AtomicUsize::new(0)),
            bytes: Arc::new(AtomicUsize::new(0)),
        }
    }

    pub(crate) async fn create_chunk(
        &self,
        contents: &[u8],
        semaphores: &Semaphores,
    ) -> Result<Nat, CreateChunkError> {
        self.chunks.fetch_add(1, Ordering::SeqCst);
        self.bytes.fetch_add(contents.len(), Ordering::SeqCst);
        create_chunk(&self.canister, &self.batch_id, contents, semaphores).await
    }

    pub(crate) fn bytes(&self) -> usize {
        self.bytes.load(Ordering::SeqCst)
    }
    pub(crate) fn chunks(&self) -> usize {
        self.chunks.load(Ordering::SeqCst)
    }
}

#[allow(clippy::too_many_arguments)]
async fn make_project_asset_encoding(
    chunk_upload_target: Option<&ChunkUploader<'_>>,
    asset_descriptor: &AssetDescriptor,
    canister_assets: &HashMap<String, AssetDetails>,
    content: &Content,
    content_encoding: &str,
    semaphores: &Semaphores,
    logger: &Logger,
) -> Result<ProjectAssetEncoding, CreateChunkError> {
    let sha256 = content.sha256();

    let already_in_place = if let Some(canister_asset) = canister_assets.get(&asset_descriptor.key)
    {
        if canister_asset.content_type != content.media_type.to_string() {
            false
        } else if let Some(canister_asset_encoding_sha256) = canister_asset
            .encodings
            .iter()
            .find(|details| details.content_encoding == content_encoding)
            .and_then(|details| details.sha256.as_ref())
        {
            canister_asset_encoding_sha256 == &sha256
        } else {
            false
        }
    } else {
        false
    };

    let chunk_ids = if already_in_place {
        info!(
            logger,
            "  {}{} ({} bytes) sha {} is already installed",
            &asset_descriptor.key,
            content_encoding_descriptive_suffix(content_encoding),
            content.data.len(),
            hex::encode(&sha256),
        );
        vec![]
    } else if let Some(target) = chunk_upload_target {
        upload_content_chunks(
            target,
            asset_descriptor,
            content,
            &sha256,
            content_encoding,
            semaphores,
            logger,
        )
        .await?
    } else {
        info!(
            logger,
            "  {}{} ({} bytes) sha {} will be uploaded",
            &asset_descriptor.key,
            content_encoding_descriptive_suffix(content_encoding),
            content.data.len(),
            hex::encode(&sha256),
        );
        vec![]
    };

    Ok(ProjectAssetEncoding {
        chunk_ids,
        sha256,
        already_in_place,
    })
}

#[allow(clippy::too_many_arguments)]
async fn make_encoding(
    chunk_upload_target: Option<&ChunkUploader<'_>>,
    asset_descriptor: &AssetDescriptor,
    canister_assets: &HashMap<String, AssetDetails>,
    content: &Content,
    encoder: &ContentEncoder,
    force_encoding: bool,
    semaphores: &Semaphores,
    logger: &Logger,
) -> Result<Option<(String, ProjectAssetEncoding)>, CreateEncodingError> {
    match encoder {
        ContentEncoder::Identity => {
            let identity_asset_encoding = make_project_asset_encoding(
                chunk_upload_target,
                asset_descriptor,
                canister_assets,
                content,
                CONTENT_ENCODING_IDENTITY,
                semaphores,
                logger,
            )
            .await
            .map_err(CreateEncodingError::CreateChunkFailed)?;
            Ok(Some((
                CONTENT_ENCODING_IDENTITY.to_string(),
                identity_asset_encoding,
            )))
        }
        encoder => {
            let encoded = content.encode(encoder).map_err(|e| {
                EncodeContentFailed(asset_descriptor.key.clone(), encoder.to_owned(), e)
            })?;
            if force_encoding || encoded.data.len() < content.data.len() {
                let content_encoding = format!("{}", encoder);
                let project_asset_encoding = make_project_asset_encoding(
                    chunk_upload_target,
                    asset_descriptor,
                    canister_assets,
                    &encoded,
                    &content_encoding,
                    semaphores,
                    logger,
                )
                .await
                .map_err(CreateEncodingError::CreateChunkFailed)?;
                Ok(Some((content_encoding, project_asset_encoding)))
            } else {
                Ok(None)
            }
        }
    }
}

async fn make_encodings(
    chunk_upload_target: Option<&ChunkUploader<'_>>,
    asset_descriptor: &AssetDescriptor,
    canister_assets: &HashMap<String, AssetDetails>,
    content: &Content,
    semaphores: &Semaphores,
    logger: &Logger,
) -> Result<HashMap<String, ProjectAssetEncoding>, CreateEncodingError> {
    let encoders = asset_descriptor
        .config
        .encodings
        .clone()
        .unwrap_or_else(|| default_encoders(&content.media_type));
    // The identity encoding is always uploaded if it's in the list of chosen encodings.
    // Other encoding are only uploaded if they save bytes compared to identity.
    // The encoding is forced through the filter if there is no identity encoding to compare against.
    let force_encoding = !encoders.contains(&ContentEncoder::Identity);

    let encoding_futures: Vec<_> = encoders
        .iter()
        .map(|encoder| {
            make_encoding(
                chunk_upload_target,
                asset_descriptor,
                canister_assets,
                content,
                encoder,
                force_encoding,
                semaphores,
                logger,
            )
        })
        .collect();

    let encodings = try_join_all(encoding_futures).await?;

    let mut result: HashMap<String, ProjectAssetEncoding> = HashMap::new();

    for (key, value) in encodings.into_iter().flatten() {
        result.insert(key, value);
    }
    Ok(result)
}

async fn make_project_asset(
    chunk_upload_target: Option<&ChunkUploader<'_>>,
    asset_descriptor: AssetDescriptor,
    canister_assets: &HashMap<String, AssetDetails>,
    semaphores: &Semaphores,
    logger: &Logger,
) -> Result<ProjectAsset, CreateProjectAssetError> {
    let file_size = dfx_core::fs::metadata(&asset_descriptor.source)?.len();
    let permits = std::cmp::max(
        1,
        std::cmp::min(
            ((file_size + 999999) / 1000000) as usize,
            MAX_COST_SINGLE_FILE_MB,
        ),
    );
    let _releaser = semaphores.file.acquire(permits).await;
    let content = Content::load(&asset_descriptor.source)
        .map_err(CreateProjectAssetError::LoadContentFailed)?;

    let encodings = make_encodings(
        chunk_upload_target,
        &asset_descriptor,
        canister_assets,
        &content,
        semaphores,
        logger,
    )
    .await?;

    Ok(ProjectAsset {
        asset_descriptor,
        media_type: content.media_type,
        encodings,
    })
}

pub(crate) async fn make_project_assets(
    chunk_upload_target: Option<&ChunkUploader<'_>>,
    asset_descriptors: Vec<AssetDescriptor>,
    canister_assets: &HashMap<String, AssetDetails>,
    logger: &Logger,
) -> Result<HashMap<String, ProjectAsset>, CreateProjectAssetError> {
    let semaphores = Semaphores::new();

    let project_asset_futures: Vec<_> = asset_descriptors
        .iter()
        .map(|loc| {
            make_project_asset(
                chunk_upload_target,
                loc.clone(),
                canister_assets,
                &semaphores,
                logger,
            )
        })
        .collect();
    let project_assets = try_join_all(project_asset_futures).await?;

    let mut hm = HashMap::new();
    for project_asset in project_assets {
        hm.insert(project_asset.asset_descriptor.key.clone(), project_asset);
    }
    Ok(hm)
}

async fn upload_content_chunks(
    chunk_uploader: &ChunkUploader<'_>,
    asset_descriptor: &AssetDescriptor,
    content: &Content,
    sha256: &Vec<u8>,
    content_encoding: &str,
    semaphores: &Semaphores,
    logger: &Logger,
) -> Result<Vec<Nat>, CreateChunkError> {
    if content.data.is_empty() {
        let empty = vec![];
        let chunk_id = chunk_uploader.create_chunk(&empty, semaphores).await?;
        info!(
            logger,
            "  {}{} 1/1 (0 bytes) sha {}",
            &asset_descriptor.key,
            content_encoding_descriptive_suffix(content_encoding),
            hex::encode(sha256)
        );
        return Ok(vec![chunk_id]);
    }

    let count = (content.data.len() + MAX_CHUNK_SIZE - 1) / MAX_CHUNK_SIZE;
    let chunks_futures: Vec<_> = content
        .data
        .chunks(MAX_CHUNK_SIZE)
        .enumerate()
        .map(|(i, data_chunk)| {
            chunk_uploader
                .create_chunk(data_chunk, semaphores)
                .map_ok(move |chunk_id| {
                    info!(
                        logger,
                        "  {}{} {}/{} ({} bytes) sha {} {}",
                        &asset_descriptor.key,
                        content_encoding_descriptive_suffix(content_encoding),
                        i + 1,
                        count,
                        data_chunk.len(),
                        hex::encode(sha256),
                        &asset_descriptor.config
                    );
                    debug!(logger, "{:?}", &asset_descriptor.config);

                    chunk_id
                })
        })
        .collect();
    try_join_all(chunks_futures).await
}

fn content_encoding_descriptive_suffix(content_encoding: &str) -> String {
    if content_encoding == CONTENT_ENCODING_IDENTITY {
        "".to_string()
    } else {
        format!(" ({})", content_encoding)
    }
}

fn default_encoders(media_type: &Mime) -> Vec<ContentEncoder> {
    match (media_type.type_(), media_type.subtype()) {
        (mime::TEXT, _) | (_, mime::JAVASCRIPT) | (_, mime::HTML) => {
            vec![ContentEncoder::Identity, ContentEncoder::Gzip]
        }
        _ => vec![ContentEncoder::Identity],
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/batch_upload/retryable.rs:
-----------------------

use ic_agent::export::reqwest::StatusCode;
use ic_agent::AgentError;

pub(crate) fn retryable(agent_error: &AgentError) -> bool {
    match agent_error {
        AgentError::TimeoutWaitingForResponse() => true,
        AgentError::TransportError(_) => true,
        AgentError::HttpError(http_error) => {
            http_error.status == StatusCode::INTERNAL_SERVER_ERROR
                || http_error.status == StatusCode::BAD_GATEWAY
                || http_error.status == StatusCode::SERVICE_UNAVAILABLE
                || http_error.status == StatusCode::GATEWAY_TIMEOUT
                || http_error.status == StatusCode::TOO_MANY_REQUESTS
        }
        _ => false,
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/batch_upload/semaphores.rs:
-----------------------

use futures_intrusive::sync::SharedSemaphore;

// Maximum MB of file data to load at once.  More memory may be used, due to encodings.
const MAX_SIMULTANEOUS_LOADED_MB: usize = 50;

// How many simultaneous chunks being created at once
const MAX_SIMULTANEOUS_CREATE_CHUNK: usize = 50;

// How many simultaneous Agent.call() to create_chunk
const MAX_SIMULTANEOUS_CREATE_CHUNK_CALLS: usize = 25;

// How many simultaneous Agent.wait() on create_chunk result
const MAX_SIMULTANEOUS_CREATE_CHUNK_WAITS: usize = 25;

pub(crate) struct Semaphores {
    // The "file" semaphore limits how much file data to load at once.  A given loaded file's data
    // may be simultaneously encoded (gzip and so forth).
    pub file: SharedSemaphore,

    // The create_chunk semaphore limits the number of chunks that can be in the process
    // of being created at one time.  Since each chunk creation can involve retries,
    // this focuses those retries on a smaller number of chunks.
    // Without this semaphore, every chunk would make its first attempt, before
    // any chunk made its second attempt.
    pub create_chunk: SharedSemaphore,

    // The create_chunk_call semaphore limits the number of simultaneous
    // agent.call()s to create_chunk.
    pub create_chunk_call: SharedSemaphore,

    // The create_chunk_wait semaphore limits the number of simultaneous
    // agent.wait() calls for outstanding create_chunk requests.
    pub create_chunk_wait: SharedSemaphore,
}

impl Semaphores {
    pub fn new() -> Semaphores {
        let file = SharedSemaphore::new(true, MAX_SIMULTANEOUS_LOADED_MB);

        let create_chunk = SharedSemaphore::new(true, MAX_SIMULTANEOUS_CREATE_CHUNK);

        let create_chunk_call = SharedSemaphore::new(true, MAX_SIMULTANEOUS_CREATE_CHUNK_CALLS);

        let create_chunk_wait = SharedSemaphore::new(true, MAX_SIMULTANEOUS_CREATE_CHUNK_WAITS);

        Semaphores {
            file,
            create_chunk,
            create_chunk_call,
            create_chunk_wait,
        }
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/methods/api_version.rs:
-----------------------

use crate::canister_api::methods::method_names::API_VERSION;
use ic_utils::call::SyncCall;
use ic_utils::Canister;

pub(crate) async fn api_version(canister: &Canister<'_>) -> u16 {
    canister
        .query(API_VERSION)
        .build()
        .call()
        .await
        .map_or(0, |v: (u16,)| v.0)
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/methods/asset_properties.rs:
-----------------------

use crate::error::GetAssetPropertiesError;
use crate::error::GetAssetPropertiesError::GetAssetPropertiesFailed;
use crate::{
    batch_upload::retryable::retryable,
    canister_api::{
        methods::method_names::GET_ASSET_PROPERTIES,
        types::asset::{AssetDetails, AssetProperties, GetAssetPropertiesArgument},
    },
};
use backoff::backoff::Backoff;
use backoff::ExponentialBackoffBuilder;
use futures_intrusive::sync::SharedSemaphore;
use ic_agent::{agent::RejectResponse, AgentError};
use ic_utils::call::SyncCall;
use ic_utils::Canister;
use std::{collections::HashMap, time::Duration};

const MAX_CONCURRENT_REQUESTS: usize = 50;

pub(crate) async fn get_assets_properties(
    canister: &Canister<'_>,
    canister_assets: &HashMap<String, AssetDetails>,
) -> Result<HashMap<String, AssetProperties>, GetAssetPropertiesError> {
    let semaphore = SharedSemaphore::new(true, MAX_CONCURRENT_REQUESTS);

    let asset_ids = canister_assets.keys().cloned().collect::<Vec<_>>();
    let futs = asset_ids
        .iter()
        .map(|asset_id| async {
            let _releaser = semaphore.acquire(1).await;

            let mut retry_policy = ExponentialBackoffBuilder::new()
                .with_initial_interval(Duration::from_secs(1))
                .with_max_interval(Duration::from_secs(16))
                .with_multiplier(2.0)
                .with_max_elapsed_time(Some(Duration::from_secs(300)))
                .build();

            loop {
                let response = get_asset_properties(canister, asset_id).await;

                match response {
                    Ok(asset_properties) => break Ok(asset_properties),
                    Err(agent_err) if !retryable(&agent_err) => {
                        break Err(agent_err);
                    }
                    Err(agent_err) => match retry_policy.next_backoff() {
                        Some(duration) => tokio::time::sleep(duration).await,
                        None => break Err(agent_err),
                    },
                };
            }
        })
        .collect::<Vec<_>>();

    let results = futures::future::join_all(futs).await;

    let mut all_assets_properties = HashMap::new();
    for (index, result) in results.into_iter().enumerate() {
        match result {
            Ok(asset_properties) => {
                all_assets_properties.insert(asset_ids[index].to_string(), asset_properties);
            }
            // older canisters don't have get_assets_properties method
            // therefore we can break the loop
            Err(AgentError::UncertifiedReject(RejectResponse { reject_message, .. }))
                if reject_message
                    .contains(&format!("has no query method '{GET_ASSET_PROPERTIES}'"))
                    || reject_message.contains("query method does not exist") =>
            {
                break;
            }
            Err(e) => {
                return Err(GetAssetPropertiesFailed(asset_ids[index].clone(), e));
            }
        }
    }

    Ok(all_assets_properties)
}

pub(crate) async fn get_asset_properties(
    canister: &Canister<'_>,
    asset_id: &str,
) -> Result<AssetProperties, AgentError> {
    let (asset_properties,): (AssetProperties,) = canister
        .query(GET_ASSET_PROPERTIES)
        .with_arg(GetAssetPropertiesArgument(asset_id.to_string()))
        .build()
        .call()
        .await?;
    Ok(asset_properties)
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/methods/batch.rs:
-----------------------

use crate::batch_upload::retryable::retryable;
use crate::canister_api::methods::method_names::{
    COMMIT_BATCH, COMPUTE_EVIDENCE, CREATE_BATCH, PROPOSE_COMMIT_BATCH,
};
use crate::canister_api::types::batch_upload::common::{
    ComputeEvidenceArguments, CreateBatchRequest, CreateBatchResponse,
};
use backoff::backoff::Backoff;
use backoff::ExponentialBackoffBuilder;
use candid::{CandidType, Nat};
use ic_agent::AgentError;
use ic_utils::Canister;
use serde_bytes::ByteBuf;
use std::time::Duration;

pub(crate) async fn create_batch(canister: &Canister<'_>) -> Result<Nat, AgentError> {
    let mut retry_policy = ExponentialBackoffBuilder::new()
        .with_initial_interval(Duration::from_secs(1))
        .with_max_interval(Duration::from_secs(16))
        .with_multiplier(2.0)
        .with_max_elapsed_time(Some(Duration::from_secs(300)))
        .build();

    let result = loop {
        let create_batch_args = CreateBatchRequest {};
        let response = canister
            .update(CREATE_BATCH)
            .with_arg(&create_batch_args)
            .build()
            .map(|result: (CreateBatchResponse,)| (result.0.batch_id,))
            .await;
        match response {
            Ok((batch_id,)) => break Ok(batch_id),
            Err(agent_err) if !retryable(&agent_err) => {
                break Err(agent_err);
            }
            Err(agent_err) => match retry_policy.next_backoff() {
                Some(duration) => tokio::time::sleep(duration).await,
                None => break Err(agent_err),
            },
        };
    }?;
    Ok(result)
}

pub(crate) async fn submit_commit_batch<T: CandidType + Sync>(
    canister: &Canister<'_>,
    method_name: &str,
    arg: T, // CommitBatchArguments_{v0,v1,etc}
) -> Result<(), AgentError> {
    let mut retry_policy = ExponentialBackoffBuilder::new()
        .with_initial_interval(Duration::from_secs(1))
        .with_max_interval(Duration::from_secs(16))
        .with_multiplier(2.0)
        .with_max_elapsed_time(Some(Duration::from_secs(300)))
        .build();

    loop {
        match canister.update(method_name).with_arg(&arg).build().await {
            Ok(()) => return Ok(()),
            Err(agent_err) if !retryable(&agent_err) => {
                return Err(agent_err);
            }
            Err(agent_err) => match retry_policy.next_backoff() {
                Some(duration) => tokio::time::sleep(duration).await,
                None => return Err(agent_err),
            },
        }
    }
}

pub(crate) async fn commit_batch<T: CandidType + Sync>(
    canister: &Canister<'_>,
    arg: T, // CommitBatchArguments_{v0,v1,etc}
) -> Result<(), AgentError> {
    submit_commit_batch(canister, COMMIT_BATCH, arg).await
}

pub(crate) async fn propose_commit_batch<T: CandidType + Sync>(
    canister: &Canister<'_>,
    arg: T, // CommitBatchArguments_{v0,v1,etc}
) -> Result<(), AgentError> {
    submit_commit_batch(canister, PROPOSE_COMMIT_BATCH, arg).await
}

pub(crate) async fn compute_evidence(
    canister: &Canister<'_>,
    arg: &ComputeEvidenceArguments,
) -> Result<Option<ByteBuf>, AgentError> {
    let mut retry_policy = ExponentialBackoffBuilder::new()
        .with_initial_interval(Duration::from_secs(1))
        .with_max_interval(Duration::from_secs(16))
        .with_multiplier(2.0)
        .with_max_elapsed_time(Some(Duration::from_secs(300)))
        .build();

    loop {
        match canister
            .update(COMPUTE_EVIDENCE)
            .with_arg(arg)
            .build()
            .map(|result: (Option<ByteBuf>,)| (result.0,))
            .await
        {
            Ok(x) => return Ok(x.0),
            Err(agent_err) if !retryable(&agent_err) => {
                return Err(agent_err);
            }
            Err(agent_err) => match retry_policy.next_backoff() {
                Some(duration) => tokio::time::sleep(duration).await,
                None => return Err(agent_err),
            },
        }
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/methods/chunk.rs:
-----------------------

use crate::batch_upload::retryable::retryable;
use crate::batch_upload::semaphores::Semaphores;
use crate::canister_api::methods::method_names::CREATE_CHUNK;
use crate::canister_api::types::batch_upload::common::{CreateChunkRequest, CreateChunkResponse};
use crate::error::CreateChunkError;
use backoff::backoff::Backoff;
use backoff::ExponentialBackoffBuilder;
use candid::{Decode, Nat};
use ic_agent::agent::CallResponse;
use ic_utils::Canister;
use std::time::Duration;

pub(crate) async fn create_chunk(
    canister: &Canister<'_>,
    batch_id: &Nat,
    content: &[u8],
    semaphores: &Semaphores,
) -> Result<Nat, CreateChunkError> {
    let _chunk_releaser = semaphores.create_chunk.acquire(1).await;
    let batch_id = batch_id.clone();
    let args = CreateChunkRequest { batch_id, content };
    let mut retry_policy = ExponentialBackoffBuilder::new()
        .with_initial_interval(Duration::from_secs(1))
        .with_max_interval(Duration::from_secs(16))
        .with_multiplier(2.0)
        .with_max_elapsed_time(Some(Duration::from_secs(300)))
        .build();

    loop {
        let builder = canister.update(CREATE_CHUNK);
        let builder = builder.with_arg(&args);
        let request_id_result = {
            let _releaser = semaphores.create_chunk_call.acquire(1).await;
            builder
                .build()
                .map(|result: (CreateChunkResponse,)| (result.0.chunk_id,))
                .call()
                .await
        };

        let wait_result = match request_id_result {
            Ok(resp) => match resp {
                CallResponse::Response(r) => Ok(r),
                CallResponse::Poll(id) => {
                    let _releaser = semaphores.create_chunk_wait.acquire(1).await;
                    canister
                        .wait(&id)
                        .await
                        .and_then(|bytes| Ok((Decode!(&bytes, CreateChunkResponse)?.chunk_id,)))
                }
            },
            Err(agent_err) => Err(agent_err),
        };

        match wait_result {
            Ok((chunk_id,)) => {
                return Ok(chunk_id);
            }
            Err(agent_err) if !retryable(&agent_err) => {
                return Err(CreateChunkError::CreateChunk(agent_err));
            }
            Err(agent_err) => match retry_policy.next_backoff() {
                Some(duration) => tokio::time::sleep(duration).await,
                None => return Err(CreateChunkError::CreateChunk(agent_err)),
            },
        }
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/methods/list.rs:
-----------------------

use crate::canister_api::methods::method_names::LIST;
use crate::canister_api::types::{asset::AssetDetails, list::ListAssetsRequest};
use ic_agent::AgentError;
use ic_utils::call::SyncCall;
use ic_utils::Canister;
use std::collections::HashMap;

pub(crate) async fn list_assets(
    canister: &Canister<'_>,
) -> Result<HashMap<String, AssetDetails>, AgentError> {
    let (entries,): (Vec<AssetDetails>,) = canister
        .query(LIST)
        .with_arg(ListAssetsRequest {})
        .build()
        .call()
        .await?;

    let assets: HashMap<_, _> = entries.into_iter().map(|d| (d.key.clone(), d)).collect();

    Ok(assets)
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/methods/method_names.rs:
-----------------------

pub(crate) const API_VERSION: &str = "api_version";
pub(crate) const COMMIT_BATCH: &str = "commit_batch";
pub(crate) const COMPUTE_EVIDENCE: &str = "compute_evidence";
pub(crate) const CREATE_BATCH: &str = "create_batch";
pub(crate) const CREATE_CHUNK: &str = "create_chunk";
pub(crate) const GET_ASSET_PROPERTIES: &str = "get_asset_properties";
pub(crate) const LIST: &str = "list";
pub(crate) const PROPOSE_COMMIT_BATCH: &str = "propose_commit_batch";


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/methods/mod.rs:
-----------------------

pub(crate) mod api_version;
pub(crate) mod asset_properties;
pub(crate) mod batch;
pub(crate) mod chunk;
pub(crate) mod list;
pub(crate) mod method_names;


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/mod.rs:
-----------------------

pub(crate) mod methods;
pub(crate) mod types;


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/types/asset.rs:
-----------------------

use candid::CandidType;
use serde::Deserialize;
use std::collections::HashMap;

/// Information about a content encoding stored for an asset.
#[derive(CandidType, Debug, Deserialize)]
pub struct AssetEncodingDetails {
    /// A content encoding, such as "gzip".
    pub content_encoding: String,

    /// By convention, the sha256 of the entire asset encoding.  This is calculated
    /// by the asset uploader.  It is not generated or validated by the canister.
    pub sha256: Option<Vec<u8>>,
}

/// Information about an asset stored in the canister.
#[derive(CandidType, Debug, Deserialize)]
pub struct AssetDetails {
    /// The key identifies the asset.
    pub key: String,
    /// A list of the encodings stored for the asset.
    pub encodings: Vec<AssetEncodingDetails>,
    /// The MIME type of the asset.
    pub content_type: String,
}

/// Information about the properties stored for an asset.
#[derive(CandidType, Debug, Deserialize, Default)]
pub struct AssetProperties {
    /// Asset's cache max_age property
    pub max_age: Option<u64>,
    /// Asset's HTTP response headers
    pub headers: Option<HashMap<String, String>>,
    /// Asset's toggle for whether to serve the asset over .raw domain
    pub allow_raw_access: Option<bool>,
    /// Asset's toggle for whether to serve the .html asset both as /route and /route.html
    pub is_aliased: Option<bool>,
}

/// Sets the asset with the given properties.
#[derive(Debug, Clone, CandidType, PartialEq, Eq, PartialOrd, Ord)]
pub struct SetAssetPropertiesArguments {
    pub key: String,
    pub max_age: Option<Option<u64>>,
    pub headers: Option<Option<Vec<(String, String)>>>,
    pub allow_raw_access: Option<Option<bool>>,
    pub is_aliased: Option<Option<bool>>,
}

/// The arguments to the `get_asset_properties` method.
#[derive(CandidType, Debug)]
pub struct GetAssetPropertiesArgument(pub String);


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/types/batch_upload/common.rs:
-----------------------

use crate::asset::config::HeadersConfig;
use candid::{CandidType, Nat};
use serde::Deserialize;

/// Create a new batch, which will expire after some time period.
/// This expiry is extended by any call to create_chunk().
/// Also, removes any expired batches.
#[derive(CandidType, Debug)]
pub struct CreateBatchRequest {}

/// The response to a CreateBatchRequest.
#[derive(CandidType, Debug, Deserialize)]
pub struct CreateBatchResponse {
    /// The ID of the created batch.
    pub batch_id: Nat,
}

/// Upload a chunk of data that is part of an asset's content.
#[derive(CandidType, Debug, Deserialize)]
pub struct CreateChunkRequest<'a> {
    /// The batch with which to associate the created chunk.
    /// The chunk will be deleted if the batch expires before being committed.
    pub batch_id: Nat,

    /// The data in this chunk.
    #[serde(with = "serde_bytes")]
    pub content: &'a [u8],
}

/// The responst to a CreateChunkRequest.
#[derive(CandidType, Debug, Deserialize)]
pub struct CreateChunkResponse {
    /// The ID of the created chunk.
    pub chunk_id: Nat,
}

/// Create a new asset.  Has no effect if the asset already exists and the content type matches.
/// Traps if the asset already exists but with a different content type.
#[derive(CandidType, Clone, Debug, PartialOrd, PartialEq, Eq, Ord)]
pub struct CreateAssetArguments {
    /// The key identifies the asset.
    pub key: String,
    /// The MIME type of this asset
    pub content_type: String,
    /// The cache HTTP header Time To Live parameter
    pub max_age: Option<u64>,
    /// The HTTP headers
    pub headers: Option<HeadersConfig>,
    /// Aliasing enabled or not
    pub enable_aliasing: Option<bool>,
    /// When set to true, don't redirect from raw to certified
    pub allow_raw_access: Option<bool>,
}

/// Set the data for a particular content encoding for the given asset.
#[derive(CandidType, Clone, Debug, PartialOrd, PartialEq, Eq, Ord)]
pub struct SetAssetContentArguments {
    /// The key identifies the asset.
    pub key: String,
    /// The content encoding for which this content applies
    pub content_encoding: String,
    /// The chunks to assign to this content
    pub chunk_ids: Vec<Nat>,
    /// The sha256 of the entire content
    pub sha256: Option<Vec<u8>>,
}

/// Remove a specific content encoding for the asset.
#[derive(CandidType, Clone, Debug, PartialOrd, PartialEq, Eq, Ord)]
pub struct UnsetAssetContentArguments {
    /// The key identifies the asset.
    pub key: String,
    /// The content encoding to remove.
    pub content_encoding: String,
}

/// Remove the specified asset.
#[derive(CandidType, Clone, Debug, PartialOrd, PartialEq, Eq, Ord)]
pub struct DeleteAssetArguments {
    /// The key identifies the asset to delete.
    pub key: String,
}

/// Remove all assets, batches, and chunks, and reset the next batch and chunk IDs.
#[derive(CandidType, Clone, Debug, PartialOrd, PartialEq, Eq, Ord)]
pub struct ClearArguments {}

/// Compute a hash over the proposed CommitBatchArguments.  This may take more than one call.
#[derive(CandidType, Debug)]
pub struct ComputeEvidenceArguments {
    /// The batch for which to compute evidence
    pub batch_id: Nat,

    /// A measure of how much work to do in one call
    pub max_iterations: Option<u16>,
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/types/batch_upload/mod.rs:
-----------------------

pub(crate) mod common;
pub(crate) mod v0;
pub(crate) mod v1;


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/types/batch_upload/v0.rs:
-----------------------

use super::common::*;
use crate::error::DowngradeCommitBatchArgumentsV1ToV0Error;
use crate::error::DowngradeCommitBatchArgumentsV1ToV0Error::V0SetAssetPropertiesNotSupported;
use candid::{CandidType, Nat};

/// Batch operations that can be applied atomically.
#[derive(CandidType, Debug, Eq, PartialEq, PartialOrd, Ord)]
#[allow(dead_code)]
pub enum BatchOperationKind {
    /// Clear all state from the asset canister.
    Clear(ClearArguments),

    /// Remove an asset altogether.
    DeleteAsset(DeleteAssetArguments),

    /// Create a new asset.
    CreateAsset(CreateAssetArguments),

    /// Remove content from an asset by encoding.
    UnsetAssetContent(UnsetAssetContentArguments),

    /// Assign content to an asset by encoding.
    SetAssetContent(SetAssetContentArguments),
}

/// Apply all of the operations in the batch, and then remove the batch.
#[derive(CandidType, Debug, Eq, PartialEq, PartialOrd, Ord)]
pub struct CommitBatchArguments {
    /// The batch to commit.
    pub batch_id: Nat,

    /// The operations to apply atomically.
    pub operations: Vec<BatchOperationKind>,
}

// impl try_from for v0::BatchOperationKind from v1::BatchOperationKind
impl TryFrom<super::v1::CommitBatchArguments> for CommitBatchArguments {
    type Error = DowngradeCommitBatchArgumentsV1ToV0Error;

    fn try_from(value: super::v1::CommitBatchArguments) -> Result<Self, Self::Error> {
        let mut operations = vec![];
        for operation in value.operations {
            let operation = match operation {
                super::v1::BatchOperationKind::CreateAsset(args) => {
                    BatchOperationKind::CreateAsset(args)
                }
                super::v1::BatchOperationKind::SetAssetContent(args) => {
                    BatchOperationKind::SetAssetContent(args)
                }
                super::v1::BatchOperationKind::UnsetAssetContent(args) => {
                    BatchOperationKind::UnsetAssetContent(args)
                }
                super::v1::BatchOperationKind::DeleteAsset(args) => {
                    BatchOperationKind::DeleteAsset(args)
                }
                super::v1::BatchOperationKind::Clear(args) => BatchOperationKind::Clear(args),
                super::v1::BatchOperationKind::SetAssetProperties(_) => {
                    return Err(V0SetAssetPropertiesNotSupported)
                }
            };
            operations.push(operation);
        }

        Ok(Self {
            batch_id: value.batch_id,
            operations,
        })
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/types/batch_upload/v1.rs:
-----------------------

use crate::canister_api::types::{
    asset::SetAssetPropertiesArguments,
    batch_upload::common::{
        ClearArguments, CreateAssetArguments, DeleteAssetArguments, SetAssetContentArguments,
        UnsetAssetContentArguments,
    },
};
use candid::{CandidType, Nat};
use std::collections::HashMap;

/// Batch operations that can be applied atomically.
#[derive(CandidType, Clone, Debug, Eq, PartialEq, PartialOrd, Ord)]
pub enum BatchOperationKind {
    #[allow(dead_code)]
    /// Clear all state from the asset canister.
    Clear(ClearArguments),

    /// Remove an asset altogether.
    DeleteAsset(DeleteAssetArguments),

    /// Create a new asset.
    CreateAsset(CreateAssetArguments),

    /// Remove content from an asset by encoding.
    UnsetAssetContent(UnsetAssetContentArguments),

    /// Assign content to an asset by encoding.
    SetAssetContent(SetAssetContentArguments),

    /// Set asset properties.
    SetAssetProperties(SetAssetPropertiesArguments),
}

/// Apply all of the operations in the batch, and then remove the batch.
#[derive(CandidType, Debug, Eq, PartialEq, PartialOrd, Ord)]
pub struct CommitBatchArguments {
    /// The batch to commit.
    pub batch_id: Nat,

    /// The operations to apply atomically.
    pub operations: Vec<BatchOperationKind>,
}

impl CommitBatchArguments {
    pub(crate) fn group_by_kind_then_count(&self) -> HashMap<String, usize> {
        self.operations
            .iter()
            .fold(HashMap::new(), |mut map: HashMap<String, usize>, op| {
                let key = match op {
                    BatchOperationKind::Clear(_) => "Clear",
                    BatchOperationKind::DeleteAsset(_) => "Delete",
                    BatchOperationKind::CreateAsset(_) => "CreateAsset",
                    BatchOperationKind::UnsetAssetContent(_) => "UnsetAssetContent",
                    BatchOperationKind::SetAssetContent(_) => "SetAssetContent",
                    BatchOperationKind::SetAssetProperties(_) => "SetAssetProperties",
                };
                *map.entry(key.to_owned()).or_default() += 1;
                map
            })
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/types/list.rs:
-----------------------

use candid::CandidType;

/// Return a list of all assets in the canister.
#[derive(CandidType, Debug)]
pub struct ListAssetsRequest {}


-----------------------

/src/canisters/frontend/ic-asset/src/canister_api/types/mod.rs:
-----------------------

pub(crate) mod asset;
pub(crate) mod batch_upload;
pub(crate) mod list;


-----------------------

/src/canisters/frontend/ic-asset/src/error/compatibility.rs:
-----------------------

use crate::error::downgrade_commit_batch_arguments::DowngradeCommitBatchArgumentsV1ToV0Error;
use thiserror::Error;

/// Errors that occur when trying to deploy to an older version of the asset canister.
#[derive(Error, Debug)]
pub enum CompatibilityError {
    /// Failed to downgrade from v1::CommitBatchArguments to v0::CommitBatchArguments.
    #[error("Failed to downgrade from v1::CommitBatchArguments to v0::CommitBatchArguments: {0}. Please upgrade your asset canister, or use older tooling (dfx<=v-0.13.1 or icx-asset<=0.20.0)")]
    DowngradeV1TOV0Failed(DowngradeCommitBatchArgumentsV1ToV0Error),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/compute_evidence.rs:
-----------------------

use crate::error::create_project_asset::CreateProjectAssetError;
use crate::error::gather_asset_descriptors::GatherAssetDescriptorsError;
use crate::error::get_asset_properties::GetAssetPropertiesError;
use crate::error::hash_content::HashContentError;
use ic_agent::AgentError;
use thiserror::Error;

/// Errors related to computing evidence for a proposed update.
#[derive(Error, Debug)]
pub enum ComputeEvidenceError {
    /// Failed when inspecting assets to be updated.
    #[error(transparent)]
    ProcessProjectAsset(#[from] CreateProjectAssetError),

    /// Failed when determining which assets and encodings changed.
    #[error(transparent)]
    GatherAssetDescriptors(#[from] GatherAssetDescriptorsError),

    /// Failed when reading assets properties from the asset canister.
    #[error(transparent)]
    GetAssetProperties(#[from] GetAssetPropertiesError),

    /// Failed when computing hashes of asset content.
    #[error(transparent)]
    HashContent(#[from] HashContentError),

    /// Failed to list assets in the asset canister.
    #[error("Failed to list assets: {0}")]
    ListAssets(AgentError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/create_chunk.rs:
-----------------------

use ic_agent::AgentError;
use thiserror::Error;

/// Errors related to creating a chunk.
#[derive(Error, Debug)]
pub enum CreateChunkError {
    /// Failed in call to create_chunk, or in waiting for response.
    #[error("Failed to create chunk: {0}")]
    CreateChunk(AgentError),

    /// Failed to decode the create chunk response.
    #[error("Failed to decode create chunk response: {0}")]
    DecodeCreateChunkResponse(candid::Error),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/create_encoding.rs:
-----------------------

use crate::asset::content_encoder::ContentEncoder;
use crate::error::create_chunk::CreateChunkError;
use thiserror::Error;

/// Errors related to creating/uploading an asset content encoding to the asset canister
#[derive(Error, Debug)]
pub enum CreateEncodingError {
    /// Failed when creating a chunk.
    #[error("Failed to create chunk: {0}")]
    CreateChunkFailed(CreateChunkError),

    /// Failed when encoding asset content.
    #[error("Failed to encode content of '{0}' with {1} encoding: {2}")]
    EncodeContentFailed(String, ContentEncoder, std::io::Error),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/create_project_asset.rs:
-----------------------

use crate::error::create_encoding::CreateEncodingError;
use dfx_core::error::fs::{ReadFileError, ReadMetadataError};
use thiserror::Error;

/// Errors related to creating an asset found in the project in the asset canister.
#[derive(Error, Debug)]
pub enum CreateProjectAssetError {
    /// Failed to create an asset encoding in the asset canister.
    #[error("Failed to create encoding: {0}")]
    CreateEncodingError(#[from] CreateEncodingError),

    /// Failed to find out the file size of an asset.
    #[error("failed to determine asset size")]
    DetermineAssetSizeFailed(#[from] ReadMetadataError),

    /// Failed to load asset content from the filesystem.
    #[error("failed to load asset content")]
    LoadContentFailed(#[from] ReadFileError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/downgrade_commit_batch_arguments.rs:
-----------------------

use thiserror::Error;

/// Errors related to downgrading CommitBatchArguments for use with a v0 asset canister.
#[derive(Error, Debug)]
pub enum DowngradeCommitBatchArgumentsV1ToV0Error {
    /// Asset canister v0 does not support SetAssetProperties.
    #[error("SetAssetProperties is not supported")]
    V0SetAssetPropertiesNotSupported,
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/gather_asset_descriptors.rs:
-----------------------

use crate::error::get_asset_config::GetAssetConfigError;
use crate::error::load_config::AssetLoadConfigError;
use dfx_core::error::fs::CanonicalizePathError;
use std::path::PathBuf;
use thiserror::Error;

/// Errors related to building asset list and reading asset configurations.
#[derive(Error, Debug)]
pub enum GatherAssetDescriptorsError {
    /// An asset with a given key exists in more than one source directory.
    #[error("Asset with key '{0}' defined at {1} and {2}")]
    DuplicateAssetKey(String, Box<PathBuf>, Box<PathBuf>),

    /// Failed to get asset configuration.
    #[error("Failed to get asset configuration: {0}")]
    GetAssetConfigFailed(#[from] GetAssetConfigError),

    /// Failed to canonicalize a directory entry.
    #[error("invalid directory entry")]
    InvalidDirectoryEntry(#[source] CanonicalizePathError),

    /// Failed to canonicalize a source directory.
    #[error("invalid source directory")]
    InvalidSourceDirectory(#[source] CanonicalizePathError),

    /// Failed to load the asset configuration for a directory.
    #[error("Failed to load asset configuration: {0}")]
    LoadConfigFailed(AssetLoadConfigError),

    /// One or more assets use the hardened security policy but don't actually specify any hardenings compared to the standard security policy.
    #[error("This project uses the hardened security policy for some assets, but does not actually configure any custom improvements over the standard policy. To get started, look at 'dfx info canister-security-policy'. It shows the default security policy along with suggestions on how to improve it.\n{0}")]
    HardenedSecurityPolicyIsNotHardened(String),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/get_asset_config.rs:
-----------------------

use dfx_core::error::fs::NoParentPathError;
use std::path::PathBuf;
use thiserror::Error;

/// Errors related to getting asset configuration.
#[derive(Error, Debug)]
pub enum GetAssetConfigError {
    /// An asset exists, but it does not have a configuration.
    #[error("No configuration found for asset '{0}'")]
    AssetConfigNotFound(PathBuf),

    /// The path to an asset was invalid.
    #[error("invalid asset path")]
    InvalidPath(#[from] NoParentPathError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/get_asset_properties.rs:
-----------------------

use ic_agent::AgentError;
use thiserror::Error;

/// Errors related to getting asset properties.
#[derive(Error, Debug)]
pub enum GetAssetPropertiesError {
    /// The call to get_asset_properties failed.
    #[error("Failed to get asset properties for {0}: {1}")]
    GetAssetPropertiesFailed(String, AgentError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/hash_content.rs:
-----------------------

use crate::asset::content_encoder::ContentEncoder;
use dfx_core::error::fs::ReadFileError;
use thiserror::Error;

/// Errors related to hashing asset content.
#[derive(Error, Debug)]
pub enum HashContentError {
    /// Failed to encode the content in order to compute the hash.
    #[error("Failed to encode content of '{0}' with {1} encoding: {2}")]
    EncodeContentFailed(String, ContentEncoder, std::io::Error),

    /// Failed to load asset content from the filesystem.
    #[error("failed to load content")]
    LoadContentFailed(#[from] ReadFileError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/load_config.rs:
-----------------------

use crate::error::load_rule::LoadRuleError;
use dfx_core::error::fs::{ReadDirError, ReadToStringError};
use std::path::PathBuf;
use thiserror::Error;

/// Errors related to loading asset configuration.
#[derive(Error, Debug)]
pub enum AssetLoadConfigError {
    /// Failed to canonicalize the root directory.
    #[error("root_dir '{0}' is expected to be a canonical path")]
    InvalidRootDir(PathBuf),

    /// Failed to load a rule from the asset configuration file.
    #[error("Failed to load rule in {0}: {1}")]
    LoadRuleFailed(PathBuf, LoadRuleError),

    /// An asset configuration file was not valid JSON5.
    #[error("Malformed JSON asset config file '{0}': {1}")]
    MalformedAssetConfigFile(PathBuf, json5::Error),

    /// both `assets.json` and `assets.json5` files exist in the same directory.
    #[error("both {} and {} files exist in the same directory (dir = {:?})",
    crate::asset::config::ASSETS_CONFIG_FILENAME_JSON,
    crate::asset::config::ASSETS_CONFIG_FILENAME_JSON5,
    .0.display())]
    MultipleConfigurationFiles(PathBuf),

    /// Couldn't read a directory when loading asset configuration.
    #[error(transparent)]
    ReadDir(#[from] ReadDirError),

    /// Couldn't read a config file.
    #[error(transparent)]
    ReadToString(#[from] ReadToStringError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/load_rule.rs:
-----------------------

use std::path::PathBuf;
use thiserror::Error;

/// Errors related to loading an asset configuration rule.
#[derive(Error, Debug)]
pub enum LoadRuleError {
    /// The match string could not be combined with the root directory to form a valid string.
    #[error("Failed to combine {0} and {1} into a string (to be later used as a glob pattern)")]
    FormGlobPatternFailed(PathBuf, String),

    /// The glob pattern was not valid.
    #[error("{0} is not a valid glob pattern: {1}")]
    InvalidGlobPattern(String, globset::Error),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/mod.rs:
-----------------------

//! Error types

mod compatibility;
mod compute_evidence;
mod create_chunk;
mod create_encoding;
mod create_project_asset;
mod downgrade_commit_batch_arguments;
mod gather_asset_descriptors;
mod get_asset_config;
mod get_asset_properties;
mod hash_content;
mod load_config;
mod load_rule;
mod prepare_sync_for_proposal;
mod sync;
mod upload;
mod upload_content;

pub use compatibility::CompatibilityError;
pub use compute_evidence::ComputeEvidenceError;
pub use create_chunk::CreateChunkError;
pub use create_encoding::CreateEncodingError;
pub use create_project_asset::CreateProjectAssetError;
pub use downgrade_commit_batch_arguments::DowngradeCommitBatchArgumentsV1ToV0Error;
pub use gather_asset_descriptors::GatherAssetDescriptorsError;
pub use get_asset_config::GetAssetConfigError;
pub use get_asset_properties::GetAssetPropertiesError;
pub use hash_content::HashContentError;
pub use load_config::AssetLoadConfigError;
pub use load_rule::LoadRuleError;
pub use prepare_sync_for_proposal::PrepareSyncForProposalError;
pub use sync::SyncError;
pub use upload::UploadError;
pub use upload_content::UploadContentError;


-----------------------

/src/canisters/frontend/ic-asset/src/error/prepare_sync_for_proposal.rs:
-----------------------

use crate::error::upload_content::UploadContentError;
use ic_agent::AgentError;
use thiserror::Error;

/// Errors related to preparing synchronization operations for a proposal.
#[derive(Error, Debug)]
pub enum PrepareSyncForProposalError {
    /// Failed while requesting that the asset canister compute evidence.
    #[error("Failed to compute evidence: {0}")]
    ComputeEvidence(AgentError),

    /// Failed while calling propose_commit_batch.
    #[error("Failed to propose batch to commit: {0}")]
    ProposeCommitBatch(AgentError),

    /// Failed while uploading content for synchronization.
    #[error(transparent)]
    UploadContent(#[from] UploadContentError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/sync.rs:
-----------------------

use crate::error::compatibility::CompatibilityError;
use crate::error::upload_content::UploadContentError;
use ic_agent::AgentError;
use thiserror::Error;

/// Errors related to the sync process.
#[derive(Error, Debug)]
pub enum SyncError {
    /// Failed when calling commit_batch
    #[error("Failed to commit batch: {0}")]
    CommitBatchFailed(AgentError),

    /// Failed when trying to work with an older asset canister.
    #[error(transparent)]
    Compatibility(#[from] CompatibilityError),

    /// Failed when uploading content for synchronization.
    #[error(transparent)]
    UploadContentFailed(#[from] UploadContentError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/upload.rs:
-----------------------

use crate::error::compatibility::CompatibilityError;
use crate::error::create_project_asset::CreateProjectAssetError;
use ic_agent::AgentError;
use thiserror::Error;

/// Errors encountered during the upload process.
#[derive(Error, Debug)]
pub enum UploadError {
    /// Failed when calling commit_batch.
    #[error("Commit batch failed: {0}")]
    CommitBatchFailed(AgentError),

    /// Failure when trying to work with an older asset canister.
    #[error(transparent)]
    Compatibility(#[from] CompatibilityError),

    /// Failed when calling create_batch.
    #[error("Create batch failed: {0}")]
    CreateBatchFailed(AgentError),

    /// Failed when creating project assets.
    #[error("Failed to create project asset: {0}")]
    CreateProjectAssetFailed(#[from] CreateProjectAssetError),

    /// Failed when calling the list method.
    #[error("List assets failed: {0}")]
    ListAssetsFailed(AgentError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/error/upload_content.rs:
-----------------------

use crate::error::create_project_asset::CreateProjectAssetError;
use crate::error::gather_asset_descriptors::GatherAssetDescriptorsError;
use crate::error::get_asset_properties::GetAssetPropertiesError;
use ic_agent::AgentError;
use thiserror::Error;

/// Errors related to uploading content to the asset canister.
#[derive(Error, Debug)]
pub enum UploadContentError {
    /// Failed when calling create_batch.
    #[error("Failed to create batch: {0}")]
    CreateBatchFailed(AgentError),

    /// Failed when creating project assets.
    #[error("Failed to create project asset: {0}")]
    CreateProjectAssetError(#[from] CreateProjectAssetError),

    /// Failed when building list of assets to synchronize.
    #[error("Failed to gather asset descriptors: {0}")]
    GatherAssetDescriptorsFailed(#[from] GatherAssetDescriptorsError),

    /// Failed when getting asset properties.
    #[error(transparent)]
    GetAssetPropertiesFailed(#[from] GetAssetPropertiesError),

    /// Failed when calling the list method.
    #[error("Failed to list assets: {0}")]
    ListAssetsFailed(AgentError),
}


-----------------------

/src/canisters/frontend/ic-asset/src/evidence/mod.rs:
-----------------------

use crate::asset::content::Content;
use crate::asset::content_encoder::ContentEncoder::{Brotli, Gzip};
use crate::batch_upload::operations::assemble_batch_operations;
use crate::batch_upload::operations::AssetDeletionReason::Obsolete;
use crate::batch_upload::plumbing::{make_project_assets, ProjectAsset};
use crate::canister_api::methods::asset_properties::get_assets_properties;
use crate::canister_api::methods::list::list_assets;
use crate::canister_api::types::asset::SetAssetPropertiesArguments;
use crate::canister_api::types::batch_upload::common::{
    ClearArguments, CreateAssetArguments, DeleteAssetArguments, SetAssetContentArguments,
    UnsetAssetContentArguments,
};
use crate::canister_api::types::batch_upload::v1::BatchOperationKind;
use crate::error::ComputeEvidenceError;
use crate::error::HashContentError;
use crate::error::HashContentError::EncodeContentFailed;
use crate::sync::gather_asset_descriptors;
use ic_utils::Canister;
use sha2::{Digest, Sha256};
use slog::{info, Logger};
use std::collections::{BTreeMap, HashMap};
use std::path::Path;

const TAG_FALSE: [u8; 1] = [0];
const TAG_TRUE: [u8; 1] = [1];

const TAG_NONE: [u8; 1] = [2];
const TAG_SOME: [u8; 1] = [3];

const TAG_CREATE_ASSET: [u8; 1] = [4];
const TAG_SET_ASSET_CONTENT: [u8; 1] = [5];
const TAG_UNSET_ASSET_CONTENT: [u8; 1] = [6];
const TAG_DELETE_ASSET: [u8; 1] = [7];
const TAG_CLEAR: [u8; 1] = [8];
const TAG_SET_ASSET_PROPERTIES: [u8; 1] = [9];

/// Compute the hash ("evidence") over the batch operations required to update the assets
pub async fn compute_evidence(
    canister: &Canister<'_>,
    dirs: &[&Path],
    logger: &Logger,
) -> Result<String, ComputeEvidenceError> {
    let asset_descriptors = gather_asset_descriptors(dirs, logger)?;

    let canister_assets = list_assets(canister)
        .await
        .map_err(ComputeEvidenceError::ListAssets)?;
    info!(
        logger,
        "Fetching properties for all assets in the canister."
    );
    let canister_asset_properties = get_assets_properties(canister, &canister_assets).await?;

    info!(
        logger,
        "Computing evidence for batch operations for assets in the project.",
    );
    let project_assets =
        make_project_assets(None, asset_descriptors, &canister_assets, logger).await?;

    let mut operations = assemble_batch_operations(
        &project_assets,
        canister_assets,
        Obsolete,
        canister_asset_properties,
    );
    operations.sort();

    let mut sha = Sha256::new();
    for op in operations {
        hash_operation(&mut sha, &op, &project_assets)?;
    }
    let evidence: [u8; 32] = sha.finalize().into();

    Ok(hex::encode(evidence))
}

fn hash_operation(
    hasher: &mut Sha256,
    op: &BatchOperationKind,
    project_assets: &HashMap<String, ProjectAsset>,
) -> Result<(), HashContentError> {
    match op {
        BatchOperationKind::CreateAsset(args) => hash_create_asset(hasher, args),
        BatchOperationKind::SetAssetContent(args) => {
            hash_set_asset_content(hasher, args, project_assets)?
        }
        BatchOperationKind::UnsetAssetContent(args) => hash_unset_asset_content(hasher, args),
        BatchOperationKind::DeleteAsset(args) => hash_delete_asset(hasher, args),
        BatchOperationKind::Clear(args) => hash_clear(hasher, args),
        BatchOperationKind::SetAssetProperties(args) => hash_set_asset_properties(hasher, args),
    };
    Ok(())
}

fn hash_create_asset(hasher: &mut Sha256, args: &CreateAssetArguments) {
    hasher.update(TAG_CREATE_ASSET);
    hasher.update(&args.key);
    hasher.update(&args.content_type);
    if let Some(max_age) = args.max_age {
        hasher.update(TAG_SOME);
        hasher.update(max_age.to_be_bytes());
    } else {
        hasher.update(TAG_NONE);
    }
    hash_headers(hasher, args.headers.as_ref());
    hash_opt_bool(hasher, args.allow_raw_access);
    hash_opt_bool(hasher, args.enable_aliasing);
}

fn hash_set_asset_content(
    hasher: &mut Sha256,
    args: &SetAssetContentArguments,
    project_assets: &HashMap<String, ProjectAsset>,
) -> Result<(), HashContentError> {
    hasher.update(TAG_SET_ASSET_CONTENT);
    hasher.update(&args.key);
    hasher.update(&args.content_encoding);
    hash_opt_vec_u8(hasher, args.sha256.as_ref());

    let project_asset = project_assets.get(&args.key).unwrap();
    let ad = &project_asset.asset_descriptor;

    let content = {
        let identity = Content::load(&ad.source)?;
        match args.content_encoding.as_str() {
            "identity" => identity,
            "br" | "brotli" => identity
                .encode(&Brotli)
                .map_err(|e| EncodeContentFailed(ad.key.clone(), Brotli, e))?,
            "gzip" => identity
                .encode(&Gzip)
                .map_err(|e| EncodeContentFailed(ad.key.clone(), Gzip, e))?,
            _ => unreachable!("unhandled content encoder"),
        }
    };
    hasher.update(&content.data);
    Ok(())
}

fn hash_unset_asset_content(hasher: &mut Sha256, args: &UnsetAssetContentArguments) {
    hasher.update(TAG_UNSET_ASSET_CONTENT);
    hasher.update(&args.key);
    hasher.update(&args.content_encoding);
}

fn hash_delete_asset(hasher: &mut Sha256, args: &DeleteAssetArguments) {
    hasher.update(TAG_DELETE_ASSET);
    hasher.update(&args.key);
}

fn hash_clear(hasher: &mut Sha256, _args: &ClearArguments) {
    hasher.update(TAG_CLEAR);
}

fn hash_opt_bool(hasher: &mut Sha256, b: Option<bool>) {
    if let Some(b) = b {
        hasher.update(TAG_SOME);
        hasher.update(if b { TAG_TRUE } else { TAG_FALSE });
    } else {
        hasher.update(TAG_NONE);
    }
}

fn hash_opt_vec_u8(hasher: &mut Sha256, buf: Option<&Vec<u8>>) {
    if let Some(buf) = buf {
        hasher.update(TAG_SOME);
        hasher.update(buf);
    } else {
        hasher.update(TAG_NONE);
    }
}

fn hash_headers(hasher: &mut Sha256, headers: Option<&BTreeMap<String, String>>) {
    if let Some(headers) = headers {
        hasher.update(TAG_SOME);
        for k in headers.keys() {
            let v = headers.get(k).unwrap();
            hasher.update(k);
            hasher.update(v);
        }
    } else {
        hasher.update(TAG_NONE);
    }
}

fn hash_set_asset_properties(hasher: &mut Sha256, args: &SetAssetPropertiesArguments) {
    hasher.update(TAG_SET_ASSET_PROPERTIES);
    hasher.update(&args.key);
    if let Some(max_age) = args.max_age {
        hasher.update(TAG_SOME);
        if let Some(max_age) = max_age {
            hasher.update(TAG_SOME);
            hasher.update(max_age.to_be_bytes());
        } else {
            hasher.update(TAG_NONE);
        }
    } else {
        hasher.update(TAG_NONE);
    }
    if let Some(headers) = args.headers.as_ref() {
        hasher.update(TAG_SOME);
        if let Some(h) = headers {
            let h = BTreeMap::from_iter(h.iter().map(|(k, v)| (k.to_string(), v.to_string())));
            hash_headers(hasher, Some(&h));
        } else {
            hash_headers(hasher, None);
        }
    } else {
        hasher.update(TAG_NONE);
    }
    if let Some(allow_raw_access) = args.allow_raw_access {
        hasher.update(TAG_SOME);
        hash_opt_bool(hasher, allow_raw_access);
    } else {
        hasher.update(TAG_NONE);
    }
    if let Some(enable_aliasing) = args.is_aliased {
        hasher.update(TAG_SOME);
        hash_opt_bool(hasher, enable_aliasing);
    } else {
        hasher.update(TAG_NONE);
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/lib.rs:
-----------------------

/src/canisters/frontend/ic-asset/src/security_policy.rs:
-----------------------

/src/canisters/frontend/ic-asset/src/sync.rs:
-----------------------

use crate::asset::config::{
    AssetConfig, AssetSourceDirectoryConfiguration, ASSETS_CONFIG_FILENAME_JSON,
};
use crate::batch_upload::operations::BATCH_UPLOAD_API_VERSION;
use crate::batch_upload::plumbing::ChunkUploader;
use crate::batch_upload::{
    self,
    operations::AssetDeletionReason,
    plumbing::{make_project_assets, AssetDescriptor},
};
use crate::canister_api::methods::batch::{compute_evidence, propose_commit_batch};
use crate::canister_api::methods::{
    api_version::api_version,
    asset_properties::get_assets_properties,
    batch::{commit_batch, create_batch},
    list::list_assets,
};
use crate::canister_api::types::batch_upload::v0;
use crate::canister_api::types::batch_upload::v1::BatchOperationKind;
use crate::canister_api::types::batch_upload::{
    common::ComputeEvidenceArguments, v1::CommitBatchArguments,
};
use crate::error::CompatibilityError::DowngradeV1TOV0Failed;
use crate::error::GatherAssetDescriptorsError;
use crate::error::GatherAssetDescriptorsError::{
    DuplicateAssetKey, InvalidDirectoryEntry, InvalidSourceDirectory, LoadConfigFailed,
};
use crate::error::PrepareSyncForProposalError;
use crate::error::SyncError;
use crate::error::SyncError::CommitBatchFailed;
use crate::error::UploadContentError;
use crate::error::UploadContentError::{CreateBatchFailed, ListAssetsFailed};
use candid::Nat;
use ic_agent::AgentError;
use ic_utils::Canister;
use itertools::Itertools;
use serde_bytes::ByteBuf;
use slog::{debug, info, trace, warn, Logger};
use std::collections::HashMap;
use std::path::Path;
use walkdir::WalkDir;

const KNOWN_DIRECTORIES: [&str; 1] = [".well-known"];

/// Sets the contents of the asset canister to the contents of a directory, including deleting old assets.
pub async fn upload_content_and_assemble_sync_operations(
    canister: &Canister<'_>,
    dirs: &[&Path],
    no_delete: bool,
    logger: &Logger,
) -> Result<CommitBatchArguments, UploadContentError> {
    let asset_descriptors = gather_asset_descriptors(dirs, logger)?;

    let canister_assets = list_assets(canister).await.map_err(ListAssetsFailed)?;
    info!(
        logger,
        "Fetching properties for all assets in the canister."
    );
    let now = std::time::Instant::now();
    let canister_asset_properties = get_assets_properties(canister, &canister_assets).await?;

    info!(
        logger,
        "Done fetching properties for all assets in the canister. Took {:?}",
        now.elapsed()
    );

    info!(logger, "Starting batch.");

    let batch_id = create_batch(canister).await.map_err(CreateBatchFailed)?;

    info!(
        logger,
        "Staging contents of new and changed assets in batch {}:", batch_id
    );

    let chunk_uploader = ChunkUploader::new(canister.clone(), batch_id.clone());

    let project_assets = make_project_assets(
        Some(&chunk_uploader),
        asset_descriptors,
        &canister_assets,
        logger,
    )
    .await?;

    let commit_batch_args = batch_upload::operations::assemble_commit_batch_arguments(
        project_assets,
        canister_assets,
        match no_delete {
            true => AssetDeletionReason::Incompatible,
            false => AssetDeletionReason::Obsolete,
        },
        canister_asset_properties,
        batch_id,
    );

    // -v
    debug!(
        logger,
        "Count of each Batch Operation Kind: {:?}",
        commit_batch_args.group_by_kind_then_count()
    );
    debug!(
        logger,
        "Chunks: {}  Bytes: {}",
        chunk_uploader.chunks(),
        chunk_uploader.bytes()
    );

    // -vv
    trace!(logger, "Value of CommitBatch: {:?}", commit_batch_args);

    Ok(commit_batch_args)
}

/// Sets the contents of the asset canister to the contents of a directory, including deleting old assets.
pub async fn sync(
    canister: &Canister<'_>,
    dirs: &[&Path],
    no_delete: bool,
    logger: &Logger,
) -> Result<(), SyncError> {
    let commit_batch_args =
        upload_content_and_assemble_sync_operations(canister, dirs, no_delete, logger).await?;
    let canister_api_version = api_version(canister).await;
    debug!(logger, "Canister API version: {canister_api_version}. ic-asset API version: {BATCH_UPLOAD_API_VERSION}");
    info!(logger, "Committing batch.");
    match canister_api_version {
        0 => {
            let commit_batch_args_v0 = v0::CommitBatchArguments::try_from(commit_batch_args).map_err(DowngradeV1TOV0Failed)?;
            warn!(logger, "The asset canister is running an old version of the API. It will not be able to set assets properties.");
            commit_batch(canister, commit_batch_args_v0).await
        }
        BATCH_UPLOAD_API_VERSION.. => commit_in_stages(canister, commit_batch_args, logger).await,
    }.map_err(CommitBatchFailed)
}

async fn commit_in_stages(
    canister: &Canister<'_>,
    commit_batch_args: CommitBatchArguments,
    logger: &Logger,
) -> Result<(), AgentError> {
    // Note that SetAssetProperties operations are only generated for assets that
    // already exist, since CreateAsset operations set all properties.
    let (set_properties_operations, other_operations): (Vec<_>, Vec<_>) = commit_batch_args
        .operations
        .into_iter()
        .partition(|op| matches!(op, BatchOperationKind::SetAssetProperties(_)));

    // This part seems reasonable in general as a separate batch
    for operations in set_properties_operations.chunks(500) {
        info!(logger, "Setting properties of {} assets.", operations.len());
        commit_batch(
            canister,
            CommitBatchArguments {
                batch_id: Nat::from(0_u8),
                operations: operations.into(),
            },
        )
        .await?
    }

    // Seen to work at 800 ({"SetAssetContent": 932, "Delete": 47, "CreateAsset": 58})
    // so 500 shouldn't exceed per-message instruction limit
    for operations in other_operations.chunks(500) {
        info!(
            logger,
            "Committing batch with {} operations.",
            operations.len()
        );
        commit_batch(
            canister,
            CommitBatchArguments {
                batch_id: Nat::from(0_u8),
                operations: operations.into(),
            },
        )
        .await?
    }

    // this just deletes the batch
    commit_batch(
        canister,
        CommitBatchArguments {
            batch_id: commit_batch_args.batch_id,
            operations: vec![],
        },
    )
    .await
}

/// Stage changes and propose the batch for commit.
pub async fn prepare_sync_for_proposal(
    canister: &Canister<'_>,
    dirs: &[&Path],
    logger: &Logger,
) -> Result<(Nat, ByteBuf), PrepareSyncForProposalError> {
    let arg = upload_content_and_assemble_sync_operations(canister, dirs, false, logger).await?;
    let arg = sort_batch_operations(arg);
    let batch_id = arg.batch_id.clone();

    info!(logger, "Preparing batch {}.", batch_id);
    propose_commit_batch(canister, arg)
        .await
        .map_err(PrepareSyncForProposalError::ProposeCommitBatch)?;

    let compute_evidence_arg = ComputeEvidenceArguments {
        batch_id: batch_id.clone(),
        max_iterations: Some(97), // 75% of max(130) = 97.5
    };
    info!(logger, "Computing evidence.");
    let evidence = loop {
        if let Some(evidence) = compute_evidence(canister, &compute_evidence_arg)
            .await
            .map_err(PrepareSyncForProposalError::ComputeEvidence)?
        {
            break evidence;
        }
    };

    info!(logger, "Proposed commit of batch {} with evidence {}.  Either commit it by proposal, or delete it.", batch_id, hex::encode(&evidence));

    Ok((batch_id, evidence))
}

fn sort_batch_operations(mut args: CommitBatchArguments) -> CommitBatchArguments {
    args.operations.sort();
    args
}

fn include_entry(entry: &walkdir::DirEntry, config: &AssetConfig) -> bool {
    if let Some(ignored) = config.ignore {
        !ignored
    } else if let Some(entry_name) = entry.file_name().to_str() {
        let is_known = if entry.path().is_dir() {
            KNOWN_DIRECTORIES.contains(&entry_name)
        } else {
            false
        };
        is_known || !entry_name.starts_with('.')
    } else {
        true
    }
}

pub(crate) fn gather_asset_descriptors(
    dirs: &[&Path],
    logger: &Logger,
) -> Result<Vec<AssetDescriptor>, GatherAssetDescriptorsError> {
    let mut asset_descriptors: HashMap<String, AssetDescriptor> = HashMap::new();
    for dir in dirs {
        let dir = dfx_core::fs::canonicalize(dir).map_err(InvalidSourceDirectory)?;
        let mut configuration =
            AssetSourceDirectoryConfiguration::load(&dir).map_err(LoadConfigFailed)?;
        let mut asset_descriptors_interim = vec![];
        let entries = WalkDir::new(&dir)
            .into_iter()
            .filter_entry(|entry| {
                if let Ok(canonical_path) = &dfx_core::fs::canonicalize(entry.path()) {
                    let config = configuration
                        .get_asset_config(canonical_path)
                        .unwrap_or_default();
                    include_entry(entry, &config)
                } else {
                    false
                }
            })
            .filter_map(|r| r.ok())
            .filter(|entry| {
                entry.file_type().is_file() && entry.file_name() != ASSETS_CONFIG_FILENAME_JSON
            })
            .collect::<Vec<_>>();

        for e in entries {
            let source = dfx_core::fs::canonicalize(e.path()).map_err(InvalidDirectoryEntry)?;
            let relative = source.strip_prefix(&dir).expect("cannot strip prefix");
            let key = String::from("/") + relative.to_string_lossy().as_ref();
            let config = configuration.get_asset_config(&source)?;

            asset_descriptors_interim.push(AssetDescriptor {
                source,
                key,
                config,
            })
        }

        for asset_descriptor in asset_descriptors_interim {
            if let Some(already_seen) = asset_descriptors.get(&asset_descriptor.key) {
                return Err(DuplicateAssetKey(
                    asset_descriptor.key.clone(),
                    Box::new(asset_descriptor.source.clone()),
                    Box::new(already_seen.source.clone()),
                ));
            }
            asset_descriptors.insert(asset_descriptor.key.clone(), asset_descriptor);
        }

        for (config_path, rules) in configuration.get_unused_configs() {
            warn!(
                logger,
                "{count} unmatched configuration{s} in {path}/.ic-assets.json config file:",
                count = rules.len(),
                s = if rules.len() > 1 { "s" } else { "" },
                path = config_path.display()
            );
            for rule in rules {
                warn!(logger, "{}", serde_json::to_string_pretty(&rule).unwrap());
            }
        }

        let no_policy_assets = asset_descriptors
            .values()
            .filter(|asset| asset.config.warn_about_no_security_policy())
            .collect_vec();
        if !no_policy_assets.is_empty() {
            warn!(
                logger,
                "This project does not define a security policy for some assets."
            );
            warn!(
                logger,
                "You should define a security policy in .ic-assets.json5. For example:"
            );
            warn!(logger, "[");
            warn!(logger, "  {{");
            warn!(logger, r#"    "match": "**/*","#);
            warn!(logger, r#"    "security_policy": "standard""#);
            warn!(logger, "  }}");
            warn!(logger, "]");

            if no_policy_assets.len() == asset_descriptors.len() {
                warn!(logger, "Assets without any security policy: all");
            } else {
                warn!(logger, "Assets without any security policy:");
                for asset in &no_policy_assets {
                    warn!(logger, "  - {}", asset.key);
                }
            }
        }
        let standard_policy_assets = asset_descriptors
            .values()
            .filter(|asset| asset.config.warn_about_standard_security_policy())
            .collect_vec();
        if !standard_policy_assets.is_empty() {
            warn!(logger, "This project uses the default security policy for some assets. While it is set up to work with many applications, it is recommended to further harden the policy to increase security against attacks like XSS.");
            warn!(logger, "To get started, have a look at 'dfx info canister-security-policy'. It shows the default security policy along with suggestions on how to improve it.");
            if standard_policy_assets.len() == asset_descriptors.len() {
                warn!(logger, "Unhardened assets: all");
            } else {
                warn!(logger, "Unhardened assets:");
                for asset in &standard_policy_assets {
                    warn!(logger, "  - {}", asset.key);
                }
            }
        }
        if !standard_policy_assets.is_empty() || !no_policy_assets.is_empty() {
            warn!(logger, "To disable the policy warning, define \"disable_security_policy_warning\": true in .ic-assets.json5.");
        }
        let missing_hardening_assets = asset_descriptors
            .values()
            .filter(|asset| asset.config.warn_about_missing_hardening_headers())
            .collect_vec();
        if !missing_hardening_assets.is_empty() {
            let mut error = String::new();
            if missing_hardening_assets.len() == asset_descriptors.len() {
                error.push_str("Unhardened assets: all");
            } else {
                error.push_str("Unhardened assets:");
                for asset in &missing_hardening_assets {
                    error.push_str(&format!("\n  - {}", asset.key));
                }
            }
            return Err(GatherAssetDescriptorsError::HardenedSecurityPolicyIsNotHardened(error));
        }
    }
    Ok(asset_descriptors.into_values().collect())
}

#[cfg(test)]
mod test_gathering_asset_descriptors_with_tempdir {

    use crate::asset::config::{CacheConfig, HeadersConfig};

    use super::AssetDescriptor;
    use std::{
        collections::HashMap,
        fs,
        path::{Path, PathBuf},
    };
    use tempfile::{Builder, TempDir};

    fn gather_asset_descriptors(dirs: &[&Path]) -> Vec<AssetDescriptor> {
        let logger = slog::Logger::root(slog::Discard, slog::o!());
        super::gather_asset_descriptors(dirs, &logger).unwrap()
    }

    impl AssetDescriptor {
        fn default_from_path(assets_dir: &Path, relative_path: &str) -> Self {
            let relative_path = relative_path.split('/').collect::<Vec<_>>();
            let relative_path = relative_path
                .iter()
                .fold(PathBuf::new(), |acc, x| acc.join(x));
            AssetDescriptor {
                source: assets_dir.join(&relative_path),
                key: format!("/{}", relative_path.to_str().unwrap()),
                config: Default::default(),
            }
        }
        fn with_headers(mut self, headers: HashMap<&str, &str>) -> Self {
            let headers = headers
                .into_iter()
                .map(|(k, v)| (k.to_string(), v.to_string()))
                .collect::<HeadersConfig>();
            let mut h = self.config.headers.unwrap_or_default();
            h.extend(headers);
            self.config.headers = Some(h);
            self
        }
        fn with_cache(mut self, cache: CacheConfig) -> Self {
            self.config.cache = Some(cache);
            self
        }
    }

    impl PartialEq for AssetDescriptor {
        fn eq(&self, other: &Self) -> bool {
            [
                self.source == other.source,
                self.key == other.key,
                self.config.cache == other.config.cache,
                self.config.headers == other.config.headers,
                self.config.ignore.unwrap_or(false) == other.config.ignore.unwrap_or(false),
            ]
            .into_iter()
            .all(|v| v)
        }
    }
    /// assets_tempdir directory structure:
    /// /assetsRAND5
    /// â”œâ”€â”€ .ic-assets.json
    /// â”œâ”€â”€ .hfile
    /// â”œâ”€â”€ file
    /// â”œâ”€- .hidden-dir
    /// â”‚  â”œâ”€â”€ .ic-assets.json
    /// â”‚  â”œâ”€â”€ .hfile
    /// â”‚  â”œâ”€â”€ file
    /// â”‚  â””â”€â”€ .hidden-dir-nested
    /// â”‚     â”œâ”€â”€ .ic-assets.json
    /// â”‚     â”œâ”€â”€ .hfile
    /// â”‚     â””â”€â”€ file
    /// â””â”€â”€ .hidden-dir-flat
    ///    â”œâ”€â”€ .ic-assets.json
    ///    â”œâ”€â”€ .hfile
    ///    â””â”€â”€ file
    fn create_temporary_assets_directory(modified_files: HashMap<PathBuf, String>) -> TempDir {
        let assets_tempdir = Builder::new()
            .prefix("assets")
            .rand_bytes(5)
            .tempdir()
            .unwrap();

        let mut default_files = HashMap::from([
            (Path::new(".ic-assets.json").to_path_buf(), "[]".to_string()),
            (Path::new(".hfile").to_path_buf(), "".to_string()),
            (Path::new("file").to_path_buf(), "".to_string()),
            (
                Path::new(".hidden-dir/.ic-assets.json").to_path_buf(),
                "[]".to_string(),
            ),
            (
                Path::new(".hidden-dir/.hfile").to_path_buf(),
                "".to_string(),
            ),
            (Path::new(".hidden-dir/file").to_path_buf(), "".to_string()),
            (
                Path::new(".hidden-dir/.hidden-dir-nested/.ic-assets.json").to_path_buf(),
                "[]".to_string(),
            ),
            (
                Path::new(".hidden-dir/.hidden-dir-nested/.hfile").to_path_buf(),
                "".to_string(),
            ),
            (
                Path::new(".hidden-dir/.hidden-dir-nested/file").to_path_buf(),
                "".to_string(),
            ),
            (
                Path::new(".hidden-dir-flat/.ic-assets.json").to_path_buf(),
                "[]".to_string(),
            ),
            (
                Path::new(".hidden-dir-flat/.hfile").to_path_buf(),
                "".to_string(),
            ),
            (
                Path::new(".hidden-dir-flat/file").to_path_buf(),
                "".to_string(),
            ),
        ]);
        default_files.extend(modified_files);

        for (k, v) in default_files {
            let path = assets_tempdir.path().join(k);
            fs::create_dir_all(path.parent().unwrap()).unwrap();
            fs::write(path, v).unwrap();
        }

        assets_tempdir
    }

    #[test]
    /// test gathering all files (including dotfiles in nested dotdirs)
    fn gather_all_files() {
        let files = HashMap::from([(
            Path::new(".ic-assets.json").to_path_buf(),
            r#"[
                {"match": ".*", "ignore": false}
            ]"#
            .to_string(),
        )]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors = vec![
            AssetDescriptor::default_from_path(&assets_dir, ".hfile"),
            AssetDescriptor::default_from_path(&assets_dir, "file"),
            AssetDescriptor::default_from_path(
                &assets_dir,
                ".hidden-dir/.hidden-dir-nested/.hfile",
            ),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/.hidden-dir-nested/file"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir-flat/.hfile"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir-flat/file"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/.hfile"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/file"),
        ];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[test]
    /// test gathering all non-dot files, from non-dot dirs
    fn gather_all_nondot_files_from_nondot_dirs() {
        let files = HashMap::from([(
            Path::new(".ic-assets.json").to_path_buf(),
            r#"[
                    {"match": ".*", "ignore": true}
                ]"#
            .to_string(),
        )]);
        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let asset_descriptors = gather_asset_descriptors(&[&assets_dir]);
        let expected_asset_descriptors =
            vec![AssetDescriptor::default_from_path(&assets_dir, "file")];
        assert_eq!(asset_descriptors, expected_asset_descriptors);

        // same but without the `ignore` flag (defaults to `true`)
        let files = HashMap::from([(
            Path::new(".ic-assets.json").to_path_buf(),
            r#"[
                    {"match": ".*"}
                ]"#
            .to_string(),
        )]);
        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let asset_descriptors = gather_asset_descriptors(&[&assets_dir]);
        let expected_asset_descriptors =
            vec![AssetDescriptor::default_from_path(&assets_dir, "file")];
        assert_eq!(asset_descriptors, expected_asset_descriptors);

        // different glob pattern
        let files = HashMap::from([(
            Path::new(".ic-assets.json").to_path_buf(),
            r#"[
                    {"match": "*"}
                ]"#
            .to_string(),
        )]);
        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let asset_descriptors = gather_asset_descriptors(&[&assets_dir]);
        let expected_asset_descriptors =
            vec![AssetDescriptor::default_from_path(&assets_dir, "file")];
        assert_eq!(asset_descriptors, expected_asset_descriptors);

        // different glob pattern
        let files = HashMap::from([(
            Path::new(".ic-assets.json").to_path_buf(),
            r#"[
                    {"match": "**/*"}
                ]"#
            .to_string(),
        )]);
        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let asset_descriptors = gather_asset_descriptors(&[&assets_dir]);
        let expected_asset_descriptors =
            vec![AssetDescriptor::default_from_path(&assets_dir, "file")];
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[cfg(target_family = "unix")]
    #[test]
    /// Cannot include files inside hidden directory using only config file
    /// inside hidden directory. Hidden directory has to be first included in
    /// config file sitting in parent dir.
    /// The behaviour will have to stay until this lands:
    /// https://github.com/BurntSushi/ripgrep/issues/2229
    fn failed_to_include_hidden_dir() {
        let files = HashMap::from([(
            Path::new(".hidden-dir/.ic-assets.json").to_path_buf(),
            r#"[
                    {"match": ".", "ignore": false},
                    {"match": "?", "ignore": false},
                    {"match": "*", "ignore": false},
                    {"match": "**", "ignore": false},
                    {"match": ".?", "ignore": false},
                    {"match": ".*", "ignore": false},
                    {"match": ".**", "ignore": false},
                    {"match": "./*", "ignore": false},
                    {"match": "./**", "ignore": false},
                    {"match": "./**/*", "ignore": false},
                    {"match": "./**/**", "ignore": false},
                    {"match": "../*", "ignore": false},
                    {"match": "../.*", "ignore": false},
                    {"match": "../.**", "ignore": false},
                    {"match": "../.**/*", "ignore": false},
                    {"match": ".hfile", "ignore": false},
                    {"match": "file", "ignore": false},
                    {"match": "file"}
                ]"#
            .to_string(),
        )]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors =
            vec![AssetDescriptor::default_from_path(&assets_dir, "file")];

        expected_asset_descriptors.sort_by_key(|v| v.key.clone());
        asset_descriptors.sort_by_key(|v| v.key.clone());

        assert_eq!(asset_descriptors, expected_asset_descriptors)
    }

    #[test]
    fn configuring_dotfiles_step_by_step() {
        let files = HashMap::from([
            (
                Path::new(".ic-assets.json").to_path_buf(),
                r#"[{"match": ".hidden-dir", "ignore": false}]"#.to_string(),
            ),
            (
                Path::new(".hidden-dir/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": ".hidden-dir-nested", "ignore": false},
                    {"match": ".*", "ignore": false, "headers": {"A": "z"}},
                    {"match": ".hfile", "headers": {"B": "y"}}
                ]"#
                .to_string(),
            ),
            (
                Path::new(".hidden-dir/.hidden-dir-nested/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "*", "ignore": false, "headers": {"C": "x"}},
                    {"match": ".hfile", "headers": {"D": "w"}}
                ]"#
                .to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors = vec![
            AssetDescriptor::default_from_path(&assets_dir, "file"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/.hfile")
                .with_headers(HashMap::from([("B", "y"), ("A", "z")])),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/file"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/.hidden-dir-nested/file")
                .with_headers(HashMap::from([("A", "z"), ("C", "x")])),
            AssetDescriptor::default_from_path(
                &assets_dir,
                ".hidden-dir/.hidden-dir-nested/.hfile",
            )
            .with_headers(HashMap::from([("D", "w"), ("A", "z"), ("C", "x")])),
        ];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors)
    }

    #[test]
    fn include_only_a_specific_dotfile() {
        let files = HashMap::from([
            (
                Path::new(".ic-assets.json").to_path_buf(),
                r#"[
                    {"match": ".hidden-dir", "ignore": false},
                    {"match": "file", "ignore": true}
                ]"#
                .to_string(),
            ),
            (
                Path::new(".hidden-dir/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "file", "ignore": true},
                    {"match": ".hidden-dir-nested", "ignore": false}
                ]"#
                .to_string(),
            ),
            (
                Path::new(".hidden-dir/.hidden-dir-nested/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "file", "ignore": true},
                    {"match": ".hfile", "ignore": false, "headers": {"D": "w"}}
                ]"#
                .to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors = vec![AssetDescriptor::default_from_path(
            &assets_dir,
            ".hidden-dir/.hidden-dir-nested/.hfile",
        )
        .with_headers(HashMap::from([("D", "w")]))];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[test]
    fn include_all_files_except_one() {
        let files = HashMap::from([
            (
                Path::new(".ic-assets.json").to_path_buf(),
                r#"[
                    {"match": ".*", "ignore": false}
                ]"#
                .to_string(),
            ),
            (
                Path::new(".hidden-dir/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "file", "ignore": true}
                ]"#
                .to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors = vec![
            AssetDescriptor::default_from_path(&assets_dir, "file"),
            AssetDescriptor::default_from_path(&assets_dir, ".hfile"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir-flat/file"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir-flat/.hfile"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/.hfile"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/.hidden-dir-nested/file"),
            AssetDescriptor::default_from_path(
                &assets_dir,
                ".hidden-dir/.hidden-dir-nested/.hfile",
            ),
        ];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[test]
    fn possible_to_reinclude_previously_ignored_file() {
        let files = HashMap::from([
            (
                Path::new(".ic-assets.json").to_path_buf(),
                r#"[
                    {"match": ".hidden-dir-flat", "ignore": false},
                    {"match": ".hidden-dir-flat/file", "ignore": true }

                ]"#
                .to_string(),
            ),
            (
                Path::new(".hidden-dir-flat/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "*", "ignore": false},
                    {"match": "file", "ignore": false}
                ]"#
                .to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors = vec![
            AssetDescriptor::default_from_path(&assets_dir, "file"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir-flat/.hfile"),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir-flat/file"),
        ];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[test]
    /// It is not possible to include a file if its parent directory has been excluded
    fn impossible_to_reinclude_file_from_already_ignored_directory() {
        let files = HashMap::from([
            // additional, non-dot dirs and files
            (Path::new("dir/file").to_path_buf(), "".to_string()),
            (Path::new("anotherdir/file").to_path_buf(), "".to_string()),
            (
                Path::new("anotherdir/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "file", "ignore": false}
                ]"#
                .to_string(),
            ),
            // end of additional, non-dot dirs and files
            (
                Path::new(".ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "anotherdir", "ignore": true}
                ]"#
                .to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors = vec![
            AssetDescriptor::default_from_path(&assets_dir, "file"),
            AssetDescriptor::default_from_path(&assets_dir, "dir/file"),
        ];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[test]
    fn known_directories_included_by_default() {
        let files = HashMap::from([
            // a typical use case of the .well-known folder
            (
                Path::new(".well-known/ic-domains").to_path_buf(),
                "foo.bar.com".to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors = vec![
            AssetDescriptor::default_from_path(&assets_dir, "file"),
            AssetDescriptor::default_from_path(&assets_dir, ".well-known/ic-domains"),
        ];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[test]
    fn known_directories_can_be_ignored() {
        let files = HashMap::from([
            // a typical use case of the .well-known folder
            (
                Path::new(".well-known/ic-domains").to_path_buf(),
                "foo.bar.com".to_string(),
            ),
            (
                Path::new(".ic-assets.json").to_path_buf(),
                r#"[
                    {"match": ".well-known", "ignore": true}
                ]"#
                .to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = dbg!(gather_asset_descriptors(&[&assets_dir]));

        let mut expected_asset_descriptors =
            vec![AssetDescriptor::default_from_path(&assets_dir, "file")];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(asset_descriptors, expected_asset_descriptors);
    }

    #[test]
    fn bonanza() {
        let files = HashMap::from([
            // additional, non-dot dirs and files
            (Path::new("dir/file").to_path_buf(), "".to_string()),
            (
                Path::new("dir/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "file", "headers": { "Access-Control-Allow-Origin": "null" }}
                ]"#
                .to_string(),
            ),
            (Path::new("anotherdir/file").to_path_buf(), "".to_string()),
            (
                Path::new("anotherdir/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "file", "cache": { "max_age": 42 }, "headers": null }
                ]"#
                .to_string(),
            ),
            // end of additional, non-dot dirs and files
            (
                Path::new(".ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "*", "cache": { "max_age": 11 }, "headers": { "X-Content-Type-Options": "nosniff" } },
                    {"match": "**/.hfile", "ignore": false, "headers": { "X-Content-Type-Options": "*" }},
                    {"match": ".hidden-dir-flat", "ignore": false },
                    {"match": ".hidden-dir", "ignore": false }

                ]"#
                .to_string(),
            ),
            (
                Path::new(".hidden-dir-flat/.ic-assets.json").to_path_buf(),
                r#"[
                    {"match": "*", "ignore": false, "headers": {"Cross-Origin-Resource-Policy": "same-origin"}},
                    {"match": ".hfile", "ignore": true}
                ]"#
                .to_string(),
            ),
        ]);

        let assets_temp_dir = create_temporary_assets_directory(files);
        let assets_dir = assets_temp_dir.path().canonicalize().unwrap();
        let mut asset_descriptors = gather_asset_descriptors(&[&assets_dir]);

        let mut expected_asset_descriptors = vec![
            AssetDescriptor::default_from_path(&assets_dir, ".hfile")
                .with_headers(HashMap::from([("X-Content-Type-Options", "*")]))
                .with_cache(CacheConfig { max_age: Some(11) }),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/.hfile")
                .with_headers(HashMap::from([("X-Content-Type-Options", "*")]))
                .with_cache(CacheConfig { max_age: Some(11) }),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir/file")
                .with_headers(HashMap::from([("X-Content-Type-Options", "nosniff")]))
                .with_cache(CacheConfig { max_age: Some(11) }),
            AssetDescriptor::default_from_path(&assets_dir, ".hidden-dir-flat/file")
                .with_headers(HashMap::from([("X-Content-Type-Options", "nosniff")]))
                .with_headers(HashMap::from([(
                    "Cross-Origin-Resource-Policy",
                    "same-origin",
                )]))
                .with_cache(CacheConfig { max_age: Some(11) }),
            AssetDescriptor::default_from_path(&assets_dir, "anotherdir/file")
                .with_cache(CacheConfig { max_age: Some(42) }),
            AssetDescriptor::default_from_path(&assets_dir, "dir/file")
                .with_headers(HashMap::from([("X-Content-Type-Options", "nosniff")]))
                .with_headers(HashMap::from([("Access-Control-Allow-Origin", "null")]))
                .with_cache(CacheConfig { max_age: Some(11) }),
            AssetDescriptor::default_from_path(&assets_dir, "file")
                .with_cache(CacheConfig { max_age: Some(11) })
                .with_headers(HashMap::from([("X-Content-Type-Options", "nosniff")])),
        ];

        expected_asset_descriptors.sort_by_key(|v| v.source.clone());
        asset_descriptors.sort_by_key(|v| v.source.clone());
        assert_eq!(dbg!(asset_descriptors), expected_asset_descriptors);
    }
}


-----------------------

/src/canisters/frontend/ic-asset/src/upload.rs:
-----------------------

use crate::asset::config::AssetConfig;
use crate::batch_upload::operations::BATCH_UPLOAD_API_VERSION;
use crate::batch_upload::{
    self,
    operations::AssetDeletionReason,
    plumbing::{make_project_assets, AssetDescriptor, ChunkUploader},
};
use crate::canister_api::methods::{
    api_version::api_version,
    batch::{commit_batch, create_batch},
    list::list_assets,
};
use crate::canister_api::types::batch_upload::v0;
use crate::error::CompatibilityError::DowngradeV1TOV0Failed;
use crate::error::UploadError;
use crate::error::UploadError::{CommitBatchFailed, CreateBatchFailed, ListAssetsFailed};
use ic_utils::Canister;
use slog::{info, Logger};
use std::collections::HashMap;
use std::path::PathBuf;

/// Upload the specified files
pub async fn upload(
    canister: &Canister<'_>,
    files: HashMap<String, PathBuf>,
    logger: &Logger,
) -> Result<(), UploadError> {
    let asset_descriptors: Vec<AssetDescriptor> = files
        .iter()
        .map(|x| AssetDescriptor {
            source: x.1.clone(),
            key: x.0.clone(),
            config: AssetConfig::default(),
        })
        .collect();

    let canister_assets = list_assets(canister).await.map_err(ListAssetsFailed)?;

    info!(logger, "Starting batch.");

    let batch_id = create_batch(canister).await.map_err(CreateBatchFailed)?;

    info!(logger, "Staging contents of new and changed assets:");

    let chunk_upload_target = ChunkUploader::new(canister.clone(), batch_id.clone());

    let project_assets = make_project_assets(
        Some(&chunk_upload_target),
        asset_descriptors,
        &canister_assets,
        logger,
    )
    .await?;

    let commit_batch_args = batch_upload::operations::assemble_commit_batch_arguments(
        project_assets,
        canister_assets,
        AssetDeletionReason::Incompatible,
        HashMap::new(),
        batch_id,
    );

    let canister_api_version = api_version(canister).await;
    info!(logger, "Committing batch.");
    match canister_api_version {
        0 => {
            let commit_batch_args_v0 = v0::CommitBatchArguments::try_from(commit_batch_args)
                .map_err(DowngradeV1TOV0Failed)?;
            commit_batch(canister, commit_batch_args_v0).await
        }
        BATCH_UPLOAD_API_VERSION.. => commit_batch(canister, commit_batch_args).await,
    }
    .map_err(CommitBatchFailed)
}


-----------------------

/src/canisters/frontend/ic-certified-assets/CHANGELOG.md:
-----------------------

# Changelog
All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.2.5] - 2022-08-22
### Removed 
- Support for asset caching based on [ETag](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag)
- Automatic redirection of all traffic from `.raw.ic0.app` domain to `.ic0.app`

## [0.2.4] - 2022-07-12
### Fixed
- headers field in Candid spec accepts mmultiple HTTP headers

## [0.2.3] - 2022-07-06
### Added
- Support for setting custom HTTP headers on asset creation 

## [0.2.2] - 2022-05-12
### Fixed
- Parse and produce ETag headers with quotes around the hash

## [0.2.1] - 2022-05-12
### Fixed
- Make StableState public again

## [0.2.0] - 2022-05-11
### Added
- Support for asset caching based on [ETag](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag)
- Support for asset caching based on [max-age](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)
- Automatic redirection of all traffic from `.raw.ic0.app` domain to `.ic0.app`

## [0.1.0] - 2022-02-02
### Added
- First release


-----------------------

/src/canisters/frontend/ic-certified-assets/Cargo.toml:
-----------------------

[package]
name = "ic-certified-assets"
version = "0.2.5"
authors.workspace = true
edition.workspace = true
repository.workspace = true
license.workspace = true
rust-version.workspace = true
description = "Rust support for asset certification."
documentation = "https://docs.rs/ic-certified-assets"
categories = ["wasm", "filesystem", "data-structures"]
keywords = ["internet-computer", "dfinity"]

[dependencies]
base64.workspace = true
candid.workspace = true
hex.workspace = true
ic-cdk.workspace = true
ic-certification = "2.3.0"
ic-response-verification = "2.3.0"
ic-representation-independent-hash = "2.3.0"
itertools.workspace = true
num-traits.workspace = true
serde.workspace = true
serde_bytes.workspace = true
serde_cbor.workspace = true
sha2.workspace = true
percent-encoding = "2.3.1"

[dev-dependencies]
ic-http-certification = "2.3.0"
candid_parser.workspace = true
anyhow.workspace = true
ic-response-verification-test-utils = { git = "https://github.com/dfinity/response-verification.git", rev = "a65009624b61736df6d2dc17756bdbd02a84f599" }
ic-certification-testing = { git = "https://github.com/dfinity/response-verification.git", rev = "a65009624b61736df6d2dc17756bdbd02a84f599" }
ic-crypto-tree-hash = { git = "https://github.com/dfinity/ic.git", rev = "1290256484f59c3d950c5e9a098e97383b248ad6" }


-----------------------

/src/canisters/frontend/ic-certified-assets/LICENSE:
-----------------------

../../../../LICENSE

-----------------------

/src/canisters/frontend/ic-certified-assets/README.md:
-----------------------

# Certified Assets Library

Rust support for asset certification.

Certified assets can also be served from any Rust canister by including this library.

## Adding to a canister

```
[dependencies]
ic-certified-assets = "0.2.2"
```

The assets are preserved over upgrades by including the corresponding functions in the `init/pre_upgrade/upgrade`
hooks which can be mixed with the other state from the canister:

```
#[derive(Clone, Debug, CandidType, Deserialize)]
struct StableState {
  my_state: MyState,
  assets: crate::assets::StableState,
}

#[init]
fn init() {
  crate::assets::init();
}

#[pre_upgrade]
fn pre_upgrade() {
  let stable_state = STATE.with(|s| StableState {
    my_state: s.my_state,
    assets: crate::assets::pre_upgrade(),
  });
  ic_cdk::storage::stable_save((stable_state,)).expect("failed to save stable state");
}

#[post_upgrade]
fn post_upgrade() {
  let (StableState { assets, my_state },): (StableState,) =
                                         ic_cdk::storage::stable_restore().expect("failed to restore stable state");
  crate::assets::post_upgrade(assets);
  STATE.with(|s| {
      s.my_state = my_state;
  };
}
```

## Uploading assets

```
cd assets
icx-asset --pem ~/.config/dfx/identity/default/identity.pem --replica https://icp0.io sync <canister_id> .
```


-----------------------

/src/canisters/frontend/ic-certified-assets/assets.did:
-----------------------

type BatchId = nat;
type ChunkId = nat;
type Key = text;
type Time = int;

type CreateAssetArguments = record {
  key: Key;
  content_type: text;
  max_age: opt nat64;
  headers: opt vec HeaderField;
  enable_aliasing: opt bool;
  allow_raw_access: opt bool;
};

// Add or change content for an asset, by content encoding
type SetAssetContentArguments = record {
  key: Key;
  content_encoding: text;
  chunk_ids: vec ChunkId;
  sha256: opt blob;
};

// Remove content for an asset, by content encoding
type UnsetAssetContentArguments = record {
  key: Key;
  content_encoding: text;
};

// Delete an asset
type DeleteAssetArguments = record {
  key: Key;
};

// Reset everything
type ClearArguments = record {};

type BatchOperationKind = variant {
  CreateAsset: CreateAssetArguments;
  SetAssetContent: SetAssetContentArguments;

  SetAssetProperties: SetAssetPropertiesArguments;

  UnsetAssetContent: UnsetAssetContentArguments;
  DeleteAsset: DeleteAssetArguments;

  Clear: ClearArguments;
};

type CommitBatchArguments = record {
  batch_id: BatchId;
  operations: vec BatchOperationKind
};

type CommitProposedBatchArguments = record {
  batch_id: BatchId;
  evidence: blob;
};

type ComputeEvidenceArguments = record {
  batch_id: BatchId;
  max_iterations: opt nat16
};

type DeleteBatchArguments = record {
  batch_id: BatchId;
};

type HeaderField = record { text; text; };

type HttpRequest = record {
  method: text;
  url: text;
  headers: vec HeaderField;
  body: blob;
  certificate_version: opt nat16;
};

type HttpResponse = record {
  status_code: nat16;
  headers: vec HeaderField;
  body: blob;
  streaming_strategy: opt StreamingStrategy;
};

type StreamingCallbackHttpResponse = record {
  body: blob;
  token: opt StreamingCallbackToken;
};

type StreamingCallbackToken = record {
  key: Key;
  content_encoding: text;
  index: nat;
  sha256: opt blob;
};

type StreamingStrategy = variant {
  Callback: record {
    callback: func (StreamingCallbackToken) -> (opt StreamingCallbackHttpResponse) query;
    token: StreamingCallbackToken;
  };
};

type SetAssetPropertiesArguments = record {
  key: Key;
  max_age: opt opt nat64;
  headers: opt opt vec HeaderField;
  allow_raw_access: opt opt bool;
  is_aliased: opt opt bool;
};

type ConfigurationResponse = record {
  max_batches: opt nat64;
  max_chunks: opt nat64;
  max_bytes: opt nat64;
};

type ConfigureArguments = record {
  max_batches: opt opt nat64;
  max_chunks: opt opt nat64;
  max_bytes: opt opt nat64;
};

type Permission = variant {
  Commit;
  ManagePermissions;
  Prepare;
};

type GrantPermission = record {
  to_principal: principal;
  permission: Permission;
};
type RevokePermission = record {
  of_principal: principal;
  permission: Permission;
};
type ListPermitted = record { permission: Permission };

type ValidationResult = variant { Ok : text; Err : text };

type AssetCanisterArgs = variant {
  Init: InitArgs;
  Upgrade: UpgradeArgs;
};

type InitArgs = record {};

type UpgradeArgs = record {
  set_permissions: opt SetPermissions;
};

/// Sets the list of principals granted each permission.
type SetPermissions = record {
  prepare: vec principal;
  commit: vec principal;
  manage_permissions: vec principal;
};

service: (asset_canister_args: opt AssetCanisterArgs) -> {
  api_version: () -> (nat16) query;

  get: (record {
    key: Key;
    accept_encodings: vec text;
  }) -> (record {
    content: blob; // may be the entirety of the content, or just chunk index 0
    content_type: text;
    content_encoding: text;
    sha256: opt blob; // sha256 of entire asset encoding, calculated by dfx and passed in SetAssetContentArguments
    total_length: nat; // all chunks except last have size == content.size()
  }) query;

  // if get() returned chunks > 1, call this to retrieve them.
  // chunks may or may not be split up at the same boundaries as presented to create_chunk().
  get_chunk: (record {
    key: Key;
    content_encoding: text;
    index: nat;
    sha256: opt blob;  // sha256 of entire asset encoding, calculated by dfx and passed in SetAssetContentArguments
  }) -> (record { content: blob }) query;

  list : (record {}) -> (vec record {
    key: Key;
    content_type: text;
    encodings: vec record {
      content_encoding: text;
      sha256: opt blob; // sha256 of entire asset encoding, calculated by dfx and passed in SetAssetContentArguments
      length: nat; // Size of this encoding's blob. Calculated when uploading assets.
      modified: Time;
    };
  }) query;

  certified_tree : (record {}) -> (record {
    certificate: blob;
    tree: blob;
  }) query;

  create_batch : (record {}) -> (record { batch_id: BatchId });

  create_chunk: (record { batch_id: BatchId; content: blob }) -> (record { chunk_id: ChunkId });
  create_chunks: (record { batch_id: BatchId; content: vec blob }) -> (record { chunk_ids: vec ChunkId });

  // Perform all operations successfully, or reject
  commit_batch: (CommitBatchArguments) -> ();

  // Save the batch operations for later commit
  propose_commit_batch: (CommitBatchArguments) -> ();

  // Given a batch already proposed, perform all operations successfully, or reject
  commit_proposed_batch: (CommitProposedBatchArguments) -> ();

  // Compute a hash over the CommitBatchArguments.  Call until it returns Some(evidence).
  compute_evidence: (ComputeEvidenceArguments) -> (opt blob);

  // Delete a batch that has been created, or proposed for commit, but not yet committed
  delete_batch: (DeleteBatchArguments) -> ();

  create_asset: (CreateAssetArguments) -> ();
  set_asset_content: (SetAssetContentArguments) -> ();
  unset_asset_content: (UnsetAssetContentArguments) -> ();

  delete_asset: (DeleteAssetArguments) -> ();

  clear: (ClearArguments) -> ();

  // Single call to create an asset with content for a single content encoding that
  // fits within the message ingress limit.
  store: (record {
    key: Key;
    content_type: text;
    content_encoding: text;
    content: blob;
    sha256: opt blob
  }) -> ();

  http_request: (request: HttpRequest) -> (HttpResponse) query;
  http_request_streaming_callback: (token: StreamingCallbackToken) -> (opt StreamingCallbackHttpResponse) query;

  authorize: (principal) -> ();
  deauthorize: (principal) -> ();
  list_authorized: () -> (vec principal);
  grant_permission: (GrantPermission) -> ();
  revoke_permission: (RevokePermission) -> ();
  list_permitted: (ListPermitted) -> (vec principal);
  take_ownership: () -> ();

  get_asset_properties : (key: Key) -> (record {
    max_age: opt nat64;
    headers: opt vec HeaderField;
    allow_raw_access: opt bool;
    is_aliased: opt bool; } ) query;
  set_asset_properties: (SetAssetPropertiesArguments) -> ();

  get_configuration: () -> (ConfigurationResponse);
  configure: (ConfigureArguments) -> ();

  validate_grant_permission: (GrantPermission) -> (ValidationResult);
  validate_revoke_permission: (RevokePermission) -> (ValidationResult);
  validate_take_ownership: () -> (ValidationResult);
  validate_commit_proposed_batch: (CommitProposedBatchArguments) -> (ValidationResult);
  validate_configure: (ConfigureArguments) -> (ValidationResult);
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/asset_certification/mod.rs:
-----------------------

use self::{
    tree::NestedTree,
    types::{
        certification::{AssetPath, HashTreePath, NestedTreeKey, RequestHash, WitnessResult},
        http::{
            build_ic_certificate_expression_from_headers, response_hash, HeaderField, FALLBACK_FILE,
        },
    },
};
use crate::asset_certification::types::http::build_ic_certificate_expression_header;
use ic_certification::merge_hash_trees;
use ic_representation_independent_hash::Value;
use serde::Serialize;
use sha2::Digest;

pub mod tree;
pub mod types;
pub use ic_certification::HashTree;

pub type CertifiedResponses = NestedTree<NestedTreeKey, Vec<u8>>;

impl CertifiedResponses {
    /// Certifies a response for a number of paths with certification v2.
    ///
    /// # Arguments
    /// * `paths`: path(s) to the resource
    /// * `status_code`: HTTP status code of the response
    /// * `headers`: All certified headers. It is possible to respond with additional headers, but only the ones supplied in this argument are certified
    /// * `body`: Response body. Ignored if `body_hash.is_some()`
    /// * `body_hash`: Hash of the response body. If supplied the response body will not be hashed, which can save a lot of computation
    ///
    /// # Return Value
    /// * `Vec<HashTreePath>`: `HashTreePath`s corresponding to the supplied `paths`. Can be used to remove or re-insert certification for a specific response without having to re-compute the full path
    pub fn certify_response(
        &mut self,
        paths: &[&str],
        status_code: u16,
        headers: &[(String, Value)],
        body: &[u8],
        body_hash: Option<[u8; 32]>,
    ) -> Vec<HashTreePath> {
        let certificate_expression = build_ic_certificate_expression_from_headers(headers);
        let request_hash = RequestHash::default(); // request certification currently not supported
        let body_hash = body_hash.unwrap_or_else(|| sha2::Sha256::digest(body).into());
        let response_hash = response_hash(headers, status_code, &body_hash);

        paths
            .iter()
            .map(|path| {
                let asset_path = AssetPath::from(path);
                let hash_tree_path = asset_path.hash_tree_path(
                    &certificate_expression,
                    &request_hash,
                    response_hash,
                );
                self.certify_response_precomputed(&hash_tree_path);
                hash_tree_path
            })
            .collect()
    }

    /// Certifies a response for a number of paths with certification v1.
    ///
    /// REPLACES a previously certified response for any given path because v1 certification only supports one certified response per path.
    ///
    /// # Arguments
    /// * `paths`: path(s) to the resource
    /// * `body`: Response body
    /// * `body_hash`: Hash of the response body. If supplied the response body will not be hashed, which can save a lot of computation
    pub fn certify_response_v1(
        &mut self,
        paths: &[&str],
        body: &[u8],
        body_hash: Option<[u8; 32]>,
    ) {
        let body_hash = body_hash.unwrap_or_else(|| sha2::Sha256::digest(body).into());
        let hash_tree_paths: Vec<HashTreePath> = paths
            .iter()
            .map(|path| {
                let asset_path = AssetPath::from(path);
                asset_path.asset_hash_path_v1()
            })
            .collect();

        for path in hash_tree_paths.iter() {
            self.insert(path.as_vec(), Vec::from(body_hash));
        }
    }

    /// Certifies a response that can be used if no certified response is available for the requested path with certification v2.
    ///
    /// # Arguments
    /// * `status_code`: HTTP status code of the response
    /// * `headers`: All certified headers. It is possible to respond with additional headers, but only the ones supplied in this argument are certified
    /// * `body`: Response body. Ignored if `body_hash.is_some()`
    /// * `body_hash`: Hash of the response body. If supplied the response body will not be hashed, which can save a lot of computation
    ///
    /// # Return Value
    /// * `HashTreePath`: `HashTreePath` corresponding to the supplied response. Can be used to remove or re-insert certification for this specific response without having to re-compute the full path
    pub fn certify_fallback_response(
        &mut self,
        status_code: u16,
        headers: &[(String, Value)],
        body: &[u8],
        body_hash: Option<[u8; 32]>,
    ) -> HashTreePath {
        let certificate_expression = build_ic_certificate_expression_from_headers(headers);
        let cert_expr_header = build_ic_certificate_expression_header(&certificate_expression);
        let cert_expr_header = (cert_expr_header.0, Value::String(cert_expr_header.1));
        let mut certified_headers = Vec::from(headers);
        certified_headers.push(cert_expr_header);
        let request_hash = RequestHash::default(); // request certification currently not supported
        let body_hash = body_hash.unwrap_or_else(|| sha2::Sha256::digest(body).into());
        let response_hash = response_hash(&certified_headers, status_code, &body_hash);

        let asset_path = AssetPath::fallback_path();
        let hash_tree_path =
            asset_path.hash_tree_path(&certificate_expression, &request_hash, response_hash);
        self.certify_response_precomputed(&hash_tree_path);
        hash_tree_path
    }

    /// Certifies a response that can be used if no certified response is available for the requested path with certification v1.
    /// Alias for `self.certify_response_v1(&["/index.html"], body, body_hash)`.
    ///
    /// REPLACES a previously certified response because v1 certification only supports one certified response per path.
    pub fn certify_fallback_response_v1(&mut self, body: &[u8], body_hash: Option<[u8; 32]>) {
        self.certify_response_v1(&[FALLBACK_FILE], body, body_hash)
    }

    /// Certifies a response. Expects a finished `HashTreePath`, skipping the (sometimes expensive) computation of the `HashTreePath`.
    pub fn certify_response_precomputed(&mut self, path: &HashTreePath) {
        self.insert(path.as_vec(), Vec::new());
    }

    /// Removes all certified responses for a path for certification v2
    pub fn remove_responses_for_path(&mut self, path: &str) {
        let key = AssetPath::from(path);
        self.delete(key.asset_hash_path_root_v2().as_vec());
    }

    /// Removes the certified response for a path for certification v1
    pub fn remove_responses_for_path_v1(&mut self, path: &str) {
        let key = AssetPath::from(path);
        self.delete(key.asset_hash_path_v1().as_vec());
    }

    /// Removes all certified fallback responses for certification v2
    pub fn remove_fallback_responses(&mut self) {
        self.delete(HashTreePath::not_found_base_path_v2().as_vec());
    }

    /// Removes the certified fallback response for certification v1
    pub fn remove_fallback_responses_v1(&mut self) {
        self.delete(HashTreePath::not_found_base_path_v1().as_vec());
    }

    /// Removes a specific response from the certified responses. Expects a finished `HashTreePath`, skipping the (sometimes expensive) computation of the `HashTreePath`.
    pub fn remove_response_precomputed(&mut self, path: &HashTreePath) {
        self.delete(path.as_vec());
    }

    /// If the path has certified responses this function creates a hash tree that proves...
    /// * The path is part of the CertifiedResponses hash tree
    /// The hash tree then includes certification for all valid responses for this path.
    ///
    /// If the path has no certified responses this function creates a hash tree that proves...
    /// * The absence of the path in the CertifiedResponses hash tree
    /// * The presence/absence of a 404 response
    /// The hash tree then includes certification for all valid responses for a 404 response.
    ///
    /// # Return Value
    /// `(found, tree)`
    /// * `found`:
    ///   * WitnessResult::Found if `path` has a certified response.
    ///   * `WitnessResult::FallbackFound` if the path has no certified response, but the fallback path has.
    ///   * `WitnessResult::NoneFound` if both `path` and the fallback path have no certified response.
    /// * `tree`: The `HashTree` as described above.
    pub fn witness_path(&self, path: &str) -> (HashTree, WitnessResult) {
        let path = AssetPath::from(path);
        let hash_tree_path_root = path.asset_hash_path_root_v2();
        if self.contains_path(hash_tree_path_root.as_vec()) {
            (
                self.witness(hash_tree_path_root.as_vec()),
                WitnessResult::PathFound,
            )
        } else {
            let absence_proof = self.witness(hash_tree_path_root.as_vec());
            let fallback_paths = hash_tree_path_root.fallback_paths_v2();

            let combined_proof =
                fallback_paths
                    .into_iter()
                    .fold(absence_proof, |accumulator, path| {
                        let new_proof = self.witness(path.as_vec());
                        merge_hash_trees(accumulator, new_proof)
                    });

            let fallback_path = HashTreePath::not_found_base_path_v2();
            if self.contains_path(fallback_path.as_vec()) {
                (combined_proof, WitnessResult::FallbackFound)
            } else {
                (combined_proof, WitnessResult::NoneFound)
            }
        }
    }

    pub fn expr_path(&self, path: &str) -> String {
        let path = AssetPath::from(path);
        let hash_tree_path_root = path.asset_hash_path_root_v2();
        if self.contains_path(hash_tree_path_root.as_vec()) {
            path.asset_hash_path_root_v2().expr_path()
        } else {
            HashTreePath::not_found_base_path_v2().expr_path()
        }
    }

    /// If the path has certified responses this function creates a hash tree that proves...
    /// * The path is part of the CertifiedResponses hash tree
    /// The hash tree then includes certification the valid certification v1 response for this path.
    ///
    /// If the path has no certified response this function creates a hash tree that proves...
    /// * The absence of the path in the CertifiedResponses hash tree
    /// * The presence/absence of a 404 response
    /// The hash tree then includes certification for the valid certification v1 response for a 404 response.
    ///
    /// # Arguments
    /// * `path`: The path to generate a proof of presence/absence for
    /// * `not_found_path`: The defined fall-back path to use if `path` is not certified by v1
    ///
    /// # Return Value
    /// `(found, tree)`
    /// * `found`:
    ///   * WitnessResult::Found if `path` has a certified response.
    ///   * `WitnessResult::FallbackFound` if the path has no certified response, but the fallback path has.
    ///   * `WitnessResult::NoneFound` if both `path` and the fallback path have no certified response.
    /// * `tree`: The `HashTree` as described above.
    pub fn witness_path_v1(&self, path: &str) -> (HashTree, WitnessResult) {
        let path = AssetPath::from(path);
        let hash_tree_path = path.asset_hash_path_v1();
        if self.contains_leaf(hash_tree_path.as_vec()) {
            (
                self.witness(hash_tree_path.as_vec()),
                WitnessResult::PathFound,
            )
        } else {
            let fallback_path = AssetPath::fallback_path_v1().asset_hash_path_v1();

            let absence_proof = self.witness(hash_tree_path.as_vec());
            let not_found_proof = self.witness(fallback_path.as_vec());
            let combined_proof = merge_hash_trees(absence_proof, not_found_proof);

            if self.contains_leaf(fallback_path.as_vec()) {
                (combined_proof, WitnessResult::FallbackFound)
            } else {
                (combined_proof, WitnessResult::NoneFound)
            }
        }
    }

    /// Same as `witness_path`, but produces a header that can be returned as a `HttpResponse` header instead of a witness `HashTree`.
    pub fn witness_to_header(
        &self,
        path: &str,
        certificate: &[u8],
    ) -> (HeaderField, WitnessResult) {
        let (witness, witness_result) = self.witness_path(path);
        let expr_path = self.expr_path(path);

        let mut serializer = serde_cbor::ser::Serializer::new(vec![]);
        serializer.self_describe().unwrap();
        witness.serialize(&mut serializer).unwrap();

        (
            (
                "IC-Certificate".to_string(),
                String::from("version=2, ")
                    + "certificate=:"
                    + &base64::encode(certificate)
                    + ":, tree=:"
                    + &base64::encode(serializer.into_inner())
                    + ":, expr_path=:"
                    + &expr_path
                    + ":",
            ),
            witness_result,
        )
    }

    /// Same as `witness_path`, but produces a header that can be returned as a `HttpResponse` header instead of a witness `HashTree`.
    pub fn witness_to_header_v1(
        &self,
        path: &str,
        certificate: &[u8],
    ) -> (HeaderField, WitnessResult) {
        let (witness, witness_result) = self.witness_path_v1(path);
        let mut serializer = serde_cbor::ser::Serializer::new(vec![]);
        serializer.self_describe().unwrap();
        witness.serialize(&mut serializer).unwrap();
        (
            (
                "IC-Certificate".to_string(),
                String::from("certificate=:")
                    + &base64::encode(certificate)
                    + ":, tree=:"
                    + &base64::encode(serializer.into_inner())
                    + ":",
            ),
            witness_result,
        )
    }
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/asset_certification/tree.rs:
-----------------------

use ic_certification::{AsHashTree, HashTree, RbTree};

pub trait NestedTreeKeyRequirements: Clone + AsRef<[u8]> + 'static {}
pub trait NestedTreeValueRequirements: AsHashTree + 'static {}
impl<T> NestedTreeKeyRequirements for T where T: Clone + AsRef<[u8]> + 'static {}
impl<T> NestedTreeValueRequirements for T where T: AsHashTree + 'static {}

#[derive(Debug, Clone)]
pub enum NestedTree<K: NestedTreeKeyRequirements, V: NestedTreeValueRequirements> {
    Leaf(V),
    Nested(RbTree<K, NestedTree<K, V>>),
}

impl<K: NestedTreeKeyRequirements, V: NestedTreeValueRequirements> Default for NestedTree<K, V> {
    fn default() -> Self {
        NestedTree::Nested(RbTree::<K, NestedTree<K, V>>::new())
    }
}

impl<K: NestedTreeKeyRequirements, V: NestedTreeValueRequirements> AsHashTree for NestedTree<K, V> {
    fn root_hash(&self) -> ic_certification::Hash {
        match self {
            NestedTree::Leaf(a) => a.root_hash(),
            NestedTree::Nested(tree) => tree.root_hash(),
        }
    }

    fn as_hash_tree(&self) -> HashTree {
        match self {
            NestedTree::Leaf(a) => a.as_hash_tree(),
            NestedTree::Nested(tree) => tree.as_hash_tree(),
        }
    }
}

impl<K: NestedTreeKeyRequirements, V: NestedTreeValueRequirements> NestedTree<K, V> {
    #[allow(dead_code)]
    pub fn get(&self, path: &[K]) -> Option<&V> {
        if let Some(key) = path.first() {
            match self {
                NestedTree::Leaf(_) => None,
                NestedTree::Nested(tree) => tree
                    .get(key.as_ref())
                    .and_then(|child| child.get(&path[1..])),
            }
        } else {
            match self {
                NestedTree::Leaf(value) => Some(value),
                NestedTree::Nested(_) => None,
            }
        }
    }

    /// Returns true if there is a leaf at the specified path
    pub fn contains_leaf(&self, path: &[K]) -> bool {
        if let Some(key) = path.first() {
            match self {
                NestedTree::Leaf(_) => false,
                NestedTree::Nested(tree) => tree
                    .get(key.as_ref())
                    .map(|child| child.contains_leaf(&path[1..]))
                    .unwrap_or(false),
            }
        } else {
            matches!(self, NestedTree::Leaf(_))
        }
    }

    /// Returns true if there is a leaf or a subtree at the specified path
    pub fn contains_path(&self, path: &[K]) -> bool {
        if let Some(key) = path.first() {
            match self {
                NestedTree::Leaf(_) => false,
                NestedTree::Nested(tree) => tree
                    .get(key.as_ref())
                    .map(|child| child.contains_path(&path[1..]))
                    .unwrap_or(false),
            }
        } else {
            true
        }
    }

    pub fn insert(&mut self, path: &[K], value: V) {
        if let Some(key) = path.first() {
            match self {
                NestedTree::Leaf(_) => {
                    *self = NestedTree::default();
                    self.insert(path, value);
                }
                NestedTree::Nested(tree) => {
                    if tree.get(key.as_ref()).is_some() {
                        tree.modify(key.as_ref(), |child| child.insert(&path[1..], value));
                    } else {
                        tree.insert(key.clone(), NestedTree::default());
                        self.insert(path, value);
                    }
                }
            }
        } else {
            *self = NestedTree::Leaf(value);
        }
    }

    pub fn delete(&mut self, path: &[K]) {
        if let Some(key) = path.first() {
            match self {
                NestedTree::Leaf(_) => {}
                NestedTree::Nested(tree) => {
                    tree.modify(key.as_ref(), |child| child.delete(&path[1..]));
                }
            }
        } else {
            *self = NestedTree::default();
        }
    }

    pub fn witness(&self, path: &[K]) -> HashTree {
        if let Some(key) = path.first() {
            match self {
                NestedTree::Leaf(value) => value.as_hash_tree(),
                NestedTree::Nested(tree) => {
                    tree.nested_witness(key.as_ref(), |tree| tree.witness(&path[1..]))
                }
            }
        } else {
            self.as_hash_tree()
        }
    }
}

#[test]
fn nested_tree_operation() {
    let mut tree: NestedTree<&str, Vec<u8>> = NestedTree::default();
    // insertion
    tree.insert(&["one", "two"], vec![2]);
    tree.insert(&["one", "three"], vec![3]);
    assert_eq!(tree.get(&["one", "two"]), Some(&vec![2]));
    assert_eq!(tree.get(&["one", "two", "three"]), None);
    assert_eq!(tree.get(&["one"]), None);
    assert!(tree.contains_leaf(&["one", "two"]));
    assert!(tree.contains_path(&["one"]));
    assert!(!tree.contains_leaf(&["one", "two", "three"]));
    assert!(!tree.contains_path(&["one", "two", "three"]));
    assert!(!tree.contains_leaf(&["one"]));

    // deleting non-existent key doesn't do anything
    tree.delete(&["one", "two", "three"]);
    assert_eq!(tree.get(&["one", "two"]), Some(&vec![2]));
    assert!(tree.contains_leaf(&["one", "two"]));

    // deleting existing key works
    tree.delete(&["one", "three"]);
    assert_eq!(tree.get(&["one", "two"]), Some(&vec![2]));
    assert_eq!(tree.get(&["one", "three"]), None);
    assert!(tree.contains_leaf(&["one", "two"]));
    assert!(!tree.contains_leaf(&["one", "three"]));

    // deleting subtree works
    tree.delete(&["one"]);
    assert_eq!(tree.get(&["one", "two"]), None);
    assert_eq!(tree.get(&["one"]), None);
    assert!(!tree.contains_leaf(&["one", "two"]));
    assert!(!tree.contains_leaf(&["one"]));
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/asset_certification/types/certification.rs:
-----------------------

use super::{
    http::{build_ic_certificate_expression_from_headers, FALLBACK_FILE},
    rc_bytes::RcBytes,
};
use candid::{CandidType, Deserialize};
use ic_representation_independent_hash::Value;
use serde_cbor::ser::IoWrite;
use serde_cbor::Serializer;
use sha2::Digest;
use std::borrow::Borrow;

pub type AssetKey = String;

#[derive(Default, Clone, Debug, CandidType, Deserialize)]
pub struct CertificateExpression {
    pub expression: String,
    pub expression_hash: [u8; 32],
}

#[derive(Clone, Debug, CandidType, Deserialize, Default)]
pub struct RequestHash(Option<[u8; 32]>);

#[derive(Default, Clone, Copy, Debug, CandidType, Deserialize)]
pub struct ResponseHash(pub [u8; 32]);

impl<T> From<T> for ResponseHash
where
    T: Borrow<[u8; 32]>,
{
    fn from(hash: T) -> Self {
        ResponseHash(*hash.borrow())
    }
}

/// AssetKey that has been split into segments.
/// E.g. `["foo", "index.html"]`
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct AssetPath(pub Vec<AssetKey>);

impl<T> From<T> for AssetPath
where
    T: AsRef<str>,
{
    fn from(key: T) -> Self {
        let mut iter = key.as_ref().split('/').peekable();
        if let Some(first_segment) = iter.peek() {
            // "/path/to/asset".split("/") produces an empty node before "path", therefore we need to skip it
            if first_segment.is_empty() {
                iter.next();
            }
        }
        Self(iter.map(|segment| segment.to_string()).collect())
    }
}

impl AssetPath {
    pub fn reconstruct_asset_key(&self) -> AssetKey {
        // this reconstructs "" as "/", but this is not a problem because no http client actually requests ""
        format!("/{}", self.0.join("/"))
    }

    pub fn asset_hash_path_v1(&self) -> HashTreePath {
        HashTreePath(vec![
            "http_assets".into(),
            self.reconstruct_asset_key().into(),
        ])
    }

    pub fn asset_hash_path_root_v2(&self) -> HashTreePath {
        let mut hash_path: Vec<NestedTreeKey> = self
            .0
            .iter()
            .map(|segment| segment.as_str().into())
            .collect();
        hash_path.push("<$>".into());
        hash_path.insert(0, "http_expr".into());
        HashTreePath(hash_path)
    }

    pub fn hash_tree_path(
        &self,
        certificate_expression: &CertificateExpression,
        RequestHash(maybe_request_hash): &RequestHash,
        ResponseHash(response_hash): ResponseHash,
    ) -> HashTreePath {
        let mut hash_path: Vec<NestedTreeKey> = vec![];
        if matches!(self.0.last(), Some(segment) if segment == "<*>") {
            // it's a v2 fallback path
            hash_path.push("http_expr".into());
            hash_path.push("<*>".into());
        } else {
            hash_path.push("http_expr".into());
            hash_path = self.0.iter().fold(hash_path, |mut path, s| {
                path.push(s.as_str().into());
                path
            });
            hash_path.push("<$>".into()); // asset path terminator
        };
        hash_path.push(certificate_expression.expression_hash.into());
        hash_path.push(
            maybe_request_hash
                .map(|request_hash| request_hash.into())
                .unwrap_or_else(|| "".into()),
        );
        hash_path.push(NestedTreeKey::Hash(response_hash));
        HashTreePath(hash_path)
    }

    pub fn fallback_path() -> Self {
        Self(vec!["http_expr".into(), "<*>".into()])
    }

    pub fn fallback_path_v1() -> Self {
        Self::from(FALLBACK_FILE)
    }
}

/// AssetPath that is ready to be inserted into asset_hashes.
/// E.g. `["http_expr", "foo", "index.html", "<$>", "<expr_hash>", "<request hash>", "<response_hash>"]`
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct HashTreePath(pub Vec<NestedTreeKey>);

impl From<Vec<NestedTreeKey>> for HashTreePath {
    fn from(vec: Vec<NestedTreeKey>) -> Self {
        Self(vec)
    }
}

impl HashTreePath {
    pub fn as_vec(&self) -> &Vec<NestedTreeKey> {
        &self.0
    }

    pub fn expr_path(&self) -> String {
        let strings = self
            .0
            .iter()
            .map(|key| match key {
                NestedTreeKey::String(k) => k.clone(),
                NestedTreeKey::Bytes(b) => hex::encode(b),
                NestedTreeKey::Hash(h) => hex::encode(h),
            })
            .collect::<Vec<String>>();
        let cbor = serialize_cbor_self_describing(&strings);
        base64::encode(cbor)
    }

    /// Produces all `HashTreePath`s required to prove
    /// - whether or not fallback file exists and
    /// - that there is no fallback file with higher priority
    /// in the hash tree.
    pub fn fallback_paths_v2(&self) -> Vec<Self> {
        let mut paths = Vec::new();

        // starting at 1 because "http_expr" is always the starting element
        for i in 1..self.0.len() {
            let mut without_trailing_slash: Vec<NestedTreeKey> = self.0.as_slice()[0..i].into();
            let mut with_trailing_slash = without_trailing_slash.clone();
            without_trailing_slash.push("<*>".into());
            with_trailing_slash.push("".into());
            with_trailing_slash.push("<*>".into());

            paths.push(without_trailing_slash.into());
            paths.push(with_trailing_slash.into());
        }

        paths
    }

    pub fn new(
        asset: &str,
        status_code: u16,
        headers: &[(String, Value)],
        body: &RcBytes,
        body_hash: Option<[u8; 32]>,
    ) -> Self {
        Self::from_parts(asset, status_code, headers, body, body_hash, None, None)
    }

    pub fn from_parts(
        path: &str,
        status_code: u16,
        headers: &[(String, Value)],
        body: &[u8],
        body_hash: Option<[u8; 32]>,
        certificate_expression: Option<&CertificateExpression>,
        response_hash: Option<ResponseHash>,
    ) -> Self {
        let certificate_expression = certificate_expression
            .cloned()
            .unwrap_or_else(|| build_ic_certificate_expression_from_headers(headers));
        let request_hash = RequestHash::default(); // request certification currently not supported
        let body_hash = body_hash.unwrap_or_else(|| sha2::Sha256::digest(body).into());
        let response_hash = response_hash
            .unwrap_or_else(|| super::http::response_hash(headers, status_code, &body_hash));

        AssetPath::from(path).hash_tree_path(&certificate_expression, &request_hash, response_hash)
    }

    pub fn not_found_base_path_v2() -> Self {
        HashTreePath::from(Vec::from([
            NestedTreeKey::String("http_expr".into()),
            NestedTreeKey::String("<*>".into()),
        ]))
    }

    pub fn not_found_base_path_v1() -> Self {
        let not_found_path = AssetPath::from(FALLBACK_FILE);
        not_found_path.asset_hash_path_v1()
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum NestedTreeKey {
    String(String),
    Bytes(Vec<u8>),
    Hash([u8; 32]),
}

impl AsRef<[u8]> for NestedTreeKey {
    fn as_ref(&self) -> &[u8] {
        match self {
            NestedTreeKey::String(s) => s.as_bytes(),
            NestedTreeKey::Bytes(b) => b.as_slice(),
            NestedTreeKey::Hash(h) => h,
        }
    }
}

impl From<&str> for NestedTreeKey {
    fn from(s: &str) -> Self {
        Self::String(s.into())
    }
}

impl From<&[u8]> for NestedTreeKey {
    fn from(slice: &[u8]) -> Self {
        Self::Bytes(slice.to_vec())
    }
}

impl From<[u8; 32]> for NestedTreeKey {
    fn from(hash: [u8; 32]) -> Self {
        Self::Hash(hash)
    }
}

impl From<String> for NestedTreeKey {
    fn from(s: String) -> Self {
        Self::String(s)
    }
}

fn serialize_cbor_self_describing<T>(value: &T) -> Vec<u8>
where
    T: serde::Serialize,
{
    let mut vec = Vec::new();
    let mut binding = IoWrite::new(&mut vec);
    let mut s = Serializer::new(&mut binding);
    s.self_describe()
        .expect("Cannot produce self-describing cbor.");
    value
        .serialize(&mut s)
        .expect("Failed to serialize self-describing CBOR.");
    vec
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum WitnessResult {
    PathFound,
    FallbackFound,
    NoneFound,
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/asset_certification/types/http.rs:
-----------------------

use super::rc_bytes::RcBytes;
use crate::{
    asset_certification::types::certification::{CertificateExpression, ResponseHash},
    state_machine::{encoding_certification_order, Asset, AssetEncoding},
};
use candid::{define_function, CandidType, Deserialize, Nat};
use ic_certification::Hash;
use ic_representation_independent_hash::{representation_independent_hash, Value};
use serde_bytes::ByteBuf;
use sha2::Digest;

/// The file to serve if the requested file wasn't found.
pub const FALLBACK_FILE: &str = "/index.html";

const HTTP_REDIRECT_PERMANENT: u16 = 308;

pub const IC_CERTIFICATE_EXPRESSION_VALUE: &str = r#"default_certification(ValidationArgs{certification: Certification{no_request_certification: Empty{}, response_certification: ResponseCertification{certified_response_headers: ResponseHeaderList{headers: ["content-type"{headers}]}}}})"#;

pub type HeaderField = (String, String);

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct HttpRequest {
    pub method: String,
    pub url: String,
    pub headers: Vec<HeaderField>,
    pub body: ByteBuf,
    pub certificate_version: Option<u16>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct HttpResponse {
    pub status_code: u16,
    pub headers: Vec<HeaderField>,
    pub body: RcBytes,
    pub upgrade: Option<bool>,
    pub streaming_strategy: Option<StreamingStrategy>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct StreamingCallbackToken {
    pub key: String,
    pub content_encoding: String,
    pub index: Nat,
    // We don't care about the sha, we just want to be backward compatible.
    pub sha256: Option<ByteBuf>,
}

define_function!(pub CallbackFunc : (StreamingCallbackToken) -> (StreamingCallbackHttpResponse) query);
#[derive(Clone, Debug, CandidType, Deserialize)]
pub enum StreamingStrategy {
    Callback {
        callback: CallbackFunc,
        token: StreamingCallbackToken,
    },
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct StreamingCallbackHttpResponse {
    pub body: RcBytes,
    pub token: Option<StreamingCallbackToken>,
}

impl StreamingCallbackToken {
    pub fn create_token(
        enc_name: &str,
        content_chunks_count: usize,
        content_sha256: [u8; 32],
        key: &str,
        chunk_index: usize,
    ) -> Option<Self> {
        if chunk_index + 1 >= content_chunks_count {
            None
        } else {
            Some(StreamingCallbackToken {
                key: key.to_string(),
                content_encoding: enc_name.to_string(),
                index: Nat::from(chunk_index + 1),
                sha256: Some(ByteBuf::from(content_sha256)),
            })
        }
    }
}

impl HttpRequest {
    pub fn get_path(&self) -> &str {
        match self.url.find('?') {
            Some(i) => &self.url[..i],
            None => &self.url[..],
        }
    }

    pub fn get_header_value(&self, header_key: &str) -> Option<&String> {
        self.headers
            .iter()
            .find_map(|(k, v)| k.eq_ignore_ascii_case(header_key).then_some(v))
    }

    // Spec:
    // If not set: assume version 1.
    // If available: use requested certificate version.
    // If requested version is not available: use latest available version.
    pub fn get_certificate_version(&self) -> u16 {
        if self.certificate_version.is_none() || self.certificate_version == Some(1) {
            1
        } else {
            2 // latest available
        }
    }

    pub fn redirect_from_raw_to_certified_domain(&self) -> HttpResponse {
        #[cfg(not(test))]
        let canister_id = ic_cdk::api::id().to_text();
        #[cfg(test)]
        let canister_id = self.get_canister_id();

        let location = match self.get_header_value("Host") {
            Some(host_header) if host_header.ends_with("ic0.app") => {
                format!("https://{canister_id}.ic0.app{path}", path = self.url)
            }
            _ => format!("https://{canister_id}.icp0.io{path}", path = self.url),
        };
        HttpResponse::build_redirect(HTTP_REDIRECT_PERMANENT, location)
    }

    #[cfg(test)]
    pub fn get_canister_id(&self) -> &str {
        if let Some(host_header) = self.get_header_value("Host") {
            if host_header.contains(".localhost")
                || host_header.contains(".io")
                || host_header.contains(".app")
            {
                return host_header.split('.').next().unwrap();
            } else if let Some(t) = self.url.split("canisterId=").nth(1) {
                let x = t.split_once('&');
                if let Some(c) = x {
                    return c.0;
                }
            }
        }
        unreachable!()
    }

    pub fn is_raw_domain(&self) -> bool {
        if let Some(host_header) = self.get_header_value("Host") {
            host_header.contains(".raw.ic")
        } else {
            false
        }
    }
}

impl HttpResponse {
    #[allow(clippy::too_many_arguments)]
    pub fn build_ok(
        asset: &Asset,
        enc_name: &str,
        enc: &AssetEncoding,
        key: &str,
        chunk_index: usize,
        certificate_header: Option<&HeaderField>,
        callback: &CallbackFunc,
        etags: &[Hash],
        cert_version: u16,
    ) -> HttpResponse {
        let mut headers = asset.get_headers_for_asset(enc_name, cert_version);
        if let Some(head) = certificate_header {
            headers.insert(head.0.clone(), head.1.clone());
        }

        let streaming_strategy = StreamingCallbackToken::create_token(
            enc_name,
            enc.content_chunks.len(),
            enc.sha256,
            key,
            chunk_index,
        )
        .map(|token| StreamingStrategy::Callback {
            callback: callback.clone(),
            token,
        });

        let (status_code, body) = if etags.contains(&enc.sha256) {
            (304, RcBytes::default())
        } else {
            if !headers
                .iter()
                .any(|(header_name, _)| header_name.eq_ignore_ascii_case("etag"))
            {
                headers.insert(
                    "etag".to_string(),
                    format!("\"{}\"", hex::encode(enc.sha256)),
                );
            }
            (200, enc.content_chunks[chunk_index].clone())
        };

        HttpResponse {
            status_code,
            headers: headers.into_iter().collect::<_>(),
            body,
            upgrade: None,
            streaming_strategy,
        }
    }

    #[allow(clippy::too_many_arguments)]
    pub fn build_ok_from_requested_encodings(
        asset: &Asset,
        requested_encodings: &[String],
        key: &str,
        chunk_index: usize,
        certificate_header: Option<&HeaderField>,
        callback: &CallbackFunc,
        etags: &[Hash],
        cert_version: u16,
    ) -> Option<HttpResponse> {
        let most_important_v1 = asset.most_important_encoding_v1();

        // Return a requested encoding that is certified
        for enc_name in requested_encodings.iter() {
            if let Some(enc) = asset.encodings.get(enc_name) {
                if enc.certified {
                    if cert_version == 1 {
                        // In v1, only the most important encoding is certified.
                        if enc_name != &most_important_v1 {
                            continue;
                        }
                    }
                    return Some(Self::build_ok(
                        asset,
                        enc_name,
                        enc,
                        key,
                        chunk_index,
                        certificate_header,
                        callback,
                        etags,
                        cert_version,
                    ));
                }
            }
        }

        // None of the requested encodings are available with certification
        // In v1, a first fall-back measure is to return a non-certified encoding, if a requested encoding is available
        if cert_version == 1 {
            for enc_name in requested_encodings.iter() {
                if let Some(enc) = asset.encodings.get(enc_name) {
                    return Some(Self::build_ok(
                        asset,
                        enc_name,
                        enc,
                        key,
                        chunk_index,
                        // we return the certificate anyways because then the service worker can try to convert the encoding (e.g. unzip)
                        // and then try to match the response hash in other encoding formats
                        certificate_header,
                        callback,
                        etags,
                        cert_version,
                    ));
                }
            }
        }

        // None of the requested encodings are available - fall back to the best we have
        for enc_name in encoding_certification_order(asset.encodings.keys()) {
            if let Some(enc) = asset.encodings.get(&enc_name) {
                // In v1, only the most important encoding is certified.
                if enc_name != most_important_v1 {
                    continue;
                }
                if enc.certified {
                    return Some(Self::build_ok(
                        asset,
                        &enc_name,
                        enc,
                        key,
                        chunk_index,
                        certificate_header,
                        callback,
                        etags,
                        cert_version,
                    ));
                }
            }
        }
        None
    }

    pub fn build_400(err_msg: &str) -> Self {
        HttpResponse {
            status_code: 400,
            headers: vec![],
            body: RcBytes::from(ByteBuf::from(err_msg)),
            upgrade: None,
            streaming_strategy: None,
        }
    }

    pub fn build_404(certificate_header: HeaderField, cert_version: u16) -> HttpResponse {
        let base_404 = Self::uncertified_404();
        let mut headers = base_404.headers.clone();
        headers.push(certificate_header);
        if cert_version == 2 {
            let certificate_expression =
                build_ic_certificate_expression_from_headers(&base_404.headers);
            let cert_expr_header = build_ic_certificate_expression_header(&certificate_expression);
            headers.push(cert_expr_header)
        }
        HttpResponse {
            headers,
            ..base_404
        }
    }

    pub fn uncertified_404() -> HttpResponse {
        HttpResponse {
            status_code: 404,
            headers: vec![("content-type".to_string(), "text/plain".to_string())],
            body: RcBytes::from(ByteBuf::from("not found")),
            upgrade: None,
            streaming_strategy: None,
        }
    }

    pub fn build_redirect(status_code: u16, location: String) -> HttpResponse {
        HttpResponse {
            status_code,
            headers: vec![("Location".to_string(), location)],
            body: RcBytes::from(ByteBuf::default()),
            upgrade: None,
            streaming_strategy: None,
        }
    }
}

pub fn response_hash(
    certified_headers: &[(String, Value)],
    status_code: u16,
    body_hash: &[u8; 32],
) -> ResponseHash {
    // certification v2 spec:
    // Response hash is the hash of the concatenation of
    //   - representation-independent hash of headers
    //   - hash of the response body
    //
    // The representation-independent hash of headers consist of
    //    - all certified headers (here all headers), plus
    //    - synthetic header `:ic-cert-status` with value <HTTP status code of response>

    let mut headers = Vec::from(certified_headers);
    headers.push((
        ":ic-cert-status".to_string(),
        Value::Number(status_code.into()),
    ));
    let header_hash = representation_independent_hash(&headers);
    let hash: [u8; 32] = sha2::Sha256::digest([header_hash.as_ref(), body_hash].concat()).into();
    ResponseHash(hash)
}

pub fn build_ic_certificate_expression_from_headers_and_encoding<T>(
    headers: &[(String, T)],
    encoding_name: Option<&str>,
) -> CertificateExpression {
    let mut headers = headers
        .iter()
        .map(|(h, _)| format!(", \"{}\"", h))
        .collect::<Vec<_>>()
        .join("");
    if let Some(encoding) = encoding_name {
        if encoding != "identity" {
            headers = format!(", \"content-encoding\"{}", headers);
        }
    }

    let expression = IC_CERTIFICATE_EXPRESSION_VALUE.replace("{headers}", &headers);
    let hash: [u8; 32] = sha2::Sha256::digest(expression.as_bytes()).into();
    CertificateExpression {
        expression,
        expression_hash: hash,
    }
}

pub fn build_ic_certificate_expression_from_headers<T>(
    headers: &[(String, T)],
) -> CertificateExpression {
    let headers = headers
        .iter()
        .map(|(h, _)| format!(", \"{}\"", h))
        .collect::<Vec<_>>()
        .join("");

    let expression = IC_CERTIFICATE_EXPRESSION_VALUE.replace("{headers}", &headers);
    let hash: [u8; 32] = sha2::Sha256::digest(expression.as_bytes()).into();
    CertificateExpression {
        expression,
        expression_hash: hash,
    }
}

pub fn build_ic_certificate_expression_header(
    certificate_expression: &CertificateExpression,
) -> HeaderField {
    (
        "ic-certificateexpression".to_string(),
        certificate_expression.expression.clone(),
    )
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/asset_certification/types/mod.rs:
-----------------------

pub mod certification;
pub mod http;
pub mod rc_bytes;


-----------------------

/src/canisters/frontend/ic-certified-assets/src/asset_certification/types/rc_bytes.rs:
-----------------------

//! This module contains an implementation of [RcBytes], a reference-counted byte array.
use candid::{
    types::{Serializer, Type, TypeInner},
    CandidType, Deserialize,
};
use serde::de::Deserializer;
use serde_bytes::ByteBuf;
use std::convert::AsRef;
use std::ops::Deref;
use std::rc::Rc;

#[derive(Clone, Debug, Default)]
pub struct RcBytes(Rc<ByteBuf>);

impl CandidType for RcBytes {
    fn _ty() -> Type {
        TypeInner::Vec(TypeInner::Nat8.into()).into()
    }

    fn idl_serialize<S>(&self, serializer: S) -> Result<(), S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_blob(&self.0)
    }
}

impl<'de> Deserialize<'de> for RcBytes {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        ByteBuf::deserialize(deserializer).map(Self::from)
    }
}

impl From<ByteBuf> for RcBytes {
    fn from(b: ByteBuf) -> Self {
        Self(Rc::new(b))
    }
}

impl AsRef<[u8]> for RcBytes {
    fn as_ref(&self) -> &[u8] {
        &self.0
    }
}

impl Deref for RcBytes {
    type Target = [u8];
    fn deref(&self) -> &[u8] {
        &self.0
    }
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/evidence.rs:
-----------------------

use crate::evidence::EvidenceComputation::{Computed, NextChunkIndex, NextOperation};
use crate::state_machine::Chunk;
use crate::types::BatchOperation::{
    Clear, CreateAsset, DeleteAsset, SetAssetContent, SetAssetProperties, UnsetAssetContent,
};
use crate::types::{
    ChunkId, ClearArguments, CommitBatchArguments, CreateAssetArguments, DeleteAssetArguments,
    SetAssetContentArguments, SetAssetPropertiesArguments, UnsetAssetContentArguments,
};
use itertools::Itertools;
use serde_bytes::ByteBuf;
use sha2::{Digest, Sha256};
use std::collections::HashMap;

const TAG_FALSE: [u8; 1] = [0];
const TAG_TRUE: [u8; 1] = [1];

const TAG_NONE: [u8; 1] = [2];
const TAG_SOME: [u8; 1] = [3];

const TAG_CREATE_ASSET: [u8; 1] = [4];
const TAG_SET_ASSET_CONTENT: [u8; 1] = [5];
const TAG_UNSET_ASSET_CONTENT: [u8; 1] = [6];
const TAG_DELETE_ASSET: [u8; 1] = [7];
const TAG_CLEAR: [u8; 1] = [8];
const TAG_SET_ASSET_PROPERTIES: [u8; 1] = [9];

pub enum EvidenceComputation {
    NextOperation {
        operation_index: usize,
        hasher: Sha256,
    },

    NextChunkIndex {
        operation_index: usize,
        chunk_index: usize,
        hasher: Sha256,
    },

    Computed(ByteBuf),
}

impl Default for EvidenceComputation {
    fn default() -> Self {
        Self::new()
    }
}

impl EvidenceComputation {
    pub fn new() -> Self {
        NextOperation {
            operation_index: 0,
            hasher: Sha256::new(),
        }
    }

    pub fn advance(self, args: &CommitBatchArguments, chunks: &HashMap<ChunkId, Chunk>) -> Self {
        match self {
            NextOperation {
                operation_index,
                hasher,
            } => next_operation(args, operation_index, hasher),
            NextChunkIndex {
                operation_index,
                chunk_index,
                hasher,
            } => next_chunk_index(args, operation_index, chunk_index, hasher, chunks),
            Computed(evidence) => Computed(evidence),
        }
    }
}

fn next_operation(
    args: &CommitBatchArguments,
    operation_index: usize,
    mut hasher: Sha256,
) -> EvidenceComputation {
    match args.operations.get(operation_index) {
        None => {
            let sha256: [u8; 32] = hasher.finalize().into();
            Computed(ByteBuf::from(sha256))
        }
        Some(CreateAsset(args)) => {
            hash_create_asset(&mut hasher, args);
            NextOperation {
                operation_index: operation_index + 1,
                hasher,
            }
        }
        Some(SetAssetContent(args)) => {
            hash_set_asset_content(&mut hasher, args);
            NextChunkIndex {
                operation_index,
                chunk_index: 0,
                hasher,
            }
        }
        Some(UnsetAssetContent(args)) => {
            hash_unset_asset_content(&mut hasher, args);
            NextOperation {
                operation_index: operation_index + 1,
                hasher,
            }
        }
        Some(DeleteAsset(args)) => {
            hash_delete_asset(&mut hasher, args);
            NextOperation {
                operation_index: operation_index + 1,
                hasher,
            }
        }
        Some(Clear(args)) => {
            hash_clear(&mut hasher, args);
            NextOperation {
                operation_index: operation_index + 1,
                hasher,
            }
        }
        Some(SetAssetProperties(args)) => {
            hash_set_asset_properties(&mut hasher, args);
            NextOperation {
                operation_index: operation_index + 1,
                hasher,
            }
        }
    }
}

fn next_chunk_index(
    args: &CommitBatchArguments,
    operation_index: usize,
    chunk_index: usize,
    mut hasher: Sha256,
    chunks: &HashMap<ChunkId, Chunk>,
) -> EvidenceComputation {
    if let Some(SetAssetContent(sac)) = args.operations.get(operation_index) {
        if let Some(chunk_id) = sac.chunk_ids.get(chunk_index) {
            hash_chunk_by_id(&mut hasher, chunk_id, chunks);
            if chunk_index + 1 < sac.chunk_ids.len() {
                return NextChunkIndex {
                    operation_index,
                    chunk_index: chunk_index + 1,
                    hasher,
                };
            }
        }
    }
    NextOperation {
        operation_index: operation_index + 1,
        hasher,
    }
}

fn hash_chunk_by_id(hasher: &mut Sha256, chunk_id: &ChunkId, chunks: &HashMap<ChunkId, Chunk>) {
    if let Some(chunk) = chunks.get(chunk_id) {
        hasher.update(&chunk.content);
    }
}

fn hash_create_asset(hasher: &mut Sha256, args: &CreateAssetArguments) {
    hasher.update(TAG_CREATE_ASSET);
    hasher.update(&args.key);
    hasher.update(&args.content_type);
    if let Some(max_age) = args.max_age {
        hasher.update(TAG_SOME);
        hasher.update(max_age.to_be_bytes());
    } else {
        hasher.update(TAG_NONE);
    }
    hash_headers(hasher, args.headers.as_ref());
    hash_opt_bool(hasher, args.allow_raw_access);
    hash_opt_bool(hasher, args.enable_aliasing);
}

fn hash_set_asset_content(hasher: &mut Sha256, args: &SetAssetContentArguments) {
    hasher.update(TAG_SET_ASSET_CONTENT);
    hasher.update(&args.key);
    hasher.update(&args.content_encoding);
    hash_opt_bytebuf(hasher, args.sha256.as_ref());
}

fn hash_unset_asset_content(hasher: &mut Sha256, args: &UnsetAssetContentArguments) {
    hasher.update(TAG_UNSET_ASSET_CONTENT);
    hasher.update(&args.key);
    hasher.update(&args.content_encoding);
}

fn hash_delete_asset(hasher: &mut Sha256, args: &DeleteAssetArguments) {
    hasher.update(TAG_DELETE_ASSET);
    hasher.update(&args.key);
}

fn hash_clear(hasher: &mut Sha256, _args: &ClearArguments) {
    hasher.update(TAG_CLEAR);
}

fn hash_set_asset_properties(hasher: &mut Sha256, args: &SetAssetPropertiesArguments) {
    hasher.update(TAG_SET_ASSET_PROPERTIES);
    hasher.update(&args.key);
    if let Some(max_age) = args.max_age {
        hasher.update(TAG_SOME);
        if let Some(max_age) = max_age {
            hasher.update(TAG_SOME);
            hasher.update(max_age.to_be_bytes());
        } else {
            hasher.update(TAG_NONE);
        }
    } else {
        hasher.update(TAG_NONE);
    }
    if let Some(headers) = args.headers.as_ref() {
        hasher.update(TAG_SOME);
        hash_headers(hasher, headers.as_ref());
    } else {
        hasher.update(TAG_NONE);
    }
    if let Some(allow_raw_access) = args.allow_raw_access {
        hasher.update(TAG_SOME);
        hash_opt_bool(hasher, allow_raw_access);
    } else {
        hasher.update(TAG_NONE);
    }
    if let Some(enable_aliasing) = args.is_aliased {
        hasher.update(TAG_SOME);
        hash_opt_bool(hasher, enable_aliasing);
    } else {
        hasher.update(TAG_NONE);
    }
}

fn hash_opt_bool(hasher: &mut Sha256, b: Option<bool>) {
    if let Some(b) = b {
        hasher.update(TAG_SOME);
        hasher.update(if b { TAG_TRUE } else { TAG_FALSE });
    } else {
        hasher.update(TAG_NONE);
    }
}

fn hash_opt_bytebuf(hasher: &mut Sha256, buf: Option<&ByteBuf>) {
    if let Some(buf) = buf {
        hasher.update(TAG_SOME);
        hasher.update(buf);
    } else {
        hasher.update(TAG_NONE);
    }
}

fn hash_headers(hasher: &mut Sha256, headers: Option<&HashMap<String, String>>) {
    if let Some(headers) = headers {
        hasher.update(TAG_SOME);
        for k in headers.keys().sorted() {
            let v = headers.get(k).unwrap();
            hasher.update(k);
            hasher.update(v);
        }
    } else {
        hasher.update(TAG_NONE);
    }
}

#[test]
fn tag_value_uniqueness() {
    let tags = include_str!("evidence.rs")
        .lines()
        .filter(|l| l.starts_with("const TAG_"))
        .map(|line| {
            line.split(": [u8; 1] = [")
                .nth(1)
                .unwrap()
                .trim_end_matches("];")
                .parse::<u8>()
                .unwrap()
        });
    assert_eq!(
        tags.clone().count(),
        tags.unique().count(),
        "tag values must be unique"
    );
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/lib.rs:
-----------------------

/src/canisters/frontend/ic-certified-assets/src/state_machine.rs:
-----------------------

/src/canisters/frontend/ic-certified-assets/src/tests.rs:
-----------------------

/src/canisters/frontend/ic-certified-assets/src/types.rs:
-----------------------

//! This module defines types shared by the certified assets state machine and the canister
//! endpoints.
use crate::asset_certification::types::{certification::AssetKey, rc_bytes::RcBytes};
use candid::{CandidType, Deserialize, Nat, Principal};
use serde_bytes::ByteBuf;
use std::collections::HashMap;

pub type BatchId = Nat;
pub type ChunkId = Nat;

// IDL Types

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct ConfigureArguments {
    pub max_batches: Option<Option<u64>>,
    pub max_chunks: Option<Option<u64>>,
    pub max_bytes: Option<Option<u64>>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct ConfigurationResponse {
    pub max_batches: Option<u64>,
    pub max_chunks: Option<u64>,
    pub max_bytes: Option<u64>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CreateAssetArguments {
    pub key: AssetKey,
    pub content_type: String,
    pub max_age: Option<u64>,
    pub headers: Option<HashMap<String, String>>,
    pub enable_aliasing: Option<bool>,
    pub allow_raw_access: Option<bool>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct SetAssetContentArguments {
    pub key: AssetKey,
    pub content_encoding: String,
    pub chunk_ids: Vec<ChunkId>,
    pub sha256: Option<ByteBuf>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct UnsetAssetContentArguments {
    pub key: AssetKey,
    pub content_encoding: String,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct DeleteAssetArguments {
    pub key: AssetKey,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct ClearArguments {}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub enum BatchOperation {
    CreateAsset(CreateAssetArguments),
    SetAssetContent(SetAssetContentArguments),
    UnsetAssetContent(UnsetAssetContentArguments),
    DeleteAsset(DeleteAssetArguments),
    Clear(ClearArguments),
    SetAssetProperties(SetAssetPropertiesArguments),
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CommitBatchArguments {
    pub batch_id: BatchId,
    pub operations: Vec<BatchOperation>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CommitProposedBatchArguments {
    pub batch_id: BatchId,
    pub evidence: ByteBuf,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct DeleteBatchArguments {
    pub batch_id: BatchId,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct ComputeEvidenceArguments {
    pub batch_id: BatchId,
    pub max_iterations: Option<u16>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct StoreArg {
    pub key: AssetKey,
    pub content_type: String,
    pub content_encoding: String,
    pub content: ByteBuf,
    pub sha256: Option<ByteBuf>,
    pub aliased: Option<bool>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct GetArg {
    pub key: AssetKey,
    pub accept_encodings: Vec<String>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct GetChunkArg {
    pub key: AssetKey,
    pub content_encoding: String,
    pub index: Nat,
    pub sha256: Option<ByteBuf>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct GetChunkResponse {
    pub content: RcBytes,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CreateBatchResponse {
    pub batch_id: BatchId,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CreateChunkArg {
    pub batch_id: BatchId,
    pub content: ByteBuf,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CreateChunkResponse {
    pub chunk_id: ChunkId,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CreateChunksArg {
    pub batch_id: BatchId,
    pub content: Vec<ByteBuf>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct CreateChunksResponse {
    pub chunk_ids: Vec<ChunkId>,
}

#[derive(Clone, Debug, CandidType, Deserialize, PartialEq, Eq)]
pub struct AssetProperties {
    pub max_age: Option<u64>,
    pub headers: Option<HashMap<String, String>>,
    pub allow_raw_access: Option<bool>,
    pub is_aliased: Option<bool>,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct SetAssetPropertiesArguments {
    pub key: AssetKey,
    pub max_age: Option<Option<u64>>,
    pub headers: Option<Option<HashMap<String, String>>>,
    pub allow_raw_access: Option<Option<bool>>,
    pub is_aliased: Option<Option<bool>>,
}

#[derive(Clone, Debug, Eq, PartialEq, CandidType, Deserialize)]
pub enum Permission {
    Commit,
    ManagePermissions,
    Prepare,
}

impl std::fmt::Display for Permission {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match *self {
            Permission::Commit => f.write_str("Commit"),
            Permission::Prepare => f.write_str("Prepare"),
            Permission::ManagePermissions => f.write_str("ManagePermissions"),
        }
    }
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct GrantPermissionArguments {
    pub to_principal: Principal,
    pub permission: Permission,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct RevokePermissionArguments {
    pub of_principal: Principal,
    pub permission: Permission,
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct ListPermittedArguments {
    pub permission: Permission,
}

/// The argument to `init` and `post_upgrade` needs to have the same argument type by definition.
/// `AssetCanisterArgs` is there so that the two functions can take different argument types.
#[derive(Clone, Debug, CandidType, Deserialize)]
pub enum AssetCanisterArgs {
    Init(InitArgs),
    Upgrade(UpgradeArgs),
}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct InitArgs {}

#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct UpgradeArgs {
    pub set_permissions: Option<SetPermissions>,
}

/// Sets the list of principals with a certain permission for every permission that is `Some`.
#[derive(Clone, Debug, CandidType, Deserialize)]
pub struct SetPermissions {
    pub prepare: Vec<Principal>,
    pub commit: Vec<Principal>,
    pub manage_permissions: Vec<Principal>,
}


-----------------------

/src/canisters/frontend/ic-certified-assets/src/url_decode.rs:
-----------------------

use std::fmt;

use percent_encoding::percent_decode_str;

#[derive(Debug, PartialEq, Eq)]
pub enum UrlDecodeError {
    InvalidPercentEncoding,
}

impl fmt::Display for UrlDecodeError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::InvalidPercentEncoding => write!(f, "invalid percent encoding"),
        }
    }
}

/// Decodes a percent encoded string according to https://url.spec.whatwg.org/#percent-decode
///
/// This is a wrapper around the percent-encoding crate.
///
/// The rules that it follow by are:
/// - Start with an empty sequence of bytes of the output
/// - Convert the input to a sequence of bytes
/// - if the byte is `%` and the next two bytes are hex, convet the hex value to a byte
///   and add it to the output, otherwise add the byte to the output
/// - convert the output byte sequence to a UTF-8 string and return it. If the conversion
///   fails return an error.
pub fn url_decode(url: &str) -> Result<String, UrlDecodeError> {
    match percent_decode_str(url).decode_utf8() {
        Ok(result) => Ok(result.to_string()),
        Err(_) => Err(UrlDecodeError::InvalidPercentEncoding),
    }
}


-----------------------

/src/canisters/frontend/ic-frontend-canister/Cargo.toml:
-----------------------

[package]
name = "ic-frontend-canister"
version = "0.2.5"
authors = ["DFINITY Stiftung <sdk@dfinity.org>"]
edition = "2021"
description = "Assets Canister for IC."
repository = "https://github.com/dfinity/sdk"
license = "Apache-2.0"
categories = ["wasm"]
keywords = ["internet-computer", "dfinity"]

[lib]
path = "src/lib.rs"
crate-type = ["cdylib"]

[dependencies]
ic-certified-assets = { path = "../ic-certified-assets" }
ic-cdk.workspace = true
candid.workspace = true


-----------------------

/src/canisters/frontend/ic-frontend-canister/LICENSE:
-----------------------

../../../../LICENSE

-----------------------

/src/canisters/frontend/ic-frontend-canister/README.md:
-----------------------

# Assets Canister

This is the implementation of the DFX assets canister in Rust with support for asset certification.


-----------------------

/src/canisters/frontend/ic-frontend-canister/src/lib.rs:
-----------------------

use ic_cdk::{init, post_upgrade, pre_upgrade};
use ic_certified_assets::types::AssetCanisterArgs;

#[init]
fn init(args: Option<AssetCanisterArgs>) {
    ic_certified_assets::init(args);
}

#[pre_upgrade]
fn pre_upgrade() {
    ic_cdk::storage::stable_save((ic_certified_assets::pre_upgrade(),))
        .expect("failed to save stable state");
}

#[post_upgrade]
fn post_upgrade(args: Option<AssetCanisterArgs>) {
    let (stable_state,): (ic_certified_assets::StableState,) =
        ic_cdk::storage::stable_restore().expect("failed to restore stable state");
    ic_certified_assets::post_upgrade(stable_state, args);
}


-----------------------

/src/canisters/frontend/icx-asset/Cargo.toml:
-----------------------

[package]
name = "icx-asset"
version = "0.21.0"
authors.workspace = true
edition.workspace = true
repository.workspace = true
license.workspace = true
rust-version.workspace = true
description = "CLI tool to manage assets on an asset canister on the Internet Computer."
documentation = "https://docs.rs/icx-asset"
categories = ["command-line-interface"]
keywords = ["internet-computer", "agent", "icp", "dfinity", "asset"]

[dependencies]
anstyle.workspace = true
anyhow.workspace = true
candid = { workspace = true }
clap = { workspace = true, features = ["derive", "cargo", "unstable-styles", "wrap_help" ] }
delay = "0.3.1"
humantime.workspace = true
ic-agent = { workspace = true }
ic-asset.workspace = true
ic-utils = { workspace = true }
libflate = "1.2.0"
num-traits.workspace = true
pem.workspace = true
serde.workspace = true
serde_bytes.workspace = true
serde_json.workspace = true
slog = { workspace = true, features = ["max_level_trace"] }
slog-async.workspace = true
slog-term.workspace = true
time = { workspace = true, features = ["formatting", "local-offset"] }
tokio = { workspace = true, features = ["full", "rt"] }
thiserror.workspace = true
walkdir.workspace = true


-----------------------

/src/canisters/frontend/icx-asset/LICENSE:
-----------------------

../../../../LICENSE

-----------------------

/src/canisters/frontend/icx-asset/README.md:
-----------------------

# `icx-asset`
A command line tool to manage an asset storage canister.

## icx-asset sync

Synchronize one or more directories to an asset canister.

Usage: `icx-asset sync <canister id> <source directory>...`

Example:
```
# same asset synchronization as dfx deploy for a default project, if you've already run dfx build
$ icx-asset --pem ~/.config/dfx/identity/default/identity.pem sync <canister id> src/prj_assets/assets dist/prj_assets  
```

## icx-asset ls

List assets in the asset canister.

## icx-asset upload

Usage: `icx-asset upload [<key>=]<file> [[<key>=]<file> ...]`

Examples:

```
# upload a single file as /a.txt
$ icx-asset upload a.txt

# upload a single file, a.txt, under another name
$ icx-asset upload /b.txt=a.txt

# upload a directory and its contents as /some-dir/*
$ icx-asset upload some-dir

# Similar to synchronization with dfx deploy, but without deleting anything:
$ icx-asset upload /=src/<project>/assets


```

-----------------------

/src/canisters/frontend/icx-asset/src/commands/list.rs:
-----------------------

use candid::{CandidType, Int, Nat};
use ic_utils::call::SyncCall;
use ic_utils::Canister;
use num_traits::ToPrimitive;
use serde::Deserialize;
use slog::{info, Logger};
use time::{format_description, OffsetDateTime};

pub async fn list(canister: &Canister<'_>, logger: &Logger) -> anyhow::Result<()> {
    #[derive(CandidType, Deserialize)]
    struct Encoding {
        modified: Int,
        content_encoding: String,
        sha256: Option<Vec<u8>>,
        length: Nat,
    }

    #[derive(CandidType, Deserialize)]
    struct ListEntry {
        key: String,
        content_type: String,
        encodings: Vec<Encoding>,
    }

    #[derive(CandidType, Deserialize)]
    struct EmptyRecord {}

    let (entries,): (Vec<ListEntry>,) = canister
        .query("list")
        .with_arg(EmptyRecord {})
        .build()
        .call()
        .await?;

    for entry in entries {
        for encoding in entry.encodings {
            let modified = encoding.modified;
            let modified =
                OffsetDateTime::from_unix_timestamp_nanos(modified.0.to_i128().unwrap())?;
            let timestamp_format =
                format_description::parse("[year]-[month]-[day] [hour]:[minute]:[second] UTC")?;

            info!(
                logger,
                "{:>20} {:>15} {:50} ({}, {})",
                modified.format(&timestamp_format)?,
                encoding.length.0,
                entry.key,
                entry.content_type,
                encoding.content_encoding
            );
        }
    }
    Ok(())
}


-----------------------

/src/canisters/frontend/icx-asset/src/commands/mod.rs:
-----------------------

pub mod list;
pub mod sync;
pub mod upload;


-----------------------

/src/canisters/frontend/icx-asset/src/commands/sync.rs:
-----------------------

use crate::SyncOpts;
use ic_utils::Canister;
use slog::Logger;
use std::path::Path;

pub(crate) async fn sync(
    canister: &Canister<'_>,
    o: &SyncOpts,
    logger: &Logger,
) -> anyhow::Result<()> {
    let dirs: Vec<&Path> = o.directory.iter().map(|d| d.as_path()).collect();
    ic_asset::sync(canister, &dirs, o.no_delete, logger).await?;
    Ok(())
}


-----------------------

/src/canisters/frontend/icx-asset/src/commands/upload.rs:
-----------------------

use crate::UploadOpts;
use ic_utils::Canister;
use slog::Logger;
use std::collections::HashMap;
use std::path::PathBuf;
use std::str::FromStr;
use walkdir::WalkDir;

pub(crate) async fn upload(
    canister: &Canister<'_>,
    opts: &UploadOpts,
    logger: &Logger,
) -> anyhow::Result<()> {
    let key_map = get_key_map(&opts.files)?;
    ic_asset::upload(canister, key_map, logger).await?;
    Ok(())
}

fn get_key_map(files: &[String]) -> anyhow::Result<HashMap<String, PathBuf>> {
    let mut key_map: HashMap<String, PathBuf> = HashMap::new();

    for arg in files {
        let (key, source): (String, PathBuf) = {
            if let Some(index) = arg.find('=') {
                (
                    arg[..index].to_string(),
                    PathBuf::from_str(&arg[index + 1..])?,
                )
            } else {
                let source = PathBuf::from_str(&arg.clone())?;
                let key = format!("/{}", source.file_name().unwrap().to_string_lossy());
                // or if we want to retain relative paths:
                // let key = if source.is_absolute() {
                //     format!("/{}", source.file_name().unwrap().to_string_lossy())
                // } else {
                //     format!("/{}", arg.clone())
                // };
                (key, source)
            }
        };

        if source.is_file() {
            key_map.insert(key, source);
        } else {
            for p in WalkDir::new(source.clone())
                .into_iter()
                .filter_map(std::result::Result::ok)
                .filter(|e| !e.file_type().is_dir())
            {
                let p = p.path().to_path_buf();
                let relative = p.strip_prefix(&source).expect("cannot strip prefix");
                let mut key = key.clone();
                if !key.ends_with('/') {
                    key.push('/');
                }
                key.push_str(relative.to_string_lossy().as_ref());
                key_map.insert(key, p);
            }
        }
    }

    Ok(key_map)
}


-----------------------

/src/canisters/frontend/icx-asset/src/main.rs:
-----------------------

mod commands;
mod support;
use crate::commands::list::list;
use crate::commands::sync::sync;
use crate::commands::upload::upload;
use anstyle::{AnsiColor, Style};
use candid::Principal;
use clap::builder::Styles;
use clap::{crate_authors, crate_version, Parser, ValueEnum};
use ic_agent::identity::{AnonymousIdentity, BasicIdentity, Secp256k1Identity};
use ic_agent::{Agent, Identity};
use slog::Level;
use std::path::PathBuf;

const DEFAULT_IC_GATEWAY: &str = "https://icp0.io";

#[derive(Parser)]
#[command(
    version = crate_version!(),
    author = crate_authors!(),
    propagate_version = true,
    styles = style(),
)]
struct Opts {
    /// Some input. Because this isn't an Option<T> it's required to be used
    #[arg(long, default_value = "http://localhost:4943/")]
    replica: String,

    /// An optional PEM file to read the identity from. If none is passed,
    /// a random identity will be created.
    #[arg(long)]
    pem: Option<PathBuf>,

    /// An optional field to set the expiry time on requests. Can be a human
    /// readable time (like `100s`) or a number of seconds.
    #[arg(long)]
    ttl: Option<humantime::Duration>,

    #[command(subcommand)]
    subcommand: SubCommand,

    #[arg(long, value_enum, default_value = "info")]
    log_level: LogLevel,
}

#[derive(ValueEnum, Clone, Debug)]
enum LogLevel {
    Trace,
    Debug,
    Info,
    Warning,
    Error,
}

impl From<LogLevel> for Level {
    fn from(log_level: LogLevel) -> Self {
        match log_level {
            LogLevel::Trace => Level::Trace,
            LogLevel::Debug => Level::Debug,
            LogLevel::Info => Level::Info,
            LogLevel::Warning => Level::Warning,
            LogLevel::Error => Level::Error,
        }
    }
}

#[derive(Parser)]
enum SubCommand {
    /// List keys from the asset canister.
    #[command(name = "ls")]
    List(ListOpts),

    /// Synchronize a directory to the asset canister
    Sync(SyncOpts),

    /// Uploads an asset to an asset canister.
    Upload(UploadOpts),
}

#[derive(Parser)]
struct ListOpts {
    /// The canister ID.
    canister_id: String,
}

#[derive(Parser)]
struct SyncOpts {
    /// The canister ID.
    canister_id: String,

    /// The directories to synchronize
    directory: Vec<PathBuf>,

    /// Do not delete files from the canister that are not present locally.
    #[arg(long)]
    no_delete: bool,
}

#[derive(Parser)]
struct UploadOpts {
    /// The asset canister ID to manage.
    canister_id: String,

    /// Files or folders to send.
    files: Vec<String>,
}

fn create_identity(maybe_pem: Option<PathBuf>) -> Box<dyn Identity + Sync + Send> {
    if let Some(pem_path) = maybe_pem {
        if let Ok(secp256k_identity) = Secp256k1Identity::from_pem_file(&pem_path) {
            Box::new(secp256k_identity)
        } else {
            Box::new(BasicIdentity::from_pem_file(pem_path).expect("Could not read the key pair."))
        }
    } else {
        Box::new(AnonymousIdentity)
    }
}

fn style() -> Styles {
    let green = Style::new().fg_color(Some(AnsiColor::Green.into()));
    let yellow = Style::new().fg_color(Some(AnsiColor::Yellow.into()));
    let red = Style::new()
        .fg_color(Some(AnsiColor::BrightRed.into()))
        .bold();
    Styles::styled()
        .literal(green)
        .placeholder(green)
        .error(red)
        .header(yellow)
        .invalid(yellow)
        .valid(green)
}

#[tokio::main(flavor = "multi_thread", worker_threads = 10)]
async fn main() -> anyhow::Result<()> {
    let opts: Opts = Opts::parse();

    let logger = support::new_logger(opts.log_level.into());

    let agent = Agent::builder()
        .with_url(&opts.replica)
        .with_boxed_identity(create_identity(opts.pem))
        .build()?;

    let normalized_replica = opts.replica.strip_suffix('/').unwrap_or(&opts.replica);
    if normalized_replica != DEFAULT_IC_GATEWAY {
        agent.fetch_root_key().await?;
    }

    match &opts.subcommand {
        SubCommand::List(o) => {
            let canister = ic_utils::Canister::builder()
                .with_agent(&agent)
                .with_canister_id(Principal::from_text(&o.canister_id)?)
                .build()?;
            list(&canister, &logger).await?;
        }
        SubCommand::Sync(o) => {
            let canister = ic_utils::Canister::builder()
                .with_agent(&agent)
                .with_canister_id(Principal::from_text(&o.canister_id)?)
                .build()?;
            sync(&canister, o, &logger).await?;
        }
        SubCommand::Upload(o) => {
            let canister = ic_utils::Canister::builder()
                .with_agent(&agent)
                .with_canister_id(Principal::from_text(&o.canister_id)?)
                .build()?;
            upload(&canister, o, &logger).await?;
        }
    }

    Ok(())
}

#[cfg(test)]
mod tests {
    use clap::CommandFactory;

    use crate::Opts;

    #[test]
    fn validate_cli() {
        Opts::command().debug_assert();
    }
}


-----------------------

/src/canisters/frontend/icx-asset/src/support.rs:
-----------------------

use slog::{Drain, Level, Logger};

pub struct TermLogFormat<D>
where
    D: slog_term::Decorator,
{
    decorator: D,
}

impl<D: slog_term::Decorator> TermLogFormat<D> {
    pub fn new(decorator: D) -> TermLogFormat<D> {
        TermLogFormat { decorator }
    }
}

impl<D: slog_term::Decorator> slog::Drain for TermLogFormat<D> {
    type Ok = ();
    type Err = std::io::Error;

    fn log(
        &self,
        record: &slog::Record<'_>,
        values: &slog::OwnedKVList,
    ) -> std::result::Result<Self::Ok, Self::Err> {
        self.decorator.with_record(record, values, |decorator| {
            if record.level() <= slog::Level::Warning {
                decorator.start_level()?;
                write!(decorator, "{}: ", record.level().as_str())?;
                // start_whitespace resets to normal coloring after printing the level
                decorator.start_whitespace()?;
            }

            decorator.start_msg()?;
            write!(decorator, "{}", record.msg())?;

            decorator.start_whitespace()?;
            writeln!(decorator)?;

            decorator.flush()?;
            Ok(())
        })
    }
}

pub(crate) fn new_logger(level: Level) -> Logger {
    let decorator = slog_term::TermDecorator::new().build();
    let drain = TermLogFormat::new(decorator).fuse();
    let drain = slog::LevelFilter::new(drain, level).fuse();
    let drain = slog_async::Async::new(drain).build().fuse();
    Logger::root(drain, slog::o!())
}


-----------------------

/src/dfx-core/Cargo.toml:
-----------------------

[package]
name = "dfx-core"
version = "0.1.0"
authors.workspace = true
edition.workspace = true
repository.workspace = true
license.workspace = true
rust-version.workspace = true
description = "dfx core library"
documentation = "https://docs.rs/dfx-core"
keywords = ["internet-computer", "icp", "dfinity"]

[dependencies]
aes-gcm.workspace = true
argon2.workspace = true
backoff.workspace = true
bip32 = "0.4.0"
byte-unit = { workspace = true, features = ["serde"] }
bytes.workspace = true
candid = { workspace = true }
clap = { workspace = true, features = ["string"] }
dialoguer = { workspace = true }
directories-next.workspace = true
dunce = "1.0"
flate2 = { workspace = true, default-features = false, features = ["zlib-ng"] }
handlebars.workspace = true
hex = { workspace = true, features = ["serde"] }
humantime-serde = "1.1.1"
ic-agent.workspace = true
ic-utils.workspace = true
ic-identity-hsm.workspace = true
itertools.workspace = true
k256 = { version = "0.11.4", features = ["pem"] }
keyring.workspace = true
lazy_static.workspace = true
reqwest = { workspace = true, features = ["blocking", "json"] }
ring.workspace = true
schemars.workspace = true
sec1 = { workspace = true, features = ["std"] }
semver = { workspace = true, features = ["serde"] }
serde.workspace = true
serde_json.workspace = true
sha2.workspace = true
slog = { workspace = true, features = ["max_level_trace"] }
tar.workspace = true
tempfile.workspace = true
thiserror.workspace = true
tiny-bip39 = "1.0.0"
time = { workspace = true, features = ["serde", "serde-human-readable"] }
url.workspace = true

[dev-dependencies]
futures.workspace = true
proptest = "1.0"
tempfile = "3.1.0"
tokio = { workspace = true, features = ["full"] }


-----------------------

/src/dfx-core/src/canister/mod.rs:
-----------------------

use crate::{
    cli::ask_for_consent,
    error::canister::{CanisterBuilderError, CanisterInstallError},
    identity::CallSender,
};
use candid::Principal;
use ic_agent::Agent;
use ic_utils::{
    interfaces::{
        management_canister::builders::{CanisterInstall, InstallMode},
        ManagementCanister, WalletCanister,
    },
    Argument,
};

pub async fn build_wallet_canister(
    id: Principal,
    agent: &Agent,
) -> Result<WalletCanister<'_>, CanisterBuilderError> {
    WalletCanister::from_canister(
        ic_utils::Canister::builder()
            .with_agent(agent)
            .with_canister_id(id)
            .build()
            .unwrap(),
    )
    .await
    .map_err(CanisterBuilderError::WalletCanisterCaller)
}

pub fn install_mode_to_prompt(mode: &InstallMode) -> &'static str {
    match mode {
        InstallMode::Install => "Installing",
        InstallMode::Reinstall => "Reinstalling",
        InstallMode::Upgrade { .. } => "Upgrading",
    }
}

pub async fn install_canister_wasm(
    agent: &Agent,
    canister_id: Principal,
    canister_name: Option<&str>,
    args: &[u8],
    mode: InstallMode,
    call_sender: &CallSender,
    wasm_module: Vec<u8>,
    skip_consent: bool,
) -> Result<(), CanisterInstallError> {
    let mgr = ManagementCanister::create(agent);
    if !skip_consent && mode == InstallMode::Reinstall {
        let msg = if let Some(name) = canister_name {
            format!("You are about to reinstall the {name} canister")
        } else {
            format!("You are about to reinstall the canister {canister_id}")
        } + r#"
This will OVERWRITE all the data and code in the canister.

YOU WILL LOSE ALL DATA IN THE CANISTER.

"#;
        ask_for_consent(&msg).map_err(CanisterInstallError::UserConsent)?;
    }

    match call_sender {
        CallSender::SelectedId => {
            let install_builder = mgr
                .install(&canister_id, &wasm_module)
                .with_raw_arg(args.to_vec())
                .with_mode(mode);
            install_builder
                .await
                .map_err(CanisterInstallError::InstallWasmError)
        }
        CallSender::Wallet(wallet_id) => {
            let wallet = build_wallet_canister(*wallet_id, agent).await?;
            let install_args = CanisterInstall {
                mode,
                canister_id,
                wasm_module,
                arg: args.to_vec(),
            };
            wallet
                .call(
                    *mgr.canister_id_(),
                    "install_code",
                    Argument::from_candid((install_args,)),
                    0,
                )
                .await
                .map_err(CanisterInstallError::InstallWasmError)
        }
    }
}


-----------------------

/src/dfx-core/src/cli/mod.rs:
-----------------------

use crate::error::cli::UserConsent;
use std::io::stdin;

pub fn ask_for_consent(message: &str) -> Result<(), UserConsent> {
    eprintln!("WARNING!");
    eprintln!("{}", message);
    eprintln!("Do you want to proceed? yes/No");
    let mut input_string = String::new();
    stdin()
        .read_line(&mut input_string)
        .map_err(UserConsent::ReadError)?;
    let input_string = input_string.trim_end().to_lowercase();
    if input_string != "yes" && input_string != "y" {
        return Err(UserConsent::Declined);
    }
    Ok(())
}


-----------------------

/src/dfx-core/src/config/cache.rs:
-----------------------

#[cfg(windows)]
use crate::config::directories::project_dirs;
use crate::error::cache::{
    DeleteCacheError, EnsureCacheVersionsDirError, GetBinaryCommandPathError, GetCacheRootError,
    IsCacheInstalledError, ListCacheVersionsError,
};
#[cfg(not(windows))]
use crate::foundation::get_user_home;
use crate::fs::composite::ensure_dir_exists;
use semver::Version;
use std::path::PathBuf;

pub trait Cache {
    fn version_str(&self) -> String;
    fn is_installed(&self) -> Result<bool, IsCacheInstalledError>;
    fn delete(&self) -> Result<(), DeleteCacheError>;
    fn get_binary_command_path(
        &self,
        binary_name: &str,
    ) -> Result<PathBuf, GetBinaryCommandPathError>;
    fn get_binary_command(
        &self,
        binary_name: &str,
    ) -> Result<std::process::Command, GetBinaryCommandPathError>;
}

pub fn get_cache_root() -> Result<PathBuf, GetCacheRootError> {
    let cache_root = std::env::var_os("DFX_CACHE_ROOT");
    // dirs-next is not used for *nix to preserve existing paths
    #[cfg(not(windows))]
    let p = {
        let home = get_user_home()?;
        let root = cache_root.unwrap_or(home);
        PathBuf::from(root).join(".cache").join("dfinity")
    };
    #[cfg(windows)]
    let p = match cache_root {
        Some(var) => PathBuf::from(var),
        None => project_dirs()?.cache_dir().to_owned(),
    };
    if p.exists() && !p.is_dir() {
        return Err(GetCacheRootError::FindCacheDirectoryFailed(p));
    }
    Ok(p)
}

/// Constructs and returns <cache root>/versions/<version> without creating any directories.
pub fn get_cache_path_for_version(v: &str) -> Result<PathBuf, GetCacheRootError> {
    let p = get_cache_root()?.join("versions").join(v);
    Ok(p)
}

/// Return the binary cache root. It constructs it if not present
/// already.
pub fn ensure_cache_versions_dir() -> Result<PathBuf, EnsureCacheVersionsDirError> {
    let p = get_cache_root()?.join("versions");

    ensure_dir_exists(&p)?;

    Ok(p)
}

/// Doesn't create the version dir, but does create everything up to it
pub fn get_bin_cache(v: &str) -> Result<PathBuf, EnsureCacheVersionsDirError> {
    let root = ensure_cache_versions_dir()?;
    Ok(root.join(v))
}

pub fn is_version_installed(v: &str) -> Result<bool, IsCacheInstalledError> {
    Ok(get_bin_cache(v)?.is_dir())
}

pub fn delete_version(v: &str) -> Result<bool, DeleteCacheError> {
    if !is_version_installed(v).unwrap_or(false) {
        return Ok(false);
    }

    let root = get_bin_cache(v)?;
    crate::fs::remove_dir_all(&root)?;

    Ok(true)
}

pub fn get_binary_path_from_version(
    version: &str,
    binary_name: &str,
) -> Result<PathBuf, EnsureCacheVersionsDirError> {
    let env_var_name = format!("DFX_{}_PATH", binary_name.replace('-', "_").to_uppercase());

    if let Ok(path) = std::env::var(env_var_name) {
        return Ok(PathBuf::from(path));
    }

    Ok(get_bin_cache(version)?.join(binary_name))
}

pub fn binary_command_from_version(
    version: &str,
    name: &str,
) -> Result<std::process::Command, EnsureCacheVersionsDirError> {
    let path = get_binary_path_from_version(version, name)?;
    let cmd = std::process::Command::new(path);

    Ok(cmd)
}

pub fn list_versions() -> Result<Vec<Version>, ListCacheVersionsError> {
    let root = ensure_cache_versions_dir()?;
    let mut result: Vec<Version> = Vec::new();

    for entry in crate::fs::read_dir(&root)? {
        let entry = entry.map_err(ListCacheVersionsError::ReadCacheEntryFailed)?;
        if let Some(version) = entry.file_name().to_str() {
            if version.starts_with('_') {
                // temp directory for version being installed
                continue;
            }
            result.push(Version::parse(version).map_err(|e| {
                ListCacheVersionsError::MalformedSemverString(version.to_string(), e)
            })?);
        }
    }

    Ok(result)
}


-----------------------

/src/dfx-core/src/config/directories.rs:
-----------------------

use crate::error::config::ConfigError;
use crate::error::config::ConfigError::{
    DetermineConfigDirectoryFailed, DetermineSharedNetworkDirectoryFailed,
    EnsureConfigDirectoryExistsFailed,
};
use crate::error::get_user_home::GetUserHomeError;
use crate::error::get_user_home::GetUserHomeError::NoHomeInEnvironment;
#[cfg(not(windows))]
use crate::foundation::get_user_home;
use crate::fs::composite::ensure_dir_exists;
use directories_next::ProjectDirs;
use std::path::PathBuf;

pub fn project_dirs() -> Result<&'static ProjectDirs, GetUserHomeError> {
    lazy_static::lazy_static! {
        static ref DIRS: Option<ProjectDirs> = ProjectDirs::from("org", "dfinity", "dfx");
    }
    DIRS.as_ref().ok_or(NoHomeInEnvironment())
}

pub fn get_shared_network_data_directory(network: &str) -> Result<PathBuf, ConfigError> {
    let project_dirs = project_dirs().map_err(DetermineSharedNetworkDirectoryFailed)?;
    Ok(project_dirs.data_local_dir().join("network").join(network))
}

pub fn get_user_dfx_config_dir() -> Result<PathBuf, ConfigError> {
    let config_root = std::env::var_os("DFX_CONFIG_ROOT");
    // dirs-next is not used for *nix to preserve existing paths
    #[cfg(not(windows))]
    let p = {
        let home = get_user_home().map_err(DetermineConfigDirectoryFailed)?;
        let root = config_root.unwrap_or(home);
        PathBuf::from(root).join(".config").join("dfx")
    };
    #[cfg(windows)]
    let p = match config_root {
        Some(var) => PathBuf::from(var),
        None => project_dirs()
            .map_err(DetermineConfigDirectoryFailed)?
            .config_dir()
            .to_owned(),
    };
    ensure_dir_exists(&p).map_err(EnsureConfigDirectoryExistsFailed)?;
    Ok(p)
}


-----------------------

/src/dfx-core/src/config/mod.rs:
-----------------------

pub mod cache;
pub mod directories;
pub mod model;
pub mod project_templates;


-----------------------

/src/dfx-core/src/config/model/bitcoin_adapter.rs:
-----------------------

use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use std::net::{IpAddr, Ipv4Addr, SocketAddr};
use std::path::PathBuf;
use std::str::FromStr;

const BITCOIND_REGTEST_DEFAULT_PORT: u16 = 18444;

pub fn default_nodes() -> Vec<SocketAddr> {
    vec![SocketAddr::new(
        IpAddr::V4(Ipv4Addr::LOCALHOST),
        BITCOIND_REGTEST_DEFAULT_PORT,
    )]
}

// These definitions come from https://gitlab.com/dfinity-lab/public/ic/-/blob/master/rs/bitcoin/adapter/src/config.rs
#[derive(Clone, Debug, Deserialize, Serialize, PartialEq, Eq, Default)]
/// The source of the unix domain socket to be used for inter-process
/// communication.
pub enum IncomingSource {
    /// We use systemd's created socket.
    #[default]
    Systemd,
    /// We use the corresponing path as socket.
    Path(PathBuf),
}

/// Represents the log level of the bitcoin adapter.
#[derive(Clone, Debug, Serialize, Deserialize, Copy, PartialEq, Eq, JsonSchema, Default)]
#[serde(rename_all = "snake_case")]
pub enum BitcoinAdapterLogLevel {
    Critical,
    Error,
    Warning,
    #[default]
    Info,
    Debug,
    Trace,
}

impl FromStr for BitcoinAdapterLogLevel {
    type Err = String;

    fn from_str(input: &str) -> Result<BitcoinAdapterLogLevel, Self::Err> {
        match input {
            "critical" => Ok(BitcoinAdapterLogLevel::Critical),
            "error" => Ok(BitcoinAdapterLogLevel::Error),
            "warning" => Ok(BitcoinAdapterLogLevel::Warning),
            "info" => Ok(BitcoinAdapterLogLevel::Info),
            "debug" => Ok(BitcoinAdapterLogLevel::Debug),
            "trace" => Ok(BitcoinAdapterLogLevel::Trace),
            other => Err(format!("Unknown log level: {}", other)),
        }
    }
}

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct LoggerConfig {
    level: BitcoinAdapterLogLevel,
}

/// This struct contains configuration options for the BTC Adapter.
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct Config {
    /// The type of Bitcoin network we plan to communicate to (e.g. "mainnet", "testnet", "regtest", etc.).
    pub network: String,
    /// Addresses of nodes to connect to (in case discovery from seeds is not possible/sufficient)
    #[serde(default)]
    pub nodes: Vec<SocketAddr>,
    /// Specifies which unix domain socket should be used for serving incoming requests.
    #[serde(default)]
    pub incoming_source: IncomingSource,

    pub logger: LoggerConfig,
}

impl Config {
    pub fn new(
        nodes: Vec<SocketAddr>,
        uds_path: PathBuf,
        log_level: BitcoinAdapterLogLevel,
    ) -> Config {
        Config {
            network: String::from("regtest"),
            nodes,
            incoming_source: IncomingSource::Path(uds_path),
            logger: LoggerConfig { level: log_level },
        }
    }

    pub fn get_socket_path(&self) -> Option<PathBuf> {
        match &self.incoming_source {
            IncomingSource::Systemd => None,
            IncomingSource::Path(path) => Some(path.clone()),
        }
    }
}


-----------------------

/src/dfx-core/src/config/model/canister_http_adapter.rs:
-----------------------

use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use std::{path::PathBuf, str::FromStr};

// These definitions come from https://gitlab.com/dfinity-lab/public/ic/-/blob/master/rs/canister_http/adapter/src/config.rs
#[derive(Clone, Debug, Deserialize, Serialize, PartialEq, Eq, Default)]
/// The source of the unix domain socket to be used for inter-process
/// communication.
pub enum IncomingSource {
    /// We use systemd's created socket.
    #[default]
    Systemd,
    /// We use the corresponing path as socket.
    Path(PathBuf),
}

/// Represents the log level of the HTTP adapter.
#[derive(Clone, Debug, Serialize, Deserialize, Copy, PartialEq, Eq, JsonSchema, Default)]
#[serde(rename_all = "snake_case")]
pub enum HttpAdapterLogLevel {
    Critical,
    #[default]
    Error,
    Warning,
    Info,
    Debug,
    Trace,
}

impl FromStr for HttpAdapterLogLevel {
    type Err = String;

    fn from_str(input: &str) -> Result<HttpAdapterLogLevel, Self::Err> {
        match input {
            "critical" => Ok(HttpAdapterLogLevel::Critical),
            "error" => Ok(HttpAdapterLogLevel::Error),
            "warning" => Ok(HttpAdapterLogLevel::Warning),
            "info" => Ok(HttpAdapterLogLevel::Info),
            "debug" => Ok(HttpAdapterLogLevel::Debug),
            "trace" => Ok(HttpAdapterLogLevel::Trace),
            other => Err(format!("Unknown log level: {}", other)),
        }
    }
}

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct LoggerConfig {
    pub level: HttpAdapterLogLevel,
}

/// This struct contains configuration options for the Canister HTTP Adapter.
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct Config {
    /// Specifies which unix domain socket should be used for serving incoming requests.
    #[serde(default)]
    pub incoming_source: IncomingSource,

    pub logger: LoggerConfig,
}

impl Config {
    pub fn new(uds_path: PathBuf, log_level: HttpAdapterLogLevel) -> Config {
        Config {
            incoming_source: IncomingSource::Path(uds_path),
            logger: LoggerConfig { level: log_level },
        }
    }

    pub fn get_socket_path(&self) -> Option<PathBuf> {
        match &self.incoming_source {
            IncomingSource::Systemd => None,
            IncomingSource::Path(path) => Some(path.clone()),
        }
    }
}


-----------------------

/src/dfx-core/src/config/model/canister_id_store.rs:
-----------------------

use crate::config::model::dfinity::Config;
use crate::config::model::network_descriptor::{NetworkDescriptor, NetworkTypeDescriptor};
use crate::error::canister_id_store::{
    AddCanisterIdError, CanisterIdStoreError, RemoveCanisterIdError, SaveIdsError,
    SaveTimestampsError,
};
use crate::network::directory::ensure_cohesive_network_directory;
use candid::Principal as CanisterId;
use ic_agent::export::Principal;
use serde::{Deserialize, Serialize, Serializer};
use slog::{warn, Logger};
use std::collections::BTreeMap;
use std::ops::{Deref, DerefMut, Sub};
use std::path::PathBuf;
use std::sync::Arc;
use std::time::{Duration, SystemTime};
use time::format_description::well_known::Rfc3339;
use time::OffsetDateTime;

pub type CanisterName = String;
pub type NetworkName = String;
pub type CanisterIdString = String;

pub type NetworkNametoCanisterId = BTreeMap<NetworkName, CanisterIdString>;
pub type CanisterIds = BTreeMap<CanisterName, NetworkNametoCanisterId>;

pub type CanisterTimestamps = BTreeMap<CanisterName, NetworkNametoCanisterTimestamp>;

// OffsetDateTime has nanosecond precision, while SystemTime is OS-dependent (100ns on Windows)
pub type AcquisitionDateTime = OffsetDateTime;

#[derive(Debug, Clone, Default)]
pub struct NetworkNametoCanisterTimestamp(BTreeMap<NetworkName, AcquisitionDateTime>);

impl Deref for NetworkNametoCanisterTimestamp {
    type Target = BTreeMap<NetworkName, AcquisitionDateTime>;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl DerefMut for NetworkNametoCanisterTimestamp {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.0
    }
}

impl Serialize for NetworkNametoCanisterTimestamp {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        let out = self.0.iter().map(|(key, time)| {
            (
                key,
                AcquisitionDateTime::from(*time)
                    .format(&Rfc3339)
                    .expect("Failed to serialise timestamp"),
            )
        });
        serializer.collect_map(out)
    }
}

impl<'de> Deserialize<'de> for NetworkNametoCanisterTimestamp {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        let map: BTreeMap<NetworkName, String> = Deserialize::deserialize(deserializer)?;
        let btree: BTreeMap<NetworkName, AcquisitionDateTime> = map
            .into_iter()
            .map(|(key, timestamp)| (key, AcquisitionDateTime::parse(&timestamp, &Rfc3339)))
            .try_fold(BTreeMap::new(), |mut map, (key, result)| match result {
                Ok(value) => {
                    map.insert(key, value);
                    Ok(map)
                }
                Err(err) => Err(err),
            })
            .map_err(|err| serde::de::Error::custom(err.to_string()))?;
        Ok(Self(btree))
    }
}

#[derive(Clone, Debug)]
pub struct CanisterIdStore {
    network_descriptor: NetworkDescriptor,
    canister_ids_path: Option<PathBuf>,
    canister_timestamps_path: Option<PathBuf>,

    // Only the canister ids read from/written to canister-ids.json
    // which does not include remote canister ids
    ids: CanisterIds,

    // Only canisters that will time out at some point have their timestamp of acquisition saved
    acquisition_timestamps: CanisterTimestamps,

    // Remote ids read from dfx.json, never written to canister_ids.json
    remote_ids: Option<CanisterIds>,

    // ids of pull dependencies in dfx.json, never written to canister_ids.json
    pull_ids: BTreeMap<CanisterName, CanisterId>,
}

impl CanisterIdStore {
    pub const DEFAULT: &'static str = "__default";

    pub fn new(
        log: &Logger,
        network_descriptor: &NetworkDescriptor,
        config: Option<Arc<Config>>,
    ) -> Result<Self, CanisterIdStoreError> {
        let canister_ids_path = match network_descriptor {
            NetworkDescriptor {
                r#type: NetworkTypeDescriptor::Persistent,
                ..
            } => config
                .as_ref()
                .map(|c| c.get_project_root().join("canister_ids.json")),
            NetworkDescriptor { name, .. } => match &config {
                None => None,
                Some(config) => {
                    let dir = config.get_temp_path()?.join(name);
                    ensure_cohesive_network_directory(network_descriptor, &dir)?;
                    Some(dir.join("canister_ids.json"))
                }
            },
        };
        let canister_timestamps_path = match network_descriptor {
            NetworkDescriptor {
                name,
                r#type: NetworkTypeDescriptor::Playground { .. },
                ..
            } => {
                if let Some(config) = config.as_ref() {
                    let dir = config.get_temp_path()?.join(name);
                    ensure_cohesive_network_directory(network_descriptor, &dir)?;
                    Some(dir.join("canister_timestamps.json"))
                } else {
                    None
                }
            }
            _ => None,
        };
        let remote_ids = get_remote_ids(config.clone());
        let pull_ids = if let Some(config) = config {
            config.get_config().get_pull_canisters()?
        } else {
            BTreeMap::new()
        };
        let ids = match &canister_ids_path {
            Some(path) if path.is_file() => crate::json::load_json_file(path)?,
            _ => CanisterIds::new(),
        };
        let acquisition_timestamps = match &canister_timestamps_path {
            Some(path) if path.is_file() => crate::json::load_json_file(path)?,
            _ => CanisterTimestamps::new(),
        };
        let mut store = CanisterIdStore {
            network_descriptor: network_descriptor.clone(),
            canister_ids_path,
            canister_timestamps_path,
            ids,
            acquisition_timestamps,
            remote_ids,
            pull_ids,
        };

        if let NetworkTypeDescriptor::Playground {
            canister_timeout_seconds,
            ..
        } = &network_descriptor.r#type
        {
            store.prune_expired_canisters(log, &Duration::from_secs(*canister_timeout_seconds))?;
        }

        Ok(store)
    }

    pub fn get_timestamp(&self, canister_name: &str) -> Option<&AcquisitionDateTime> {
        self.acquisition_timestamps
            .get(canister_name)
            .and_then(|timestamp_map| timestamp_map.get(&self.network_descriptor.name))
    }

    pub fn get_name(&self, canister_id: &str) -> Option<&String> {
        self.remote_ids
            .as_ref()
            .and_then(|remote_ids| self.get_name_in(canister_id, remote_ids))
            .or_else(|| self.get_name_in_project(canister_id))
            .or_else(|| self.get_name_in_pull_ids(canister_id))
    }

    pub fn get_name_in_project(&self, canister_id: &str) -> Option<&String> {
        self.get_name_in(canister_id, &self.ids)
    }

    pub fn get_name_in<'a>(
        &'a self,
        canister_id: &str,
        canister_ids: &'a CanisterIds,
    ) -> Option<&'a String> {
        canister_ids
            .iter()
            .find(|(_, nn)| nn.get(&self.network_descriptor.name) == Some(&canister_id.to_string()))
            .map(|(canister_name, _)| canister_name)
    }

    fn get_name_in_pull_ids(&self, canister_id: &str) -> Option<&String> {
        self.pull_ids
            .iter()
            .find(|(_, id)| id.to_text() == canister_id)
            .map(|(canister_name, _)| canister_name)
    }

    pub fn save_ids(&self) -> Result<(), SaveIdsError> {
        let path = self
            .canister_ids_path
            .as_ref()
            .unwrap_or_else(|| {
                // the only callers of this method have already called Environment::get_config_or_anyhow
                unreachable!("Must be in a project (call Environment::get_config_or_anyhow()) to save canister ids")
            });
        crate::fs::composite::ensure_parent_dir_exists(path)?;
        crate::json::save_json_file(path, &self.ids)?;
        Ok(())
    }

    fn save_timestamps(&self) -> Result<(), SaveTimestampsError> {
        let path = self
            .canister_timestamps_path
            .as_ref()
            .unwrap_or_else(|| {
                // the only callers of this method have already called Environment::get_config_or_anyhow
                unreachable!("Must be in a project (call Environment::get_config_or_anyhow()) to save canister timestamps")
            });
        crate::fs::composite::ensure_parent_dir_exists(path)?;
        crate::json::save_json_file(path, &self.acquisition_timestamps)?;
        Ok(())
    }

    pub fn find(&self, canister_name: &str) -> Option<CanisterId> {
        self.remote_ids
            .as_ref()
            .and_then(|remote_ids| self.find_in(canister_name, remote_ids))
            .or_else(|| self.find_in(canister_name, &self.ids))
            .or_else(|| self.pull_ids.get(canister_name).copied())
    }
    pub fn get_name_id_map(&self) -> BTreeMap<String, String> {
        let mut ids: BTreeMap<_, _> = self
            .ids
            .iter()
            .filter_map(|(name, network_to_id)| {
                Some((
                    name.clone(),
                    network_to_id.get(&self.network_descriptor.name).cloned()?,
                ))
            })
            .collect();
        if let Some(remote_ids) = &self.remote_ids {
            let mut remote = remote_ids
                .iter()
                .filter_map(|(name, network_to_id)| {
                    Some((
                        name.clone(),
                        network_to_id.get(&self.network_descriptor.name).cloned()?,
                    ))
                })
                .collect();
            ids.append(&mut remote);
        }
        let mut pull_ids = self
            .pull_ids
            .iter()
            .map(|(name, id)| (name.clone(), id.to_text()))
            .collect();
        ids.append(&mut pull_ids);
        ids.into_iter()
            .filter(|(name, _)| !name.starts_with("__"))
            .collect()
    }

    fn find_in(&self, canister_name: &str, canister_ids: &CanisterIds) -> Option<CanisterId> {
        canister_ids
            .get(canister_name)
            .and_then(|network_name_to_canister_id| {
                network_name_to_canister_id
                    .get(&self.network_descriptor.name)
                    .or_else(|| network_name_to_canister_id.get(CanisterIdStore::DEFAULT))
            })
            .and_then(|s| CanisterId::from_text(s).ok())
    }

    pub fn get(&self, canister_name: &str) -> Result<CanisterId, CanisterIdStoreError> {
        self.find(canister_name).ok_or_else(|| {
            let network = if self.network_descriptor.name == "local" {
                "".to_string()
            } else {
                format!(" --network {}", self.network_descriptor.name)
            };
            CanisterIdStoreError::CanisterIdNotFound {
                canister_name: canister_name.to_string(),
                network,
            }
        })
    }

    pub fn add(
        &mut self,
        canister_name: &str,
        canister_id: &str,
        timestamp: Option<AcquisitionDateTime>,
    ) -> Result<(), AddCanisterIdError> {
        let network_name = &self.network_descriptor.name;
        match self.ids.get_mut(canister_name) {
            Some(network_name_to_canister_id) => {
                network_name_to_canister_id
                    .insert(network_name.to_string(), canister_id.to_string());
            }
            None => {
                let mut network_name_to_canister_id = NetworkNametoCanisterId::new();
                network_name_to_canister_id
                    .insert(network_name.to_string(), canister_id.to_string());
                self.ids
                    .insert(canister_name.to_string(), network_name_to_canister_id);
            }
        }
        self.save_ids()
            .map_err(|source| AddCanisterIdError::SaveIds {
                canister_name: canister_name.to_string(),
                canister_id: canister_id.to_string(),
                source,
            })?;
        if let Some(timestamp) = timestamp {
            match self.acquisition_timestamps.get_mut(canister_name) {
                Some(network_name_to_timestamp) => {
                    network_name_to_timestamp.insert(network_name.to_string(), timestamp);
                }
                None => {
                    let mut network_name_to_timestamp = NetworkNametoCanisterTimestamp::default();
                    network_name_to_timestamp.insert(network_name.to_string(), timestamp);
                    self.acquisition_timestamps
                        .insert(canister_name.to_string(), network_name_to_timestamp);
                }
            }
            self.save_timestamps()?;
        }
        Ok(())
    }

    pub fn remove(&mut self, canister_name: &str) -> Result<(), RemoveCanisterIdError> {
        let network_name = &self.network_descriptor.name;
        if let Some(network_name_to_canister_id) = self.ids.get_mut(canister_name) {
            network_name_to_canister_id.remove(network_name);
            self.save_ids()
                .map_err(|e| RemoveCanisterIdError::SaveIds {
                    canister_name: canister_name.to_string(),
                    source: e,
                })?
        };
        if let Some(network_name_to_timestamp) = self.acquisition_timestamps.get_mut(canister_name)
        {
            network_name_to_timestamp.remove(network_name);
            self.save_timestamps()?;
        }
        Ok(())
    }

    fn prune_expired_canisters(
        &mut self,
        log: &Logger,
        timeout: &Duration,
    ) -> Result<(), RemoveCanisterIdError> {
        let network_name = &self.network_descriptor.name;
        let now = SystemTime::now();
        let prune_cutoff = now.sub(*timeout);

        let mut canisters_to_prune: Vec<String> = Vec::new();
        for (canister_name, timestamp) in self.acquisition_timestamps.iter().filter_map(
            |(canister_name, network_to_timestamp)| {
                network_to_timestamp
                    .get(network_name)
                    .map(|timestamp| (canister_name, timestamp))
            },
        ) {
            if *timestamp <= prune_cutoff {
                canisters_to_prune.push(canister_name.clone());
            }
        }

        for canister in canisters_to_prune {
            warn!(log, "Canister '{}' has timed out.", &canister);
            self.remove(&canister)?;
        }

        Ok(())
    }

    pub fn non_remote_user_canisters(&self) -> Vec<(String, Principal)> {
        self.ids
            .iter()
            .filter_map(|(name, network_to_id)| {
                network_to_id
                    .get(&self.network_descriptor.name)
                    .and_then(|principal| Principal::from_text(principal).ok())
                    .map(|principal| (name.clone(), principal))
            })
            .collect()
    }
}

fn get_remote_ids(config: Option<Arc<Config>>) -> Option<CanisterIds> {
    let config = config?;
    let config = config.get_config();

    let mut remote_ids = CanisterIds::new();
    if let Some(canisters) = &config.canisters {
        for (canister_name, canister_config) in canisters {
            if let Some(remote) = &canister_config.remote {
                for (network_name, canister_id) in &remote.id {
                    let canister_id = canister_id.to_string();
                    match remote_ids.get_mut(canister_name) {
                        Some(network_name_to_canister_id) => {
                            network_name_to_canister_id
                                .insert(network_name.to_string(), canister_id);
                        }
                        None => {
                            let mut network_name_to_canister_id = NetworkNametoCanisterId::new();
                            network_name_to_canister_id
                                .insert(network_name.to_string(), canister_id);
                            remote_ids
                                .insert(canister_name.to_string(), network_name_to_canister_id);
                        }
                    }
                }
            }
        }
    }
    if remote_ids.is_empty() {
        None
    } else {
        Some(remote_ids)
    }
}


-----------------------

/src/dfx-core/src/config/model/dfinity.rs:
-----------------------

#![allow(dead_code)]
#![allow(clippy::should_implement_trait)] // for from_str.  why now?
use crate::config::directories::get_user_dfx_config_dir;
use crate::config::model::bitcoin_adapter::BitcoinAdapterLogLevel;
use crate::config::model::canister_http_adapter::HttpAdapterLogLevel;
use crate::config::model::extension_canister_type::apply_extension_canister_types;
use crate::error::config::{GetOutputEnvFileError, GetTempPathError};
use crate::error::dfx_config::AddDependenciesError::CanisterCircularDependency;
use crate::error::dfx_config::GetCanisterNamesWithDependenciesError::AddDependenciesFailed;
use crate::error::dfx_config::GetComputeAllocationError::GetComputeAllocationFailed;
use crate::error::dfx_config::GetFreezingThresholdError::GetFreezingThresholdFailed;
use crate::error::dfx_config::GetLogVisibilityError::GetLogVisibilityFailed;
use crate::error::dfx_config::GetMemoryAllocationError::GetMemoryAllocationFailed;
use crate::error::dfx_config::GetPullCanistersError::PullCanistersSameId;
use crate::error::dfx_config::GetRemoteCanisterIdError::GetRemoteCanisterIdFailed;
use crate::error::dfx_config::GetReservedCyclesLimitError::GetReservedCyclesLimitFailed;
use crate::error::dfx_config::GetSpecifiedIdError::GetSpecifiedIdFailed;
use crate::error::dfx_config::GetWasmMemoryLimitError::GetWasmMemoryLimitFailed;
use crate::error::dfx_config::{
    AddDependenciesError, GetCanisterConfigError, GetCanisterNamesWithDependenciesError,
    GetComputeAllocationError, GetFreezingThresholdError, GetLogVisibilityError,
    GetMemoryAllocationError, GetPullCanistersError, GetRemoteCanisterIdError,
    GetReservedCyclesLimitError, GetSpecifiedIdError, GetWasmMemoryLimitError,
};
use crate::error::fs::CanonicalizePathError;
use crate::error::load_dfx_config::LoadDfxConfigError;
use crate::error::load_dfx_config::LoadDfxConfigError::{
    DetermineCurrentWorkingDirFailed, ResolveConfigPath,
};
use crate::error::load_networks_config::LoadNetworksConfigError;
use crate::error::load_networks_config::LoadNetworksConfigError::{
    GetConfigPathFailed, LoadConfigFromFileFailed,
};
use crate::error::socket_addr_conversion::SocketAddrConversionError;
use crate::error::socket_addr_conversion::SocketAddrConversionError::{
    EmptyIterator, ParseSocketAddrFailed,
};
use crate::error::structured_file::StructuredFileError;
use crate::error::structured_file::StructuredFileError::DeserializeJsonFileFailed;
use crate::extension::manager::ExtensionManager;
use crate::fs::create_dir_all;
use crate::json::save_json_file;
use crate::json::structure::{PossiblyStr, SerdeVec};
use crate::util::ByteSchema;
use byte_unit::Byte;
use candid::Principal;
use ic_utils::interfaces::management_canister::LogVisibility;
use schemars::JsonSchema;
use serde::de::{Error as _, MapAccess, Visitor};
use serde::{Deserialize, Deserializer, Serialize};
use serde_json::Value;
use std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};
use std::default::Default;
use std::fmt;
use std::net::{IpAddr, Ipv4Addr, SocketAddr, ToSocketAddrs};
use std::path::{Path, PathBuf};
use std::time::Duration;

use super::network_descriptor::MOTOKO_PLAYGROUND_CANISTER_TIMEOUT_SECONDS;

pub const CONFIG_FILE_NAME: &str = "dfx.json";

pub const BUILTIN_CANISTER_TYPES: [&str; 5] = ["rust", "motoko", "assets", "custom", "pull"];

const EMPTY_CONFIG_DEFAULTS: ConfigDefaults = ConfigDefaults {
    bitcoin: None,
    bootstrap: None,
    build: None,
    canister_http: None,
    proxy: None,
    replica: None,
};

const EMPTY_CONFIG_DEFAULTS_BUILD: ConfigDefaultsBuild = ConfigDefaultsBuild {
    packtool: None,
    args: None,
};

/// # Remote Canister Configuration
/// This field allows canisters to be marked 'remote' for certain networks.
/// On networks where this canister contains a remote ID, the canister is not deployed.
/// Instead it is assumed to exist already under control of a different project.
#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
pub struct ConfigCanistersCanisterRemote {
    /// # Remote Candid File
    /// On networks where this canister is marked 'remote', this candid file is used instead of the one declared in the canister settings.
    pub candid: Option<PathBuf>,

    /// # Network to Remote ID Mapping
    /// This field contains mappings from network names to remote canister IDs (Principals).
    /// For all networks listed here, this canister is considered 'remote'.
    #[schemars(with = "BTreeMap<String, String>")]
    pub id: BTreeMap<String, Principal>,
}

/// # Wasm Optimization Levels
/// Wasm optimization levels that are passed to `wasm-opt`. "cycles" defaults to O3, "size" defaults to Oz.
/// O4 through O0 focus on performance (with O0 performing no optimizations), and Oz and Os focus on reducing binary size, where Oz is more aggressive than Os.
/// O3 and Oz empirically give best cycle savings and code size savings respectively.
#[derive(Copy, Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub enum WasmOptLevel {
    #[serde(rename = "cycles")]
    Cycles,
    #[serde(rename = "size")]
    Size,
    O4,
    O3,
    O2,
    O1,
    O0,
    Oz,
    Os,
}
impl std::fmt::Display for WasmOptLevel {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        std::fmt::Debug::fmt(self, f)
    }
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema, Default)]
#[serde(rename_all = "lowercase")]
pub enum MetadataVisibility {
    /// Anyone can query the metadata
    #[default]
    Public,

    /// Only the controllers of the canister can query the metadata.
    Private,
}

/// # Canister Metadata Configuration
/// Configures a custom metadata section for the canister wasm.
/// dfx uses the first definition of a given name matching the current network, ignoring any of the same name that follow.
#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
pub struct CanisterMetadataSection {
    /// # Name
    /// The name of the wasm section
    pub name: String,

    /// # Visibility
    #[serde(default)]
    pub visibility: MetadataVisibility,

    /// # Networks
    /// Networks this section applies to.
    /// If this field is absent, then it applies to all networks.
    /// An empty array means this element will not apply to any network.
    pub networks: Option<BTreeSet<String>>,

    /// # Path
    /// Path to file containing section contents.
    /// Conflicts with `content`.
    /// For sections with name=`candid:service`, this field is optional, and if not specified, dfx will use
    /// the canister's candid definition.
    /// If specified for a Motoko canister, the service defined in the specified path must be a valid subtype of the canister's
    /// actual candid service definition.
    pub path: Option<PathBuf>,

    /// # Content
    /// Content of this metadata section.
    /// Conflicts with `path`.
    pub content: Option<String>,
}

impl CanisterMetadataSection {
    pub fn applies_to_network(&self, network: &str) -> bool {
        self.networks
            .as_ref()
            .map(|networks| networks.contains(network))
            .unwrap_or(true)
    }
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
pub struct Pullable {
    /// # wasm_url
    /// The Url to download canister wasm.
    pub wasm_url: String,
    /// # wasm_hash
    /// SHA256 hash of the wasm module located at wasm_url.
    /// Only define this if the on-chain canister wasm is expected not to match the wasm at wasm_url.
    /// The hash can also be specified via a URL using the `wasm_hash_url` field.
    /// If both are defined, the `wasm_hash_url` field will be ignored.
    pub wasm_hash: Option<String>,
    /// # wasm_hash_url
    /// Specify the SHA256 hash of the wasm module via this URL.
    /// Only define this if the on-chain canister wasm is expected not to match the wasm at wasm_url.
    /// The hash can also be specified directly using the `wasm_hash` field.
    /// If both are defined, the `wasm_hash_url` field will be ignored.
    pub wasm_hash_url: Option<String>,
    /// # dependencies
    /// Canister IDs (Principal) of direct dependencies.
    #[schemars(with = "Vec::<String>")]
    pub dependencies: Vec<Principal>,
    /// # init_guide
    /// A message to guide consumers how to initialize the canister.
    pub init_guide: String,
    /// # init_arg
    /// A default initialization argument for the canister that consumers can use.
    pub init_arg: Option<String>,
}

pub type TechStackCategoryMap = HashMap<String, HashMap<String, String>>;

/// # Tech Stack
/// The tech stack used to build a canister.
#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
pub struct TechStack {
    /// # cdk
    #[serde(skip_serializing_if = "Option::is_none")]
    pub cdk: Option<TechStackCategoryMap>,
    /// # language
    #[serde(skip_serializing_if = "Option::is_none")]
    pub language: Option<TechStackCategoryMap>,
    /// # lib
    #[serde(skip_serializing_if = "Option::is_none")]
    pub lib: Option<TechStackCategoryMap>,
    /// # tool
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool: Option<TechStackCategoryMap>,
    /// # other
    #[serde(skip_serializing_if = "Option::is_none")]
    pub other: Option<TechStackCategoryMap>,
}

pub const DEFAULT_SHARED_LOCAL_BIND: &str = "127.0.0.1:4943"; // hex for "IC"
pub const DEFAULT_PROJECT_LOCAL_BIND: &str = "127.0.0.1:8000";
pub const DEFAULT_IC_GATEWAY: &str = "https://icp0.io";
pub const DEFAULT_IC_GATEWAY_TRAILING_SLASH: &str = "https://icp0.io/";
pub const DEFAULT_REPLICA_PORT: u16 = 8080;

/// # Canister Configuration
/// Configurations for a single canister.
#[derive(Clone, Debug, Serialize, Deserialize, JsonSchema)]
pub struct ConfigCanistersCanister {
    /// # Declarations Configuration
    /// Defines which canister interface declarations to generate,
    /// and where to generate them.
    #[serde(default)]
    pub declarations: CanisterDeclarationsConfig,

    /// # Remote Configuration
    /// Used to mark the canister as 'remote' on certain networks.
    #[serde(default)]
    pub remote: Option<ConfigCanistersCanisterRemote>,

    /// # Canister-Specific Build Argument
    /// This field defines an additional argument to pass to the Motoko compiler when building the canister.
    pub args: Option<String>,

    /// # Resource Allocation Settings
    /// Defines initial values for resource allocation settings.
    #[serde(default)]
    pub initialization_values: InitializationValues,

    /// # Dependencies
    /// Defines on which canisters this canister depends on.
    #[serde(default)]
    pub dependencies: Vec<String>,

    /// # Force Frontend URL
    /// Mostly unused.
    /// If this value is not null, a frontend URL is displayed after deployment even if the canister type is not 'asset'.
    pub frontend: Option<BTreeMap<String, String>>,

    /// # Type-Specific Canister Properties
    /// Depending on the canister type, different fields are required.
    /// These are defined in this object.
    #[serde(flatten)]
    pub type_specific: CanisterTypeProperties,

    /// # Post-Install Commands
    /// One or more commands to run post canister installation.
    /// These commands are executed in the root of the project.
    #[serde(default)]
    pub post_install: SerdeVec<String>,

    /// # Path to Canister Entry Point
    /// Entry point for e.g. Motoko Compiler.
    pub main: Option<PathBuf>,

    /// # Shrink Canister Wasm
    /// Whether run `ic-wasm shrink` after building the Canister.
    /// Enabled by default for Rust/Motoko canisters.
    /// Disabled by default for custom canisters.
    pub shrink: Option<bool>,

    /// # Optimize Canister Wasm
    /// Invoke wasm level optimizations after building the canister. Optimization level can be set to "cycles" to optimize for cycle usage, "size" to optimize for binary size, or any of "O4, O3, O2, O1, O0, Oz, Os".
    /// Disabled by default.
    /// If this option is specified, the `shrink` option will be ignored.
    #[serde(default)]
    pub optimize: Option<WasmOptLevel>,

    /// # Metadata
    /// Defines metadata sections to set in the canister .wasm
    #[serde(default)]
    pub metadata: Vec<CanisterMetadataSection>,

    /// # Pullable
    /// Defines required properties so that this canister is ready for `dfx deps pull` by other projects.
    #[serde(default)]
    pub pullable: Option<Pullable>,

    /// # Tech Stack
    /// Defines the tech stack used to build this canister.
    #[serde(default)]
    pub tech_stack: Option<TechStack>,

    /// # Gzip Canister Wasm
    /// Disabled by default.
    pub gzip: Option<bool>,

    /// # Specified Canister ID
    /// Attempts to create the canister with this Canister ID.
    /// This option only works with non-mainnet replica.
    /// If the `--specified-id` argument is also provided, this `specified_id` field will be ignored.
    #[schemars(with = "Option<String>")]
    pub specified_id: Option<Principal>,

    /// # Init Arg
    /// The Candid initialization argument for installing the canister.
    /// If the `--argument` or `--argument-file` argument is also provided, this `init_arg` field will be ignored.
    pub init_arg: Option<String>,

    /// # Init Arg File
    /// The Candid initialization argument file for installing the canister.
    /// If the `--argument` or `--argument-file` argument is also provided, this `init_arg_file` field will be ignored.
    pub init_arg_file: Option<String>,
}

#[derive(Clone, Debug, Serialize, JsonSchema)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum CanisterTypeProperties {
    /// # Rust-Specific Properties
    Rust {
        /// # Package Name
        /// Name of the Rust package that compiles this canister's Wasm.
        package: String,

        /// # Crate name
        /// Name of the Rust crate that compiles to this canister's Wasm.
        /// If left unspecified, defaults to the crate with the same name as the package.
        #[serde(rename = "crate")]
        crate_name: Option<String>,

        /// # Candid File
        /// Path of this canister's candid interface declaration.
        candid: PathBuf,
    },
    /// # Asset-Specific Properties
    Assets {
        /// # Asset Source Folder
        /// Folders from which assets are uploaded.
        source: Vec<PathBuf>,

        /// # Build Commands
        /// Commands that are executed in order to produce this canister's assets.
        /// Expected to produce assets in one of the paths specified by the 'source' field.
        /// Optional if there is no build necessary or the assets can be built using the default `npm run build` command.
        #[schemars(default)]
        build: SerdeVec<String>,

        /// # NPM workspace
        /// The workspace in package.json that this canister is in, if it is not in the root workspace.
        workspace: Option<String>,
    },
    /// # Custom-Specific Properties
    Custom {
        /// # Wasm Path
        /// Path to Wasm to be installed. URLs to a Wasm module are also acceptable.
        /// A canister that has a URL to a Wasm module can not also have `build` steps.
        wasm: String,

        /// # Candid File
        /// Path to this canister's candid interface declaration.  A URL to a candid file is also acceptable.
        candid: String,

        /// # Build Commands
        /// Commands that are executed in order to produce this canister's Wasm module.
        /// Expected to produce the Wasm in the path specified by the 'wasm' field.
        /// No build commands are allowed if the `wasm` field is a URL.
        /// These commands are executed in the root of the project.
        #[schemars(default)]
        build: SerdeVec<String>,
    },
    /// # Motoko-Specific Properties
    Motoko,
    /// # Pull-Specific Properties
    Pull {
        /// # Canister ID
        /// Principal of the canister on the ic network.
        #[schemars(with = "String")]
        id: Principal,
    },
}

impl CanisterTypeProperties {
    pub fn name(&self) -> &'static str {
        match self {
            Self::Rust { .. } => "rust",
            Self::Motoko { .. } => "motoko",
            Self::Assets { .. } => "assets",
            Self::Custom { .. } => "custom",
            Self::Pull { .. } => "pull",
        }
    }
}

#[derive(Clone, Default, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "snake_case")]
pub enum CanisterLogVisibility {
    #[default]
    Controllers,
    Public,
    #[schemars(with = "Vec::<String>")]
    AllowedViewers(Vec<Principal>),
}

impl From<CanisterLogVisibility> for LogVisibility {
    fn from(value: CanisterLogVisibility) -> Self {
        match value {
            CanisterLogVisibility::Controllers => LogVisibility::Controllers,
            CanisterLogVisibility::Public => LogVisibility::Public,
            CanisterLogVisibility::AllowedViewers(viewers) => {
                LogVisibility::AllowedViewers(viewers)
            }
        }
    }
}

/// # Initial Resource Allocations
#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
#[serde(default)]
pub struct InitializationValues {
    /// # Compute Allocation
    /// Must be a number between 0 and 100, inclusively.
    /// It indicates how much compute power should be guaranteed to this canister, expressed as a percentage of the maximum compute power that a single canister can allocate.
    pub compute_allocation: Option<PossiblyStr<u64>>,

    /// # Memory Allocation
    /// Maximum memory (in bytes) this canister is allowed to occupy.
    /// Can be specified as an integer, or as an SI unit string (e.g. "4KB", "2 MiB")
    #[schemars(with = "Option<ByteSchema>")]
    pub memory_allocation: Option<Byte>,

    /// # Freezing Threshold
    /// Freezing threshould of the canister, measured in seconds.
    /// Valid inputs are numbers (seconds) or strings parsable by humantime (e.g. "15days 2min 2s").
    #[serde(with = "humantime_serde")]
    #[schemars(with = "Option<String>")]
    pub freezing_threshold: Option<Duration>,

    /// # Reserved Cycles Limit
    /// Specifies the upper limit of the canister's reserved cycles balance.
    ///
    /// Reserved cycles are cycles that the system sets aside for future use by the canister.
    /// If a subnet's storage exceeds 450 GiB, then every time a canister allocates new storage bytes,
    /// the system sets aside some amount of cycles from the main balance of the canister.
    /// These reserved cycles will be used to cover future payments for the newly allocated bytes.
    /// The reserved cycles are not transferable and the amount of reserved cycles depends on how full the subnet is.
    ///
    /// A setting of 0 means that the canister will trap if it tries to allocate new storage while the subnet's memory usage exceeds 450 GiB.
    #[schemars(with = "Option<u128>")]
    pub reserved_cycles_limit: Option<u128>,

    /// # Wasm Memory Limit
    /// Specifies a soft limit (in bytes) on the Wasm memory usage of the canister.
    ///
    /// Update calls, timers, heartbeats, installs, and post-upgrades fail if the
    /// Wasm memory usage exceeds this limit. The main purpose of this setting is
    /// to protect against the case when the canister reaches the hard 4GiB
    /// limit.
    ///
    /// Must be a number of bytes between 0 and 2^48 (i.e. 256 TiB), inclusive.
    /// Can be specified as an integer, or as an SI unit string (e.g. "4KB", "2 MiB")
    #[schemars(with = "Option<ByteSchema>")]
    pub wasm_memory_limit: Option<Byte>,

    /// # Log Visibility
    /// Specifies who is allowed to read the canister's logs.
    ///
    /// Can be "public", "controllers" or "allowed_viewers" with a list of principals.
    #[schemars(with = "Option<CanisterLogVisibility>")]
    pub log_visibility: Option<CanisterLogVisibility>,
}

/// # Declarations Configuration
/// Configurations about which canister interface declarations to generate,
/// and where to generate them.
#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
pub struct CanisterDeclarationsConfig {
    /// # Declaration Output Directory
    /// Directory to place declarations for that canister.
    /// Default is 'src/declarations/<canister_name>'.
    pub output: Option<PathBuf>,

    /// # Languages to generate
    /// A list of languages to generate type declarations.
    /// Supported options are 'js', 'ts', 'did', 'mo'.
    /// Default is ['js', 'ts', 'did'].
    pub bindings: Option<Vec<String>>,

    /// # Canister ID ENV Override
    /// A string that will replace process.env.CANISTER_ID_{canister_name_uppercase}
    /// in the 'src/dfx/assets/language_bindings/canister.js' template.
    pub env_override: Option<String>,

    /// # Node compatibility flag
    /// Flag to pre-populate generated declarations with better defaults for various types of projects
    /// Default is false
    #[serde(default)]
    pub node_compatibility: bool,
}

/// # Bitcoin Adapter Configuration
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]
pub struct ConfigDefaultsBitcoin {
    /// # Enable Bitcoin Adapter
    #[serde(default)]
    pub enabled: bool,

    /// # Available Nodes
    /// Addresses of nodes to connect to (in case discovery from seeds is not possible/sufficient).
    #[serde(default)]
    pub nodes: Option<Vec<SocketAddr>>,

    /// # Logging Level
    /// The logging level of the adapter.
    #[serde(default = "default_bitcoin_log_level")]
    pub log_level: BitcoinAdapterLogLevel,

    /// # Initialization Argument
    /// The initialization argument for the bitcoin canister.
    #[serde(default = "default_bitcoin_canister_init_arg")]
    pub canister_init_arg: String,
}

pub fn default_bitcoin_log_level() -> BitcoinAdapterLogLevel {
    BitcoinAdapterLogLevel::Info
}

pub fn default_bitcoin_canister_init_arg() -> String {
    "(record { stability_threshold = 0 : nat; network = variant { regtest }; blocks_source = principal \"aaaaa-aa\"; fees = record { get_utxos_base = 0 : nat; get_utxos_cycles_per_ten_instructions = 0 : nat; get_utxos_maximum = 0 : nat; get_balance = 0 : nat; get_balance_maximum = 0 : nat; get_current_fee_percentiles = 0 : nat; get_current_fee_percentiles_maximum = 0 : nat;  send_transaction_base = 0 : nat; send_transaction_per_byte = 0 : nat; }; syncing = variant { enabled }; api_access = variant { enabled }; disable_api_if_not_fully_synced = variant { enabled }})".to_string()
}

impl Default for ConfigDefaultsBitcoin {
    fn default() -> Self {
        ConfigDefaultsBitcoin {
            enabled: false,
            nodes: None,
            log_level: default_bitcoin_log_level(),
            canister_init_arg: default_bitcoin_canister_init_arg(),
        }
    }
}

/// # HTTP Adapter Configuration
#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub struct ConfigDefaultsCanisterHttp {
    /// # Enable HTTP Adapter
    #[serde(default = "default_as_true")]
    pub enabled: bool,

    /// # Logging Level
    /// The logging level of the adapter.
    #[serde(default)]
    pub log_level: HttpAdapterLogLevel,
}

impl Default for ConfigDefaultsCanisterHttp {
    fn default() -> Self {
        ConfigDefaultsCanisterHttp {
            enabled: true,
            log_level: HttpAdapterLogLevel::default(),
        }
    }
}

fn default_as_true() -> bool {
    // sigh https://github.com/serde-rs/serde/issues/368
    true
}

/// # Bootstrap Server Configuration
/// The bootstrap command has been removed.  All of these fields are ignored.
#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub struct ConfigDefaultsBootstrap {
    /// Specifies the IP address that the bootstrap server listens on. Defaults to 127.0.0.1.
    #[serde(default = "default_bootstrap_ip")]
    pub ip: IpAddr,

    /// Specifies the port number that the bootstrap server listens on. Defaults to 8081.
    #[serde(default = "default_bootstrap_port")]
    pub port: u16,

    /// Specifies the maximum number of seconds that the bootstrap server
    /// will wait for upstream requests to complete. Defaults to 30.
    #[serde(default = "default_bootstrap_timeout")]
    pub timeout: u64,
}

pub fn default_bootstrap_ip() -> IpAddr {
    IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1))
}
pub fn default_bootstrap_port() -> u16 {
    8081
}
pub fn default_bootstrap_timeout() -> u64 {
    30
}

impl Default for ConfigDefaultsBootstrap {
    fn default() -> Self {
        ConfigDefaultsBootstrap {
            ip: default_bootstrap_ip(),
            port: default_bootstrap_port(),
            timeout: default_bootstrap_timeout(),
        }
    }
}

/// # Build Process Configuration
#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
pub struct ConfigDefaultsBuild {
    /// Main command to run the packtool.
    /// This command is executed in the root of the project.
    pub packtool: Option<String>,

    /// Arguments for packtool.
    pub args: Option<String>,
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "lowercase")]
pub enum ReplicaLogLevel {
    Critical,
    Error,
    Warning,
    Info,
    Debug,
    Trace,
}

impl Default for ReplicaLogLevel {
    fn default() -> Self {
        Self::Error
    }
}

impl ReplicaLogLevel {
    pub fn to_ic_starter_string(&self) -> String {
        match self {
            Self::Critical => "critical".to_string(),
            Self::Error => "error".to_string(),
            Self::Warning => "warning".to_string(),
            Self::Info => "info".to_string(),
            Self::Debug => "debug".to_string(),
            Self::Trace => "trace".to_string(),
        }
    }
}

/// # Local Replica Configuration
#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub struct ConfigDefaultsReplica {
    /// Port the replica listens on.
    pub port: Option<u16>,

    /// # Subnet Type
    /// Determines the subnet type the replica will run as.
    /// Affects things like cycles accounting, message size limits, cycle limits.
    /// Defaults to 'application'.
    pub subnet_type: Option<ReplicaSubnetType>,

    /// Run replica with the provided log level. Default is 'error'. Debug prints still get displayed
    pub log_level: Option<ReplicaLogLevel>,
}

/// Configuration for the HTTP gateway.
#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub struct ConfigDefaultsProxy {
    /// A list of domains that can be served. These are used for canister resolution [default: localhost]
    pub domain: Option<SerdeVec<String>>,
}

// Schemars doesn't add the enum value's docstrings. Therefore the explanations have to be up here.
/// # Network Type
/// Type 'ephemeral' is used for networks that are regularly reset.
/// Type 'persistent' is used for networks that last for a long time and where it is preferred that canister IDs get stored in source control.
#[derive(Copy, Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema, Default)]
#[serde(rename_all = "lowercase")]
pub enum NetworkType {
    // We store ephemeral canister ids in .dfx/{network}/canister_ids.json
    #[default]
    Ephemeral,

    // We store persistent canister ids in canister_ids.json (adjacent to dfx.json)
    Persistent,
}

impl NetworkType {
    fn ephemeral() -> Self {
        NetworkType::Ephemeral
    }
    fn persistent() -> Self {
        NetworkType::Persistent
    }
}

#[derive(Copy, Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema, Default)]
#[serde(rename_all = "lowercase")]
pub enum ReplicaSubnetType {
    System,
    #[default]
    Application,
    VerifiedApplication,
}

impl ReplicaSubnetType {
    /// Converts the value to the string expected by ic-starter for its --subnet-type argument
    pub fn as_ic_starter_string(&self) -> String {
        match self {
            ReplicaSubnetType::System => "system".to_string(),
            ReplicaSubnetType::Application => "application".to_string(),
            ReplicaSubnetType::VerifiedApplication => "verified_application".to_string(),
        }
    }
}

fn default_playground_timeout_seconds() -> u64 {
    MOTOKO_PLAYGROUND_CANISTER_TIMEOUT_SECONDS
}

/// Playground config to borrow canister from instead of creating new canisters.
#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub struct PlaygroundConfig {
    /// Canister ID of the playground canister
    pub playground_canister: String,

    /// How many seconds a canister can be borrowed for
    #[serde(default = "default_playground_timeout_seconds")]
    pub timeout_seconds: u64,
}

/// # Custom Network Configuration
#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub struct ConfigNetworkProvider {
    /// The URL(s) this network can be reached at.
    pub providers: Vec<String>,

    /// Persistence type of this network.
    #[serde(default = "NetworkType::persistent")]
    pub r#type: NetworkType,
    pub playground: Option<PlaygroundConfig>,
}

/// # Local Replica Configuration
#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]
pub struct ConfigLocalProvider {
    /// Bind address for the webserver.
    /// For the shared local network, the default is 127.0.0.1:4943.
    /// For project-specific local networks, the default is 127.0.0.1:8000.
    pub bind: Option<String>,

    /// Persistence type of this network.
    #[serde(default = "NetworkType::ephemeral")]
    pub r#type: NetworkType,

    pub bitcoin: Option<ConfigDefaultsBitcoin>,
    pub bootstrap: Option<ConfigDefaultsBootstrap>,
    pub canister_http: Option<ConfigDefaultsCanisterHttp>,
    pub replica: Option<ConfigDefaultsReplica>,
    pub playground: Option<PlaygroundConfig>,
    pub proxy: Option<ConfigDefaultsProxy>,
}

#[derive(Debug, Clone, PartialEq, Eq, Deserialize, Serialize, JsonSchema)]
#[serde(untagged)]
pub enum ConfigNetwork {
    ConfigNetworkProvider(ConfigNetworkProvider),
    ConfigLocalProvider(ConfigLocalProvider),
}

#[derive(Copy, Clone, Debug, Serialize, Deserialize, JsonSchema)]
pub enum Profile {
    // debug is for development only
    Debug,
    // release is for production
    Release,
}

/// Defaults to use on dfx start.
#[derive(Clone, Debug, Default, Serialize, Deserialize, JsonSchema)]
pub struct ConfigDefaults {
    pub bitcoin: Option<ConfigDefaultsBitcoin>,
    pub bootstrap: Option<ConfigDefaultsBootstrap>,
    pub build: Option<ConfigDefaultsBuild>,
    pub canister_http: Option<ConfigDefaultsCanisterHttp>,
    pub proxy: Option<ConfigDefaultsProxy>,
    pub replica: Option<ConfigDefaultsReplica>,
}

/// # dfx.json
#[derive(Clone, Debug, Serialize, Deserialize, JsonSchema)]
pub struct ConfigInterface {
    pub profile: Option<Profile>,

    /// Used to keep track of dfx.json versions.
    pub version: Option<u32>,

    /// # dfx version
    /// Pins the dfx version for this project.
    pub dfx: Option<String>,

    /// Mapping between canisters and their settings.
    pub canisters: Option<BTreeMap<String, ConfigCanistersCanister>>,

    /// Defaults for dfx start.
    pub defaults: Option<ConfigDefaults>,

    /// Mapping between network names and their configurations.
    /// Networks 'ic' and 'local' are implicitly defined.
    pub networks: Option<BTreeMap<String, ConfigNetwork>>,

    /// If set, environment variables will be output to this file (without overwriting any user-defined variables, if the file already exists).
    pub output_env_file: Option<PathBuf>,
}

pub type TopLevelConfigNetworks = BTreeMap<String, ConfigNetwork>;

#[derive(Clone, Debug, Serialize, Deserialize, JsonSchema)]
pub struct NetworksConfigInterface {
    pub networks: TopLevelConfigNetworks,
}

impl ConfigCanistersCanister {}

pub fn to_socket_addr(s: &str) -> Result<SocketAddr, SocketAddrConversionError> {
    match s.to_socket_addrs() {
        Ok(mut a) => match a.next() {
            Some(res) => Ok(res),
            None => Err(EmptyIterator(s.to_string())),
        },
        Err(err) => Err(ParseSocketAddrFailed(s.to_string(), err)),
    }
}

impl ConfigDefaultsBuild {
    pub fn get_packtool(&self) -> Option<String> {
        match &self.packtool {
            Some(v) if !v.is_empty() => self.packtool.to_owned(),
            _ => None,
        }
    }
    pub fn get_args(&self) -> Option<String> {
        match &self.args {
            Some(v) if !v.is_empty() => self.args.to_owned(),
            _ => None,
        }
    }
}

impl ConfigDefaults {
    pub fn get_build(&self) -> &ConfigDefaultsBuild {
        match &self.build {
            Some(x) => x,
            None => &EMPTY_CONFIG_DEFAULTS_BUILD,
        }
    }
}

impl NetworksConfigInterface {
    pub fn get_network(&self, name: &str) -> Option<&ConfigNetwork> {
        self.networks.get(name)
    }
}

impl ConfigInterface {
    pub fn get_defaults(&self) -> &ConfigDefaults {
        match &self.defaults {
            Some(v) => v,
            _ => &EMPTY_CONFIG_DEFAULTS,
        }
    }

    pub fn get_network(&self, name: &str) -> Option<&ConfigNetwork> {
        self.networks
            .as_ref()
            .and_then(|networks| networks.get(name))
    }

    pub fn get_version(&self) -> u32 {
        self.version.unwrap_or(1)
    }
    pub fn get_dfx(&self) -> Option<String> {
        self.dfx.to_owned()
    }

    /// Return the names of the specified canister and all of its dependencies.
    /// If none specified, return the names of all canisters.
    pub fn get_canister_names_with_dependencies(
        &self,
        some_canister: Option<&str>,
    ) -> Result<Vec<String>, GetCanisterNamesWithDependenciesError> {
        self.canisters
            .as_ref()
            .ok_or(GetCanisterNamesWithDependenciesError::CanistersFieldDoesNotExist())
            .and_then(|canister_map| match some_canister {
                Some(specific_canister) => {
                    let mut names = HashSet::new();
                    let mut path = vec![];
                    add_dependencies(canister_map, &mut names, &mut path, specific_canister)
                        .map(|_| names.into_iter().collect())
                        .map_err(|err| AddDependenciesFailed(specific_canister.to_string(), err))
                }
                None => Ok(canister_map.keys().cloned().collect()),
            })
    }

    pub fn get_remote_canister_id(
        &self,
        canister: &str,
        network: &str,
    ) -> Result<Option<Principal>, GetRemoteCanisterIdError> {
        let maybe_principal = self
            .get_canister_config(canister)
            .map_err(|e| {
                GetRemoteCanisterIdFailed(
                    Box::new(canister.to_string()),
                    Box::new(network.to_string()),
                    e,
                )
            })?
            .remote
            .as_ref()
            .and_then(|r| r.id.get(network))
            .copied();

        Ok(maybe_principal)
    }

    pub fn is_remote_canister(
        &self,
        canister: &str,
        network: &str,
    ) -> Result<bool, GetRemoteCanisterIdError> {
        Ok(self.get_remote_canister_id(canister, network)?.is_some())
    }

    pub fn get_compute_allocation(
        &self,
        canister_name: &str,
    ) -> Result<Option<u64>, GetComputeAllocationError> {
        Ok(self
            .get_canister_config(canister_name)
            .map_err(|e| GetComputeAllocationFailed(canister_name.to_string(), e))?
            .initialization_values
            .compute_allocation
            .map(|x| x.0))
    }

    pub fn get_memory_allocation(
        &self,
        canister_name: &str,
    ) -> Result<Option<Byte>, GetMemoryAllocationError> {
        Ok(self
            .get_canister_config(canister_name)
            .map_err(|e| GetMemoryAllocationFailed(canister_name.to_string(), e))?
            .initialization_values
            .memory_allocation)
    }

    pub fn get_freezing_threshold(
        &self,
        canister_name: &str,
    ) -> Result<Option<Duration>, GetFreezingThresholdError> {
        Ok(self
            .get_canister_config(canister_name)
            .map_err(|e| GetFreezingThresholdFailed(canister_name.to_string(), e))?
            .initialization_values
            .freezing_threshold)
    }

    pub fn get_reserved_cycles_limit(
        &self,
        canister_name: &str,
    ) -> Result<Option<u128>, GetReservedCyclesLimitError> {
        Ok(self
            .get_canister_config(canister_name)
            .map_err(|e| GetReservedCyclesLimitFailed(canister_name.to_string(), e))?
            .initialization_values
            .reserved_cycles_limit)
    }

    pub fn get_wasm_memory_limit(
        &self,
        canister_name: &str,
    ) -> Result<Option<Byte>, GetWasmMemoryLimitError> {
        Ok(self
            .get_canister_config(canister_name)
            .map_err(|e| GetWasmMemoryLimitFailed(canister_name.to_string(), e))?
            .initialization_values
            .wasm_memory_limit)
    }

    pub fn get_log_visibility(
        &self,
        canister_name: &str,
    ) -> Result<Option<LogVisibility>, GetLogVisibilityError> {
        Ok(self
            .get_canister_config(canister_name)
            .map_err(|e| GetLogVisibilityFailed(canister_name.to_string(), e))?
            .initialization_values
            .log_visibility
            .clone()
            .map(|visibility| visibility.into()))
    }

    fn get_canister_config(
        &self,
        canister_name: &str,
    ) -> Result<&ConfigCanistersCanister, GetCanisterConfigError> {
        self.canisters
            .as_ref()
            .ok_or(GetCanisterConfigError::CanistersFieldDoesNotExist())?
            .get(canister_name)
            .ok_or_else(|| GetCanisterConfigError::CanisterNotFound(canister_name.to_string()))
    }

    pub fn get_pull_canisters(&self) -> Result<BTreeMap<String, Principal>, GetPullCanistersError> {
        let mut res = BTreeMap::new();
        let mut id_to_name: BTreeMap<Principal, &String> = BTreeMap::new();
        if let Some(map) = &self.canisters {
            for (k, v) in map {
                if let CanisterTypeProperties::Pull { id } = v.type_specific {
                    if let Some(other_name) = id_to_name.get(&id) {
                        return Err(PullCanistersSameId(other_name.to_string(), k.clone(), id));
                    }
                    res.insert(k.clone(), id);
                    id_to_name.insert(id, k);
                }
            }
        };
        Ok(res)
    }

    pub fn get_specified_id(
        &self,
        canister_name: &str,
    ) -> Result<Option<Principal>, GetSpecifiedIdError> {
        Ok(self
            .get_canister_config(canister_name)
            .map_err(|e| GetSpecifiedIdFailed(canister_name.to_string(), e))?
            .specified_id)
    }
}

fn add_dependencies(
    all_canisters: &BTreeMap<String, ConfigCanistersCanister>,
    names: &mut HashSet<String>,
    path: &mut Vec<String>,
    canister_name: &str,
) -> Result<(), AddDependenciesError> {
    let inserted = names.insert(String::from(canister_name));

    if !inserted {
        return if path.contains(&String::from(canister_name)) {
            path.push(String::from(canister_name));
            Err(CanisterCircularDependency(path.clone()))
        } else {
            Ok(())
        };
    }

    let canister_config = all_canisters
        .get(canister_name)
        .ok_or_else(|| AddDependenciesError::CanisterNotFound(canister_name.to_string()))?;

    path.push(String::from(canister_name));

    for canister in &canister_config.dependencies {
        add_dependencies(all_canisters, names, path, canister)?;
    }

    path.pop();

    Ok(())
}

#[derive(Clone, Debug)]
pub struct Config {
    path: PathBuf,
    json: Value,
    // public interface to the config:
    pub config: ConfigInterface,
}

#[allow(dead_code)]
impl Config {
    fn resolve_config_path(working_dir: &Path) -> Result<Option<PathBuf>, CanonicalizePathError> {
        let mut curr = crate::fs::canonicalize(working_dir)?;
        while curr.parent().is_some() {
            if curr.join(CONFIG_FILE_NAME).is_file() {
                return Ok(Some(curr.join(CONFIG_FILE_NAME)));
            } else {
                curr.pop();
            }
        }

        // Have to check if the config could be in the root (e.g. on VMs / CI).
        if curr.join(CONFIG_FILE_NAME).is_file() {
            return Ok(Some(curr.join(CONFIG_FILE_NAME)));
        }

        Ok(None)
    }

    fn from_file(
        path: &Path,
        extension_manager: Option<&ExtensionManager>,
    ) -> Result<Config, LoadDfxConfigError> {
        let content = crate::fs::read(path)?;
        Config::from_slice(path.to_path_buf(), &content, extension_manager)
    }

    pub fn from_dir(
        working_dir: &Path,
        extension_manager: Option<&ExtensionManager>,
    ) -> Result<Option<Config>, LoadDfxConfigError> {
        let path = Config::resolve_config_path(working_dir).map_err(ResolveConfigPath)?;
        path.map(|path| Config::from_file(&path, extension_manager))
            .transpose()
    }

    pub fn from_current_dir(
        extension_manager: Option<&ExtensionManager>,
    ) -> Result<Option<Config>, LoadDfxConfigError> {
        let working_dir = std::env::current_dir().map_err(DetermineCurrentWorkingDirFailed)?;
        Config::from_dir(&working_dir, extension_manager)
    }

    fn from_slice(
        path: PathBuf,
        content: &[u8],
        extension_manager: Option<&ExtensionManager>,
    ) -> Result<Config, LoadDfxConfigError> {
        let json: Value = serde_json::from_slice(content)
            .map_err(|e| LoadDfxConfigError::DeserializeValueFailed(Box::new(path.clone()), e))?;
        let effective_json = apply_extension_canister_types(json.clone(), extension_manager)?;

        let config = serde_json::from_value(effective_json)
            .map_err(|e| LoadDfxConfigError::DeserializeValueFailed(Box::new(path.clone()), e))?;
        Ok(Config { path, json, config })
    }

    /// Create a configuration from a string.
    #[cfg(test)]
    pub(crate) fn from_str(content: &str) -> Result<Config, StructuredFileError> {
        Ok(Config::from_slice(PathBuf::from("-"), content.as_bytes(), None).unwrap())
    }

    #[cfg(test)]
    pub(crate) fn from_str_and_path(
        path: PathBuf,
        content: &str,
    ) -> Result<Config, StructuredFileError> {
        Ok(Config::from_slice(path, content.as_bytes(), None).unwrap())
    }

    pub fn get_path(&self) -> &PathBuf {
        &self.path
    }
    pub fn get_temp_path(&self) -> Result<PathBuf, GetTempPathError> {
        let path = self.get_path().parent().unwrap().join(".dfx");
        create_dir_all(&path)?;
        Ok(path)
    }
    pub fn get_json(&self) -> &Value {
        &self.json
    }
    pub fn get_mut_json(&mut self) -> &mut Value {
        &mut self.json
    }
    pub fn get_config(&self) -> &ConfigInterface {
        &self.config
    }

    pub fn get_project_root(&self) -> &Path {
        // a configuration path contains a file name specifically. As
        // such we should be returning at least root as parent. If
        // this is invariance is broken, we must fail.
        self.path.parent().expect(
            "An incorrect configuration path was set with no parent, i.e. did not include root",
        )
    }

    // returns the path to the output env file if any, guaranteed to be
    // a child relative to the project root
    pub fn get_output_env_file(
        &self,
        from_cmdline: Option<PathBuf>,
    ) -> Result<Option<PathBuf>, GetOutputEnvFileError> {
        from_cmdline
            .or(self.config.output_env_file.clone())
            .map(|p| {
                if p.is_relative() {
                    let p = self.get_project_root().join(p);

                    // cannot canonicalize a path that doesn't exist, but the parent should exist
                    let env_parent = crate::fs::parent(&p)?;
                    let env_parent = crate::fs::canonicalize(&env_parent)?;
                    if !env_parent.starts_with(self.get_project_root()) {
                        Err(GetOutputEnvFileError::OutputEnvFileMustBeInProjectRoot(p))
                    } else {
                        Ok(self.get_project_root().join(p))
                    }
                } else {
                    Err(GetOutputEnvFileError::OutputEnvFileMustBeRelative(p))
                }
            })
            .transpose()
    }

    pub fn save(&self) -> Result<(), StructuredFileError> {
        save_json_file(&self.path, &self.json)
    }
}

// grumble grumble https://github.com/serde-rs/serde/issues/2231
impl<'de> Deserialize<'de> for CanisterTypeProperties {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        deserializer.deserialize_map(PropertiesVisitor)
    }
}

struct PropertiesVisitor;

impl<'de> Visitor<'de> for PropertiesVisitor {
    type Value = CanisterTypeProperties;
    fn expecting(&self, f: &mut fmt::Formatter) -> fmt::Result {
        f.write_str("canister type metadata")
    }
    fn visit_map<A>(self, mut map: A) -> Result<Self::Value, A::Error>
    where
        A: MapAccess<'de>,
    {
        let missing_field = A::Error::missing_field;
        let mut wasm = None;
        let mut candid = None;
        let mut package = None;
        let mut crate_name = None;
        let mut source = None;
        let mut build = None;
        let mut r#type = None;
        let mut id = None;
        let mut workspace = None;
        while let Some(key) = map.next_key::<String>()? {
            match &*key {
                "package" => package = Some(map.next_value()?),
                "crate" => crate_name = Some(map.next_value()?),
                "source" => source = Some(map.next_value()?),
                "candid" => candid = Some(map.next_value()?),
                "build" => build = Some(map.next_value()?),
                "wasm" => wasm = Some(map.next_value()?),
                "type" => r#type = Some(map.next_value::<String>()?),
                "id" => id = Some(map.next_value()?),
                "workspace" => workspace = Some(map.next_value()?),
                _ => continue,
            }
        }
        let props = match r#type.as_deref() {
            Some("motoko") | None => CanisterTypeProperties::Motoko,
            Some("rust") => CanisterTypeProperties::Rust {
                candid: PathBuf::from(candid.ok_or_else(|| missing_field("candid"))?),
                package: package.ok_or_else(|| missing_field("package"))?,
                crate_name,
            },
            Some("assets") => CanisterTypeProperties::Assets {
                source: source.ok_or_else(|| missing_field("source"))?,
                build: build.unwrap_or_default(),
                workspace,
            },
            Some("custom") => CanisterTypeProperties::Custom {
                build: build.unwrap_or_default(),
                candid: candid.ok_or_else(|| missing_field("candid"))?,
                wasm: wasm.ok_or_else(|| missing_field("wasm"))?,
            },
            Some("pull") => CanisterTypeProperties::Pull {
                id: id.ok_or_else(|| missing_field("id"))?,
            },
            Some(x) => return Err(A::Error::unknown_variant(x, &BUILTIN_CANISTER_TYPES)),
        };
        Ok(props)
    }
}

#[derive(Clone)]
pub struct NetworksConfig {
    path: PathBuf,
    json: Value,
    // public interface to the networsk config:
    networks_config: NetworksConfigInterface,
}

impl NetworksConfig {
    pub fn get_path(&self) -> &PathBuf {
        &self.path
    }
    pub fn get_interface(&self) -> &NetworksConfigInterface {
        &self.networks_config
    }

    pub fn new() -> Result<NetworksConfig, LoadNetworksConfigError> {
        let dir = get_user_dfx_config_dir().map_err(GetConfigPathFailed)?;

        let path = dir.join("networks.json");
        if path.exists() {
            NetworksConfig::from_file(&path).map_err(LoadConfigFromFileFailed)
        } else {
            Ok(NetworksConfig {
                path,
                json: Default::default(),
                networks_config: NetworksConfigInterface {
                    networks: BTreeMap::new(),
                },
            })
        }
    }

    fn from_file(path: &Path) -> Result<NetworksConfig, StructuredFileError> {
        let content = crate::fs::read(path)?;

        let networks: BTreeMap<String, ConfigNetwork> = serde_json::from_slice(&content)
            .map_err(|e| DeserializeJsonFileFailed(Box::new(path.to_path_buf()), e))?;
        let networks_config = NetworksConfigInterface { networks };
        let json = serde_json::from_slice(&content)
            .map_err(|e| DeserializeJsonFileFailed(Box::new(path.to_path_buf()), e))?;
        let path = PathBuf::from(path);
        Ok(NetworksConfig {
            path,
            json,
            networks_config,
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn find_dfinity_config_current_path() {
        let root_dir = tempfile::tempdir().unwrap();
        let root_path = root_dir.into_path().canonicalize().unwrap();
        let config_path = root_path.join("foo/fah/bar").join(CONFIG_FILE_NAME);

        std::fs::create_dir_all(config_path.parent().unwrap()).unwrap();
        std::fs::write(&config_path, "{}").unwrap();

        assert_eq!(
            config_path,
            Config::resolve_config_path(config_path.parent().unwrap())
                .unwrap()
                .unwrap(),
        );
    }

    #[test]
    fn find_dfinity_config_parent() {
        let root_dir = tempfile::tempdir().unwrap();
        let root_path = root_dir.into_path().canonicalize().unwrap();
        let config_path = root_path.join("foo/fah/bar").join(CONFIG_FILE_NAME);

        std::fs::create_dir_all(config_path.parent().unwrap()).unwrap();
        std::fs::write(&config_path, "{}").unwrap();

        assert!(
            Config::resolve_config_path(config_path.parent().unwrap().parent().unwrap())
                .unwrap()
                .is_none()
        );
    }

    #[test]
    fn find_dfinity_config_subdir() {
        let root_dir = tempfile::tempdir().unwrap();
        let root_path = root_dir.into_path().canonicalize().unwrap();
        let config_path = root_path.join("foo/fah/bar").join(CONFIG_FILE_NAME);
        let subdir_path = config_path.parent().unwrap().join("baz/blue");

        std::fs::create_dir_all(&subdir_path).unwrap();
        std::fs::write(&config_path, "{}").unwrap();

        assert_eq!(
            config_path,
            Config::resolve_config_path(subdir_path.as_path())
                .unwrap()
                .unwrap(),
        );
    }

    #[test]
    fn local_defaults_to_ephemeral() {
        let config = Config::from_str(
            r#"{
            "networks": {
                "local": {
                    "bind": "localhost:8000"
                }
            }
        }"#,
        )
        .unwrap();

        let network = config.get_config().get_network("local").unwrap();
        if let ConfigNetwork::ConfigLocalProvider(local_network) = network {
            assert_eq!(local_network.r#type, NetworkType::Ephemeral);
        } else {
            panic!("not a local provider");
        }
    }

    #[test]
    fn local_can_override_to_persistent() {
        let config = Config::from_str(
            r#"{
            "networks": {
                "local": {
                    "bind": "localhost:8000",
                    "type": "persistent"
                }
            }
        }"#,
        )
        .unwrap();

        let network = config.get_config().get_network("local").unwrap();
        if let ConfigNetwork::ConfigLocalProvider(local_network) = network {
            assert_eq!(local_network.r#type, NetworkType::Persistent);
        } else {
            panic!("not a local provider");
        }
    }

    #[test]
    fn network_defaults_to_persistent() {
        let config = Config::from_str(
            r#"{
            "networks": {
                "somewhere": {
                    "providers": [ "https://1.2.3.4:5000" ]
                }
            }
        }"#,
        )
        .unwrap();

        let network = config.get_config().get_network("somewhere").unwrap();
        if let ConfigNetwork::ConfigNetworkProvider(network_provider) = network {
            assert_eq!(network_provider.r#type, NetworkType::Persistent);
        } else {
            panic!("not a network provider");
        }
    }

    #[test]
    fn network_can_override_to_ephemeral() {
        let config = Config::from_str(
            r#"{
            "networks": {
                "staging": {
                    "providers": [ "https://1.2.3.4:5000" ],
                    "type": "ephemeral"
                }
            }
        }"#,
        )
        .unwrap();

        let network = config.get_config().get_network("staging").unwrap();
        if let ConfigNetwork::ConfigNetworkProvider(network_provider) = network {
            assert_eq!(network_provider.r#type, NetworkType::Ephemeral);
        } else {
            panic!("not a network provider");
        }

        assert_eq!(
            config.get_config().get_network("staging").unwrap(),
            &ConfigNetwork::ConfigNetworkProvider(ConfigNetworkProvider {
                providers: vec![String::from("https://1.2.3.4:5000")],
                r#type: NetworkType::Ephemeral,
                playground: None,
            })
        );
    }

    #[test]
    fn get_correct_initialization_values() {
        let config = Config::from_str(
            r#"{
              "canisters": {
                "test_project": {
                  "initialization_values": {
                    "compute_allocation" : "100",
                    "memory_allocation": "8GB"
                  }
                }
              }
        }"#,
        )
        .unwrap();

        let config_interface = config.get_config();
        let compute_allocation = config_interface
            .get_compute_allocation("test_project")
            .unwrap()
            .unwrap();
        assert_eq!(100, compute_allocation);

        let memory_allocation = config_interface
            .get_memory_allocation("test_project")
            .unwrap()
            .unwrap();
        assert_eq!("8GB".parse::<Byte>().unwrap(), memory_allocation);

        let config_no_values = Config::from_str(
            r#"{
              "canisters": {
                "test_project_two": {
                }
              }
        }"#,
        )
        .unwrap();
        let config_interface = config_no_values.get_config();
        let compute_allocation = config_interface
            .get_compute_allocation("test_project_two")
            .unwrap();
        let memory_allocation = config_interface
            .get_memory_allocation("test_project_two")
            .unwrap();
        assert_eq!(None, compute_allocation);
        assert_eq!(None, memory_allocation);
    }
}


-----------------------

/src/dfx-core/src/config/model/extension_canister_type.rs:
-----------------------

use crate::config::model::dfinity::BUILTIN_CANISTER_TYPES;
use crate::error::config::{
    AppendMetadataError, ApplyExtensionCanisterTypeDefaultsError, ApplyExtensionCanisterTypeError,
    ApplyExtensionCanisterTypesError, MergeTechStackError, RenderErrorWithContext,
};
use crate::extension::manager::ExtensionManager;
use crate::extension::manifest::{extension::ExtensionCanisterType, ExtensionManifest};
use handlebars::{Handlebars, RenderError};
use serde_json::{Map, Value};
use std::collections::{BTreeMap, BTreeSet};

pub fn apply_extension_canister_types(
    mut json: Value,
    extension_manager: Option<&ExtensionManager>,
) -> Result<Value, ApplyExtensionCanisterTypesError> {
    let Some(extension_manager) = extension_manager else {
        return Ok(json);
    };
    let Some(canisters) = json.get_mut("canisters") else {
        return Ok(json);
    };

    let canisters: &mut Map<String, Value> = canisters
        .as_object_mut()
        .ok_or(ApplyExtensionCanisterTypesError::CanistersFieldIsNotAnObject())?;
    for (canister_name, v) in canisters.iter_mut() {
        let canister_json =
            v.as_object_mut()
                .ok_or(ApplyExtensionCanisterTypesError::CanisterIsNotAnObject(
                    canister_name.to_string(),
                ))?;
        apply_extension_canister_type(canister_name, canister_json, extension_manager)?
    }
    Ok(json)
}

fn apply_extension_canister_type(
    canister_name: &str,
    fields: &mut Map<String, Value>,
    extension_manager: &ExtensionManager,
) -> Result<(), ApplyExtensionCanisterTypeError> {
    let canister_type = fields
        .get("type")
        .and_then(|t| t.as_str())
        .unwrap_or("custom")
        .to_string();

    if BUILTIN_CANISTER_TYPES.contains(&canister_type.as_str()) {
        return Ok(());
    }

    let extension = canister_type;
    if !ExtensionManifest::exists(&extension, &extension_manager.dir) {
        return Err(
            ApplyExtensionCanisterTypeError::NoExtensionForUnknownCanisterType {
                canister: canister_name.to_string(),
                extension,
            },
        );
    }
    let manifest = ExtensionManifest::load(&extension, &extension_manager.dir)
        .map_err(ApplyExtensionCanisterTypeError::LoadExtensionManifest)?;
    let extension_canister_type = manifest.canister_type.ok_or(
        ApplyExtensionCanisterTypeError::ExtensionDoesNotDefineCanisterType {
            canister: canister_name.to_string(),
            extension: extension.clone(),
        },
    )?;

    apply_defaults(canister_name, fields, &extension_canister_type).map_err(|source| {
        ApplyExtensionCanisterTypeError::ApplyDefaults {
            canister: Box::new(canister_name.to_string()),
            extension: Box::new(extension),
            source,
        }
    })?;
    fields.insert("type".to_string(), Value::String("custom".to_string()));
    Ok(())
}

fn apply_defaults(
    canister_name: &str,
    canister_json: &mut Map<String, Value>,
    extension_canister_type: &ExtensionCanisterType,
) -> Result<(), ApplyExtensionCanisterTypeDefaultsError> {
    let evaluate_keys = keys_in_evaluation_order(extension_canister_type);

    let handlebars = Handlebars::new();

    for k in evaluate_keys {
        let data = build_render_data(canister_name, canister_json, extension_canister_type);
        let v = extension_canister_type.defaults.get(&k).unwrap();
        let v = recursive_render_templates(v.clone(), &handlebars, &data).map_err(|source| {
            ApplyExtensionCanisterTypeDefaultsError::Render(Box::new(RenderErrorWithContext {
                field: k.to_string(),
                value: v.to_string(),
                source,
            }))
        })?;

        let handled_metadata = k == "metadata" && append_metadata(canister_json, &v)?;
        let handled_tech_stack = k == "tech_stack" && merge_tech_stack(canister_json, &v)?;
        let handled = handled_metadata || handled_tech_stack;
        let already_in_dfx_json = canister_json.contains_key(&k);
        if !handled && !already_in_dfx_json {
            canister_json.insert(k.clone(), v);
        }
    }
    Ok(())
}

fn keys_in_evaluation_order(extension_canister_type: &ExtensionCanisterType) -> Vec<String> {
    let mut remaining_keys = extension_canister_type
        .defaults
        .keys()
        .cloned()
        .collect::<BTreeSet<String>>();
    let mut evaluate_keys = extension_canister_type.evaluation_order.clone();
    for k in evaluate_keys.iter() {
        remaining_keys.remove(k);
    }
    evaluate_keys.extend(remaining_keys);
    evaluate_keys
}

fn recursive_render_templates(
    v: Value,
    handlebars: &Handlebars,
    data: &BTreeMap<String, Value>,
) -> Result<Value, RenderError> {
    match v {
        Value::String(s) => {
            let s = handlebars.render_template(&s, data)?;
            Ok(Value::String(s))
        }
        Value::Array(arr) => {
            let arr = arr
                .into_iter()
                .map(|v| recursive_render_templates(v, handlebars, data))
                .collect::<Result<Vec<Value>, _>>()?;
            Ok(Value::Array(arr))
        }
        Value::Object(obj) => {
            let obj: Result<Map<String, Value>, RenderError> = obj
                .into_iter()
                .map(|(k, v)| {
                    let v: Value = recursive_render_templates(v, handlebars, data)?;
                    Ok((k, v))
                })
                .collect();
            Ok(Value::Object(obj?))
        }
        _ => Ok(v),
    }
}

fn build_render_data(
    canister_name: &str,
    canister_json: &Map<String, Value>,
    extension_canister_type: &ExtensionCanisterType,
) -> BTreeMap<String, Value> {
    let mut data = BTreeMap::new();
    data.insert(
        "canister_name".to_string(),
        Value::String(canister_name.to_string()),
    );
    let mut canister = Map::new();
    canister.extend(extension_canister_type.defaults.clone());
    canister.extend(canister_json.clone());
    data.insert("canister".to_string(), Value::Object(canister));
    data
}

fn append_metadata(
    canister_json: &mut Map<String, Value>,
    extension_metadata: &Value,
) -> Result<bool, AppendMetadataError> {
    let Some(metadata) = canister_json.get_mut("metadata") else {
        return Ok(false);
    };

    let metadata = metadata
        .as_array_mut()
        .ok_or(AppendMetadataError::ExpectedCanisterMetadataArray)?;
    let mut extension_metadata = extension_metadata
        .as_array()
        .ok_or(AppendMetadataError::ExpectedExtensionCanisterTypeMetadataArray)?
        .clone();
    metadata.append(&mut extension_metadata);
    Ok(true)
}

fn merge_tech_stack(
    canister_json: &mut Map<String, Value>,
    extension_tech_stack: &Value,
) -> Result<bool, MergeTechStackError> {
    let Some(canister_tech_stack) = canister_json.get_mut("tech_stack") else {
        return Ok(false);
    };

    merge_tech_stack_maps(canister_tech_stack, extension_tech_stack)?;

    Ok(true)
}

fn merge_tech_stack_maps(
    canister_tech_stack: &mut Value,
    extension_tech_stack: &Value,
) -> Result<(), MergeTechStackError> {
    let canister_tech_stack = canister_tech_stack
        .as_object_mut()
        .ok_or(MergeTechStackError::ExpectedCanisterTechStackObject)?;
    let extension_tech_stack = extension_tech_stack
        .as_object()
        .ok_or(MergeTechStackError::ExpectedExtensionCanisterTypeTechStackObject)?;
    for (k, v) in extension_tech_stack.iter() {
        if let Some(canister_value) = canister_tech_stack.get_mut(k) {
            if canister_value.is_object() {
                merge_tech_stack_maps(canister_value, v)?;
            }
        } else {
            canister_tech_stack.insert(k.clone(), v.clone());
        }
    }
    Ok(())
}


-----------------------

/src/dfx-core/src/config/model/local_server_descriptor.rs:
-----------------------

use crate::config::model::bitcoin_adapter;
use crate::config::model::canister_http_adapter::HttpAdapterLogLevel;
use crate::config::model::dfinity::{
    to_socket_addr, ConfigDefaultsBitcoin, ConfigDefaultsCanisterHttp, ConfigDefaultsProxy,
    ConfigDefaultsReplica, ReplicaLogLevel, ReplicaSubnetType, DEFAULT_PROJECT_LOCAL_BIND,
    DEFAULT_SHARED_LOCAL_BIND,
};
use crate::config::model::replica_config::CachedConfig;
use crate::error::network_config::{
    NetworkConfigError, NetworkConfigError::ParseBindAddressFailed,
};
use crate::error::structured_file::StructuredFileError;
use crate::json::load_json_file;
use crate::json::structure::SerdeVec;
use serde::{Deserialize, Serialize};
use slog::{debug, info, Logger};
use std::net::SocketAddr;
use std::path::{Path, PathBuf};
use time::OffsetDateTime;

#[derive(Deserialize, Serialize, Debug)]
pub struct NetworkMetadata {
    pub created: OffsetDateTime,
    pub settings_digest: String,
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub enum LocalNetworkScopeDescriptor {
    Project,
    Shared { network_id_path: PathBuf },
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub struct LocalServerDescriptor {
    /// The data directory is one of the following:
    ///     <project directory>/.dfx/network/local
    ///     $HOME/Library/Application Support/org.dfinity.dfx/network/local
    ///     $HOME/.local/share/dfx/network/local
    ///     $APPDATA/dfx/network/local
    pub data_directory: PathBuf,
    pub settings_digest: Option<String>,

    pub bind_address: SocketAddr,

    pub bitcoin: ConfigDefaultsBitcoin,
    pub canister_http: ConfigDefaultsCanisterHttp,
    pub proxy: ConfigDefaultsProxy,
    pub replica: ConfigDefaultsReplica,

    pub scope: LocalNetworkScopeDescriptor,

    legacy_pid_path: Option<PathBuf>,
}

impl LocalNetworkScopeDescriptor {
    pub fn shared(data_directory: &Path) -> Self {
        LocalNetworkScopeDescriptor::Shared {
            network_id_path: data_directory.join("network-id"),
        }
    }
}

impl LocalServerDescriptor {
    pub fn new(
        data_directory: PathBuf,
        bind: String,
        bitcoin: ConfigDefaultsBitcoin,
        canister_http: ConfigDefaultsCanisterHttp,
        proxy: ConfigDefaultsProxy,
        replica: ConfigDefaultsReplica,
        scope: LocalNetworkScopeDescriptor,
        legacy_pid_path: Option<PathBuf>,
    ) -> Result<Self, NetworkConfigError> {
        let settings_digest = None;
        let bind_address = to_socket_addr(&bind).map_err(ParseBindAddressFailed)?;
        Ok(LocalServerDescriptor {
            data_directory,
            settings_digest,
            bind_address,
            bitcoin,
            canister_http,
            proxy,
            replica,
            scope,
            legacy_pid_path,
        })
    }

    /// The contents of this file are different for each `dfx start --clean`
    /// or `dfx start` when the network data directory doesn't already exist
    pub fn network_id_path(&self) -> PathBuf {
        self.data_dir_by_settings_digest().join("network-id")
    }

    /// This file contains the pid of the process started with `dfx start`
    pub fn dfx_pid_path(&self) -> PathBuf {
        self.data_directory.join("pid")
    }

    /// The path of the pid file, as well as one that dfx <= 0.11.x would have created
    pub fn dfx_pid_paths(&self) -> Vec<PathBuf> {
        let mut pid_paths: Vec<PathBuf> = vec![];
        if let Some(legacy_pid_path) = &self.legacy_pid_path {
            pid_paths.push(legacy_pid_path.clone());
        }
        pid_paths.push(self.dfx_pid_path());
        pid_paths
    }

    /// This file contains the pid of the ic-btc-adapter process
    pub fn btc_adapter_pid_path(&self) -> PathBuf {
        self.data_directory.join("ic-btc-adapter-pid")
    }

    /// This file contains the configuration for the ic-btc-adapter
    pub fn btc_adapter_config_path(&self) -> PathBuf {
        self.data_directory.join("ic-btc-adapter-config.json")
    }

    /// This file contains the PATH of the unix domain socket for the ic-btc-adapter
    pub fn btc_adapter_socket_holder_path(&self) -> PathBuf {
        self.data_directory.join("ic-btc-adapter-socket-path")
    }

    /// This file contains the configuration for the ic-https-outcalls-adapter
    pub fn canister_http_adapter_config_path(&self) -> PathBuf {
        self.data_directory.join("ic-canister-http-config.json")
    }

    /// This file contains the pid of the ic-https-outcalls-adapter process
    pub fn canister_http_adapter_pid_path(&self) -> PathBuf {
        self.data_directory.join("ic-https-outcalls-adapter-pid")
    }

    /// This file contains the PATH of the unix domain socket for the ic-https-outcalls-adapter
    pub fn canister_http_adapter_socket_holder_path(&self) -> PathBuf {
        self.data_directory.join("ic-canister-http-socket-path")
    }

    /// The replica configuration directory doesn't actually contain replica configuration.
    /// It contains two files:
    ///   - replica-1.port  contains the listening port of the running replica process
    ///   - replica.pid     contains the pid of the running replica process
    pub fn replica_configuration_dir(&self) -> PathBuf {
        self.data_directory.join("replica-configuration")
    }

    /// This file contains the listening port of the replica
    pub fn replica_port_path(&self) -> PathBuf {
        self.replica_configuration_dir().join("replica-1.port")
    }

    /// This file contains the pid of the replica process
    pub fn replica_pid_path(&self) -> PathBuf {
        self.replica_configuration_dir().join("replica-pid")
    }

    /// This file contains the configuration/API port of the pocket-ic replica process
    pub fn pocketic_port_path(&self) -> PathBuf {
        self.data_directory.join("pocket-ic-port")
    }

    /// This file contains the pid of the pocket-ic replica process
    pub fn pocketic_pid_path(&self) -> PathBuf {
        self.data_directory.join("pocket-ic-pid")
    }

    /// This file contains the configuration port of the pocket-ic gateway process
    pub fn pocketic_proxy_port_path(&self) -> PathBuf {
        self.data_directory.join("pocket-ic-proxy-port")
    }

    /// This file contains the pid of the pocket-ic gateway process
    pub fn pocketic_proxy_pid_path(&self) -> PathBuf {
        self.data_directory.join("pocket-ic-proxy-pid")
    }

    /// Returns whether the local server is PocketIC (as opposed to the replica)
    pub fn effective_config(&self) -> Result<Option<CachedConfig<'static>>, StructuredFileError> {
        let path = self.effective_config_path();
        path.exists().then(|| load_json_file(&path)).transpose()
    }

    pub fn load_settings_digest(&mut self) -> Result<(), StructuredFileError> {
        if self.settings_digest.is_none() {
            let network_id_path = self.data_directory.join("network-id");
            let network_metadata: NetworkMetadata = load_json_file(&network_id_path)?;
            self.settings_digest = Some(network_metadata.settings_digest);
        }
        Ok(())
    }

    pub fn settings_digest(&self) -> &str {
        self.settings_digest
            .as_ref()
            .expect("settings_digest must be set")
    }

    pub fn data_dir_by_settings_digest(&self) -> PathBuf {
        if self.scope == LocalNetworkScopeDescriptor::Project {
            self.data_directory.clone()
        } else {
            self.data_directory.join(self.settings_digest())
        }
    }

    /// The top-level directory holding state for the replica.
    pub fn state_dir(&self) -> PathBuf {
        self.data_dir_by_settings_digest().join("state")
    }

    /// The replicated state of the replica.
    pub fn replicated_state_dir(&self) -> PathBuf {
        self.state_dir().join("replicated_state")
    }

    /// This file contains the listening port of the HTTP gateway.
    /// This is the port that the agent connects to.
    pub fn webserver_port_path(&self) -> PathBuf {
        self.data_directory.join("webserver-port")
    }

    /// This file contains the effective config the replica was started with.
    pub fn effective_config_path(&self) -> PathBuf {
        self.data_directory.join("replica-effective-config.json")
    }

    pub fn effective_config_path_by_settings_digest(&self) -> PathBuf {
        self.data_dir_by_settings_digest()
            .join("replica-effective-config.json")
    }
}

impl LocalServerDescriptor {
    pub fn with_bind_address(self, bind_address: SocketAddr) -> Self {
        Self {
            bind_address,
            ..self
        }
    }

    pub fn with_replica_port(self, port: u16) -> Self {
        let replica = ConfigDefaultsReplica {
            port: Some(port),
            ..self.replica
        };
        Self { replica, ..self }
    }

    pub fn with_bitcoin_enabled(self) -> LocalServerDescriptor {
        let bitcoin = ConfigDefaultsBitcoin {
            enabled: true,
            ..self.bitcoin
        };
        Self { bitcoin, ..self }
    }

    pub fn with_bitcoin_nodes(self, nodes: Vec<SocketAddr>) -> LocalServerDescriptor {
        let bitcoin = ConfigDefaultsBitcoin {
            nodes: Some(nodes),
            ..self.bitcoin
        };
        Self { bitcoin, ..self }
    }

    pub fn with_proxy_domains(self, domains: Vec<String>) -> LocalServerDescriptor {
        let proxy = ConfigDefaultsProxy {
            domain: Some(SerdeVec::Many(domains)),
        };
        Self { proxy, ..self }
    }

    pub fn with_settings_digest(self, settings_digest: String) -> Self {
        Self {
            settings_digest: Some(settings_digest),
            ..self
        }
    }
}

impl LocalServerDescriptor {
    pub fn describe(&self, log: &Logger) {
        debug!(log, "Local server configuration:");
        let default_bind: SocketAddr = match self.scope {
            LocalNetworkScopeDescriptor::Project => DEFAULT_PROJECT_LOCAL_BIND,
            LocalNetworkScopeDescriptor::Shared { .. } => DEFAULT_SHARED_LOCAL_BIND,
        }
        .parse()
        .unwrap();

        let diffs = if self.bind_address != default_bind {
            format!(" (default: {:?})", default_bind)
        } else {
            "".to_string()
        };
        debug!(log, "  bind address: {:?}{}", self.bind_address, diffs);
        if self.bitcoin.enabled {
            let default_nodes = bitcoin_adapter::default_nodes();
            debug!(log, "  bitcoin: enabled (default: disabled)");
            let nodes: Vec<SocketAddr> = if let Some(ref nodes) = self.bitcoin.nodes {
                nodes.clone()
            } else {
                default_nodes.clone()
            };
            let diffs: String = if nodes != default_nodes {
                format!(" (default: {:?})", default_nodes)
            } else {
                "".to_string()
            };
            debug!(log, "    nodes: {:?}{}", nodes, diffs);
        } else {
            debug!(log, "  bitcoin: disabled");
        }

        if self.canister_http.enabled {
            debug!(log, "  canister http: enabled");
            let diffs: String = if self.canister_http.log_level != HttpAdapterLogLevel::default() {
                format!(" (default: {:?})", HttpAdapterLogLevel::default())
            } else {
                "".to_string()
            };
            debug!(
                log,
                "    log level: {:?}{}", self.canister_http.log_level, diffs
            );
        } else {
            debug!(log, "  canister http: disabled (default: enabled)");
        }

        debug!(log, "  replica:");
        if let Some(port) = self.replica.port {
            debug!(log, "    port: {}", port);
        }
        let subnet_type = self
            .replica
            .subnet_type
            .unwrap_or(ReplicaSubnetType::Application);
        let diffs: String = if subnet_type != ReplicaSubnetType::Application {
            format!(" (default: {:?})", ReplicaSubnetType::Application)
        } else {
            "".to_string()
        };
        debug!(log, "    subnet type: {:?}{}", subnet_type, diffs);

        let log_level = self.replica.log_level.unwrap_or_default();
        let diffs: String = if log_level != ReplicaLogLevel::default() {
            format!(" (default: {:?})", ReplicaLogLevel::default())
        } else {
            "".to_string()
        };
        debug!(log, "    log level: {:?}{}", log_level, diffs);

        debug!(log, "  data directory: {}", self.data_directory.display());
        let scope = match self.scope {
            LocalNetworkScopeDescriptor::Project => "project",
            LocalNetworkScopeDescriptor::Shared { .. } => "shared",
        };
        debug!(log, "  scope: {}", scope);
        debug!(log, "");
    }

    /// Gets the port of a local replica.
    ///
    /// # Prerequisites
    /// - A local replica or emulator needs to be running, e.g. with `dfx start`.
    pub fn get_running_replica_port(
        &self,
        logger: Option<&Logger>,
    ) -> Result<Option<u16>, NetworkConfigError> {
        let replica_port_path = self.replica_port_path();
        let pocketic_port_path = self.pocketic_port_path();
        match read_port_from(&replica_port_path)? {
            Some(port) => {
                if let Some(logger) = logger {
                    info!(logger, "Found local replica running on port {}", port);
                }
                Ok(Some(port))
            }
            None => match read_port_from(&pocketic_port_path)? {
                Some(port) => {
                    if let Some(logger) = logger {
                        info!(logger, "Found local PocketIC running on port {}", port);
                    }
                    Ok(Some(port))
                }
                None => Ok(self.replica.port),
            },
        }
    }
}

/// Reads a port number from a file.
///
/// # Prerequisites
/// The file is expected to contain the port number only, as utf8 text.
fn read_port_from(path: &Path) -> Result<Option<u16>, NetworkConfigError> {
    if path.exists() {
        let s = crate::fs::read_to_string(path)?;
        let s = s.trim();
        if s.is_empty() {
            Ok(None)
        } else {
            let port = s.parse::<u16>().map_err(|e| {
                NetworkConfigError::ParsePortValueFailed(Box::new(path.to_path_buf()), Box::new(e))
            })?;
            Ok(Some(port))
        }
    } else {
        Ok(None)
    }
}


-----------------------

/src/dfx-core/src/config/model/mod.rs:
-----------------------

pub mod bitcoin_adapter;
pub mod canister_http_adapter;
pub mod canister_id_store;
pub mod dfinity;
pub mod extension_canister_type;
pub mod local_server_descriptor;
pub mod network_descriptor;
pub mod replica_config;
pub mod settings_digest;


-----------------------

/src/dfx-core/src/config/model/network_descriptor.rs:
-----------------------

use crate::config::model::dfinity::{
    NetworkType, PlaygroundConfig, DEFAULT_IC_GATEWAY, DEFAULT_IC_GATEWAY_TRAILING_SLASH,
};
use crate::config::model::local_server_descriptor::LocalServerDescriptor;
use crate::error::network_config::NetworkConfigError;
use crate::error::network_config::NetworkConfigError::{NetworkHasNoProviders, NetworkMustBeLocal};
use crate::error::uri::UriError;
use candid::Principal;
use slog::Logger;
use std::path::{Path, PathBuf};
use url::Url;

pub const MAINNET_MOTOKO_PLAYGROUND_CANISTER_ID: Principal =
    Principal::from_slice(&[0, 0, 0, 0, 0, 48, 0, 97, 1, 1]);
pub const PLAYGROUND_NETWORK_NAME: &str = "playground";
pub const MOTOKO_PLAYGROUND_CANISTER_TIMEOUT_SECONDS: u64 = 1200;

#[derive(Clone, Debug, PartialEq, Eq)]
pub enum NetworkTypeDescriptor {
    Ephemeral {
        wallet_config_path: PathBuf,
    },
    Playground {
        playground_canister: Principal,
        canister_timeout_seconds: u64,
    },
    Persistent,
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub struct NetworkDescriptor {
    pub name: String,
    pub providers: Vec<String>,
    pub r#type: NetworkTypeDescriptor,
    pub is_ic: bool,
    pub local_server_descriptor: Option<LocalServerDescriptor>,
}

impl NetworkTypeDescriptor {
    pub fn new(
        r#type: NetworkType,
        ephemeral_wallet_config_path: &Path,
        playground: Option<PlaygroundConfig>,
    ) -> Result<Self, NetworkConfigError> {
        if let Some(playground_config) = playground {
            Ok(NetworkTypeDescriptor::Playground {
                playground_canister: Principal::from_text(&playground_config.playground_canister)
                    .map_err(|e| {
                    NetworkConfigError::ParsePrincipalFailed(
                        playground_config.playground_canister,
                        e,
                    )
                })?,
                canister_timeout_seconds: playground_config.timeout_seconds,
            })
        } else {
            match r#type {
                NetworkType::Ephemeral => Ok(NetworkTypeDescriptor::Ephemeral {
                    wallet_config_path: ephemeral_wallet_config_path.to_path_buf(),
                }),
                NetworkType::Persistent => Ok(NetworkTypeDescriptor::Persistent),
            }
        }
    }
}

impl NetworkDescriptor {
    pub fn ic() -> Self {
        NetworkDescriptor {
            name: "ic".to_string(),
            providers: vec![DEFAULT_IC_GATEWAY.to_string()],
            r#type: NetworkTypeDescriptor::Persistent,
            is_ic: true,
            local_server_descriptor: None,
        }
    }

    /// Determines whether the provided connection is the official IC or not.
    #[allow(clippy::ptr_arg)]
    pub fn is_ic(network_name: &str, providers: &Vec<String>) -> bool {
        let name_match = matches!(
            network_name,
            "ic" | DEFAULT_IC_GATEWAY | DEFAULT_IC_GATEWAY_TRAILING_SLASH
        );
        let provider_match = {
            providers.len() == 1
                && matches!(
                    providers.first().unwrap().as_str(),
                    DEFAULT_IC_GATEWAY | DEFAULT_IC_GATEWAY_TRAILING_SLASH
                )
        };
        name_match || provider_match
    }

    pub fn is_playground(&self) -> bool {
        matches!(self.r#type, NetworkTypeDescriptor::Playground { .. })
    }

    /// Return the first provider in the list
    pub fn first_provider(&self) -> Result<&str, NetworkConfigError> {
        match self.providers.first() {
            Some(provider) => Ok(provider),
            None => Err(NetworkHasNoProviders(self.name.clone())),
        }
    }

    pub fn local_server_descriptor(&self) -> Result<&LocalServerDescriptor, NetworkConfigError> {
        match &self.local_server_descriptor {
            Some(p) => Ok(p),
            None => Err(NetworkMustBeLocal(self.name.clone())),
        }
    }

    /// Playground on mainnet
    pub(crate) fn default_playground_network() -> Self {
        Self {
            name: PLAYGROUND_NETWORK_NAME.to_string(),
            providers: vec![DEFAULT_IC_GATEWAY.to_string()],
            r#type: NetworkTypeDescriptor::Playground {
                playground_canister: MAINNET_MOTOKO_PLAYGROUND_CANISTER_ID,
                canister_timeout_seconds: MOTOKO_PLAYGROUND_CANISTER_TIMEOUT_SECONDS,
            },
            is_ic: true,
            local_server_descriptor: None,
        }
    }

    fn replica_endpoints(&self) -> Result<Vec<Url>, NetworkConfigError> {
        self.providers
            .iter()
            .map(|s| {
                Url::parse(s).map_err(|e| {
                    NetworkConfigError::ParseProviderUrlFailed(Box::new(s.to_string()), e)
                })
            })
            .collect()
    }

    pub fn get_replica_urls(
        &self,
        logger: Option<&Logger>,
    ) -> Result<Vec<Url>, NetworkConfigError> {
        if self.name == "local" {
            let local_server_descriptor = self.local_server_descriptor()?;

            if let Some(port) = local_server_descriptor.get_running_replica_port(logger)? {
                let mut socket_addr = local_server_descriptor.bind_address;
                socket_addr.set_port(port);
                let url = format!("http://{}", socket_addr);
                let url =
                    Url::parse(&url).map_err(|e| UriError::UrlParseError(url.to_string(), e))?;
                return Ok(vec![url]);
            }
        }
        self.replica_endpoints()
    }
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    fn ic_by_netname() {
        assert!(NetworkDescriptor::is_ic("ic", &vec![]));
        assert!(NetworkDescriptor::is_ic(DEFAULT_IC_GATEWAY, &vec![]));
        assert!(NetworkDescriptor::is_ic(
            DEFAULT_IC_GATEWAY_TRAILING_SLASH,
            &vec![]
        ));
    }

    #[test]
    fn ic_by_provider() {
        assert!(NetworkDescriptor::is_ic(
            "not_ic",
            &vec![DEFAULT_IC_GATEWAY.to_string()]
        ));
        assert!(NetworkDescriptor::is_ic(
            "not_ic",
            &vec![DEFAULT_IC_GATEWAY_TRAILING_SLASH.to_string()]
        ));
    }

    #[test]
    fn ic_by_netname_fail() {
        assert!(!NetworkDescriptor::is_ic("not_ic", &vec![]));
    }

    #[test]
    fn ic_by_provider_fail_string() {
        assert!(!NetworkDescriptor::is_ic(
            "not_ic",
            &vec!["not_ic_provider".to_string()]
        ));
    }

    #[test]
    fn ic_by_provider_fail_unique() {
        assert!(!NetworkDescriptor::is_ic(
            "not_ic",
            &vec![
                DEFAULT_IC_GATEWAY.to_string(),
                "some_other_provider".to_string()
            ]
        ));
    }

    #[test]
    fn playground_canister_id() {
        assert_eq!(
            MAINNET_MOTOKO_PLAYGROUND_CANISTER_ID,
            Principal::from_text("mwrha-maaaa-aaaab-qabqq-cai").unwrap()
        )
    }
}


-----------------------

/src/dfx-core/src/config/model/replica_config.rs:
-----------------------

use crate::config::model::dfinity::{ReplicaLogLevel, ReplicaSubnetType};
use candid::Principal;
use serde::{Deserialize, Serialize};
use std::borrow::Cow;
use std::default::Default;
use std::path::{Path, PathBuf};

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
pub struct HttpHandlerConfig {
    /// Instructs the HTTP handler to use the specified port
    pub port: Option<u16>,

    /// Instructs the HTTP handler to bind to any open port and report the port
    /// to the specified file.
    /// The port is written in its textual representation, no newline at the
    /// end.
    pub write_port_to: Option<PathBuf>,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
pub struct BtcAdapterConfig {
    pub enabled: bool,
    pub socket_path: Option<PathBuf>,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
pub struct CanisterHttpAdapterConfig {
    pub enabled: bool,
    pub socket_path: Option<PathBuf>,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
pub struct ArtifactPoolConfig {
    pub consensus_pool_path: PathBuf,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
pub struct CryptoConfig {
    pub crypto_root: PathBuf,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
pub struct StateManagerConfig {
    pub state_root: PathBuf,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
pub struct ReplicaConfig {
    pub http_handler: HttpHandlerConfig,
    pub state_manager: StateManagerConfig,
    pub crypto: CryptoConfig,
    pub artifact_pool: ArtifactPoolConfig,
    pub subnet_type: ReplicaSubnetType,
    pub btc_adapter: BtcAdapterConfig,
    pub canister_http_adapter: CanisterHttpAdapterConfig,
    pub log_level: ReplicaLogLevel,
    pub artificial_delay: u32,
}

impl ReplicaConfig {
    pub fn new(
        state_root: &Path,
        subnet_type: ReplicaSubnetType,
        log_level: ReplicaLogLevel,
        artificial_delay: u32,
    ) -> Self {
        ReplicaConfig {
            http_handler: HttpHandlerConfig {
                write_port_to: None,
                port: None,
            },
            state_manager: StateManagerConfig {
                state_root: state_root.join("replicated_state"),
            },
            crypto: CryptoConfig {
                crypto_root: state_root.join("crypto_store"),
            },
            artifact_pool: ArtifactPoolConfig {
                consensus_pool_path: state_root.join("consensus_pool"),
            },
            subnet_type,
            btc_adapter: BtcAdapterConfig {
                enabled: false,
                socket_path: None,
            },
            canister_http_adapter: CanisterHttpAdapterConfig {
                enabled: false,
                socket_path: None,
            },
            log_level,
            artificial_delay,
        }
    }

    pub fn with_port(self, port: u16) -> Self {
        ReplicaConfig {
            http_handler: self.http_handler.with_port(port),
            ..self
        }
    }

    pub fn with_random_port(self, write_port_to: &Path) -> Self {
        ReplicaConfig {
            http_handler: self.http_handler.with_random_port(write_port_to),
            ..self
        }
    }

    pub fn with_btc_adapter_enabled(self) -> Self {
        ReplicaConfig {
            btc_adapter: self.btc_adapter.with_enabled(),
            ..self
        }
    }

    pub fn with_btc_adapter_socket(self, socket_path: PathBuf) -> Self {
        ReplicaConfig {
            btc_adapter: self.btc_adapter.with_socket_path(socket_path),
            ..self
        }
    }

    pub fn with_canister_http_adapter_enabled(self) -> Self {
        ReplicaConfig {
            canister_http_adapter: self.canister_http_adapter.with_enabled(),
            ..self
        }
    }
    pub fn with_canister_http_adapter_socket(self, socket_path: PathBuf) -> Self {
        ReplicaConfig {
            canister_http_adapter: self.canister_http_adapter.with_socket_path(socket_path),
            ..self
        }
    }
}

impl BtcAdapterConfig {
    pub fn with_enabled(self) -> Self {
        BtcAdapterConfig {
            enabled: true,
            ..self
        }
    }

    pub fn with_socket_path(self, socket_path: PathBuf) -> Self {
        BtcAdapterConfig {
            socket_path: Some(socket_path),
            ..self
        }
    }
}

impl CanisterHttpAdapterConfig {
    pub fn with_enabled(self) -> Self {
        CanisterHttpAdapterConfig {
            enabled: true,
            ..self
        }
    }

    pub fn with_socket_path(self, socket_path: PathBuf) -> Self {
        CanisterHttpAdapterConfig {
            socket_path: Some(socket_path),
            ..self
        }
    }
}

impl HttpHandlerConfig {
    pub fn with_port(self, port: u16) -> Self {
        HttpHandlerConfig {
            port: Some(port),
            write_port_to: None,
        }
    }

    pub fn with_random_port(self, write_port_to: &Path) -> Self {
        HttpHandlerConfig {
            port: None,
            write_port_to: Some(write_port_to.to_path_buf()),
        }
    }
}

#[derive(Serialize, Deserialize, Debug, PartialEq, Eq)]
#[serde(tag = "type", rename_all = "snake_case")]
#[allow(clippy::large_enum_variant)]
pub enum CachedReplicaConfig<'a> {
    Replica { config: Cow<'a, ReplicaConfig> },
    PocketIc { config: Cow<'a, ReplicaConfig> },
}

#[derive(Serialize, Deserialize, Debug, PartialEq, Eq)]
pub struct CachedConfig<'a> {
    pub replica_rev: String,
    pub effective_canister_id: Option<Principal>,
    #[serde(flatten)]
    pub config: CachedReplicaConfig<'a>,
}

impl<'a> CachedConfig<'a> {
    pub fn replica(config: &'a ReplicaConfig, replica_rev: String) -> Self {
        Self {
            replica_rev,
            effective_canister_id: None,
            config: CachedReplicaConfig::Replica {
                config: Cow::Borrowed(config),
            },
        }
    }
    pub fn pocketic(
        config: &'a ReplicaConfig,
        replica_rev: String,
        effective_canister_id: Option<Principal>,
    ) -> Self {
        Self {
            replica_rev,
            effective_canister_id,
            config: CachedReplicaConfig::PocketIc {
                config: Cow::Borrowed(config),
            },
        }
    }
    pub fn can_share_state(&self, other: &Self) -> bool {
        self == other
    }
    pub fn get_effective_canister_id(&self) -> Option<Principal> {
        self.effective_canister_id
    }
}


-----------------------

/src/dfx-core/src/config/model/settings_digest.rs:
-----------------------

use crate::config::model::dfinity::{ReplicaLogLevel, ReplicaSubnetType};
use crate::config::model::local_server_descriptor::LocalServerDescriptor;
use candid::Deserialize;
use serde::Serialize;
use sha2::{Digest, Sha256};
use std::borrow::Cow;

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
#[serde(tag = "type", rename_all = "snake_case")]
enum HttpHandlerPortSetting {
    Port {
        port: u16,
    },
    #[default]
    WritePortToPath,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
struct HttpHandlerSettings {
    pub port: HttpHandlerPortSetting,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
struct BtcAdapterSettings {
    pub enabled: bool,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
struct CanisterHttpAdapterSettings {
    pub enabled: bool,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize, PartialEq, Eq)]
struct ReplicaSettings {
    pub http_handler: HttpHandlerSettings,
    pub subnet_type: ReplicaSubnetType,
    pub btc_adapter: BtcAdapterSettings,
    pub canister_http_adapter: CanisterHttpAdapterSettings,
    pub log_level: ReplicaLogLevel,
    pub artificial_delay: u32,
}

#[derive(Serialize, Deserialize, PartialEq, Eq)]
#[serde(tag = "type", rename_all = "snake_case")]
enum BackendSettings<'a> {
    Replica { settings: Cow<'a, ReplicaSettings> },
    PocketIc,
}

#[derive(Serialize, Deserialize, PartialEq, Eq)]
struct Settings<'a> {
    pub ic_repo_commit: String,
    #[serde(flatten)]
    pub backend: BackendSettings<'a>,
}

pub fn get_settings_digest(
    ic_repo_commit: &str,
    local_server_descriptor: &LocalServerDescriptor,
    artificial_delay: u32,
    pocketic: bool,
) -> String {
    let backend = if pocketic {
        BackendSettings::PocketIc
    } else {
        get_replica_backend_settings(local_server_descriptor, artificial_delay)
    };
    let settings = Settings {
        ic_repo_commit: ic_repo_commit.into(),
        backend,
    };
    let normalized = serde_json::to_string_pretty(&settings).unwrap();
    let hash: Vec<u8> = Sha256::digest(normalized).to_vec();
    hex::encode(hash)
}

fn get_replica_backend_settings(
    local_server_descriptor: &LocalServerDescriptor,
    artificial_delay: u32,
) -> BackendSettings {
    let http_handler = HttpHandlerSettings {
        port: if let Some(port) = local_server_descriptor.replica.port {
            HttpHandlerPortSetting::Port { port }
        } else {
            HttpHandlerPortSetting::WritePortToPath
        },
    };
    let btc_adapter = BtcAdapterSettings {
        enabled: local_server_descriptor.bitcoin.enabled,
    };
    let canister_http_adapter = CanisterHttpAdapterSettings {
        enabled: local_server_descriptor.canister_http.enabled,
    };
    let replica_settings = ReplicaSettings {
        http_handler,
        subnet_type: local_server_descriptor
            .replica
            .subnet_type
            .unwrap_or_default(),
        btc_adapter,
        canister_http_adapter,
        log_level: local_server_descriptor
            .replica
            .log_level
            .unwrap_or_default(),
        artificial_delay,
    };
    BackendSettings::Replica {
        settings: Cow::Owned(replica_settings),
    }
}


-----------------------

/src/dfx-core/src/config/project_templates.rs:
-----------------------

use itertools::Itertools;
use std::collections::BTreeMap;
use std::fmt::Display;
use std::io;
use std::sync::OnceLock;

type GetArchiveFn = fn() -> Result<tar::Archive<flate2::read::GzDecoder<&'static [u8]>>, io::Error>;

#[derive(Debug, Clone)]
pub enum ResourceLocation {
    /// The template's assets are compiled into the dfx binary
    Bundled { get_archive_fn: GetArchiveFn },
}

#[derive(Debug, Clone, Eq, PartialEq)]
pub enum Category {
    Backend,
    Frontend,
    FrontendTest,
    Extra,
    Support,
}

#[derive(Debug, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct ProjectTemplateName(pub String);

impl Display for ProjectTemplateName {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

#[derive(Debug, Clone)]
pub struct ProjectTemplate {
    /// The name of the template as specified on the command line,
    /// for example `--type rust`
    pub name: ProjectTemplateName,

    /// The name used for display and sorting
    pub display: String,

    /// How to obtain the template's files
    pub resource_location: ResourceLocation,

    /// Used to determine which CLI group (`--type`, `--backend`, `--frontend`)
    /// as well as for interactive selection
    pub category: Category,

    /// Other project templates to patch in alongside this one
    pub requirements: Vec<ProjectTemplateName>,

    /// Run a command after adding the canister to dfx.json
    pub post_create: Vec<String>,

    /// If set, display a spinner while this command runs
    pub post_create_spinner_message: Option<String>,

    /// If the post-create command fails, display this warning but don't fail
    pub post_create_failure_warning: Option<String>,

    /// The sort order is fixed rather than settable in properties:
    /// For backend:
    ///   - motoko=0
    ///   - rust=1
    ///   - everything else=2 (and then by display name)
    /// For frontend:
    ///   - SvelteKit=0
    ///   - React=1
    ///   - Vue=2
    ///   - Vanilla JS=3
    ///   - No JS Template=4
    ///   - everything else=5 (and then by display name)
    /// For extras:
    ///   - Internet Identity=0
    ///   - Bitcoin=1
    ///   - everything else=2 (and then by display name)
    ///   - Frontend Tests
    pub sort_order: u32,
}

type ProjectTemplates = BTreeMap<ProjectTemplateName, ProjectTemplate>;

static PROJECT_TEMPLATES: OnceLock<ProjectTemplates> = OnceLock::new();

pub fn populate(builtin_templates: Vec<ProjectTemplate>) {
    let templates = builtin_templates
        .iter()
        .map(|t| (t.name.clone(), t.clone()))
        .collect();

    PROJECT_TEMPLATES.set(templates).unwrap();
}

pub fn get_project_template(name: &ProjectTemplateName) -> ProjectTemplate {
    PROJECT_TEMPLATES.get().unwrap().get(name).cloned().unwrap()
}

pub fn find_project_template(name: &ProjectTemplateName) -> Option<ProjectTemplate> {
    PROJECT_TEMPLATES.get().unwrap().get(name).cloned()
}

pub fn get_sorted_templates(category: Category) -> Vec<ProjectTemplate> {
    PROJECT_TEMPLATES
        .get()
        .unwrap()
        .values()
        .filter(|t| t.category == category)
        .cloned()
        .sorted_by(|a, b| {
            a.sort_order
                .cmp(&b.sort_order)
                .then_with(|| a.display.cmp(&b.display))
        })
        .collect()
}

pub fn project_template_cli_names(category: Category) -> Vec<String> {
    PROJECT_TEMPLATES
        .get()
        .unwrap()
        .values()
        .filter(|t| t.category == category)
        .map(|t| t.name.0.clone())
        .collect()
}


-----------------------

/src/dfx-core/src/error/archive.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
#[error("Failed to read archive path")]
pub struct GetArchivePathError(#[source] pub std::io::Error);


-----------------------

/src/dfx-core/src/error/builder.rs:
-----------------------

use crate::error::identity::{InstantiateIdentityFromNameError, NewIdentityManagerError};
use crate::error::{
    load_dfx_config::LoadDfxConfigError, load_networks_config::LoadNetworksConfigError,
    network_config::NetworkConfigError, root_key::FetchRootKeyError,
};
use ic_agent::AgentError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum BuildDfxInterfaceError {
    #[error(transparent)]
    BuildAgent(#[from] BuildAgentError),

    #[error(transparent)]
    BuildIdentity(#[from] BuildIdentityError),

    #[error(transparent)]
    LoadNetworksConfig(#[from] LoadNetworksConfigError),

    #[error(transparent)]
    LoadDfxConfig(#[from] LoadDfxConfigError),

    #[error(transparent)]
    NetworkConfig(#[from] NetworkConfigError),

    #[error(transparent)]
    FetchRootKey(#[from] FetchRootKeyError),
}

#[derive(Error, Debug)]
pub enum BuildIdentityError {
    #[error(transparent)]
    NewIdentityManager(#[from] NewIdentityManagerError),

    #[error(transparent)]
    InstantiateIdentityFromName(#[from] InstantiateIdentityFromNameError),
}

#[derive(Error, Debug)]
pub enum BuildAgentError {
    #[error("failed to create http client")]
    CreateHttpClient(#[source] reqwest::Error),

    #[error("failed to create route provider")]
    CreateRouteProvider(#[source] AgentError),

    #[error("failed to create transportr")]
    CreateTransport(#[source] AgentError),

    #[error("failed to create agent")]
    CreateAgent(#[source] AgentError),
}


-----------------------

/src/dfx-core/src/error/cache.rs:
-----------------------

use crate::error::archive::GetArchivePathError;
use crate::error::fs::{
    CreateDirAllError, EnsureDirExistsError, ReadDirError, ReadFileError, ReadPermissionsError,
    RemoveDirectoryAndContentsError, SetPermissionsError, UnpackingArchiveError, WriteFileError,
};
use crate::error::get_current_exe::GetCurrentExeError;
use crate::error::get_user_home::GetUserHomeError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum DeleteCacheError {
    #[error(transparent)]
    GetBinCache(#[from] EnsureCacheVersionsDirError),

    #[error(transparent)]
    RemoveDirectoryAndContents(#[from] RemoveDirectoryAndContentsError),
}

#[derive(Error, Debug)]
pub enum GetBinaryCommandPathError {
    #[error(transparent)]
    Install(#[from] InstallCacheError),

    #[error(transparent)]
    GetBinCacheRoot(#[from] EnsureCacheVersionsDirError),
}

#[derive(Error, Debug)]
pub enum EnsureCacheVersionsDirError {
    #[error(transparent)]
    EnsureDirExists(#[from] EnsureDirExistsError),

    #[error(transparent)]
    GetCacheRoot(#[from] GetCacheRootError),
}

#[derive(Error, Debug)]
pub enum GetCacheRootError {
    #[error(transparent)]
    GetUserHomeError(#[from] GetUserHomeError),

    #[error("failed to find cache directory at '{0}'.")]
    FindCacheDirectoryFailed(std::path::PathBuf),
}

#[derive(Error, Debug)]
pub enum InstallCacheError {
    #[error(transparent)]
    CreateDirAll(#[from] CreateDirAllError),

    #[error(transparent)]
    GetArchivePath(#[from] GetArchivePathError),

    #[error(transparent)]
    GetBinCache(#[from] EnsureCacheVersionsDirError),

    #[error(transparent)]
    GetCurrentExeError(#[from] GetCurrentExeError),

    #[error("invalid cache for version '{0}'.")]
    InvalidCacheForDfxVersion(String),

    #[error("failed to parse '{0}' as Semantic Version")]
    MalformedSemverString(String, #[source] semver::Error),

    #[error("failed to iterate through binary cache")]
    ReadBinaryCacheEntriesFailed(#[source] std::io::Error),

    #[error("failed to read binary cache entry")]
    ReadBinaryCacheEntryFailed(#[source] std::io::Error),

    #[error("failed to read binary cache")]
    ReadBinaryCacheStoreFailed(#[source] std::io::Error),

    #[error(transparent)]
    ReadFile(#[from] ReadFileError),

    #[error(transparent)]
    ReadPermissions(#[from] ReadPermissionsError),

    #[error(transparent)]
    RemoveDirectoryAndContents(#[from] RemoveDirectoryAndContentsError),

    #[error(transparent)]
    SetPermissions(#[from] SetPermissionsError),

    #[error(transparent)]
    UnpackingArchive(#[from] UnpackingArchiveError),

    #[error(transparent)]
    WriteFile(#[from] WriteFileError),
}

#[derive(Error, Debug)]
pub enum IsCacheInstalledError {
    #[error(transparent)]
    GetBinCache(#[from] EnsureCacheVersionsDirError),
}

#[derive(Error, Debug)]
pub enum ListCacheVersionsError {
    #[error(transparent)]
    ReadDir(#[from] ReadDirError),

    #[error(transparent)]
    GetBinCacheRoot(#[from] EnsureCacheVersionsDirError),

    #[error("failed to parse '{0}' as Semantic Version")]
    MalformedSemverString(String, #[source] semver::Error),

    #[error("failed to read entry in cache directory")]
    ReadCacheEntryFailed(#[source] std::io::Error),
}


-----------------------

/src/dfx-core/src/error/canister.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum CanisterBuilderError {
    #[error("Failed to construct wallet canister caller")]
    WalletCanisterCaller(#[source] ic_agent::AgentError),

    #[error("Failed to build call sender")]
    CallSenderBuildError(#[source] ic_agent::AgentError),
}

#[derive(Error, Debug)]
pub enum CanisterInstallError {
    #[error("Refusing to install canister without approval")]
    UserConsent(#[source] crate::error::cli::UserConsent),

    #[error(transparent)]
    CanisterBuilderError(#[from] CanisterBuilderError),

    #[error("Failed during wasm installation call")]
    InstallWasmError(#[source] ic_agent::AgentError),
}


-----------------------

/src/dfx-core/src/error/canister_id_store.rs:
-----------------------

use crate::error::fs::{ReadToStringError, RemoveDirectoryAndContentsError, WriteFileError};
use crate::error::{
    config::GetTempPathError,
    dfx_config::GetPullCanistersError,
    fs::{CreateDirAllError, EnsureParentDirExistsError},
    load_dfx_config::LoadDfxConfigError,
    structured_file::StructuredFileError,
};
use thiserror::Error;

#[derive(Error, Debug)]
pub enum CanisterIdStoreError {
    #[error(
        "Cannot find canister id. Please issue 'dfx canister create {canister_name}{network}'."
    )]
    CanisterIdNotFound {
        canister_name: String,
        network: String,
    },

    #[error("failed to ensure cohesive network directory")]
    EnsureCohesiveNetworkDirectoryFailed(#[from] EnsureCohesiveNetworkDirectoryError),

    #[error(transparent)]
    RemoveCanisterId(#[from] RemoveCanisterIdError),

    #[error(transparent)]
    GetPullCanistersFailed(#[from] GetPullCanistersError),

    #[error(transparent)]
    GetTempPath(#[from] GetTempPathError),

    #[error(transparent)]
    LoadDfxConfig(#[from] LoadDfxConfigError),

    #[error(transparent)]
    StructuredFileError(#[from] StructuredFileError),
}

#[derive(Error, Debug)]
pub enum AddCanisterIdError {
    #[error("failed to add canister with name '{canister_name}' and id '{canister_id}' to canister id store")]
    SaveIds {
        canister_name: String,
        canister_id: String,
        source: SaveIdsError,
    },

    #[error(transparent)]
    SaveTimestamps(#[from] SaveTimestampsError),
}

#[derive(Error, Debug)]
pub enum EnsureCohesiveNetworkDirectoryError {
    #[error(transparent)]
    CreateDirAll(#[from] CreateDirAllError),

    #[error(transparent)]
    ReadToString(#[from] ReadToStringError),

    #[error(transparent)]
    RemoveDirAll(#[from] RemoveDirectoryAndContentsError),

    #[error(transparent)]
    Write(#[from] WriteFileError),
}

#[derive(Error, Debug)]
pub enum RemoveCanisterIdError {
    #[error("failed to remove canister '{canister_name}' from id store")]
    SaveIds {
        canister_name: String,
        source: SaveIdsError,
    },

    #[error(transparent)]
    SaveTimestamps(#[from] SaveTimestampsError),
}

#[derive(Error, Debug)]
pub enum SaveTimestampsError {
    #[error(transparent)]
    EnsureParentDirExists(#[from] EnsureParentDirExistsError),

    #[error(transparent)]
    SaveJsonFile(#[from] StructuredFileError),
}

#[derive(Error, Debug)]
pub enum SaveIdsError {
    #[error(transparent)]
    EnsureParentDirExists(#[from] EnsureParentDirExistsError),

    #[error(transparent)]
    SaveJsonFile(#[from] StructuredFileError),
}


-----------------------

/src/dfx-core/src/error/cli.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum UserConsent {
    #[error("Unable to read input")]
    ReadError(#[source] std::io::Error),

    #[error("User declined consent.")]
    Declined,
}


-----------------------

/src/dfx-core/src/error/config.rs:
-----------------------

use crate::error::extension::LoadExtensionManifestError;
use crate::error::fs::{
    CanonicalizePathError, CreateDirAllError, EnsureDirExistsError, NoParentPathError,
};
use crate::error::get_user_home::GetUserHomeError;
use handlebars::RenderError;
use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum ConfigError {
    #[error("failed to ensure config directory exists")]
    EnsureConfigDirectoryExistsFailed(#[source] EnsureDirExistsError),

    #[error("Failed to determine config directory path")]
    DetermineConfigDirectoryFailed(#[source] GetUserHomeError),

    #[error("Failed to determine shared network data directory")]
    DetermineSharedNetworkDirectoryFailed(#[source] GetUserHomeError),
}

#[derive(Error, Debug)]
pub enum GetOutputEnvFileError {
    #[error("failed to canonicalize output_env_file")]
    CanonicalizePath(#[from] CanonicalizePathError),

    #[error("The output_env_file must be within the project root, but is {}", .0.display())]
    OutputEnvFileMustBeInProjectRoot(PathBuf),

    #[error("The output_env_file must be a relative path, but is {}", .0.display())]
    OutputEnvFileMustBeRelative(PathBuf),

    #[error(transparent)]
    NoParentPath(#[from] NoParentPathError),
}

#[derive(Error, Debug)]
pub enum GetTempPathError {
    #[error(transparent)]
    CreateDirAll(#[from] CreateDirAllError),
}

#[derive(Error, Debug)]
#[error("failed to render field '{field}' with value '{value}'")]
pub struct RenderErrorWithContext {
    pub field: String,
    pub value: String,
    pub source: RenderError,
}

#[derive(Error, Debug)]
#[error("failed to apply extension canister type '{extension}' to canister '{canister}'")]
pub struct ApplyExtensionCanisterTypeErrorWithContext {
    pub canister: Box<String>,
    pub extension: Box<String>,
    pub source: ApplyExtensionCanisterTypeError,
}

#[derive(Error, Debug)]
pub enum ApplyExtensionCanisterTypesError {
    #[error("the canisters field in dfx.json must be an object")]
    CanistersFieldIsNotAnObject(),

    #[error("canister '{0}' in dfx.json must be an object")]
    CanisterIsNotAnObject(String),

    #[error(transparent)]
    ApplyExtensionCanisterType(#[from] ApplyExtensionCanisterTypeError),
}

#[derive(Error, Debug)]
pub enum ApplyExtensionCanisterTypeError {
    #[error("failed to apply defaults from extension '{extension}' to canister '{canister}'")]
    ApplyDefaults {
        canister: Box<String>,
        extension: Box<String>,
        source: ApplyExtensionCanisterTypeDefaultsError,
    },

    #[error("canister '{canister}' has unknown type '{extension}' and there is no installed extension by that name which could define it")]
    NoExtensionForUnknownCanisterType { canister: String, extension: String },

    #[error(transparent)]
    LoadExtensionManifest(LoadExtensionManifestError),

    #[error("canister '{canister}' has type '{extension}', but that extension does not define a canister type")]
    ExtensionDoesNotDefineCanisterType { canister: String, extension: String },
}

#[derive(Error, Debug)]
pub enum ApplyExtensionCanisterTypeDefaultsError {
    #[error(transparent)]
    AppendMetadata(#[from] AppendMetadataError),

    #[error(transparent)]
    MergeTechStackError(#[from] MergeTechStackError),

    #[error(transparent)]
    Render(Box<RenderErrorWithContext>),
}

#[derive(Error, Debug)]
pub enum AppendMetadataError {
    #[error("expected canister metadata to be an array")]
    ExpectedCanisterMetadataArray,

    #[error("expected extension canister type metadata to be an array")]
    ExpectedExtensionCanisterTypeMetadataArray,
}

#[derive(Error, Debug)]
pub enum MergeTechStackError {
    #[error("expected canister tech_stack to be an object")]
    ExpectedCanisterTechStackObject,

    #[error("expected extension canister type tech_stack to be an object")]
    ExpectedExtensionCanisterTypeTechStackObject,
}


-----------------------

/src/dfx-core/src/error/dfx_config.rs:
-----------------------

use candid::Principal;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum AddDependenciesError {
    #[error("Circular canister dependencies: {}", _0.join(" -> "))]
    CanisterCircularDependency(Vec<String>),

    #[error("Canister '{0}' not found in dfx.json.")]
    CanisterNotFound(String),
}

#[derive(Error, Debug)]
pub enum GetCanisterConfigError {
    #[error("No canisters in the configuration file.")]
    CanistersFieldDoesNotExist(),

    #[error("Canister '{0}' not found in dfx.json.")]
    CanisterNotFound(String),
}

#[derive(Error, Debug)]
pub enum GetCanisterNamesWithDependenciesError {
    #[error("No canisters in the configuration file.")]
    CanistersFieldDoesNotExist(),

    #[error("Failed to add dependencies for canister '{0}'")]
    AddDependenciesFailed(String, #[source] AddDependenciesError),
}

#[derive(Error, Debug)]
pub enum GetComputeAllocationError {
    #[error("Failed to get compute allocation for canister '{0}'")]
    GetComputeAllocationFailed(String, #[source] GetCanisterConfigError),
}

#[derive(Error, Debug)]
pub enum GetFreezingThresholdError {
    #[error("Failed to get freezing threshold for canister '{0}'")]
    GetFreezingThresholdFailed(String, #[source] GetCanisterConfigError),
}

#[derive(Error, Debug)]
pub enum GetReservedCyclesLimitError {
    #[error("Failed to get reserved cycles limit for canister '{0}'")]
    GetReservedCyclesLimitFailed(String, #[source] GetCanisterConfigError),
}

#[derive(Error, Debug)]
pub enum GetMemoryAllocationError {
    #[error("Failed to get memory allocation for canister '{0}'")]
    GetMemoryAllocationFailed(String, #[source] GetCanisterConfigError),
}

#[derive(Error, Debug)]
pub enum GetWasmMemoryLimitError {
    #[error("Failed to get Wasm memory limit for canister '{0}'")]
    GetWasmMemoryLimitFailed(String, #[source] GetCanisterConfigError),
}

#[derive(Error, Debug)]
pub enum GetLogVisibilityError {
    #[error("Failed to get log visibility for canister '{0}'")]
    GetLogVisibilityFailed(String, #[source] GetCanisterConfigError),
}

#[derive(Error, Debug)]
pub enum GetPullCanistersError {
    #[error("Pull dependencies '{0}' and '{1}' have the same canister ID: {2}")]
    PullCanistersSameId(String, String, Principal),
}

#[derive(Error, Debug)]
pub enum GetRemoteCanisterIdError {
    #[error("Failed to figure out if canister '{0}' has a remote id on network '{1}'")]
    GetRemoteCanisterIdFailed(Box<String>, Box<String>, #[source] GetCanisterConfigError),
}

#[derive(Error, Debug)]
pub enum GetSpecifiedIdError {
    #[error("Failed to get specified_id for canister '{0}'")]
    GetSpecifiedIdFailed(String, #[source] GetCanisterConfigError),
}


-----------------------

/src/dfx-core/src/error/encryption.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum EncryptionError {
    #[error("Failed to decrypt content: {0}")]
    DecryptContentFailed(aes_gcm::Error),

    #[error("Failed to encrypt content: {0}")]
    EncryptContentFailed(aes_gcm::Error),

    #[error("Failed to hash password: {0}")]
    HashPasswordFailed(argon2::password_hash::Error),

    #[error("Failed to generate nonce: {0}")]
    NonceGenerationFailed(ring::error::Unspecified),

    #[error("Failed to read user input")]
    ReadUserPasswordFailed(#[source] dialoguer::Error),

    #[error("Failed to generate salt: {0}")]
    SaltGenerationFailed(ring::error::Unspecified),
}


-----------------------

/src/dfx-core/src/error/extension.rs:
-----------------------

#![allow(dead_code)]

use crate::error::cache::{EnsureCacheVersionsDirError, GetCacheRootError};
use crate::error::fs::{
    EnsureDirExistsError, ReadDirError, RemoveDirectoryAndContentsError, RenameError,
    SetPermissionsError,
};
use crate::error::structured_file::StructuredFileError;
use semver::Version;
use thiserror::Error;

#[derive(Error, Debug)]
#[error("Failed to load extension manifest")]
pub struct LoadExtensionManifestError(#[from] StructuredFileError);

#[derive(Error, Debug)]
pub enum ConvertExtensionIntoClapCommandError {
    #[error(transparent)]
    LoadExtensionManifest(#[from] LoadExtensionManifestError),

    #[error(transparent)]
    ListInstalledExtensionsError(#[from] ListInstalledExtensionsError),

    #[error(transparent)]
    ConvertExtensionSubcommandIntoClapCommandError(
        #[from] ConvertExtensionSubcommandIntoClapCommandError,
    ),
}

#[derive(Error, Debug)]
pub enum ConvertExtensionSubcommandIntoClapCommandError {
    #[error(transparent)]
    ConvertExtensionSubcommandIntoClapArgError(#[from] ConvertExtensionSubcommandIntoClapArgError),
}

#[derive(Error, Debug)]
pub enum ListAvailableExtensionsError {
    #[error(transparent)]
    FetchCatalog(#[from] FetchCatalogError),
}

#[derive(Error, Debug)]
pub enum ListInstalledExtensionsError {
    #[error(transparent)]
    ExtensionsDirectoryIsNotReadable(#[from] ReadDirError),
}

#[derive(Error, Debug)]
pub enum LoadExtensionManifestsError {
    #[error(transparent)]
    ListInstalledExtensions(#[from] ListInstalledExtensionsError),

    #[error(transparent)]
    LoadExtensionManifest(#[from] LoadExtensionManifestError),
}

#[derive(Error, Debug)]
pub enum ConvertExtensionSubcommandIntoClapArgError {
    #[error("Extension's subcommand argument '{0}' is missing description.")]
    ExtensionSubcommandArgMissingDescription(String),
}

#[derive(Error, Debug)]
pub enum RunExtensionError {
    #[error("Invalid extension name '{0:?}'.")]
    InvalidExtensionName(std::ffi::OsString),

    #[error("Cannot find cache directory")]
    FindCacheDirectoryFailed(#[from] EnsureCacheVersionsDirError),

    #[error("Failed to run extension '{0}'")]
    FailedToLaunchExtension(String, #[source] std::io::Error),

    #[error("Extension '{0}' never finished")]
    ExtensionNeverFinishedExecuting(String, #[source] std::io::Error),

    #[error("Extension terminated by signal.")]
    ExtensionExecutionTerminatedViaSignal,

    #[error("Extension exited with non-zero status code '{0}'.")]
    ExtensionExitedWithNonZeroStatus(i32),

    #[error(transparent)]
    GetExtensionBinaryError(#[from] GetExtensionBinaryError),
}

#[derive(Error, Debug)]
pub enum GetExtensionBinaryError {
    #[error("Extension '{0}' not installed.")]
    ExtensionNotInstalled(String),

    #[error("Cannot find extension binary at '{0}'.")]
    ExtensionBinaryDoesNotExist(std::path::PathBuf),

    #[error("Extension binary at {0} is not an executable file.")]
    ExtensionBinaryIsNotAFile(std::path::PathBuf),
}

#[derive(Error, Debug)]
pub enum NewExtensionManagerError {
    #[error("Cannot find cache directory")]
    FindCacheDirectoryFailed(#[from] GetCacheRootError),
}

#[derive(Error, Debug)]
pub enum DownloadAndInstallExtensionToTempdirError {
    #[error(transparent)]
    ExtensionDownloadFailed(reqwest::Error),

    #[error(transparent)]
    EnsureExtensionDirExistsFailed(#[from] EnsureDirExistsError),

    #[error("Cannot create temporary directory at '{0}'")]
    CreateTemporaryDirectoryFailed(std::path::PathBuf, #[source] std::io::Error),

    #[error("Cannot decompress extension archive (downloaded from: '{0}')")]
    DecompressFailed(url::Url, #[source] std::io::Error),
}

#[derive(Error, Debug)]
pub enum InstallExtensionError {
    #[error("extension '{0}' not found in catalog")]
    ExtensionNotFound(String),

    #[error("Extension '{0}' is already installed at version {1}.")]
    OtherVersionAlreadyInstalled(String, Version),

    #[error(transparent)]
    FetchCatalog(#[from] FetchCatalogError),

    #[error(transparent)]
    GetExtensionArchiveName(#[from] GetExtensionArchiveNameError),

    #[error(transparent)]
    GetHighestCompatibleVersion(#[from] GetHighestCompatibleVersionError),

    #[error(transparent)]
    GetExtensionDownloadUrl(#[from] GetExtensionDownloadUrlError),

    #[error(transparent)]
    GetExtensionManifest(#[from] GetExtensionManifestError),

    #[error(transparent)]
    DownloadAndInstallExtensionToTempdir(#[from] DownloadAndInstallExtensionToTempdirError),

    #[error(transparent)]
    FinalizeInstallation(#[from] FinalizeInstallationError),

    #[error(transparent)]
    LoadManifest(#[from] LoadExtensionManifestError),
}

#[derive(Error, Debug)]
pub enum GetExtensionArchiveNameError {
    #[error("Platform '{0}' is not supported.")]
    PlatformNotSupported(String),
}

#[derive(Error, Debug)]
pub enum GetHighestCompatibleVersionError {
    #[error(transparent)]
    GetDependencies(#[from] GetDependenciesError),

    #[error("No compatible version found.")]
    NoCompatibleVersionFound(),

    #[error(transparent)]
    DfxOnlyPossibleDependency(#[from] DfxOnlySupportedDependency),
}

#[derive(Error, Debug)]
pub enum GetDependenciesError {
    #[error(transparent)]
    ParseUrl(#[from] url::ParseError),

    #[error(transparent)]
    Get(reqwest::Error),

    #[error(transparent)]
    ParseJson(reqwest::Error),
}

#[derive(Error, Debug)]
pub enum GetExtensionManifestError {
    #[error(transparent)]
    Get(reqwest::Error),

    #[error(transparent)]
    ParseJson(reqwest::Error),
}

#[derive(Error, Debug)]
#[error("'dfx' is the only supported dependency")]
pub struct DfxOnlySupportedDependency;

#[derive(Error, Debug)]
#[error("Failed to parse extension manifest URL '{url}'")]
pub struct GetExtensionDownloadUrlError {
    pub url: String,
    pub source: url::ParseError,
}

#[derive(Error, Debug)]
pub enum GetTopLevelDirectoryError {
    #[error(transparent)]
    ReadDir(#[from] ReadDirError),

    #[error("No top-level directory found in archive")]
    NoTopLevelDirectoryEntry,

    #[error("Cannot read directory entry")]
    ReadDirEntry(#[source] std::io::Error),
}

#[derive(Error, Debug)]
#[error(transparent)]
pub enum FinalizeInstallationError {
    #[error(transparent)]
    GetTopLevelDirectory(#[from] GetTopLevelDirectoryError),

    #[error(transparent)]
    LoadExtensionManifest(#[from] LoadExtensionManifestError),

    #[error(transparent)]
    Rename(#[from] RenameError),

    #[error(transparent)]
    SetPermissions(#[from] SetPermissionsError),
}

#[derive(Error, Debug)]
pub enum FetchExtensionCompatibilityMatrixError {
    #[error("Cannot fetch compatibility.json from '{0}'")]
    CompatibilityMatrixFetchError(String, #[source] reqwest::Error),

    #[error("Cannot parse compatibility.json")]
    MalformedCompatibilityMatrix(#[source] reqwest::Error),
}

#[derive(Error, Debug)]
#[error(transparent)]
pub struct UninstallExtensionError(#[from] RemoveDirectoryAndContentsError);

#[derive(Error, Debug)]
pub enum FetchCatalogError {
    #[error(transparent)]
    ParseUrl(#[from] url::ParseError),

    #[error(transparent)]
    Get(reqwest::Error),

    #[error(transparent)]
    ParseJson(reqwest::Error),
}


-----------------------

/src/dfx-core/src/error/fs.rs:
-----------------------

use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
#[error("failed to canonicalize '{path}'")]
pub struct CanonicalizePathError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to copy {from} to {to}")]
pub struct CopyFileError {
    pub from: PathBuf,
    pub to: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to create directory {path} and parents")]
pub struct CreateDirAllError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
pub enum EnsureDirExistsError {
    #[error(transparent)]
    CreateDirAll(#[from] CreateDirAllError),

    #[error("path {0} is not a directory")]
    NotADirectory(PathBuf),
}

#[derive(Error, Debug)]
pub enum EnsureParentDirExistsError {
    #[error(transparent)]
    EnsureDirExists(#[from] EnsureDirExistsError),

    #[error(transparent)]
    NoParentPath(#[from] NoParentPathError),
}

#[derive(Error, Debug)]
#[error("failed to determine parent path for '{0}'")]
pub struct NoParentPathError(pub PathBuf);

#[derive(Error, Debug)]
#[error("failed to read directory {path}")]
pub struct ReadDirError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to read from {path}")]
pub struct ReadFileError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to remove directory {path}")]
pub struct RemoveDirectoryError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to write to {path}")]
pub struct WriteFileError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to read metadata of {path}")]
pub struct ReadMetadataError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to read permissions of {path}")]
pub struct ReadPermissionsError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to read {path} as string")]
pub struct ReadToStringError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to remove directory {path} and its contents")]
pub struct RemoveDirectoryAndContentsError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to remove file {path}")]
pub struct RemoveFileError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("Failed to rename {from} to {to}")]
pub struct RenameError {
    pub from: PathBuf,
    pub to: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
#[error("failed to set permissions of {path}")]
pub struct SetPermissionsError {
    pub path: PathBuf,
    pub source: std::io::Error,
}

#[derive(Error, Debug)]
pub enum SetPermissionsReadWriteError {
    #[error(transparent)]
    ReadPermissions(#[from] ReadPermissionsError),

    #[error(transparent)]
    SetPermissions(#[from] SetPermissionsError),
}

#[derive(Error, Debug)]
#[error("failed to unpack archive in {path}")]
pub struct UnpackingArchiveError {
    pub path: PathBuf,
    pub source: std::io::Error,
}


-----------------------

/src/dfx-core/src/error/get_current_exe.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum GetCurrentExeError {
    #[error("Failed to identify currently running executable")]
    NoCurrentExe(#[source] std::io::Error),
}


-----------------------

/src/dfx-core/src/error/get_user_home.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum GetUserHomeError {
    #[error("Cannot find home directory (no HOME environment variable).")]
    NoHomeInEnvironment(),
}


-----------------------

/src/dfx-core/src/error/identity.rs:
-----------------------

use crate::error::fs::{
    ReadFileError, ReadPermissionsError, RemoveDirectoryAndContentsError, RemoveDirectoryError,
    RemoveFileError, RenameError, SetPermissionsError, WriteFileError,
};
use crate::error::{
    config::ConfigError,
    encryption::EncryptionError,
    fs::{CopyFileError, CreateDirAllError, EnsureParentDirExistsError, NoParentPathError},
    get_user_home::GetUserHomeError,
    keyring::KeyringError,
    structured_file::StructuredFileError,
    wallet_config::{SaveWalletConfigError, WalletConfigError},
};
use candid::types::principal::PrincipalError;
use ic_agent::identity::PemError;
use ic_identity_hsm::HardwareIdentityError;
use std::path::PathBuf;
use std::string::FromUtf8Error;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum CallSenderFromWalletError {
    #[error("Failed to read principal from id '{0}', and did not find a wallet for that identity")]
    ParsePrincipalFromIdFailedAndNoWallet(String, #[source] PrincipalError),

    #[error("Failed to read principal from id '{0}' ({1}), and failed to load the wallet for that identity"
    )]
    ParsePrincipalFromIdFailedAndGetWalletCanisterIdFailed(
        String,
        PrincipalError,
        #[source] WalletConfigError,
    ),
}

#[derive(Error, Debug)]
pub enum ConvertMnemonicToKeyError {
    #[error("Failed to derive extended secret key from path")]
    DeriveExtendedKeyFromPathFailed(#[source] bip32::Error),
}

#[derive(Error, Debug)]
pub enum CreateIdentityConfigError {
    #[error("Failed to generate a fresh encryption configuration")]
    GenerateFreshEncryptionConfigurationFailed(#[source] EncryptionError),
}

#[derive(Error, Debug)]
pub enum CreateNewIdentityError {
    #[error("Cannot create an anonymous identity.")]
    CannotCreateAnonymousIdentity(),

    #[error("Failed to clean up previous creation attempts")]
    CleanupPreviousCreationAttemptsFailed(#[from] RemoveDirectoryAndContentsError),

    #[error("Failed to create identity config")]
    ConvertMnemonicToKeyFailed(#[source] ConvertMnemonicToKeyError),

    #[error("Convert secret key to sec1 Pem failed")]
    ConvertSecretKeyToSec1PemFailed(#[source] Box<sec1::Error>),

    #[error("Failed to create identity config")]
    CreateIdentityConfigFailed(#[source] CreateIdentityConfigError),

    #[error("Failed to create mnemonic from phrase: {0}")]
    CreateMnemonicFromPhraseFailed(String),

    #[error("failed to create temporary identity directory")]
    CreateTemporaryIdentityDirectoryFailed(#[source] CreateDirAllError),

    #[error("Failed to generate key")]
    GenerateKeyFailed(#[source] GenerateKeyError),

    #[error("Identity already exists.")]
    IdentityAlreadyExists(),

    #[error("Failed to load pem file")]
    LoadPemFromFileFailed(#[source] LoadPemFromFileError),

    #[error("Failed to remove identity")]
    RemoveIdentityFailed(#[source] RemoveIdentityError),

    #[error("Failed to rename temporary directory to permanent identity directory")]
    RenameTemporaryIdentityDirectoryFailed(#[from] RenameError),

    #[error("Failed to save identity configuration")]
    SaveIdentityConfigurationFailed(#[source] SaveIdentityConfigurationError),

    #[error("Failed to save pem")]
    SavePemFailed(#[source] SavePemError),

    #[error("Failed to switch back over to the identity you're replacing")]
    SwitchBackToIdentityFailed(#[source] UseIdentityByNameError),

    #[error("Failed to temporarily switch over to anonymous identity")]
    SwitchToAnonymousIdentityFailed(#[source] UseIdentityByNameError),

    #[error("Failed to validate pem file")]
    ValidatePemFileFailed(#[source] ValidatePemFileError),
}

#[derive(Error, Debug)]
pub enum ExportIdentityError {
    #[error("Failed to get identity config")]
    GetIdentityConfigFailed(#[source] GetIdentityConfigOrDefaultError),

    #[error("The specified identity does not exist")]
    IdentityDoesNotExist(#[source] RequireIdentityExistsError),

    #[error("Failed to load pem file")]
    LoadPemFailed(#[source] LoadPemError),

    #[error("Could not translate pem file to text")]
    TranslatePemContentToTextFailed(#[source] FromUtf8Error),

    #[error("Failed to validate pem file")]
    ValidatePemFileFailed(#[source] ValidatePemFileError),
}

#[derive(Error, Debug)]
pub enum GenerateKeyError {
    #[error("Failed to convert mnemonic to key")]
    ConvertMnemonicToKeyFailed(#[source] ConvertMnemonicToKeyError),

    #[error("Failed to generate a fresh secp256k1 key")]
    GenerateFreshSecp256k1KeyFailed(#[source] Box<sec1::Error>),
}

#[derive(Error, Debug)]
pub enum GetIdentityConfigOrDefaultError {
    #[error("Failed to load configuration for identity '{0}'")]
    LoadIdentityConfigurationFailed(String, #[source] StructuredFileError),
}

#[derive(Error, Debug)]
pub enum GetLegacyCredentialsPemPathError {
    #[error("Failed to get legacy pem path")]
    GetLegacyPemPathFailed(#[source] GetUserHomeError),
}

#[derive(Error, Debug)]
pub enum InitializeIdentityManagerError {
    #[error("Cannot create identity directory")]
    CreateIdentityDirectoryFailed(#[source] CreateDirAllError),

    #[error("Failed to generate key")]
    GenerateKeyFailed(#[source] GenerateKeyError),

    #[error(transparent)]
    GetLegacyCredentialsPemPathFailed(#[from] GetLegacyCredentialsPemPathError),

    #[error("failed to migrate legacy identity")]
    MigrateLegacyIdentityFailed(#[source] CopyFileError),

    #[error("Failed to save configuration")]
    SaveConfigurationFailed(#[source] StructuredFileError),

    #[error("Failed to write pem to file")]
    WritePemToFileFailed(#[source] WritePemToFileError),
}

#[derive(Error, Debug)]
pub enum InstantiateIdentityFromNameError {
    #[error("Failed to get principal of identity: {0}")]
    GetIdentityPrincipalFailed(String),

    #[error("Failed to load identity")]
    LoadIdentityFailed(#[source] LoadIdentityError),

    #[error("Identity must exist")]
    RequireIdentityExistsFailed(#[source] RequireIdentityExistsError),
}

#[derive(Error, Debug)]
pub enum LoadIdentityError {
    #[error("Failed to get identity config")]
    GetIdentityConfigOrDefaultFailed(#[source] GetIdentityConfigOrDefaultError),

    #[error("Failed to instantiate identity")]
    NewIdentityFailed(#[source] NewIdentityError),
}

#[derive(Error, Debug)]
pub enum LoadPemError {
    #[error("Failed to load PEM file from file")]
    LoadFromFileFailed(#[source] LoadPemFromFileError),

    #[error("Failed to load PEM file from keyring for identity '{0}'")]
    LoadFromKeyringFailed(Box<String>, #[source] KeyringError),
}

#[derive(Error, Debug)]
pub enum LoadPemFromFileError {
    #[error("Failed to decrypt PEM file at {0}")]
    DecryptPemFileFailed(PathBuf, #[source] EncryptionError),

    #[error("failed to read pem file")]
    ReadPemFileFailed(#[from] ReadFileError),
}

#[derive(Error, Debug)]
pub enum LoadPemIdentityError {
    #[error("Cannot read identity file '{0}'")]
    ReadIdentityFileFailed(String, #[source] Box<PemError>),
}

#[derive(Error, Debug)]
pub enum MapWalletsToRenamedIdentityError {
    #[error("Failed to get config directory for identity manager")]
    GetConfigDirectoryFailed(#[source] ConfigError),

    #[error("Failed to get shared network data directory")]
    GetSharedNetworkDataDirectoryFailed(#[source] ConfigError),

    #[error("Failed to rename wallet global config key")]
    RenameWalletGlobalConfigKeyFailed(#[source] RenameWalletGlobalConfigKeyError),
}

#[derive(Error, Debug)]
pub enum NewHardwareIdentityError {
    #[error("Failed to instantiate hardware identity for identity '{0}'")]
    InstantiateHardwareIdentityFailed(String, #[source] Box<HardwareIdentityError>),
}

#[derive(Error, Debug)]
pub enum NewIdentityError {
    #[error("Failed to load PEM")]
    LoadPemFailed(#[source] LoadPemError),

    #[error("Failed to load PEM identity")]
    LoadPemIdentityFailed(#[source] LoadPemIdentityError),

    #[error("Failed to instantiate hardware identity")]
    NewHardwareIdentityFailed(#[source] NewHardwareIdentityError),
}

#[derive(Error, Debug)]
pub enum NewIdentityManagerError {
    #[error("Failed to get config directory for identity manager")]
    GetConfigDirectoryFailed(#[source] ConfigError),

    #[error("Failed to load identity manager configuration")]
    LoadIdentityManagerConfigurationFailed(#[source] StructuredFileError),

    #[error("Failed to initialize identity manager")]
    InitializeFailed(#[source] InitializeIdentityManagerError),

    #[error("The specified identity must exist")]
    OverrideIdentityMustExist(#[source] RequireIdentityExistsError),

    #[error(r#"No identity configuration found.  Please run "dfx identity get-principal" or "dfx identity new <identity name>" to create a new identity."#)]
    NoIdentityConfigurationFound,
}

#[derive(Error, Debug)]
pub enum RemoveIdentityError {
    #[error("Cannot delete the anonymous identity.")]
    CannotDeleteAnonymousIdentity(),

    #[error("Cannot delete the default identity.")]
    CannotDeleteDefaultIdentity(),

    #[error("Failed to display linked wallets")]
    DisplayLinkedWalletsFailed(#[source] WalletConfigError),

    #[error("If you want to remove an identity with configured wallets, please use the --drop-wallets flag.")]
    DropWalletsFlagRequiredToRemoveIdentityWithWallets(),

    #[error("failed to remove identity directory")]
    RemoveIdentityDirectoryFailed(#[source] RemoveDirectoryError),

    #[error("Failed to remove identity file")]
    RemoveIdentityFileFailed(#[from] RemoveFileError),

    #[error("Failed to remove identity from keyring")]
    RemoveIdentityFromKeyringFailed(#[source] KeyringError),

    #[error("Identity must exist")]
    RequireIdentityExistsFailed(#[source] RequireIdentityExistsError),
}

#[derive(Error, Debug)]
pub enum RenameIdentityError {
    #[error("Cannot create an anonymous identity.")]
    CannotCreateAnonymousIdentity(),

    #[error("Failed to get identity config")]
    GetIdentityConfigFailed(#[source] GetIdentityConfigOrDefaultError),

    #[error("Identity already exists.")]
    IdentityAlreadyExists(),

    #[error("Identity does not exist")]
    IdentityDoesNotExist(#[source] RequireIdentityExistsError),

    #[error("Failed to load pem")]
    LoadPemFailed(#[source] LoadPemError),

    #[error("Failed to map wallets to renamed identity")]
    MapWalletsToRenamedIdentityFailed(#[source] MapWalletsToRenamedIdentityError),

    #[error("Failed to remove identity from keyring")]
    RemoveIdentityFromKeyringFailed(#[source] KeyringError),

    #[error("failed to rename identity directory")]
    RenameIdentityDirectoryFailed(#[from] RenameError),

    #[error("Failed to save identity configuration")]
    SaveIdentityConfigurationFailed(#[source] SaveIdentityConfigurationError),

    #[error("Failed to save pem")]
    SavePemFailed(#[source] SavePemError),

    #[error("Failed to switch over default identity settings")]
    SwitchDefaultIdentitySettingsFailed(#[source] WriteDefaultIdentityError),
}

#[derive(Error, Debug)]
pub enum RenameWalletGlobalConfigKeyError {
    #[error("Failed to rename '{0}' to '{1}' in the global wallet config")]
    RenameWalletFailed(Box<String>, Box<String>, #[source] WalletConfigError),

    #[error(transparent)]
    SaveWalletConfig(#[from] SaveWalletConfigError),
}

#[derive(Error, Debug)]
pub enum RequireIdentityExistsError {
    #[error("Identity {0} does not exist at '{1}'.")]
    IdentityDoesNotExist(String, PathBuf),

    #[error("An Identity named {0} cannot be created as it is reserved for internal use.")]
    ReservedIdentityName(String),
}

#[derive(Error, Debug)]
pub enum SaveIdentityConfigurationError {
    #[error("failed to ensure identity configuration directory exists")]
    EnsureIdentityConfigurationDirExistsFailed(#[source] EnsureParentDirExistsError),

    #[error("Failed to save identity configuration")]
    SaveIdentityConfigurationFailed(#[source] StructuredFileError),
}

#[derive(Error, Debug)]
pub enum SavePemError {
    #[error("Cannot save PEM content for an HSM.")]
    CannotSavePemContentForHsm(),

    #[error("Failed to write PEM to file")]
    WritePemToFileFailed(#[source] WritePemToFileError),

    #[error("Failed to write PEM to keyring")]
    WritePemToKeyringFailed(#[source] KeyringError),
}

#[derive(Error, Debug)]
pub enum UseIdentityByNameError {
    #[error("Identity must exist")]
    RequireIdentityExistsFailed(#[source] RequireIdentityExistsError),

    #[error("Failed to write default identity")]
    WriteDefaultIdentityFailed(#[source] WriteDefaultIdentityError),
}

#[derive(Error, Debug)]
pub enum ValidatePemFileError {
    #[error(transparent)]
    PemError(#[from] ic_agent::identity::PemError),

    #[error(
        "Ed25519 v1 keys (those generated by OpenSSL) are not supported. Try again with a v2 key"
    )]
    UnsupportedKeyVersion(),

    #[error("Failed to validate PEM content")]
    ValidatePemContentFailed(#[source] Box<PemError>),
}

#[derive(Error, Debug)]
pub enum WriteDefaultIdentityError {
    #[error("Failed to save identity manager configuration")]
    SaveIdentityManagerConfigurationFailed(#[source] StructuredFileError),
}

#[derive(Error, Debug)]
pub enum WritePemContentError {
    #[error(transparent)]
    CreateDirAll(#[from] CreateDirAllError),

    #[error(transparent)]
    NoParent(#[from] NoParentPathError),

    #[error(transparent)]
    ReadPermissions(#[from] ReadPermissionsError),

    #[error(transparent)]
    SetPermissions(#[from] SetPermissionsError),

    #[error(transparent)]
    Write(#[from] WriteFileError),
}

#[derive(Error, Debug)]
pub enum WritePemToFileError {
    #[error("Failed to encrypt PEM file")]
    EncryptPemFileFailed(PathBuf, #[source] EncryptionError),

    #[error("failed to write PEM content")]
    WritePemContentFailed(#[source] WritePemContentError),
}


-----------------------

/src/dfx-core/src/error/keyring.rs:
-----------------------

use crate::error::structured_file::StructuredFileError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum KeyringError {
    #[error("Failed to decode pem from keyring")]
    DecodePemFailed(#[source] hex::FromHexError),

    #[error("Failed to delete password from keyring")]
    DeletePasswordFailed(#[source] keyring::Error),

    #[error("Failed to get password for keyring")]
    GetPasswordFailed(#[source] keyring::Error),

    #[error("Failed to load mock keyring")]
    LoadMockKeyringFailed(#[source] StructuredFileError),

    #[error("Mock Keyring: key {0} not found")]
    MockKeyNotFound(String),

    #[error("Mock keyring unavailable - access rejected.")]
    MockUnavailable(),

    #[error("Failed to save mock keyring")]
    SaveMockKeyringFailed(#[source] StructuredFileError),

    #[error("Failed to set password for keyring")]
    SetPasswordFailed(#[source] keyring::Error),
}


-----------------------

/src/dfx-core/src/error/load_dfx_config.rs:
-----------------------

use crate::error::config::ApplyExtensionCanisterTypesError;
use crate::error::fs::{CanonicalizePathError, ReadFileError};
use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum LoadDfxConfigError {
    #[error(transparent)]
    ApplyExtensionCanisterTypesError(#[from] ApplyExtensionCanisterTypesError),

    #[error("Failed to deserialize json from {0}")]
    DeserializeValueFailed(Box<PathBuf>, #[source] serde_json::Error),

    #[error("failed to resolve config path")]
    ResolveConfigPath(#[source] CanonicalizePathError),

    #[error("Failed to load dfx configuration")]
    ReadFile(#[from] ReadFileError),

    #[error("Failed to determine current working dir")]
    DetermineCurrentWorkingDirFailed(#[source] std::io::Error),
}


-----------------------

/src/dfx-core/src/error/load_networks_config.rs:
-----------------------

use crate::error::config::ConfigError;
use crate::error::structured_file::StructuredFileError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum LoadNetworksConfigError {
    #[error("Failed to get path for network configuration")]
    GetConfigPathFailed(#[source] ConfigError),

    #[error("Failed to load network configuration")]
    LoadConfigFromFileFailed(#[source] StructuredFileError),
}


-----------------------

/src/dfx-core/src/error/mod.rs:
-----------------------

pub mod archive;
pub mod builder;
pub mod cache;
pub mod canister;
pub mod canister_id_store;
pub mod cli;
pub mod config;
pub mod dfx_config;
pub mod encryption;
pub mod extension;
pub mod fs;
pub mod get_current_exe;
pub mod get_user_home;
pub mod identity;
pub mod keyring;
pub mod load_dfx_config;
pub mod load_networks_config;
pub mod network_config;
pub mod process;
pub mod reqwest;
pub mod root_key;
pub mod socket_addr_conversion;
pub mod structured_file;
pub mod uri;
pub mod wallet_config;


-----------------------

/src/dfx-core/src/error/network_config.rs:
-----------------------

use crate::error::config::{ConfigError, GetTempPathError};
use crate::error::fs::ReadToStringError;
use crate::error::socket_addr_conversion::SocketAddrConversionError;
use crate::error::uri::UriError;

use candid::types::principal::PrincipalError;
use std::num::ParseIntError;
use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum NetworkConfigError {
    #[error(transparent)]
    ReadToString(#[from] ReadToStringError),

    #[error(transparent)]
    Config(#[from] ConfigError),

    #[error(transparent)]
    UriError(#[from] UriError),

    #[error("Failed to get replica endpoint for network '{network_name}'")]
    GettingReplicaUrlsFailed {
        network_name: String,
        source: UriError,
    },

    #[error(transparent)]
    GetTempPath(#[from] GetTempPathError),

    #[error("Network '{0}' does not specify any network providers.")]
    NetworkHasNoProviders(String),

    #[error("The '{0}' network must be a local network.")]
    NetworkMustBeLocal(String),

    #[error("Network not found: {0}")]
    NetworkNotFound(String),

    #[error("Cannot find network context.")]
    NoNetworkContext(),

    #[error("Did not find any providers for network '{0}'")]
    NoProvidersForNetwork(String),

    #[error("Failed to parse bind address")]
    ParseBindAddressFailed(#[source] SocketAddrConversionError),

    #[error("Failed to parse contents of {0} as a port value")]
    ParsePortValueFailed(Box<PathBuf>, #[source] Box<ParseIntError>),

    #[error("Failed to parse URL '{0}'")]
    ParseProviderUrlFailed(Box<String>, #[source] url::ParseError),

    #[error("Failed to read webserver port")]
    ReadWebserverPortFailed(#[source] ReadToStringError),

    #[error("Failed to parse principal '{0}'")]
    ParsePrincipalFailed(String, #[source] PrincipalError),
}


-----------------------

/src/dfx-core/src/error/process.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum ProcessError {
    #[error("Execution of '{0:?}' failed")]
    ExecutionFailed(std::ffi::OsString, #[source] std::io::Error),
}


-----------------------

/src/dfx-core/src/error/reqwest.rs:
-----------------------

use reqwest::StatusCode;

pub fn is_retryable(err: &reqwest::Error) -> bool {
    err.is_timeout()
        || err.is_connect()
        || matches!(
            err.status(),
            Some(
                StatusCode::INTERNAL_SERVER_ERROR
                    | StatusCode::BAD_GATEWAY
                    | StatusCode::SERVICE_UNAVAILABLE
                    | StatusCode::GATEWAY_TIMEOUT
                    | StatusCode::TOO_MANY_REQUESTS
            )
        )
}


-----------------------

/src/dfx-core/src/error/root_key.rs:
-----------------------

use ic_agent;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum FetchRootKeyError {
    #[error(transparent)]
    AgentError(#[from] ic_agent::AgentError),

    #[error("This command only runs on local instances. Cannot run this on the real IC.")]
    MustNotFetchRootKeyOnMainnet,
}


-----------------------

/src/dfx-core/src/error/socket_addr_conversion.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum SocketAddrConversionError {
    #[error("Did not find any socket addresses in string '{0}'")]
    EmptyIterator(String),

    #[error("Failed to convert {0} to a socket address")]
    ParseSocketAddrFailed(String, #[source] std::io::Error),
}


-----------------------

/src/dfx-core/src/error/structured_file.rs:
-----------------------

use crate::error::fs::{ReadFileError, WriteFileError};
use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum StructuredFileError {
    #[error("failed to parse contents of {0} as json")]
    DeserializeJsonFileFailed(Box<PathBuf>, #[source] serde_json::Error),

    #[error("failed to read JSON file")]
    ReadJsonFileFailed(#[from] ReadFileError),

    #[error("failed to serialize JSON to {0}")]
    SerializeJsonFileFailed(Box<PathBuf>, #[source] serde_json::Error),

    #[error("failed to write JSON file")]
    WriteJsonFileFailed(#[from] WriteFileError),
}


-----------------------

/src/dfx-core/src/error/uri.rs:
-----------------------

use thiserror::Error;

#[derive(Error, Debug)]
pub enum UriError {
    #[error("Failed to parse url '{0}'")]
    UrlParseError(String, #[source] url::ParseError),
}


-----------------------

/src/dfx-core/src/error/wallet_config.rs:
-----------------------

use crate::error::config::ConfigError;
use crate::error::fs::{CreateDirAllError, EnsureParentDirExistsError};
use crate::error::structured_file::StructuredFileError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum WalletConfigError {
    #[error("failed to ensure existence of parent directory for wallet configuration")]
    EnsureWalletConfigDirFailed(#[source] CreateDirAllError),

    #[error("Failed to get wallet configuration path")]
    GetWalletConfigPathFailed(Box<String>, Box<String>, #[source] ConfigError),

    #[error("Failed to load wallet configuration")]
    LoadWalletConfigFailed(#[source] StructuredFileError),

    #[error("Failed to save wallet configuration")]
    SaveWalletConfig(#[from] SaveWalletConfigError),
}

#[derive(Error, Debug)]
pub enum SaveWalletConfigError {
    #[error(transparent)]
    EnsureParentDirExists(#[from] EnsureParentDirExistsError),

    #[error(transparent)]
    SaveJsonFile(#[from] StructuredFileError),
}


-----------------------

/src/dfx-core/src/extension/catalog.rs:
-----------------------

use crate::error::extension::FetchCatalogError;
use crate::extension::url::ExtensionJsonUrl;
use crate::http::get::get_with_retries;
use crate::json::structure::UrlWithJsonSchema;
use backoff::exponential::ExponentialBackoff;
use schemars::JsonSchema;
use serde::Deserialize;
use std::collections::HashMap;
use std::time::Duration;
use url::Url;

const DEFAULT_CATALOG_URL: &str =
    "https://raw.githubusercontent.com/dfinity/dfx-extensions/main/catalog.json";

#[derive(Deserialize, Debug, JsonSchema)]
pub struct ExtensionCatalog(pub HashMap<String, UrlWithJsonSchema>);

impl ExtensionCatalog {
    pub async fn fetch(url: Option<&Url>) -> Result<Self, FetchCatalogError> {
        let url: Option<Url> = url.cloned();
        let url = url.unwrap_or_else(|| Url::parse(DEFAULT_CATALOG_URL).unwrap());
        let retry_policy = ExponentialBackoff {
            max_elapsed_time: Some(Duration::from_secs(60)),
            ..Default::default()
        };
        let resp = get_with_retries(url, retry_policy)
            .await
            .map_err(FetchCatalogError::Get)?;

        resp.json().await.map_err(FetchCatalogError::ParseJson)
    }

    pub fn lookup(&self, name: &str) -> Option<ExtensionJsonUrl> {
        self.0
            .get(name)
            .map(|url| ExtensionJsonUrl::new(url.0.clone()))
    }
}


-----------------------

/src/dfx-core/src/extension/installed.rs:
-----------------------

use crate::error::extension::ConvertExtensionIntoClapCommandError;
use crate::extension::manifest::ExtensionManifest;
use crate::extension::ExtensionName;
use clap::Command;
use std::collections::HashMap;

pub type InstalledExtensionList = Vec<ExtensionName>;
pub struct InstalledExtensionManifests(pub HashMap<ExtensionName, ExtensionManifest>);

impl InstalledExtensionManifests {
    pub fn as_clap_commands(&self) -> Result<Vec<Command>, ConvertExtensionIntoClapCommandError> {
        let commands = self
            .0
            .values()
            .map(|manifest| {
                manifest.into_clap_commands().map(|subcommands| {
                    Command::new(&manifest.name)
                        .allow_missing_positional(false) // don't accept unknown options
                        .allow_external_subcommands(false) // don't accept unknown subcommands
                        .about(&manifest.summary)
                        .subcommands(subcommands)
                })
            })
            .collect::<Result<Vec<_>, _>>()?;
        Ok(commands)
    }

    pub fn contains(&self, extension: &str) -> bool {
        self.0.contains_key(extension)
    }
}


-----------------------

/src/dfx-core/src/extension/manager/execute.rs:
-----------------------

use super::ExtensionManager;
use crate::config::cache::get_bin_cache;
use crate::error::extension::RunExtensionError;
use std::ffi::OsString;
use std::path::PathBuf;

impl ExtensionManager {
    pub fn run_extension(
        &self,
        extension_name: OsString,
        mut params: Vec<OsString>,
        project_root: Option<PathBuf>,
    ) -> Result<(), RunExtensionError> {
        let extension_name = extension_name
            .into_string()
            .map_err(RunExtensionError::InvalidExtensionName)?;

        let mut extension_binary = self.get_extension_binary(&extension_name)?;
        if let Some(project_root) = project_root {
            extension_binary.current_dir(project_root);
        }
        let dfx_cache = get_bin_cache(self.dfx_version.to_string().as_str())?;

        params.extend(["--dfx-cache-path".into(), dfx_cache.into_os_string()]);

        let mut child = extension_binary
            .args(&params)
            .spawn()
            .map_err(|e| RunExtensionError::FailedToLaunchExtension(extension_name.clone(), e))?;

        let exit_status = child.wait().map_err(|e| {
            RunExtensionError::ExtensionNeverFinishedExecuting(extension_name.clone(), e)
        })?;

        let code = exit_status
            .code()
            .ok_or(RunExtensionError::ExtensionExecutionTerminatedViaSignal)?;

        if code != 0 {
            Err(RunExtensionError::ExtensionExitedWithNonZeroStatus(code))
        } else {
            Ok(())
        }
    }
}


-----------------------

/src/dfx-core/src/extension/manager/install.rs:
-----------------------

use crate::error::extension::{
    DownloadAndInstallExtensionToTempdirError, FinalizeInstallationError,
    GetExtensionArchiveNameError, GetExtensionDownloadUrlError, GetExtensionManifestError,
    GetHighestCompatibleVersionError, GetTopLevelDirectoryError, InstallExtensionError,
};
use crate::extension::{
    catalog::ExtensionCatalog,
    manager::ExtensionManager,
    manifest::{ExtensionDependencies, ExtensionManifest},
    url::ExtensionJsonUrl,
};
use crate::http::get::get_with_retries;
use backoff::exponential::ExponentialBackoff;
use flate2::read::GzDecoder;
use reqwest::Url;
use semver::{BuildMetadata, Prerelease, Version};
use std::io::Cursor;
#[cfg(unix)]
use std::os::unix::fs::PermissionsExt;
use std::path::{Path, PathBuf};
use std::time::Duration;
use tar::Archive;
use tempfile::{tempdir_in, TempDir};

pub enum InstallOutcome {
    Installed(String, Version),
    AlreadyInstalled(String, Version),
}

impl ExtensionManager {
    pub async fn install_extension(
        &self,
        name: &str,
        catalog_url: Option<&Url>,
        install_as: Option<&str>,
        version: Option<&Version>,
    ) -> Result<InstallOutcome, InstallExtensionError> {
        let url = if let Ok(url) = Url::parse(name) {
            ExtensionJsonUrl::new(url)
        } else {
            ExtensionCatalog::fetch(catalog_url)
                .await?
                .lookup(name)
                .ok_or(InstallExtensionError::ExtensionNotFound(name.to_string()))?
        };

        let manifest = Self::get_extension_manifest(&url).await?;
        let extension_name: &str = &manifest.name;

        let effective_extension_name = install_as.unwrap_or(extension_name);

        if self
            .get_extension_directory(effective_extension_name)
            .exists()
        {
            let installed_manifest = ExtensionManifest::load(effective_extension_name, &self.dir)?;

            return if matches!(version, Some(v) if *v != *installed_manifest.version) {
                Err(InstallExtensionError::OtherVersionAlreadyInstalled(
                    extension_name.to_string(),
                    installed_manifest.version.clone(),
                ))
            } else {
                Ok(InstallOutcome::AlreadyInstalled(
                    extension_name.to_string(),
                    installed_manifest.version.clone(),
                ))
            };
        }

        let extension_version = match version {
            Some(version) => version.clone(),
            None => self.get_highest_compatible_version(&url).await?,
        };
        let github_release_tag = get_git_release_tag(extension_name, &extension_version);
        let extension_archive = get_extension_archive_name(extension_name)?;
        let archive_url = get_extension_download_url(
            &manifest.download_url_template(),
            &github_release_tag,
            &extension_archive,
        )?;

        let temp_dir = self
            .download_and_unpack_extension_to_tempdir(archive_url)
            .await?;

        self.finalize_installation(extension_name, effective_extension_name, temp_dir)?;

        Ok(InstallOutcome::Installed(
            extension_name.to_string(),
            extension_version,
        ))
    }

    /// Removing the prerelease tag and build metadata, because they should
    /// not be allowed in extension manifests, and semver crate won't match
    /// a semver with a prerelease tag or build metadata against a semver without.
    fn dfx_version_strip_semver(&self) -> Version {
        let mut dfx_version = self.dfx_version.clone();
        dfx_version.pre = Prerelease::EMPTY;
        dfx_version.build = BuildMetadata::EMPTY;
        dfx_version
    }

    async fn get_highest_compatible_version(
        &self,
        url: &ExtensionJsonUrl,
    ) -> Result<Version, GetHighestCompatibleVersionError> {
        let dependencies = ExtensionDependencies::fetch(url).await?;
        let dfx_version = self.dfx_version_strip_semver();
        dependencies
            .find_highest_compatible_version(&dfx_version)?
            .ok_or(GetHighestCompatibleVersionError::NoCompatibleVersionFound())
    }

    async fn get_extension_manifest(
        url: &ExtensionJsonUrl,
    ) -> Result<ExtensionManifest, GetExtensionManifestError> {
        let retry_policy = ExponentialBackoff {
            max_elapsed_time: Some(Duration::from_secs(60)),
            ..Default::default()
        };
        let resp = get_with_retries(url.as_url().clone(), retry_policy)
            .await
            .map_err(GetExtensionManifestError::Get)?;

        resp.json()
            .await
            .map_err(GetExtensionManifestError::ParseJson)
    }

    async fn download_and_unpack_extension_to_tempdir(
        &self,
        download_url: Url,
    ) -> Result<TempDir, DownloadAndInstallExtensionToTempdirError> {
        let retry_policy = ExponentialBackoff {
            max_elapsed_time: Some(Duration::from_secs(60)),
            ..Default::default()
        };
        let response = get_with_retries(download_url.clone(), retry_policy)
            .await
            .map_err(DownloadAndInstallExtensionToTempdirError::ExtensionDownloadFailed)?;

        let bytes = response
            .bytes()
            .await
            .map_err(DownloadAndInstallExtensionToTempdirError::ExtensionDownloadFailed)?;

        crate::fs::composite::ensure_dir_exists(&self.dir)
            .map_err(DownloadAndInstallExtensionToTempdirError::EnsureExtensionDirExistsFailed)?;

        let temp_dir = tempdir_in(&self.dir).map_err(|e| {
            DownloadAndInstallExtensionToTempdirError::CreateTemporaryDirectoryFailed(
                self.dir.to_path_buf(),
                e,
            )
        })?;

        let mut archive = Archive::new(GzDecoder::new(Cursor::new(bytes)));
        archive.unpack(temp_dir.path()).map_err(|e| {
            DownloadAndInstallExtensionToTempdirError::DecompressFailed(download_url, e)
        })?;

        Ok(temp_dir)
    }

    #[allow(clippy::collapsible_if)]
    fn finalize_installation(
        &self,
        extension_name: &str,
        effective_extension_name: &str,
        temp_dir: TempDir,
    ) -> Result<(), FinalizeInstallationError> {
        let effective_extension_dir = &self.get_extension_directory(effective_extension_name);
        let top_level_dir = get_top_level_directory(temp_dir.path())?;
        crate::fs::rename(&top_level_dir, effective_extension_dir)?;

        let installed_manifest = ExtensionManifest::load(effective_extension_name, &self.dir)?;

        if matches!(installed_manifest.subcommands, Some(subcommands) if !subcommands.0.is_empty())
        {
            if extension_name != effective_extension_name {
                // rename the binary
                crate::fs::rename(
                    &effective_extension_dir.join(extension_name),
                    &effective_extension_dir.join(effective_extension_name),
                )?;
            }
            #[cfg(unix)]
            {
                let bin = effective_extension_dir.join(effective_extension_name);
                crate::fs::set_permissions(&bin, std::fs::Permissions::from_mode(0o500))?;
            }
        }
        Ok(())
    }
}

fn get_top_level_directory(dir: &Path) -> Result<PathBuf, GetTopLevelDirectoryError> {
    // the archive will have a single top-level subdirectory
    // return that subdirectory
    Ok(crate::fs::read_dir(dir)?
        .next()
        .ok_or(GetTopLevelDirectoryError::NoTopLevelDirectoryEntry)?
        .map_err(GetTopLevelDirectoryError::ReadDirEntry)?
        .path())
}

fn get_extension_download_url(
    download_url_template: &str,
    github_release_tag: &str,
    extension_archive_name: &str,
) -> Result<Url, GetExtensionDownloadUrlError> {
    let download_url = download_url_template
        .replace("{{tag}}", github_release_tag)
        .replace("{{basename}}", extension_archive_name)
        .replace("{{archive-format}}", "tar.gz");

    Url::parse(&download_url).map_err(|source| GetExtensionDownloadUrlError {
        url: download_url,
        source,
    })
}

fn get_git_release_tag(extension_name: &str, extension_verion: &Version) -> String {
    format!("{extension_name}-v{extension_verion}",)
}

fn get_extension_archive_name(
    extension_name: &str,
) -> Result<String, GetExtensionArchiveNameError> {
    Ok(format!(
        "{extension_name}-{arch}-{platform}",
        platform = match std::env::consts::OS {
            "linux" => "unknown-linux-gnu",
            "macos" => "apple-darwin",
            // "windows" => "pc-windows-msvc",
            unsupported_platform =>
                return Err(GetExtensionArchiveNameError::PlatformNotSupported(
                    unsupported_platform.to_string()
                )),
        },
        arch = std::env::consts::ARCH,
    ))
}


-----------------------

/src/dfx-core/src/extension/manager/list.rs:
-----------------------

use super::ExtensionManager;
use crate::error::extension::{ListAvailableExtensionsError, ListInstalledExtensionsError};
use crate::extension::catalog::ExtensionCatalog;
use crate::extension::installed::InstalledExtensionList;
use crate::extension::ExtensionName;
use std::vec;
use url::Url;

pub type AvailableExtensionList = Vec<ExtensionName>;

impl ExtensionManager {
    pub fn list_installed_extensions(
        &self,
    ) -> Result<InstalledExtensionList, ListInstalledExtensionsError> {
        if !self.dir.exists() {
            return Ok(vec![]);
        }
        let dir_content = crate::fs::read_dir(&self.dir)?;

        let extensions = dir_content
            .filter_map(|v| {
                let dir_entry = v.ok()?;
                if dir_entry.file_type().map_or(false, |e| e.is_dir())
                    && !dir_entry.file_name().to_str()?.starts_with(".tmp")
                {
                    let name = dir_entry.file_name().to_string_lossy().to_string();
                    Some(name)
                } else {
                    None
                }
            })
            .collect();
        Ok(extensions)
    }

    pub async fn list_available_extensions(
        &self,
        catalog_url: Option<&Url>,
    ) -> Result<AvailableExtensionList, ListAvailableExtensionsError> {
        let catalog = ExtensionCatalog::fetch(catalog_url)
            .await
            .map_err(ListAvailableExtensionsError::FetchCatalog)?;
        let extensions: Vec<String> = catalog.0.into_keys().collect();

        Ok(extensions)
    }
}


-----------------------

/src/dfx-core/src/extension/manager/mod.rs:
-----------------------

use crate::config::cache::get_cache_path_for_version;
use crate::error::extension::{
    GetExtensionBinaryError, LoadExtensionManifestsError, NewExtensionManagerError,
};
use crate::extension::{installed::InstalledExtensionManifests, manifest::ExtensionManifest};
pub use install::InstallOutcome;
use semver::Version;
use std::collections::HashMap;
use std::path::PathBuf;

mod execute;
mod install;
mod list;
mod uninstall;

pub struct ExtensionManager {
    pub dir: PathBuf,
    pub dfx_version: Version,
}

impl ExtensionManager {
    pub fn new(version: &Version) -> Result<Self, NewExtensionManagerError> {
        let extensions_dir = get_cache_path_for_version(&version.to_string())?.join("extensions");

        Ok(Self {
            dir: extensions_dir,
            dfx_version: version.clone(),
        })
    }

    pub fn get_extension_directory(&self, extension_name: &str) -> PathBuf {
        self.dir.join(extension_name)
    }

    pub fn get_extension_binary(
        &self,
        extension_name: &str,
    ) -> Result<std::process::Command, GetExtensionBinaryError> {
        let dir = self.get_extension_directory(extension_name);
        if !dir.exists() {
            return Err(GetExtensionBinaryError::ExtensionNotInstalled(
                extension_name.to_string(),
            ));
        }
        let bin = dir.join(extension_name);
        if !bin.exists() {
            Err(GetExtensionBinaryError::ExtensionBinaryDoesNotExist(bin))
        } else if !bin.is_file() {
            Err(GetExtensionBinaryError::ExtensionBinaryIsNotAFile(bin))
        } else {
            Ok(std::process::Command::new(bin))
        }
    }

    pub fn is_extension_installed(&self, extension_name: &str) -> bool {
        self.get_extension_directory(extension_name).exists()
    }

    pub fn load_installed_extension_manifests(
        &self,
    ) -> Result<InstalledExtensionManifests, LoadExtensionManifestsError> {
        let manifests = self
            .list_installed_extensions()?
            .into_iter()
            .map(|name| {
                ExtensionManifest::load(&name, &self.dir)
                    .map(|manifest| (manifest.name.clone(), manifest))
            })
            .collect::<Result<HashMap<_, _>, _>>()?;
        Ok(InstalledExtensionManifests(manifests))
    }
}


-----------------------

/src/dfx-core/src/extension/manager/uninstall.rs:
-----------------------

use super::ExtensionManager;
use crate::error::extension::UninstallExtensionError;

impl ExtensionManager {
    pub fn uninstall_extension(&self, extension_name: &str) -> Result<(), UninstallExtensionError> {
        let path = self.get_extension_directory(extension_name);
        crate::fs::remove_dir_all(&path)?;
        Ok(())
    }
}


-----------------------

/src/dfx-core/src/extension/manifest/dependencies.rs:
-----------------------

use crate::error::extension::{DfxOnlySupportedDependency, GetDependenciesError};
use crate::extension::url::ExtensionJsonUrl;
use crate::http::get::get_with_retries;
use crate::json::structure::VersionReqWithJsonSchema;
use backoff::exponential::ExponentialBackoff;
use candid::Deserialize;
use schemars::JsonSchema;
use semver::Version;
use std::collections::HashMap;
use std::time::Duration;

type ExtensionVersion = Version;
type DependencyName = String;

#[derive(Debug, Deserialize, JsonSchema)]
#[serde(rename_all = "lowercase")]
pub enum DependencyRequirement {
    /// A SemVer version requirement, for example ">=0.17.0, <0.19.0".
    Version(VersionReqWithJsonSchema),
}

#[derive(Deserialize, Debug, JsonSchema)]
pub struct ExtensionDependencies(
    pub HashMap<ExtensionVersion, HashMap<DependencyName, DependencyRequirement>>,
);

impl ExtensionDependencies {
    pub async fn fetch(url: &ExtensionJsonUrl) -> Result<Self, GetDependenciesError> {
        let dependencies_json_url = url.to_dependencies_json()?;
        let retry_policy = ExponentialBackoff {
            max_elapsed_time: Some(Duration::from_secs(60)),
            ..Default::default()
        };
        let resp = get_with_retries(dependencies_json_url, retry_policy)
            .await
            .map_err(GetDependenciesError::Get)?;

        resp.json().await.map_err(GetDependenciesError::ParseJson)
    }

    pub fn find_highest_compatible_version(
        &self,
        dfx_version: &Version,
    ) -> Result<Option<Version>, DfxOnlySupportedDependency> {
        let mut keys: Vec<&Version> = self.0.keys().collect();
        keys.sort();
        keys.reverse(); // check higher extension versions first

        for key in keys {
            let dependencies = self.0.get(key).unwrap();
            for (dependency, requirements) in dependencies {
                if dependency == "dfx" {
                    match requirements {
                        DependencyRequirement::Version(req) => {
                            if req.matches(dfx_version) {
                                return Ok(Some(key.clone()));
                            }
                        }
                    }
                } else {
                    return Err(DfxOnlySupportedDependency);
                }
            }
        }

        Ok(None)
    }
}

#[test]
fn parse_test_file() {
    let f = r#"
{
  "0.3.4": {
    "dfx": {
      "version": ">=0.8, <0.9"
    }
  },
  "0.6.2": {
    "dfx": {
      "version": ">=0.9.6"
    }
  },
  "0.7.0": {
    "dfx": {
      "version": ">=0.9.9"
    }
  }
}
"#;
    let m: Result<ExtensionDependencies, serde_json::Error> = dbg!(serde_json::from_str(f));
    assert!(m.is_ok());
    let manifest = m.unwrap();

    let versions = manifest.0.keys().collect::<Vec<_>>();
    assert_eq!(versions.len(), 3);
    assert!(versions.contains(&&Version::new(0, 3, 4)));
    assert!(versions.contains(&&Version::new(0, 6, 2)));
    assert!(versions.contains(&&Version::new(0, 7, 0)));

    let v_3_4 = manifest.0.get(&Version::new(0, 3, 4)).unwrap();
    let dfx = v_3_4.get("dfx").unwrap();
    let DependencyRequirement::Version(req) = dfx;
    assert!(req.matches(&semver::Version::new(0, 8, 5)));
    assert!(!req.matches(&semver::Version::new(0, 9, 0)));

    let v_6_2 = manifest.0.get(&Version::new(0, 6, 2)).unwrap();
    let dfx = v_6_2.get("dfx").unwrap();
    let DependencyRequirement::Version(req) = dfx;
    assert!(req.matches(&semver::Version::new(0, 9, 6)));
    assert!(!req.matches(&semver::Version::new(0, 9, 5)));

    assert_eq!(
        manifest
            .find_highest_compatible_version(&Version::new(0, 8, 5))
            .unwrap(),
        Some(Version::new(0, 3, 4))
    );
    assert_eq!(
        manifest
            .find_highest_compatible_version(&Version::new(0, 9, 6))
            .unwrap(),
        Some(Version::new(0, 6, 2))
    );
    assert_eq!(
        manifest
            .find_highest_compatible_version(&Version::new(0, 9, 10))
            .unwrap(),
        Some(Version::new(0, 7, 0))
    );
}


-----------------------

/src/dfx-core/src/extension/manifest/extension.rs:
-----------------------

use crate::error::extension::{
    ConvertExtensionSubcommandIntoClapArgError, ConvertExtensionSubcommandIntoClapCommandError,
    LoadExtensionManifestError,
};
use crate::json::structure::{VersionReqWithJsonSchema, VersionWithJsonSchema};
use schemars::JsonSchema;
use serde::{Deserialize, Deserializer, Serialize, Serializer};
use serde_json::Value;
use std::path::PathBuf;
use std::{
    collections::{BTreeMap, HashMap},
    path::Path,
};

pub static MANIFEST_FILE_NAME: &str = "extension.json";
const DEFAULT_DOWNLOAD_URL_TEMPLATE: &str =
    "https://github.com/dfinity/dfx-extensions/releases/download/{{tag}}/{{basename}}.{{archive-format}}";

type SubcmdName = String;
type ArgName = String;

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct ExtensionManifest {
    pub name: String,
    pub version: VersionWithJsonSchema,
    pub homepage: String,
    pub authors: Option<String>,
    pub summary: String,
    pub categories: Vec<String>,
    pub keywords: Option<Vec<String>>,
    pub description: Option<String>,
    pub subcommands: Option<ExtensionSubcommandsOpts>,
    pub dependencies: Option<HashMap<String, ExtensionDependency>>,
    pub canister_type: Option<ExtensionCanisterType>,

    /// Components of the download url template are:
    /// - `{{tag}}`: the tag of the extension release, which will follow the form "<extension name>-v<extension version>"
    /// - `{{basename}}`: The basename of the release filename, which will follow the form "<extension name>-<arch>-<platform>", for example "nns-x86_64-unknown-linux-gnu"
    /// - `{{archive-format}}`: the format of the archive, for example "tar.gz"
    #[serde(
        default = "default_download_url_template",
        skip_serializing_if = "Option::is_none"
    )]
    pub download_url_template: Option<String>,
}

fn default_download_url_template() -> Option<String> {
    Some(DEFAULT_DOWNLOAD_URL_TEMPLATE.to_string())
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
#[serde(untagged)]
pub enum ExtensionDependency {
    /// A SemVer version requirement, for example ">=0.17.0".
    Version(VersionReqWithJsonSchema),
}

impl ExtensionManifest {
    pub fn load(
        name: &str,
        extensions_root_dir: &Path,
    ) -> Result<Self, LoadExtensionManifestError> {
        let manifest_path = Self::manifest_path(name, extensions_root_dir);
        let mut m: ExtensionManifest = crate::json::load_json_file(&manifest_path)?;
        m.name = name.to_string();
        Ok(m)
    }

    pub fn exists(name: &str, extensions_root_dir: &Path) -> bool {
        Self::manifest_path(name, extensions_root_dir).exists()
    }

    fn manifest_path(name: &str, extensions_root_dir: &Path) -> PathBuf {
        extensions_root_dir.join(name).join(MANIFEST_FILE_NAME)
    }

    pub fn download_url_template(&self) -> String {
        self.download_url_template
            .clone()
            .unwrap_or_else(|| DEFAULT_DOWNLOAD_URL_TEMPLATE.to_string())
    }

    pub fn into_clap_commands(
        &self,
    ) -> Result<Vec<clap::Command>, ConvertExtensionSubcommandIntoClapCommandError> {
        if let Some(sc) = self.subcommands.as_ref() {
            sc.0.iter()
                .map(|(subcmd, opts)| opts.as_clap_command(subcmd))
                .collect::<Result<Vec<_>, _>>()
        } else {
            Ok(vec![])
        }
    }
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
pub struct ExtensionCanisterType {
    /// If one field depends on another and both specify a handlebars expression,
    /// list the fields in the order that they should be evaluated.
    #[serde(default)]
    pub evaluation_order: Vec<String>,

    /// Default values for the canister type. These values are used when the user does not provide
    /// values in dfx.json.
    /// The "metadata" field, if present, is appended to the metadata field from dfx.json, which
    /// has the effect of providing defaults.
    /// The "tech_stack field, if present, it merged with the tech_stack field from dfx.json,
    /// which also has the effect of providing defaults.
    #[serde(default)]
    pub defaults: BTreeMap<String, Value>,
}

#[derive(Debug, Serialize, Deserialize, Default, JsonSchema)]
pub struct ExtensionSubcommandsOpts(pub BTreeMap<SubcmdName, ExtensionSubcommandOpts>);

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct ExtensionSubcommandOpts {
    pub about: Option<String>,
    pub args: Option<BTreeMap<ArgName, ExtensionSubcommandArgOpts>>,
    pub subcommands: Option<ExtensionSubcommandsOpts>,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct ExtensionSubcommandArgOpts {
    pub about: Option<String>,
    pub long: Option<String>,
    pub short: Option<char>,
    #[serde(default)]
    #[deprecated(note = "use `values` instead")]
    pub multiple: bool,
    #[serde(default)]
    pub values: ArgNumberOfValues,
}

#[derive(Debug, JsonSchema, Eq, PartialEq)]
pub enum ArgNumberOfValues {
    /// zero or more values
    Number(usize),
    /// non-inclusive range
    Range(std::ops::Range<usize>),
    /// unlimited values
    Unlimited,
}

impl Default for ArgNumberOfValues {
    fn default() -> Self {
        Self::Number(1)
    }
}

impl<'de> Deserialize<'de> for ArgNumberOfValues {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        #[derive(Deserialize)]
        #[serde(untagged)]
        enum StrOrUsize<'a> {
            Str(&'a str),
            Usize(usize),
        }

        match StrOrUsize::deserialize(deserializer)? {
            StrOrUsize::Usize(n) => Ok(Self::Number(n)),
            StrOrUsize::Str(s) => {
                if s == "unlimited" {
                    return Ok(Self::Unlimited);
                }
                if s.contains("..=") {
                    let msg = format!("Inclusive ranges are not supported: {}", s);
                    return Err(serde::de::Error::custom(msg));
                }
                if s.contains("..") {
                    let parts: Vec<&str> = s.split("..").collect();
                    if let (Ok(start), Ok(end)) =
                        (parts[0].parse::<usize>(), parts[1].parse::<usize>())
                    {
                        return Ok(Self::Range(start..end + 1));
                    }
                }
                Err(serde::de::Error::custom(format!(
            "Invalid format for values: '{}'. Expected 'unlimited' or a positive integer or a range (for example '1..3')",
            s
        )))
            }
        }
    }
}

impl Serialize for ArgNumberOfValues {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        match self {
            Self::Number(n) => serializer.serialize_u64(*n as u64),
            Self::Unlimited => serializer.serialize_str("unlimited"),
            Self::Range(range) => {
                let s = format!("{}..{}", range.start, range.end - 1);
                serializer.serialize_str(&s)
            }
        }
    }
}

impl ExtensionSubcommandArgOpts {
    pub fn as_clap_arg(
        &self,
        name: &str,
    ) -> Result<clap::Arg, ConvertExtensionSubcommandIntoClapArgError> {
        let mut arg = clap::Arg::new(name.to_string());
        if let Some(about) = &self.about {
            arg = arg.help(about);
        } else {
            return Err(ConvertExtensionSubcommandIntoClapArgError::ExtensionSubcommandArgMissingDescription(
                name.to_string(),
            ));
        }
        if let Some(l) = &self.long {
            arg = arg.long(l);
        }
        if let Some(s) = &self.short {
            arg = arg.short(*s);
        }
        #[allow(deprecated)]
        if self.multiple {
            arg = arg.num_args(0..);
        } else {
            arg = match &self.values {
                ArgNumberOfValues::Number(n) => arg.num_args(*n),
                ArgNumberOfValues::Range(r) => arg.num_args(r.clone()),
                ArgNumberOfValues::Unlimited => arg.num_args(0..),
            };
        }
        Ok(arg
            // let's allow values that start with a hyphen for args (for example, --calculator -2+2)
            .allow_hyphen_values(true)
            // don't enforce that args are required
            .required(false))
    }
}

impl ExtensionSubcommandOpts {
    pub fn as_clap_command(
        &self,
        name: &str,
    ) -> Result<clap::Command, ConvertExtensionSubcommandIntoClapCommandError> {
        let mut cmd = clap::Command::new(name.to_string());

        if let Some(about) = &self.about {
            cmd = cmd.about(about);
        }

        if let Some(args) = &self.args {
            for (name, opts) in args {
                cmd = cmd.arg(opts.as_clap_arg(name)?);
            }
        }

        if let Some(subcommands) = &self.subcommands {
            for (name, subcommand) in &subcommands.0 {
                cmd = cmd.subcommand(subcommand.as_clap_command(name)?);
            }
        }

        Ok(cmd)
    }
}

#[test]
fn parse_test_file() {
    let f = r#"
{
  "name": "sns",
  "version": "0.1.0",
  "homepage": "https://github.com/dfinity/dfx-extensions",
  "authors": "DFINITY",
  "summary": "Toolkit for simulating decentralizing a dapp via SNS.",
  "categories": [
    "sns",
    "nns"
  ],
  "dependencies": {
    "dfx": ">=0.8, <0.9"
  },
  "keywords": [
    "sns",
    "nns",
    "deployment"
  ],
  "subcommands": {
    "config": {
      "about": "About for config command. You're looking at the output of parsing test extension.json.",
      "subcommands": {
        "create": {
          "about": "Command line options for creating an SNS configuration."
        },
        "validate": {
          "about": "Command line options for validating an SNS configuration."
        }
      }
    },
    "deploy": {
      "about": "About for deploy command. You're looking at the output of parsing test extension.json."
    },
    "import": {
      "about": "About for import command. You're looking at the output of parsing test extension.json.",
      "args": {
        "network_mapping": {
          "about": "Networks to import canisters ids for.\n  --network-mapping <network name in both places>\n  --network-mapping <network name here>=<network name in project being imported>\nExamples:\n  --network-mapping ic\n  --network-mapping ic=mainnet",
          "long": "network-mapping"
        }
      }
    },
    "download": {
      "about": "About for download command. You're looking at the output of parsing test extension.json.",
      "args": {
        "ic_commit": {
          "about": "IC commit of SNS canister Wasm binaries to download",
          "long": "ic-commit"
        },
        "wasms_dir": {
          "about": "Path to store downloaded SNS canister Wasm binaries",
          "long": "wasms-dir"
        }
      }
    },
    "install": {
      "about": "About for install command. You're looking at the output of parsing test extension.json.",
      "args": {
        "account": {
          "about": "some arg that accepts multiple values separated by spaces",
          "long": "account"
        },
        "accounts": {
          "about": "some arg that accepts multiple values separated by spaces",
          "long": "accounts",
          "multiple": true
        },
        "two-accounts": {
          "about": "some arg that accepts multiple values separated by spaces",
          "long": "two-accounts",
          "values": 2
        },
        "two-or-three-accounts": {
          "about": "some arg that accepts multiple values separated by spaces",
          "long": "two-or-three-accounts",
          "values": "2..3"
        }
      }
    },
    "init-canister": {
      "about": "About for init-canister command. You're looking at the output of parsing test extension.json.",
      "args": {
        "canister_id": {
          "about": "some arg that accepts multiple values separated by spaces"
        }
      }
    },
    "init-canisters": {
      "about": "About for init-canisters command. You're looking at the output of parsing test extension.json.",
      "args": {
        "canister_ids": {
          "about": "some arg that accepts multiple values separated by spaces",
          "values": "unlimited"
        }
      }
    },
    "init-two-canisters": {
      "about": "About for init-two-canisters command. You're looking at the output of parsing test extension.json.",
      "args": {
        "canister_ids": {
          "about": "some arg that accepts multiple values separated by spaces",
          "values": 2
        }
      }
    },
    "init-two-or-three-canisters": {
      "about": "About for init-two-or-three-canisters command. You're looking at the output of parsing test extension.json.",
      "args": {
        "canister_ids": {
          "about": "some arg that accepts multiple values separated by spaces",
          "values": "2..3"
        }
      }
    }
  }
}
"#;
    macro_rules! test_cmd {
        ($cmd:expr, [$($cmds:expr),*], $arg_name:expr => [$($expected:expr),*]) => {{
            let commands = vec![$($cmds),*];
            let expected_values: Vec<&str> = vec![$($expected),*];
            let matches = $cmd.clone().get_matches_from(commands);
            let output = matches
                .get_many::<String>(&$arg_name)
                .unwrap()
                .map(|s| s.as_str())
                .collect::<Vec<&str>>();
            assert_eq!(expected_values, output, "Arg: {}", $arg_name);
        }};
        ($cmd:expr, [$($cmds:expr),*], $err_kind:expr) => {{
            let commands = vec![$($cmds),*];
            let matches = dbg!($cmd.clone().try_get_matches_from(commands));
            assert_eq!(matches.as_ref().map_err(|e| e.kind()), Err($err_kind));
        }};
    }

    let m: Result<ExtensionManifest, serde_json::Error> = dbg!(serde_json::from_str(f));
    assert!(m.is_ok());
    let manifest = m.unwrap();

    let dependencies = manifest.dependencies.as_ref().unwrap();
    let dfx_dep = dependencies.get("dfx").unwrap();
    let ExtensionDependency::Version(req) = dfx_dep;
    assert!(req.matches(&semver::Version::new(0, 8, 5)));
    assert!(!req.matches(&semver::Version::new(0, 9, 0)));

    let mut subcmds = dbg!(manifest.into_clap_commands().unwrap());

    use clap::error::ErrorKind::*;
    for c in &mut subcmds {
        c.print_long_help().unwrap();
        match c.get_name() {
            subcmd @ "download" => {
                test_cmd!(c, [subcmd, "--ic-commit", "C"], "ic_commit" => ["C"]);
                test_cmd!(c, [subcmd, "--ic-commit", "c1", "c2"], UnknownArgument);
                test_cmd!(c, [subcmd, "--dosent-extist", "c1", "c2"], UnknownArgument);
            }
            #[rustfmt::skip]
            subcmd @ "install" => {
                test_cmd!(c, [subcmd, "--account", "A"], "account" => ["A"]);
                test_cmd!(c, [subcmd, "--account", "A", "B"], UnknownArgument);
                test_cmd!(c, [subcmd, "--accounts"], "accounts" => []);
                test_cmd!(c, [subcmd, "--accounts", "A", "B"], "accounts" => ["A", "B"]);
                test_cmd!(c, [subcmd, "--two-accounts", "A"], WrongNumberOfValues);
                test_cmd!(c, [subcmd, "--two-accounts", "A", "B"], "two-accounts" => ["A", "B"]);
                test_cmd!(c, [subcmd, "--two-accounts", "A", "B", "C"], UnknownArgument);
                test_cmd!(c, [subcmd, "--two-or-three-accounts", "A"], TooFewValues);
                test_cmd!(c, [subcmd, "--two-or-three-accounts", "A", "B"], "two-or-three-accounts" => ["A", "B"]);
                test_cmd!(c, [subcmd, "--two-or-three-accounts", "A", "B", "C"], "two-or-three-accounts" => ["A", "B", "C"]);
                test_cmd!(c, [subcmd, "--two-or-three-accounts", "A", "B", "C", "D"], UnknownArgument);
            }
            subcmd @ "init-canister" => {
                test_cmd!(c, [subcmd, "x1"], "canister_id" => ["x1"]);
                test_cmd!(c, [subcmd, "x1", "x2"], UnknownArgument);
            }
            subcmd @ "init-canisters" => {
                test_cmd!(c, [subcmd, "y1", "y2", "y3", "y4", "y5"], "canister_ids" => ["y1", "y2", "y3", "y4", "y5"]);
            }
            subcmd @ "init-two-canisters" => {
                test_cmd!(c, [subcmd, "z1"], WrongNumberOfValues);
                test_cmd!(c, [subcmd, "z1", "z2"], "canister_ids" => ["z1", "z2"]);
            }
            subcmd @ "init-two-or-three-canisters" => {
                test_cmd!(c, [subcmd, "1"], TooFewValues);
                test_cmd!(c, [subcmd, "1", "2"], "canister_ids" => ["1", "2"]);
                test_cmd!(c, [subcmd, "1", "2", "3"], "canister_ids" => ["1", "2", "3"]);
                test_cmd!(c, [subcmd, "1", "2", "3", "4"], TooManyValues);
            }
            _ => {}
        }
    }
    clap::Command::new("sns")
        .subcommands(&subcmds)
        .print_help()
        .unwrap();
    clap::Command::new("sns")
        .subcommands(&subcmds)
        .debug_assert();
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json;

    #[test]
    fn test_arg_number_of_values_number_serialization_deserialization() {
        let original = ArgNumberOfValues::Number(5);
        let serialized = serde_json::to_string(&original).unwrap();
        let deserialized: ArgNumberOfValues = serde_json::from_str(&serialized).unwrap();

        assert_eq!(serialized, "5");
        assert_eq!(deserialized, ArgNumberOfValues::Number(5));
        assert_eq!(original, deserialized);
    }

    #[test]
    fn test_arg_number_of_values_unlimited_serialization_deserialization() {
        let original = ArgNumberOfValues::Unlimited;
        let serialized = serde_json::to_string(&original).unwrap();
        let deserialized: ArgNumberOfValues = serde_json::from_str(&serialized).unwrap();

        assert_eq!(serialized, "\"unlimited\"");
        assert_eq!(deserialized, ArgNumberOfValues::Unlimited);
        assert_eq!(original, deserialized);
    }

    #[test]
    fn test_arg_number_of_values_range_serialization_deserialization() {
        let original = ArgNumberOfValues::Range(1..4);
        let serialized = serde_json::to_string(&original).unwrap();
        let deserialized: ArgNumberOfValues = serde_json::from_str(&serialized).unwrap();

        assert_eq!(serialized, "\"1..3\"");
        assert_eq!(deserialized, ArgNumberOfValues::Range(1_usize..4_usize));
        assert_eq!(original, deserialized);
    }
}


-----------------------

/src/dfx-core/src/extension/manifest/mod.rs:
-----------------------

//! Directory contains code that parses the .json files.

pub mod dependencies;
pub mod extension;

/// A file that lists the dependencies of all versions of an extension.
pub use dependencies::ExtensionDependencies;

/// A file that describes an extension.
pub use extension::ExtensionManifest;


-----------------------

/src/dfx-core/src/extension/mod.rs:
-----------------------

pub mod catalog;
pub mod installed;
pub mod manager;
pub mod manifest;
pub mod url;

type ExtensionName = String;


-----------------------

/src/dfx-core/src/extension/url.rs:
-----------------------

use url::Url;

pub struct ExtensionJsonUrl(Url);

impl ExtensionJsonUrl {
    pub fn new(url: Url) -> Self {
        ExtensionJsonUrl(url)
    }

    pub fn registered(name: &str) -> Result<Self, url::ParseError> {
        let s = format!(
            "https://raw.githubusercontent.com/dfinity/dfx-extensions/main/extensions/{name}/extension.json"
        );
        Url::parse(&s).map(ExtensionJsonUrl)
    }

    pub fn to_dependencies_json(&self) -> Result<Url, url::ParseError> {
        self.as_url().join("dependencies.json")
    }

    pub fn as_url(&self) -> &Url {
        &self.0
    }
}


-----------------------

/src/dfx-core/src/foundation/mod.rs:
-----------------------

use crate::error::get_current_exe::GetCurrentExeError;
use crate::error::get_current_exe::GetCurrentExeError::NoCurrentExe;
use crate::error::get_user_home::GetUserHomeError;
use crate::error::get_user_home::GetUserHomeError::NoHomeInEnvironment;
use std::ffi::OsString;
use std::path::PathBuf;

pub fn get_user_home() -> Result<OsString, GetUserHomeError> {
    std::env::var_os("HOME").ok_or(NoHomeInEnvironment())
}

pub fn get_current_exe() -> Result<PathBuf, GetCurrentExeError> {
    std::env::current_exe().map_err(NoCurrentExe)
}


-----------------------

/src/dfx-core/src/fs/composite.rs:
-----------------------

use crate::error::fs::EnsureDirExistsError::NotADirectory;
use crate::error::fs::{EnsureDirExistsError, EnsureParentDirExistsError};
use std::path::Path;

pub fn ensure_dir_exists(p: &Path) -> Result<(), EnsureDirExistsError> {
    if !p.exists() {
        crate::fs::create_dir_all(p)?;
        Ok(())
    } else if !p.is_dir() {
        Err(NotADirectory(p.to_path_buf()))
    } else {
        Ok(())
    }
}

pub fn ensure_parent_dir_exists(d: &Path) -> Result<(), EnsureParentDirExistsError> {
    let parent = crate::fs::parent(d)?;
    ensure_dir_exists(&parent)?;
    Ok(())
}


-----------------------

/src/dfx-core/src/fs/mod.rs:
-----------------------

pub mod composite;
use crate::error::archive::GetArchivePathError;
use crate::error::fs::{
    CanonicalizePathError, CopyFileError, CreateDirAllError, NoParentPathError, ReadDirError,
    ReadFileError, ReadMetadataError, ReadPermissionsError, ReadToStringError,
    RemoveDirectoryAndContentsError, RemoveDirectoryError, RemoveFileError, RenameError,
    SetPermissionsError, SetPermissionsReadWriteError, UnpackingArchiveError, WriteFileError,
};
use std::fs::{Metadata, Permissions, ReadDir};
use std::path::{Path, PathBuf};

pub fn canonicalize(path: &Path) -> Result<PathBuf, CanonicalizePathError> {
    dunce::canonicalize(path).map_err(|source| CanonicalizePathError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn copy(from: &Path, to: &Path) -> Result<u64, CopyFileError> {
    std::fs::copy(from, to).map_err(|source| CopyFileError {
        from: from.to_path_buf(),
        to: to.to_path_buf(),
        source,
    })
}

pub fn create_dir_all(path: &Path) -> Result<(), CreateDirAllError> {
    std::fs::create_dir_all(path).map_err(|source| CreateDirAllError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn get_archive_path(
    archive: &tar::Entry<flate2::read::GzDecoder<&'static [u8]>>,
) -> Result<PathBuf, GetArchivePathError> {
    let path = archive.path().map_err(GetArchivePathError)?;
    Ok(path.to_path_buf())
}

pub fn metadata(path: &Path) -> Result<Metadata, ReadMetadataError> {
    std::fs::metadata(path).map_err(|source| ReadMetadataError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn parent(path: &Path) -> Result<PathBuf, NoParentPathError> {
    match path.parent() {
        None => Err(NoParentPathError(path.to_path_buf())),
        Some(parent) => Ok(parent.to_path_buf()),
    }
}

pub fn read(path: &Path) -> Result<Vec<u8>, ReadFileError> {
    std::fs::read(path).map_err(|source| ReadFileError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn read_to_string(path: &Path) -> Result<String, ReadToStringError> {
    std::fs::read_to_string(path).map_err(|source| ReadToStringError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn read_dir(path: &Path) -> Result<ReadDir, ReadDirError> {
    path.read_dir().map_err(|source| ReadDirError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn rename(from: &Path, to: &Path) -> Result<(), RenameError> {
    std::fs::rename(from, to).map_err(|source| RenameError {
        from: from.to_path_buf(),
        to: to.to_path_buf(),
        source,
    })
}

pub fn read_permissions(path: &Path) -> Result<Permissions, ReadPermissionsError> {
    std::fs::metadata(path)
        .map_err(|source| ReadPermissionsError {
            path: path.to_path_buf(),
            source,
        })
        .map(|x| x.permissions())
}

pub fn remove_dir(path: &Path) -> Result<(), RemoveDirectoryError> {
    std::fs::remove_dir(path).map_err(|source| RemoveDirectoryError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn remove_dir_all(path: &Path) -> Result<(), RemoveDirectoryAndContentsError> {
    std::fs::remove_dir_all(path).map_err(|source| RemoveDirectoryAndContentsError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn remove_file(path: &Path) -> Result<(), RemoveFileError> {
    std::fs::remove_file(path).map_err(|source| RemoveFileError {
        path: path.to_path_buf(),
        source,
    })
}

pub fn set_permissions(path: &Path, permissions: Permissions) -> Result<(), SetPermissionsError> {
    std::fs::set_permissions(path, permissions).map_err(|source| SetPermissionsError {
        path: path.to_path_buf(),
        source,
    })
}

#[cfg_attr(not(unix), allow(unused_variables))]
pub fn set_permissions_readwrite(path: &Path) -> Result<(), SetPermissionsReadWriteError> {
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        let mut permissions = read_permissions(path)?;
        permissions.set_mode(permissions.mode() | 0o600);
        set_permissions(path, permissions)?;
    }
    Ok(())
}

pub fn tar_unpack_in<P: AsRef<Path>>(
    path: P,
    tar: &mut tar::Entry<flate2::read::GzDecoder<&'static [u8]>>,
) -> Result<(), UnpackingArchiveError> {
    tar.unpack_in(&path)
        .map_err(|source| UnpackingArchiveError {
            path: path.as_ref().to_path_buf(),
            source,
        })?;
    Ok(())
}

pub fn write<P: AsRef<Path>, C: AsRef<[u8]>>(path: P, contents: C) -> Result<(), WriteFileError> {
    std::fs::write(path.as_ref(), contents).map_err(|source| WriteFileError {
        path: path.as_ref().to_path_buf(),
        source,
    })
}


-----------------------

/src/dfx-core/src/http/get.rs:
-----------------------

use backoff::exponential::ExponentialBackoff;
use backoff::future::retry;
use backoff::SystemClock;
use reqwest::Response;
use url::Url;

pub async fn get_with_retries(
    url: Url,
    retry_policy: ExponentialBackoff<SystemClock>,
) -> Result<Response, reqwest::Error> {
    let operation = || async {
        let response = reqwest::get(url.clone())
            .await
            .and_then(|resp| resp.error_for_status());
        match response {
            Ok(doc) => Ok(doc),
            Err(e) if crate::error::reqwest::is_retryable(&e) => Err(backoff::Error::transient(e)),
            Err(e) => Err(backoff::Error::permanent(e)),
        }
    };
    retry(retry_policy, operation).await
}


-----------------------

/src/dfx-core/src/http/mod.rs:
-----------------------

pub mod get;


-----------------------

/src/dfx-core/src/identity/identity_file_locations.rs:
-----------------------

use crate::identity::IdentityConfiguration;
use std::path::{Path, PathBuf};

pub const IDENTITY_PEM: &str = "identity.pem";
pub const IDENTITY_PEM_ENCRYPTED: &str = "identity.pem.encrypted";

#[derive(Clone, Debug)]
pub(crate) struct IdentityFileLocations {
    root_dir: PathBuf,
}

impl IdentityFileLocations {
    pub fn new(root_dir: PathBuf) -> Self {
        Self { root_dir }
    }

    pub fn root(&self) -> &Path {
        &self.root_dir
    }

    /// Determines the path of the (potentially encrypted) PEM file.
    pub fn get_identity_pem_path(
        &self,
        identity_name: &str,
        identity_config: &IdentityConfiguration,
    ) -> PathBuf {
        if identity_config.encryption.is_some() {
            self.get_encrypted_identity_pem_path(identity_name)
        } else {
            self.get_plaintext_identity_pem_path(identity_name)
        }
    }

    /// Determines the path of the clear-text PEM file.
    pub fn get_plaintext_identity_pem_path(&self, identity_name: &str) -> PathBuf {
        self.get_identity_dir_path(identity_name).join(IDENTITY_PEM)
    }

    /// Determines the path of the encrypted PEM file.
    pub fn get_encrypted_identity_pem_path(&self, identity_name: &str) -> PathBuf {
        self.get_identity_dir_path(identity_name)
            .join(IDENTITY_PEM_ENCRYPTED)
    }

    pub fn get_identity_dir_path(&self, identity: &str) -> PathBuf {
        self.root_dir.join(identity)
    }
}


-----------------------

/src/dfx-core/src/identity/identity_manager.rs:
-----------------------

use super::pem_utils::validate_pem_file;
use super::{keyring_mock, WALLET_CONFIG_FILENAME};
use crate::config::directories::get_user_dfx_config_dir;
use crate::error::encryption::EncryptionError;
use crate::error::encryption::EncryptionError::{NonceGenerationFailed, SaltGenerationFailed};
use crate::error::fs::ReadDirError;
use crate::error::identity::{
    ConvertMnemonicToKeyError,
    ConvertMnemonicToKeyError::DeriveExtendedKeyFromPathFailed,
    CreateIdentityConfigError,
    CreateIdentityConfigError::GenerateFreshEncryptionConfigurationFailed,
    CreateNewIdentityError,
    CreateNewIdentityError::{
        ConvertSecretKeyToSec1PemFailed, CreateMnemonicFromPhraseFailed,
        CreateTemporaryIdentityDirectoryFailed, SwitchBackToIdentityFailed,
        SwitchToAnonymousIdentityFailed,
    },
    ExportIdentityError,
    ExportIdentityError::TranslatePemContentToTextFailed,
    GenerateKeyError,
    GenerateKeyError::GenerateFreshSecp256k1KeyFailed,
    GetIdentityConfigOrDefaultError,
    GetIdentityConfigOrDefaultError::LoadIdentityConfigurationFailed,
    GetLegacyCredentialsPemPathError,
    GetLegacyCredentialsPemPathError::GetLegacyPemPathFailed,
    InitializeIdentityManagerError,
    InitializeIdentityManagerError::{
        CreateIdentityDirectoryFailed, GenerateKeyFailed, MigrateLegacyIdentityFailed,
        WritePemToFileFailed,
    },
    InstantiateIdentityFromNameError,
    InstantiateIdentityFromNameError::{GetIdentityPrincipalFailed, LoadIdentityFailed},
    LoadIdentityError, NewIdentityManagerError,
    NewIdentityManagerError::LoadIdentityManagerConfigurationFailed,
    RemoveIdentityError,
    RemoveIdentityError::{
        DisplayLinkedWalletsFailed, DropWalletsFlagRequiredToRemoveIdentityWithWallets,
        RemoveIdentityDirectoryFailed,
    },
    RenameIdentityError,
    RenameIdentityError::{
        GetIdentityConfigFailed, LoadPemFailed, MapWalletsToRenamedIdentityFailed, SavePemFailed,
        SwitchDefaultIdentitySettingsFailed,
    },
    RequireIdentityExistsError, SaveIdentityConfigurationError,
    SaveIdentityConfigurationError::EnsureIdentityConfigurationDirExistsFailed,
    UseIdentityByNameError,
    UseIdentityByNameError::WriteDefaultIdentityFailed,
    WriteDefaultIdentityError,
    WriteDefaultIdentityError::SaveIdentityManagerConfigurationFailed,
};
use crate::error::structured_file::StructuredFileError;
use crate::foundation::get_user_home;
use crate::fs::composite::ensure_parent_dir_exists;
use crate::identity::identity_file_locations::{IdentityFileLocations, IDENTITY_PEM};
use crate::identity::identity_manager::IdentityStorageModeError::UnknownStorageMode;
use crate::identity::{
    pem_safekeeping, pem_utils, Identity as DfxIdentity, ANONYMOUS_IDENTITY_NAME, IDENTITY_JSON,
    TEMP_IDENTITY_PREFIX,
};
use crate::json::{load_json_file, save_json_file};
use bip32::XPrv;
use bip39::{Language, Mnemonic, MnemonicType, Seed};
use candid::Principal;
use k256::pkcs8::LineEnding;
use k256::SecretKey;
use ring::{rand, rand::SecureRandom};
use sec1::EncodeEcPrivateKey;
use serde::{Deserialize, Serialize};
use slog::{debug, trace, Logger};
use std::boxed::Box;
use std::collections::BTreeMap;
use std::path::{Path, PathBuf};
use std::str::FromStr;
use thiserror::Error;

const DEFAULT_IDENTITY_NAME: &str = "default";

#[derive(Clone, Debug, Default, Serialize, Deserialize)]
struct Configuration {
    #[serde(default = "default_identity")]
    pub default: String,
}

fn default_identity() -> String {
    String::from(DEFAULT_IDENTITY_NAME)
}

#[derive(Clone, Debug, Default, Serialize, Deserialize)]
pub struct IdentityConfiguration {
    pub hsm: Option<HardwareIdentityConfiguration>,

    /// If the identity's PEM file is encrypted on disk this contains everything (except the password) to decrypt the file.
    pub encryption: Option<EncryptionConfiguration>,

    /// If the identity's PEM file is stored in the system's keyring, this field contains the identity's name WITHOUT the common prefix.
    pub keyring_identity_suffix: Option<String>,
}

/// The information necessary to de- and encrypt (except the password) the identity's .pem file
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct EncryptionConfiguration {
    /// Salt used for deriving the key from the password
    pub pw_salt: String,

    /// 96 bit Nonce used to decrypt the file
    pub file_nonce: Vec<u8>,
}

impl EncryptionConfiguration {
    /// Generates a random salt and nonce. Use this for every new identity
    pub fn new() -> Result<Self, EncryptionError> {
        let mut nonce: [u8; 12] = [0; 12];
        let mut salt: [u8; 32] = [0; 32];
        let sr = rand::SystemRandom::new();
        sr.fill(&mut nonce).map_err(NonceGenerationFailed)?;
        sr.fill(&mut salt).map_err(SaltGenerationFailed)?;

        let pw_salt = hex::encode(salt);
        let file_nonce = nonce.into();

        Ok(Self {
            pw_salt,
            file_nonce,
        })
    }
}

#[derive(Clone, Debug, Default, Serialize, Deserialize)]
pub struct HardwareIdentityConfiguration {
    #[cfg_attr(
        not(windows),
        doc = r#"The file path to the opensc-pkcs11 library e.g. "/usr/local/lib/opensc-pkcs11.so""#
    )]
    #[cfg_attr(
        windows,
        doc = r#"The file path to the opensc-pkcs11 library e.g. "C:\Program Files (x86)\OpenSC Project\OpenSC\pkcs11\opensc-pkcs11.dll"#
    )]
    pub pkcs11_lib_path: String,

    /// A sequence of pairs of hex digits
    pub key_id: String,
}

#[derive(Clone, Debug, Serialize, Deserialize, Copy, PartialEq, Eq)]
pub enum IdentityStorageMode {
    Keyring,
    PasswordProtected,
    Plaintext,
}

#[derive(Error, Debug)]
pub enum IdentityStorageModeError {
    #[error("Unknown storage mode: {0}")]
    UnknownStorageMode(String),
}

impl FromStr for IdentityStorageMode {
    type Err = IdentityStorageModeError;

    fn from_str(input: &str) -> Result<Self, Self::Err> {
        match input {
            "keyring" => Ok(IdentityStorageMode::Keyring),
            "password-protected" => Ok(IdentityStorageMode::PasswordProtected),
            "plaintext" => Ok(IdentityStorageMode::Plaintext),
            other => Err(UnknownStorageMode(other.to_string())),
        }
    }
}

impl Default for IdentityStorageMode {
    fn default() -> Self {
        Self::Keyring
    }
}

pub enum IdentityCreationParameters {
    Pem {
        mode: IdentityStorageMode,
    },
    PemFile {
        src_pem_file: PathBuf,
        mode: IdentityStorageMode,
    },
    SeedPhrase {
        mnemonic: String,
        mode: IdentityStorageMode,
    },
    Hardware {
        hsm: HardwareIdentityConfiguration,
    },
}

#[derive(Clone, Debug)]
pub struct IdentityManager {
    identity_json_path: PathBuf,
    file_locations: IdentityFileLocations,
    configuration: Configuration,
    selected_identity: String,
    selected_identity_principal: Option<Principal>,
}

#[derive(PartialEq)]
pub enum InitializeIdentity {
    Allow,
    Disallow,
}

impl IdentityManager {
    pub fn new(
        logger: &Logger,
        identity_override: Option<&str>,
        initialize_identity: InitializeIdentity,
    ) -> Result<Self, NewIdentityManagerError> {
        let config_dfx_dir_path =
            get_user_dfx_config_dir().map_err(NewIdentityManagerError::GetConfigDirectoryFailed)?;
        let identity_root_path = config_dfx_dir_path.join("identity");
        let identity_json_path = config_dfx_dir_path.join("identity.json");

        let configuration = if identity_json_path.exists() {
            load_configuration(&identity_json_path)
                .map_err(LoadIdentityManagerConfigurationFailed)?
        } else if initialize_identity == InitializeIdentity::Disallow {
            return Err(NewIdentityManagerError::NoIdentityConfigurationFound);
        } else {
            initialize(logger, &identity_json_path, &identity_root_path)
                .map_err(NewIdentityManagerError::InitializeFailed)?
        };

        let selected_identity = identity_override
            .unwrap_or(&configuration.default)
            .to_string();

        let file_locations = IdentityFileLocations::new(identity_root_path);

        let mgr = IdentityManager {
            identity_json_path,
            file_locations,
            configuration,
            selected_identity,
            selected_identity_principal: None,
        };

        if let Some(identity) = identity_override {
            mgr.require_identity_exists(logger, identity)
                .map_err(NewIdentityManagerError::OverrideIdentityMustExist)?;
        }

        Ok(mgr)
    }

    pub fn get_selected_identity_principal(&self) -> Option<Principal> {
        self.selected_identity_principal
    }

    /// Create an Identity instance for use with an Agent
    pub fn instantiate_selected_identity(
        &mut self,
        log: &Logger,
    ) -> Result<Box<DfxIdentity>, InstantiateIdentityFromNameError> {
        let name = self.selected_identity.clone();
        self.instantiate_identity_from_name(name.as_str(), log)
    }

    /// Provide a valid Identity name and create its Identity instance for use with an Agent
    pub fn instantiate_identity_from_name(
        &mut self,
        identity_name: &str,
        log: &Logger,
    ) -> Result<Box<DfxIdentity>, InstantiateIdentityFromNameError> {
        let identity = match identity_name {
            ANONYMOUS_IDENTITY_NAME => Box::new(DfxIdentity::anonymous()),
            identity_name => {
                self.require_identity_exists(log, identity_name)
                    .map_err(InstantiateIdentityFromNameError::RequireIdentityExistsFailed)?;
                Box::new(
                    self.load_identity(identity_name, log)
                        .map_err(LoadIdentityFailed)?,
                )
            }
        };
        use ic_agent::identity::Identity;
        self.selected_identity_principal =
            Some(identity.sender().map_err(GetIdentityPrincipalFailed)?);
        Ok(identity)
    }

    fn load_identity(&self, name: &str, log: &Logger) -> Result<DfxIdentity, LoadIdentityError> {
        let config = self
            .get_identity_config_or_default(name)
            .map_err(LoadIdentityError::GetIdentityConfigOrDefaultFailed)?;
        DfxIdentity::new(name, config, self.file_locations(), log)
            .map_err(LoadIdentityError::NewIdentityFailed)
    }

    /// Create a new identity (name -> generated key)
    ///
    /// `force`: If the identity already exists, remove and re-create it.
    pub fn create_new_identity(
        &mut self,
        log: &Logger,
        name: &str,
        parameters: IdentityCreationParameters,
        force: bool,
    ) -> Result<(), CreateNewIdentityError> {
        if name == ANONYMOUS_IDENTITY_NAME {
            return Err(CreateNewIdentityError::CannotCreateAnonymousIdentity());
        }

        trace!(log, "Creating identity '{name}'.");
        let identity_in_use = self.get_selected_identity_name().clone();
        // cannot delete an identity in use. Use anonymous identity temporarily if we force-overwrite the identity currently in use
        let temporarily_use_anonymous_identity = identity_in_use == name && force;

        if self.require_identity_exists(log, name).is_ok() {
            trace!(log, "Identity already exists.");
            if force {
                if temporarily_use_anonymous_identity {
                    self.use_identity_named(log, ANONYMOUS_IDENTITY_NAME)
                        .map_err(SwitchToAnonymousIdentityFailed)?;
                }
                self.remove(log, name, true, None)
                    .map_err(CreateNewIdentityError::RemoveIdentityFailed)?;
            } else {
                return Err(CreateNewIdentityError::IdentityAlreadyExists());
            }
        }

        fn create_identity_config(
            log: &Logger,
            mode: IdentityStorageMode,
            name: &str,
            hardware_config: Option<HardwareIdentityConfiguration>,
        ) -> Result<IdentityConfiguration, CreateIdentityConfigError> {
            if let Some(hsm) = hardware_config {
                Ok(IdentityConfiguration {
                    hsm: Some(hsm),
                    ..Default::default()
                })
            } else {
                match mode {
                    IdentityStorageMode::Keyring => {
                        if keyring_mock::keyring_available(log) {
                            Ok(IdentityConfiguration {
                                keyring_identity_suffix: Some(String::from(name)),
                                ..Default::default()
                            })
                        } else {
                            Ok(IdentityConfiguration {
                                encryption: Some(
                                    EncryptionConfiguration::new()
                                        .map_err(GenerateFreshEncryptionConfigurationFailed)?,
                                ),
                                ..Default::default()
                            })
                        }
                    }
                    IdentityStorageMode::PasswordProtected => Ok(IdentityConfiguration {
                        encryption: Some(
                            EncryptionConfiguration::new()
                                .map_err(GenerateFreshEncryptionConfigurationFailed)?,
                        ),
                        ..Default::default()
                    }),
                    IdentityStorageMode::Plaintext => Ok(IdentityConfiguration::default()),
                }
            }
        }

        // Use a temporary directory to prepare all identity parts in so that we don't end up with broken parts if the
        // creation process fails half-way through.
        let temp_identity_name = format!("{}{}", TEMP_IDENTITY_PREFIX, name);
        let temp_identity_dir = self.get_identity_dir_path(&temp_identity_name);
        if temp_identity_dir.exists() {
            // clean traces from previous identity creation attempts
            crate::fs::remove_dir_all(&temp_identity_dir)?;
        }

        let identity_config;
        match parameters {
            IdentityCreationParameters::Pem { mode } => {
                let (pem_content, mnemonic) =
                    generate_key().map_err(CreateNewIdentityError::GenerateKeyFailed)?;
                identity_config = create_identity_config(log, mode, name, None)
                    .map_err(CreateNewIdentityError::CreateIdentityConfigFailed)?;
                pem_safekeeping::save_pem(
                    log,
                    self.file_locations(),
                    &temp_identity_name,
                    &identity_config,
                    pem_content.as_slice(),
                )
                .map_err(CreateNewIdentityError::SavePemFailed)?;
                eprintln!("Your seed phrase for identity '{name}': {}\nThis can be used to reconstruct your key in case of emergency, so write it down in a safe place.", mnemonic.phrase());
            }
            IdentityCreationParameters::PemFile { src_pem_file, mode } => {
                identity_config = create_identity_config(log, mode, name, None)
                    .map_err(CreateNewIdentityError::CreateIdentityConfigFailed)?;
                let (src_pem_content, _) = pem_safekeeping::load_pem_from_file(&src_pem_file, None)
                    .map_err(CreateNewIdentityError::LoadPemFromFileFailed)?;
                pem_utils::validate_pem_file(&src_pem_content)
                    .map_err(CreateNewIdentityError::ValidatePemFileFailed)?;
                pem_safekeeping::save_pem(
                    log,
                    self.file_locations(),
                    &temp_identity_name,
                    &identity_config,
                    src_pem_content.as_slice(),
                )
                .map_err(CreateNewIdentityError::SavePemFailed)?;
            }
            IdentityCreationParameters::Hardware { hsm } => {
                identity_config =
                    create_identity_config(log, IdentityStorageMode::default(), name, Some(hsm))
                        .map_err(CreateNewIdentityError::CreateIdentityConfigFailed)?;
                crate::fs::create_dir_all(&temp_identity_dir)
                    .map_err(CreateTemporaryIdentityDirectoryFailed)?;
            }
            IdentityCreationParameters::SeedPhrase { mnemonic, mode } => {
                identity_config = create_identity_config(log, mode, name, None)
                    .map_err(CreateNewIdentityError::CreateIdentityConfigFailed)?;
                let mnemonic = Mnemonic::from_phrase(&mnemonic, Language::English)
                    .map_err(|e| CreateMnemonicFromPhraseFailed(format!("{}", e)))?;
                let key = mnemonic_to_key(&mnemonic)
                    .map_err(CreateNewIdentityError::ConvertMnemonicToKeyFailed)?;
                let pem = key
                    .to_sec1_pem(k256::pkcs8::LineEnding::CRLF)
                    .map_err(|e| ConvertSecretKeyToSec1PemFailed(Box::new(e)))?;
                let pem_content = pem.as_bytes();
                pem_safekeeping::save_pem(
                    log,
                    self.file_locations(),
                    &temp_identity_name,
                    &identity_config,
                    pem_content,
                )
                .map_err(CreateNewIdentityError::SavePemFailed)?;
            }
        }
        let identity_config_location = self.get_identity_json_path(&temp_identity_name);
        save_identity_configuration(log, &identity_config_location, &identity_config)
            .map_err(CreateNewIdentityError::SaveIdentityConfigurationFailed)?;

        // Everything is created. Now move from the temporary directory to the actual identity location.
        let identity_dir = self.get_identity_dir_path(name);
        crate::fs::rename(&temp_identity_dir, &identity_dir)?;

        if temporarily_use_anonymous_identity {
            self.use_identity_named(log, &identity_in_use)
                .map_err(SwitchBackToIdentityFailed)?;
        }
        Ok(())
    }

    /// Return a sorted list of all available identity names
    pub fn get_identity_names(&self, log: &Logger) -> Result<Vec<String>, ReadDirError> {
        let mut names = crate::fs::read_dir(self.file_locations.root())?
            .filter_map(|entry_result| match entry_result {
                Ok(dir_entry) => match dir_entry.file_type() {
                    Ok(file_type) if file_type.is_dir() => Some(dir_entry),
                    _ => None,
                },
                _ => None,
            })
            .map(|entry| entry.file_name().to_string_lossy().to_string())
            .filter(|identity_name| self.require_identity_exists(log, identity_name).is_ok())
            .collect::<Vec<_>>();
        names.push(ANONYMOUS_IDENTITY_NAME.to_string());

        names.sort();

        Ok(names)
    }

    /// Return the name of the currently selected (active) identity
    pub fn get_selected_identity_name(&self) -> &String {
        &self.selected_identity
    }

    pub(crate) fn file_locations(&self) -> &IdentityFileLocations {
        &self.file_locations
    }

    /// Returns the pem file content of the selected identity
    pub fn export(&self, log: &Logger, name: &str) -> Result<String, ExportIdentityError> {
        self.require_identity_exists(log, name)
            .map_err(ExportIdentityError::IdentityDoesNotExist)?;
        let config = self
            .get_identity_config_or_default(name)
            .map_err(ExportIdentityError::GetIdentityConfigFailed)?;
        let (pem_content, _) = pem_safekeeping::load_pem(log, &self.file_locations, name, &config)
            .map_err(ExportIdentityError::LoadPemFailed)?;

        validate_pem_file(&pem_content).map_err(ExportIdentityError::ValidatePemFileFailed)?;
        String::from_utf8(pem_content).map_err(TranslatePemContentToTextFailed)
    }

    /// Remove a named identity.
    /// Removing the selected identity is not allowed.
    /// Removing an identity that is connected to non-ephemeral wallets is only allowed if drop_wallets is true.
    /// If display_linked_wallets_to contains a logger, this will log all the wallets the identity is connected to.
    pub fn remove(
        &self,
        log: &Logger,
        name: &str,
        drop_wallets: bool,
        display_linked_wallets_to: Option<&Logger>,
    ) -> Result<(), RemoveIdentityError> {
        self.require_identity_exists(log, name)
            .map_err(RemoveIdentityError::RequireIdentityExistsFailed)?;

        if name == ANONYMOUS_IDENTITY_NAME {
            return Err(RemoveIdentityError::CannotDeleteAnonymousIdentity());
        }

        if self.configuration.default == name {
            return Err(RemoveIdentityError::CannotDeleteDefaultIdentity());
        }

        let wallet_config_file = self.get_persistent_wallet_config_file(name);
        if wallet_config_file.exists() {
            if let Some(logger) = display_linked_wallets_to {
                DfxIdentity::display_linked_wallets(logger, &wallet_config_file)
                    .map_err(DisplayLinkedWalletsFailed)?;
            }
            if drop_wallets {
                remove_identity_file(&wallet_config_file)?;
            } else {
                return Err(DropWalletsFlagRequiredToRemoveIdentityWithWallets());
            }
        }

        if let Ok(config) = self.get_identity_config_or_default(name) {
            if let Some(suffix) = config.keyring_identity_suffix {
                keyring_mock::delete_pem_from_keyring(&suffix)
                    .map_err(RemoveIdentityError::RemoveIdentityFromKeyringFailed)?;
            }
        }
        remove_identity_file(&self.get_identity_json_path(name))?;
        remove_identity_file(&self.file_locations.get_plaintext_identity_pem_path(name))?;
        remove_identity_file(&self.file_locations.get_encrypted_identity_pem_path(name))?;

        let dir = self.get_identity_dir_path(name);
        if dir.exists() {
            crate::fs::remove_dir(&dir).map_err(RemoveIdentityDirectoryFailed)?;
        }

        Ok(())
    }

    /// Rename an identity.
    /// If renaming the selected (default) identity, changes that
    /// to refer to the new identity name.
    pub fn rename(
        &mut self,
        log: &Logger,
        project_temp_dir: Option<PathBuf>,
        from: &str,
        to: &str,
    ) -> Result<bool, RenameIdentityError> {
        if to == ANONYMOUS_IDENTITY_NAME {
            return Err(RenameIdentityError::CannotCreateAnonymousIdentity());
        }
        self.require_identity_exists(log, from)
            .map_err(RenameIdentityError::IdentityDoesNotExist)?;

        let identity_config = self
            .get_identity_config_or_default(from)
            .map_err(GetIdentityConfigFailed)?;
        let from_dir = self.get_identity_dir_path(from);
        let to_dir = self.get_identity_dir_path(to);

        if to_dir.exists() {
            return Err(RenameIdentityError::IdentityAlreadyExists());
        }

        DfxIdentity::map_wallets_to_renamed_identity(project_temp_dir, from, to)
            .map_err(MapWalletsToRenamedIdentityFailed)?;
        crate::fs::rename(&from_dir, &to_dir)?;
        if let Some(keyring_identity_suffix) = &identity_config.keyring_identity_suffix {
            debug!(log, "Migrating keyring content.");
            let (pem, _) =
                pem_safekeeping::load_pem(log, &self.file_locations, from, &identity_config)
                    .map_err(LoadPemFailed)?;
            let new_config = IdentityConfiguration {
                keyring_identity_suffix: Some(to.to_string()),
                ..identity_config
            };
            pem_safekeeping::save_pem(log, &self.file_locations, to, &new_config, pem.as_ref())
                .map_err(SavePemFailed)?;
            let config_path = self.get_identity_json_path(to);
            save_identity_configuration(log, &config_path, &new_config)
                .map_err(RenameIdentityError::SaveIdentityConfigurationFailed)?;
            keyring_mock::delete_pem_from_keyring(keyring_identity_suffix)
                .map_err(RenameIdentityError::RemoveIdentityFromKeyringFailed)?;
        }

        if from == self.configuration.default {
            self.write_default_identity(to)
                .map_err(SwitchDefaultIdentitySettingsFailed)?;
            Ok(true)
        } else {
            Ok(false)
        }
    }

    /// Select an identity by name to use by default
    pub fn use_identity_named(
        &mut self,
        log: &Logger,
        name: &str,
    ) -> Result<(), UseIdentityByNameError> {
        self.require_identity_exists(log, name)
            .map_err(UseIdentityByNameError::RequireIdentityExistsFailed)?;
        self.write_default_identity(name)
            .map_err(WriteDefaultIdentityFailed)?;
        self.configuration.default = name.to_string();
        Ok(())
    }

    fn write_default_identity(&self, name: &str) -> Result<(), WriteDefaultIdentityError> {
        let config = Configuration {
            default: String::from(name),
        };
        save_configuration(&self.identity_json_path, &config)
            .map_err(SaveIdentityManagerConfigurationFailed)?;
        Ok(())
    }

    /// Determines if there are enough files present to consider the identity as existing.
    /// Does NOT guarantee that the identity will load correctly.
    pub fn require_identity_exists(
        &self,
        log: &Logger,
        name: &str,
    ) -> Result<(), RequireIdentityExistsError> {
        trace!(log, "Checking if identity '{name}' exists.");
        if name == ANONYMOUS_IDENTITY_NAME {
            return Ok(());
        }

        if name.starts_with(TEMP_IDENTITY_PREFIX) {
            return Err(RequireIdentityExistsError::ReservedIdentityName(
                String::from(name),
            ));
        }

        let json_path = self.get_identity_json_path(name);
        let plaintext_pem_path = self.file_locations.get_plaintext_identity_pem_path(name);
        let encrypted_pem_path = self.file_locations.get_encrypted_identity_pem_path(name);

        if !plaintext_pem_path.exists() && !encrypted_pem_path.exists() && !json_path.exists() {
            Err(RequireIdentityExistsError::IdentityDoesNotExist(
                String::from(name),
                json_path,
            ))
        } else {
            Ok(())
        }
    }

    pub fn get_identity_dir_path(&self, identity: &str) -> PathBuf {
        self.file_locations.get_identity_dir_path(identity)
    }

    /// Returns the path where wallets on persistent/non-ephemeral networks are stored.
    fn get_persistent_wallet_config_file(&self, identity: &str) -> PathBuf {
        self.get_identity_dir_path(identity)
            .join(WALLET_CONFIG_FILENAME)
    }

    /// Returns the path where an identity's `IdentityConfiguration` is stored.
    pub fn get_identity_json_path(&self, identity: &str) -> PathBuf {
        self.get_identity_dir_path(identity).join(IDENTITY_JSON)
    }

    /// Returns a map of (name, principal) pairs for unencrypted identities.
    /// In the future, we may refactor the code to include encrypted principals as well.
    pub fn get_unencrypted_principal_map(&self, log: &Logger) -> BTreeMap<String, String> {
        use ic_agent::Identity;
        let mut res = if let Ok(names) = self.get_identity_names(log) {
            names
                .iter()
                .filter_map(|name| {
                    let config = self.get_identity_config_or_default(name).ok()?;
                    if let IdentityConfiguration {
                        encryption: None,
                        keyring_identity_suffix: None,
                        hsm: None,
                    } = config
                    {
                        let sender = self.load_identity(name, log).ok()?.sender().ok()?;
                        Some((name.clone(), sender.to_text()))
                    } else {
                        None
                    }
                })
                .collect()
        } else {
            BTreeMap::new()
        };
        res.insert(
            ANONYMOUS_IDENTITY_NAME.to_string(),
            Principal::anonymous().to_string(),
        );
        res
    }

    pub fn get_identity_config_or_default(
        &self,
        identity: &str,
    ) -> Result<IdentityConfiguration, GetIdentityConfigOrDefaultError> {
        let json_path = self.get_identity_json_path(identity);
        if json_path.exists() {
            load_json_file(&json_path)
                .map_err(|err| LoadIdentityConfigurationFailed(identity.to_string(), err))
        } else {
            Ok(IdentityConfiguration::default())
        }
    }
}

pub(super) fn get_dfx_hsm_pin() -> Result<String, String> {
    std::env::var("DFX_HSM_PIN")
        .map_err(|_| "There is no DFX_HSM_PIN environment variable.".to_string())
}

fn initialize(
    logger: &Logger,
    identity_json_path: &Path,
    identity_root_path: &Path,
) -> Result<Configuration, InitializeIdentityManagerError> {
    slog::info!(
        logger,
        r#"Creating the "default" identity.
WARNING: The "default" identity is not stored securely. Do not use it to control a lot of cycles/ICP.
To create a more secure identity, create and use an identity that is protected by a password using the following commands:
    dfx identity new <my-secure-identity-name> # creates a password protected identity
    dfx identity use <my-secure-identity-name> # uses this identity by default
"#
    );

    let identity_dir = identity_root_path.join(DEFAULT_IDENTITY_NAME);
    let identity_pem_path = identity_dir.join(IDENTITY_PEM);
    if !identity_pem_path.exists() {
        if !identity_dir.exists() {
            crate::fs::create_dir_all(&identity_dir).map_err(CreateIdentityDirectoryFailed)?;
        }

        let maybe_creds_pem_path = get_legacy_creds_pem_path()?;
        if maybe_creds_pem_path
            .as_ref()
            .map(|p| p.exists())
            .unwrap_or_default()
        {
            let creds_pem_path =
                maybe_creds_pem_path.expect("Unreachable - Just checked for existence.");
            slog::info!(
                logger,
                "  - migrating key from {} to {}",
                creds_pem_path.display(),
                identity_pem_path.display()
            );
            crate::fs::copy(&creds_pem_path, &identity_pem_path)
                .map_err(MigrateLegacyIdentityFailed)?;
        } else {
            slog::info!(
                logger,
                "  - generating new key at {}",
                identity_pem_path.display()
            );
            let (key, mnemonic) = generate_key().map_err(GenerateKeyFailed)?;
            pem_safekeeping::write_pem_to_file(&identity_pem_path, None, key.as_slice())
                .map_err(WritePemToFileFailed)?;
            eprintln!("Your seed phrase: {}\nThis can be used to reconstruct your key in case of emergency, so write it down in a safe place.", mnemonic.phrase());
        }
    } else {
        slog::info!(
            logger,
            "  - using key already in place at {}",
            identity_pem_path.display()
        );
    }

    let config = Configuration {
        default: String::from(DEFAULT_IDENTITY_NAME),
    };
    save_configuration(identity_json_path, &config)
        .map_err(InitializeIdentityManagerError::SaveConfigurationFailed)?;
    slog::info!(logger, r#"Created the "default" identity."#);

    Ok(config)
}

fn get_legacy_creds_pem_path() -> Result<Option<PathBuf>, GetLegacyCredentialsPemPathError> {
    if cfg!(windows) {
        // No legacy path on Windows - there was no Windows support when paths were changed
        Ok(None)
    } else {
        let config_root = std::env::var_os("DFX_CONFIG_ROOT");
        let home = get_user_home().map_err(GetLegacyPemPathFailed)?;
        let root = config_root.unwrap_or(home);

        Ok(Some(
            PathBuf::from(root)
                .join(".dfinity")
                .join("identity")
                .join("creds.pem"),
        ))
    }
}

fn load_configuration(path: &Path) -> Result<Configuration, StructuredFileError> {
    load_json_file(path)
}

fn save_configuration(path: &Path, config: &Configuration) -> Result<(), StructuredFileError> {
    save_json_file(path, config)
}

pub(super) fn save_identity_configuration(
    log: &Logger,
    path: &Path,
    config: &IdentityConfiguration,
) -> Result<(), SaveIdentityConfigurationError> {
    trace!(log, "Writing identity configuration to {}", path.display());
    ensure_parent_dir_exists(path).map_err(EnsureIdentityConfigurationDirExistsFailed)?;

    save_json_file(path, &config)
        .map_err(SaveIdentityConfigurationError::SaveIdentityConfigurationFailed)
}

/// Removes the file if it exists.
fn remove_identity_file(file: &Path) -> Result<(), RemoveIdentityError> {
    if file.exists() {
        crate::fs::remove_file(file)?;
    }
    Ok(())
}

/// Generates a new secp256k1 key.
pub(super) fn generate_key() -> Result<(Vec<u8>, Mnemonic), GenerateKeyError> {
    let mnemonic = Mnemonic::new(MnemonicType::for_key_size(256).unwrap(), Language::English);
    let secret =
        mnemonic_to_key(&mnemonic).map_err(GenerateKeyError::ConvertMnemonicToKeyFailed)?;
    let pem = secret
        .to_sec1_pem(LineEnding::CRLF)
        .map_err(|e| GenerateFreshSecp256k1KeyFailed(Box::new(e)))?;
    Ok((pem.as_bytes().to_vec(), mnemonic))
}

pub fn mnemonic_to_key(mnemonic: &Mnemonic) -> Result<SecretKey, ConvertMnemonicToKeyError> {
    const DEFAULT_DERIVATION_PATH: &str = "m/44'/223'/0'/0/0";
    let path = DEFAULT_DERIVATION_PATH.parse().unwrap();
    let seed = Seed::new(mnemonic, "");
    let pk =
        XPrv::derive_from_path(seed.as_bytes(), &path).map_err(DeriveExtendedKeyFromPathFailed)?;
    Ok(SecretKey::from(pk.private_key()))
}


-----------------------

/src/dfx-core/src/identity/keyring_mock.rs:
-----------------------

use super::TEMP_IDENTITY_PREFIX;
use crate::error::keyring::KeyringError;
use crate::error::keyring::KeyringError::{
    DecodePemFailed, DeletePasswordFailed, GetPasswordFailed, LoadMockKeyringFailed,
    MockKeyNotFound, MockUnavailable, SaveMockKeyringFailed, SetPasswordFailed,
};
use crate::json::{load_json_file, save_json_file};
use keyring;
use serde::{Deserialize, Serialize};
use slog::{trace, Logger};
use std::{collections::HashMap, path::PathBuf};

pub const KEYRING_SERVICE_NAME: &str = "internet_computer_identities";
pub const KEYRING_IDENTITY_PREFIX: &str = "internet_computer_identity_";
pub const USE_KEYRING_MOCK_ENV_VAR: &str = "DFX_CI_MOCK_KEYRING_LOCATION";
fn keyring_identity_name_from_suffix(suffix: &str) -> String {
    format!("{}{}", KEYRING_IDENTITY_PREFIX, suffix)
}

enum KeyringMockMode {
    /// Use system keyring
    NoMock,
    /// Simulate keyring where access is granted
    MockAvailable,
    /// Simulate keyring where access is rejected
    MockReject,
}

impl KeyringMockMode {
    fn current_mode() -> Self {
        match std::env::var(USE_KEYRING_MOCK_ENV_VAR) {
            Err(_) => Self::NoMock,
            Ok(location) => match location.as_str() {
                "" => Self::MockReject,
                _ => Self::MockAvailable,
            },
        }
    }
}

#[derive(Debug, Default, Serialize, Deserialize)]
struct KeyringMock {
    pub kv_store: HashMap<String, String>,
}

impl KeyringMock {
    fn get_location() -> Result<PathBuf, KeyringError> {
        match std::env::var(USE_KEYRING_MOCK_ENV_VAR) {
            Ok(filename) => match filename.as_str() {
                "" => Err(MockUnavailable()),
                _ => Ok(PathBuf::from(filename)),
            },
            _ => unreachable!("Mock keyring unavailable."),
        }
    }

    pub fn load() -> Result<Self, KeyringError> {
        let location = Self::get_location()?;
        if location.exists() {
            load_json_file(&location).map_err(LoadMockKeyringFailed)
        } else {
            Ok(Self::default())
        }
    }

    pub fn save(&self) -> Result<(), KeyringError> {
        let location = Self::get_location()?;
        save_json_file(&location, self).map_err(SaveMockKeyringFailed)
    }
}

pub fn load_pem_from_keyring(identity_name_suffix: &str) -> Result<Vec<u8>, KeyringError> {
    let keyring_identity_name = keyring_identity_name_from_suffix(identity_name_suffix);
    match KeyringMockMode::current_mode() {
        KeyringMockMode::NoMock => {
            let entry = keyring::Entry::new(KEYRING_SERVICE_NAME, &keyring_identity_name);
            let encoded_pem = entry.get_password().map_err(GetPasswordFailed)?;
            let pem = hex::decode(encoded_pem).map_err(DecodePemFailed)?;
            Ok(pem)
        }
        KeyringMockMode::MockAvailable => {
            let mock = KeyringMock::load()?;
            let encoded_pem = mock
                .kv_store
                .get(&keyring_identity_name)
                .ok_or(MockKeyNotFound(keyring_identity_name))?;
            let pem = hex::decode(encoded_pem).map_err(DecodePemFailed)?;
            Ok(pem)
        }
        KeyringMockMode::MockReject => Err(MockUnavailable()),
    }
}

pub fn write_pem_to_keyring(
    identity_name_suffix: &str,
    pem_content: &[u8],
) -> Result<(), KeyringError> {
    let keyring_identity_name = keyring_identity_name_from_suffix(identity_name_suffix);
    let encoded_pem = hex::encode(pem_content);
    match KeyringMockMode::current_mode() {
        KeyringMockMode::NoMock => {
            let entry = keyring::Entry::new(KEYRING_SERVICE_NAME, &keyring_identity_name);
            entry
                .set_password(&encoded_pem)
                .map_err(SetPasswordFailed)?;
            Ok(())
        }
        KeyringMockMode::MockAvailable => {
            let mut mock = KeyringMock::load()?;
            mock.kv_store.insert(keyring_identity_name, encoded_pem);
            mock.save()?;
            Ok(())
        }
        KeyringMockMode::MockReject => Err(MockUnavailable()),
    }
}

/// Determines if keyring is available by trying to write a dummy entry.
pub fn keyring_available(log: &Logger) -> bool {
    match KeyringMockMode::current_mode() {
        KeyringMockMode::NoMock => {
            trace!(log, "Checking for keyring availability.");
            // by using the temp identity prefix this will not clash with real identities since that would be an invalid identity name
            let dummy_entry_name = format!(
                "{}{}{}",
                KEYRING_IDENTITY_PREFIX, TEMP_IDENTITY_PREFIX, "dummy"
            );
            let entry = keyring::Entry::new(KEYRING_SERVICE_NAME, &dummy_entry_name);
            entry.set_password("dummy entry").is_ok()
        }
        KeyringMockMode::MockReject => false,
        KeyringMockMode::MockAvailable => true,
    }
}

pub fn delete_pem_from_keyring(identity_name_suffix: &str) -> Result<(), KeyringError> {
    let keyring_identity_name = keyring_identity_name_from_suffix(identity_name_suffix);
    match KeyringMockMode::current_mode() {
        KeyringMockMode::NoMock => {
            let entry = keyring::Entry::new(KEYRING_SERVICE_NAME, &keyring_identity_name);
            if entry.get_password().is_ok() {
                entry.delete_password().map_err(DeletePasswordFailed)?;
            }
        }
        KeyringMockMode::MockAvailable => {
            let mut mock = KeyringMock::load()?;
            mock.kv_store.remove(&keyring_identity_name);
            mock.save()?;
        }
        KeyringMockMode::MockReject => return Err(MockUnavailable()),
    }
    Ok(())
}


-----------------------

/src/dfx-core/src/identity/mod.rs:
-----------------------

/src/dfx-core/src/identity/pem_safekeeping.rs:
-----------------------

use super::identity_manager::EncryptionConfiguration;
use super::IdentityConfiguration;
use crate::error::identity::WritePemContentError;
use crate::error::{
    encryption::{
        EncryptionError,
        EncryptionError::{DecryptContentFailed, HashPasswordFailed},
    },
    identity::{
        LoadPemError,
        LoadPemError::LoadFromKeyringFailed,
        LoadPemFromFileError,
        LoadPemFromFileError::DecryptPemFileFailed,
        SavePemError,
        SavePemError::{CannotSavePemContentForHsm, WritePemToKeyringFailed},
        WritePemToFileError,
        WritePemToFileError::{EncryptPemFileFailed, WritePemContentFailed},
    },
};
use crate::identity::identity_file_locations::IdentityFileLocations;
use crate::identity::keyring_mock;
use crate::identity::pem_safekeeping::PromptMode::{DecryptingToUse, EncryptingToCreate};
use aes_gcm::aead::{Aead, KeyInit};
use aes_gcm::{Aes256Gcm, Key, Nonce};
use argon2::{password_hash::PasswordHasher, Argon2};
use slog::{debug, trace, Logger};
use std::path::Path;

/// Loads an identity's PEM file content.
pub(crate) fn load_pem(
    log: &Logger,
    locations: &IdentityFileLocations,
    identity_name: &str,
    identity_config: &IdentityConfiguration,
) -> Result<(Vec<u8>, bool), LoadPemError> {
    if identity_config.hsm.is_some() {
        unreachable!("Cannot load pem content for an HSM identity.")
    } else if identity_config.keyring_identity_suffix.is_some() {
        debug!(
            log,
            "Found keyring identity suffix - PEM file is stored in keyring."
        );
        let pem = keyring_mock::load_pem_from_keyring(identity_name)
            .map_err(|err| LoadFromKeyringFailed(Box::new(identity_name.to_string()), err))?;
        Ok((pem, true))
    } else {
        let pem_path = locations.get_identity_pem_path(identity_name, identity_config);
        load_pem_from_file(&pem_path, Some(identity_config))
            .map_err(LoadPemError::LoadFromFileFailed)
    }
}

pub(crate) fn save_pem(
    log: &Logger,
    locations: &IdentityFileLocations,
    name: &str,
    identity_config: &IdentityConfiguration,
    pem_content: &[u8],
) -> Result<(), SavePemError> {
    trace!(
        log,
        "Saving pem with input identity name '{name}' and identity config {:?}",
        identity_config
    );
    if identity_config.hsm.is_some() {
        Err(CannotSavePemContentForHsm())
    } else if let Some(keyring_identity) = &identity_config.keyring_identity_suffix {
        debug!(log, "Saving keyring identity.");
        keyring_mock::write_pem_to_keyring(keyring_identity, pem_content)
            .map_err(WritePemToKeyringFailed)
    } else {
        let path = locations.get_identity_pem_path(name, identity_config);
        write_pem_to_file(&path, Some(identity_config), pem_content)
            .map_err(SavePemError::WritePemToFileFailed)
    }
}

/// Loads a pem file, no matter if it is a plaintext pem file or if it is encrypted with a password.
/// Transparently handles all complexities regarding pem file encryption, including prompting the user for the password.
/// Returns the pem and whether the original was encrypted.
///
/// Try to only load the pem file once, as the user may be prompted for the password every single time you call this function.
pub fn load_pem_from_file(
    path: &Path,
    config: Option<&IdentityConfiguration>,
) -> Result<(Vec<u8>, bool), LoadPemFromFileError> {
    let content = crate::fs::read(path)?;

    let (content, was_encrypted) = maybe_decrypt_pem(content.as_slice(), config)
        .map_err(|err| DecryptPemFileFailed(path.to_path_buf(), err))?;
    Ok((content, was_encrypted))
}

/// Transparently handles all complexities regarding pem file encryption, including prompting the user for the password.
///
/// Automatically creates required directories.
pub fn write_pem_to_file(
    path: &Path,
    config: Option<&IdentityConfiguration>,
    pem_content: &[u8],
) -> Result<(), WritePemToFileError> {
    let pem_content = maybe_encrypt_pem(pem_content, config)
        .map_err(|err| EncryptPemFileFailed(path.to_path_buf(), err))?;

    write_pem_content(path, &pem_content).map_err(WritePemContentFailed)
}

fn write_pem_content(path: &Path, pem_content: &[u8]) -> Result<(), WritePemContentError> {
    let containing_folder = crate::fs::parent(path)?;
    crate::fs::create_dir_all(&containing_folder)?;
    crate::fs::write(path, pem_content)?;

    let mut permissions = crate::fs::read_permissions(path)?;

    permissions.set_readonly(true);
    // On *nix, set the read permission to owner-only.
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        permissions.set_mode(0o400);
    }

    crate::fs::set_permissions(path, permissions)?;
    Ok(())
}

/// If the IndentityConfiguration suggests that the content of the pem file should be encrypted,
/// then the user is prompted for the password to the pem file.
/// The encrypted pem file content is then returned.
///
/// If the pem file should not be encrypted, then the content is returned as is.
///
/// `maybe_decrypt_pem` does the opposite.
fn maybe_encrypt_pem(
    pem_content: &[u8],
    config: Option<&IdentityConfiguration>,
) -> Result<Vec<u8>, EncryptionError> {
    if let Some(encryption_config) = config.and_then(|c| c.encryption.as_ref()) {
        let password = password_prompt(EncryptingToCreate)?;
        let result = encrypt(pem_content, encryption_config, &password);
        println!("Encryption complete.");
        result
    } else {
        Ok(Vec::from(pem_content))
    }
}

/// If the IndentityConfiguration suggests that the content of the pem file is encrypted,
/// then the user is prompted for the password to the pem file.
/// The decrypted pem file content is then returned.
///
/// If the pem file should not be encrypted, then the content is returned as is.
///
/// Additionally returns whether or not it was necessary to decrypt the file.
///
/// `maybe_encrypt_pem` does the opposite.
fn maybe_decrypt_pem(
    pem_content: &[u8],
    config: Option<&IdentityConfiguration>,
) -> Result<(Vec<u8>, bool), EncryptionError> {
    if let Some(decryption_config) = config.and_then(|c| c.encryption.as_ref()) {
        let password = password_prompt(DecryptingToUse)?;
        let pem = decrypt(pem_content, decryption_config, &password)?;
        // print to stderr so that output redirection works for the identity export command
        eprintln!("Decryption complete.");
        Ok((pem, true))
    } else {
        Ok((Vec::from(pem_content), false))
    }
}

#[derive(PartialEq, Eq)]
enum PromptMode {
    EncryptingToCreate,
    DecryptingToUse,
}

fn password_prompt(mode: PromptMode) -> Result<String, EncryptionError> {
    let prompt = match mode {
        PromptMode::EncryptingToCreate => "Please enter a passphrase for your identity",
        PromptMode::DecryptingToUse => "Please enter the passphrase for your identity",
    };
    dialoguer::Password::new()
        .with_prompt(prompt)
        .validate_with(|password: &String| -> Result<(), &str> {
            // Password may have been set before length check has been implemented, so only reject bad passwords during identity creation
            if password.chars().count() > 8 || mode == PromptMode::DecryptingToUse {
                Ok(())
            } else {
                Err("Password must be longer than 8 characters.")
            }
        })
        .interact()
        .map_err(EncryptionError::ReadUserPasswordFailed)
}

fn get_argon_params() -> argon2::Params {
    argon2::Params::new(64000 /* in kb */, 3, 1, Some(32 /* in bytes */)).unwrap()
}

fn encrypt(
    content: &[u8],
    config: &EncryptionConfiguration,
    password: &str,
) -> Result<Vec<u8>, EncryptionError> {
    let argon2 = Argon2::new(
        argon2::Algorithm::Argon2id,
        argon2::Version::V0x13,
        get_argon_params(),
    );
    let hash = argon2
        .hash_password(password.as_bytes(), &config.pw_salt)
        .map_err(EncryptionError::HashPasswordFailed)?;
    let key = Key::<Aes256Gcm>::clone_from_slice(hash.hash.unwrap().as_ref());
    let cipher = Aes256Gcm::new(&key);
    let nonce = Nonce::from_slice(config.file_nonce.as_slice());

    let encrypted = cipher
        .encrypt(nonce, content)
        .map_err(EncryptionError::EncryptContentFailed)?;

    Ok(encrypted)
}

fn decrypt(
    encrypted_content: &[u8],
    config: &EncryptionConfiguration,
    password: &str,
) -> Result<Vec<u8>, EncryptionError> {
    let argon2 = Argon2::new(
        argon2::Algorithm::Argon2id,
        argon2::Version::V0x13,
        get_argon_params(),
    );
    let hash = argon2
        .hash_password(password.as_bytes(), &config.pw_salt)
        .map_err(HashPasswordFailed)?;
    let key = Key::<Aes256Gcm>::clone_from_slice(hash.hash.unwrap().as_ref());
    let cipher = Aes256Gcm::new(&key);
    let nonce = Nonce::from_slice(config.file_nonce.as_slice());

    cipher
        .decrypt(nonce, encrypted_content.as_ref())
        .map_err(DecryptContentFailed)
}

#[cfg(test)]
mod test {
    use super::*;
    use proptest::prelude::*;
    proptest! {
        #![proptest_config(ProptestConfig::with_cases(90))] // takes ~0.3s per case
        #[test]
        fn decrypt_reverts_encrypt(pass in ".*", content in ".*") {
            let config = EncryptionConfiguration::new().unwrap();
            let encrypted = encrypt(content.as_bytes(), &config, &pass).unwrap();
            let decrypted = decrypt(&encrypted, &config, &pass).unwrap();

            assert_eq!(content.as_bytes(), decrypted.as_slice());
        }
    }
}


-----------------------

/src/dfx-core/src/identity/pem_utils.rs:
-----------------------

use crate::error::identity::{
    ValidatePemFileError,
    ValidatePemFileError::{UnsupportedKeyVersion, ValidatePemContentFailed},
};
use ic_agent::identity::BasicIdentity;
use ic_agent::identity::PemError;
use ic_agent::identity::Secp256k1Identity;

pub fn validate_pem_file(pem_content: &[u8]) -> Result<(), ValidatePemFileError> {
    let secp_res =
        Secp256k1Identity::from_pem(pem_content).map_err(|e| ValidatePemContentFailed(Box::new(e)));
    if let Err(e) = secp_res {
        let basic_identity_res = BasicIdentity::from_pem(pem_content);
        match basic_identity_res {
            Err(PemError::KeyRejected(rj)) if rj.to_string() == "VersionNotSupported" => {
                return Err(UnsupportedKeyVersion());
            }
            Err(_) => return Err(e),
            _ => {}
        }
    }

    Ok(())
}


-----------------------

/src/dfx-core/src/identity/wallet.rs:
-----------------------

use crate::config::directories::get_user_dfx_config_dir;
use crate::config::model::network_descriptor::{NetworkDescriptor, NetworkTypeDescriptor};
use crate::error::wallet_config::WalletConfigError;
use crate::error::wallet_config::WalletConfigError::GetWalletConfigPathFailed;
use crate::identity::{Identity, WALLET_CONFIG_FILENAME};
use candid::Principal;
use std::path::PathBuf;

pub fn get_wallet_config_path(
    network: &NetworkDescriptor,
    name: &str,
) -> Result<PathBuf, WalletConfigError> {
    Ok(match &network.r#type {
        NetworkTypeDescriptor::Persistent | NetworkTypeDescriptor::Playground { .. } => {
            // Using the global
            get_user_dfx_config_dir()
                .map_err(|e| {
                    GetWalletConfigPathFailed(
                        Box::new(name.to_string()),
                        Box::new(network.name.clone()),
                        e,
                    )
                })?
                .join("identity")
                .join(name)
                .join(WALLET_CONFIG_FILENAME)
        }
        NetworkTypeDescriptor::Ephemeral { wallet_config_path } => wallet_config_path.clone(),
    })
}

pub fn wallet_canister_id(
    network: &NetworkDescriptor,
    name: &str,
) -> Result<Option<Principal>, WalletConfigError> {
    let wallet_path = get_wallet_config_path(network, name)?;
    if !wallet_path.exists() {
        return Ok(None);
    }

    let config = Identity::load_wallet_config(&wallet_path)?;

    let maybe_wallet_principal = config
        .identities
        .get(name)
        .and_then(|wallet_network| wallet_network.networks.get(&network.name).cloned());
    Ok(maybe_wallet_principal)
}


-----------------------

/src/dfx-core/src/interface/builder.rs:
-----------------------

use crate::{
    config::model::{
        dfinity::{Config, NetworksConfig},
        network_descriptor::NetworkDescriptor,
    },
    error::{
        builder::{BuildAgentError, BuildDfxInterfaceError, BuildIdentityError},
        network_config::NetworkConfigError,
    },
    identity::{identity_manager::InitializeIdentity, IdentityManager},
    network::{
        provider::{create_network_descriptor, LocalBindDetermination},
        root_key::fetch_root_key_when_non_mainnet_or_error,
    },
    DfxInterface,
};
use ic_agent::{agent::route_provider::RoundRobinRouteProvider, Agent, Identity};
use reqwest::Client;
use std::sync::Arc;

#[derive(PartialEq)]
pub enum IdentityPicker {
    Anonymous,
    Selected,
    Named(String),
}

#[derive(PartialEq)]
pub enum NetworkPicker {
    Local,
    Mainnet,
    Named(String),
}

pub struct DfxInterfaceBuilder {
    identity: IdentityPicker,

    network: NetworkPicker,

    /// Force fetching of the root key.
    /// This is insecure and should only be set for non-mainnet networks.
    /// There is no need to set this for the local network, where the root key is fetched by default.
    /// This would typically be set for a testnet, or an alias for the local network.
    force_fetch_root_key_insecure_non_mainnet_only: bool,
}

impl DfxInterfaceBuilder {
    pub(crate) fn new() -> Self {
        Self {
            identity: IdentityPicker::Selected,
            network: NetworkPicker::Local,
            force_fetch_root_key_insecure_non_mainnet_only: false,
        }
    }

    pub fn anonymous(self) -> Self {
        self.with_identity(IdentityPicker::Anonymous)
    }

    pub fn with_identity_named(self, name: &str) -> Self {
        self.with_identity(IdentityPicker::Named(name.to_string()))
    }

    pub fn with_identity(self, identity: IdentityPicker) -> Self {
        Self { identity, ..self }
    }

    pub fn mainnet(self) -> Self {
        self.with_network(NetworkPicker::Mainnet)
    }

    pub fn with_network(self, network: NetworkPicker) -> Self {
        Self { network, ..self }
    }

    pub fn with_network_named(self, name: &str) -> Self {
        self.with_network(NetworkPicker::Named(name.to_string()))
    }

    pub fn with_force_fetch_root_key_insecure_non_mainnet_only(self) -> Self {
        Self {
            force_fetch_root_key_insecure_non_mainnet_only: true,
            ..self
        }
    }

    pub async fn build(&self) -> Result<DfxInterface, BuildDfxInterfaceError> {
        let fetch_root_key = self.network == NetworkPicker::Local
            || self.force_fetch_root_key_insecure_non_mainnet_only;
        let networks_config = NetworksConfig::new()?;
        let config = Config::from_current_dir(None)?.map(Arc::new);
        let network_descriptor = self.build_network_descriptor(config.clone(), &networks_config)?;
        let identity = self.build_identity()?;
        let agent = self.build_agent(identity.clone(), &network_descriptor)?;

        if fetch_root_key {
            fetch_root_key_when_non_mainnet_or_error(&agent, &network_descriptor).await?;
        }

        Ok(DfxInterface {
            config,
            identity,
            agent,
            networks_config,
            network_descriptor,
        })
    }

    fn build_agent(
        &self,
        identity: Arc<dyn Identity>,
        network_descriptor: &NetworkDescriptor,
    ) -> Result<Agent, BuildAgentError> {
        let route_provider = RoundRobinRouteProvider::new(network_descriptor.providers.clone())
            .map_err(BuildAgentError::CreateRouteProvider)?;
        let client = Client::builder()
            .use_rustls_tls()
            .build()
            .map_err(BuildAgentError::CreateHttpClient)?;
        let agent = Agent::builder()
            .with_http_client(client)
            .with_route_provider(route_provider)
            .with_arc_identity(identity)
            .build()
            .map_err(BuildAgentError::CreateAgent)?;
        Ok(agent)
    }

    fn build_identity(&self) -> Result<Arc<dyn Identity>, BuildIdentityError> {
        if self.identity == IdentityPicker::Anonymous {
            return Ok(Arc::new(ic_agent::identity::AnonymousIdentity));
        }

        let identity_override = match &self.identity {
            IdentityPicker::Named(name) => Some(name.clone()),
            IdentityPicker::Selected => None,
            IdentityPicker::Anonymous => unreachable!(),
        };

        let logger = slog::Logger::root(slog::Discard, slog::o!());
        let mut identity_manager = IdentityManager::new(
            &logger,
            identity_override.as_deref(),
            InitializeIdentity::Disallow,
        )?;
        let identity: Box<dyn Identity> =
            identity_manager.instantiate_selected_identity(&logger)?;
        Ok(Arc::from(identity))
    }

    fn build_network_descriptor(
        &self,
        config: Option<Arc<Config>>,
        networks_config: &NetworksConfig,
    ) -> Result<NetworkDescriptor, NetworkConfigError> {
        let network = match &self.network {
            NetworkPicker::Local => None,
            NetworkPicker::Mainnet => Some("ic".to_string()),
            NetworkPicker::Named(name) => Some(name.clone()),
        }
        .map(String::from);
        let logger = None;
        create_network_descriptor(
            config,
            Arc::new(networks_config.clone()),
            network,
            logger,
            LocalBindDetermination::ApplyRunningWebserverPort,
        )
    }
}

impl Default for DfxInterfaceBuilder {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use crate::error::{
        builder::{
            BuildDfxInterfaceError,
            BuildDfxInterfaceError::{FetchRootKey, NetworkConfig},
            BuildIdentityError::NewIdentityManager,
        },
        identity::NewIdentityManagerError,
        network_config::NetworkConfigError::NetworkNotFound,
        root_key::FetchRootKeyError,
        root_key::FetchRootKeyError::AgentError,
    };
    use crate::identity::{
        identity_manager::IdentityStorageMode::Plaintext, IdentityCreationParameters,
        IdentityManager,
    };
    use crate::DfxInterface;
    use candid::Principal;
    use futures::Future;
    use ic_agent::{AgentError::TransportError, Identity};
    use serde_json::json;
    use std::path::Path;
    use std::sync::Arc;
    use tempfile::TempDir;
    use tokio::sync::Semaphore;

    lazy_static::lazy_static! {
        static ref SEMAPHORE: Semaphore = Semaphore::new(1);
    }

    async fn run_test<F, Fut>(test_function: F)
    where
        F: FnOnce(Arc<TempDir>) -> Fut + Send,
        Fut: Future<Output = ()> + Send,
    {
        run_test_with_settings(TestSettings { testnet: None }, test_function).await;
    }

    pub struct TestNetSettings {
        pub name: String,
        pub providers: Vec<String>,
    }
    pub struct TestSettings {
        pub testnet: Option<TestNetSettings>,
    }
    async fn run_test_with_settings<F, Fut>(settings: TestSettings, test_function: F)
    where
        F: FnOnce(Arc<TempDir>) -> Fut + Send,
        Fut: Future<Output = ()> + Send,
    {
        let _permit = SEMAPHORE.acquire().await.unwrap();

        let temp_dir = TempDir::new().unwrap();

        if let Some(testnet) = settings.testnet {
            let networks_config = json!({
                testnet.name: {
                    "providers": testnet.providers,
                }
            });

            let config_dir = temp_dir.path().join(".config/dfx");
            let networks_config_path = config_dir.join("networks.json");
            crate::fs::create_dir_all(&config_dir).unwrap();
            crate::fs::write(networks_config_path, networks_config.to_string()).unwrap();
        }

        // so tests don't clobber each other in the environment
        std::env::set_var("DFX_CONFIG_ROOT", temp_dir.path());

        let temp_dir = Arc::new(temp_dir);
        test_function(temp_dir.clone()).await;
    }

    #[tokio::test]
    async fn anonymous() {
        run_test(|td| async move {
            let d = DfxInterface::builder()
                .anonymous()
                .mainnet()
                .build()
                .await
                .unwrap();
            assert!(d.identity().public_key().is_none());
            assert_eq!(d.identity().sender().unwrap(), Principal::anonymous());

            let actual = all_children(td.path());
            let expected: Vec<String> = vec![".config/".into(), ".config/dfx/".into()];
            assert_eq!(actual, expected);
        })
        .await;
    }

    #[tokio::test]
    async fn no_config_does_not_create_default_identity() {
        run_test(|_| async {
            assert!(matches!(
                DfxInterface::builder().build().await,
                Err(BuildDfxInterfaceError::BuildIdentity(NewIdentityManager(
                    NewIdentityManagerError::NoIdentityConfigurationFound
                )))
            ));
        })
        .await;
    }

    #[tokio::test]
    async fn default_identity() {
        run_test(|_| async {
            let default_principal = {
                let logger = slog::Logger::root(slog::Discard, slog::o!());
                let mut im = IdentityManager::new(
                    &logger,
                    None,
                    crate::identity::identity_manager::InitializeIdentity::Allow,
                )
                .unwrap();
                let id: Box<dyn Identity> = im.instantiate_selected_identity(&logger).unwrap();
                id.sender().unwrap()
            };
            let d = DfxInterface::builder().mainnet().build().await.unwrap();
            assert_eq!(d.identity.sender().unwrap(), default_principal);
        })
        .await;
    }

    #[tokio::test]
    async fn select_identity_by_name() {
        run_test(|_| async {
            let alice = "alice";
            let bob = "bob";
            let (alice_principal_from_mgr, bob_principal_from_mgr) = {
                let logger = slog::Logger::root(slog::Discard, slog::o!());
                let mut im = IdentityManager::new(
                    &logger,
                    None,
                    crate::identity::identity_manager::InitializeIdentity::Allow,
                )
                .unwrap();
                im.create_new_identity(&logger, alice, plaintext(), false)
                    .unwrap();
                im.create_new_identity(&logger, bob, plaintext(), false)
                    .unwrap();

                let alice: Box<dyn Identity> =
                    im.instantiate_identity_from_name(alice, &logger).unwrap();

                let bob: Box<dyn Identity> =
                    im.instantiate_identity_from_name(bob, &logger).unwrap();
                (alice.sender().unwrap(), bob.sender().unwrap())
            };
            assert_ne!(alice_principal_from_mgr, bob_principal_from_mgr);
            let alice_interface = DfxInterface::builder()
                .with_identity_named(alice)
                .mainnet()
                .build()
                .await
                .unwrap();
            assert_eq!(
                alice_interface.identity.sender().unwrap(),
                alice_principal_from_mgr
            );

            let bob_interface = DfxInterface::builder()
                .with_identity_named(bob)
                .mainnet()
                .build()
                .await
                .unwrap();
            assert_eq!(
                bob_interface.identity.sender().unwrap(),
                bob_principal_from_mgr
            );
        })
        .await;
    }

    #[tokio::test]
    async fn selected_non_default() {
        run_test(|_| async {
            let alice = "alice";
            let bob = "bob";
            let bob_principal_from_mgr = {
                let logger = slog::Logger::root(slog::Discard, slog::o!());
                let mut im = IdentityManager::new(
                    &logger,
                    None,
                    crate::identity::identity_manager::InitializeIdentity::Allow,
                )
                .unwrap();
                im.create_new_identity(&logger, alice, plaintext(), false)
                    .unwrap();
                im.create_new_identity(&logger, bob, plaintext(), false)
                    .unwrap();

                let _alice_identity: Box<dyn Identity> =
                    im.instantiate_identity_from_name(alice, &logger).unwrap();

                let bob_identity: Box<dyn Identity> =
                    im.instantiate_identity_from_name(bob, &logger).unwrap();

                im.use_identity_named(&logger, bob).unwrap();
                bob_identity.sender().unwrap()
            };

            let selected_interface = DfxInterface::builder().mainnet().build().await.unwrap();
            assert_eq!(
                selected_interface.identity.sender().unwrap(),
                bob_principal_from_mgr
            );
        })
        .await;
    }

    fn plaintext() -> IdentityCreationParameters {
        IdentityCreationParameters::Pem { mode: Plaintext }
    }

    #[tokio::test]
    async fn local_network() {
        run_test(|_| async {
            match DfxInterface::builder().anonymous().build().await {
                Ok(d) => {
                    assert_eq!(d.network_descriptor.name, "local");
                    assert!(!d.network_descriptor.is_ic);
                    assert!(d.network_descriptor.local_server_descriptor.is_some());
                }
                Err(FetchRootKey(AgentError(TransportError(_)))) => {
                    // local replica isn't running, so this is expected,
                    // but we can't check anything else
                }
                Err(e) => panic!("unexpected error: {:?}", e),
            }
        })
        .await;
    }

    #[tokio::test]
    async fn mainnet() {
        run_test(|_| async {
            let d = DfxInterface::builder()
                .anonymous()
                .mainnet()
                .build()
                .await
                .unwrap();
            let network_descriptor = d.network_descriptor;
            assert!(network_descriptor.is_ic);
            assert_eq!(network_descriptor.name, "ic");
        })
        .await;
    }

    #[tokio::test]
    async fn try_to_fetch_root_key_on_mainnet() {
        run_test(|_| async {
            assert!(matches!(
                DfxInterface::builder()
                    .anonymous()
                    .mainnet()
                    .with_force_fetch_root_key_insecure_non_mainnet_only()
                    .build()
                    .await,
                Err(FetchRootKey(
                    FetchRootKeyError::MustNotFetchRootKeyOnMainnet
                ))
            ));
        })
        .await;
    }

    #[tokio::test]
    async fn named_network_not_found() {
        run_test(|_| async {
            assert!(
                matches!(DfxInterface::builder().with_network_named("testnet").build().await,
                Err(NetworkConfig(NetworkNotFound(network_name))) if network_name == "testnet")
            );
        })
        .await;
    }

    #[tokio::test]
    async fn named_network() {
        let settings = TestSettings {
            testnet: Some(TestNetSettings {
                name: "testnet".to_string(),
                providers: vec!["http://localhost:1234".to_string()],
            }),
        };
        run_test_with_settings(settings, |_| async move {
            let d = DfxInterface::builder()
                .anonymous()
                .with_network_named("testnet")
                .build()
                .await
                .unwrap();
            let network_descriptor = d.network_descriptor;
            assert_eq!(network_descriptor.name, "testnet");
            assert_eq!(network_descriptor.providers, vec!["http://localhost:1234"]);

            // Notice that the above did not fail, because it did not try to fetch the root key.
            // It only does so if we tell it to:
            assert!(matches!(
                DfxInterface::builder()
                    .anonymous()
                    .with_network_named("testnet")
                    .with_force_fetch_root_key_insecure_non_mainnet_only()
                    .build()
                    .await,
                Err(FetchRootKey(AgentError(TransportError(_))))
            ));
        })
        .await;
    }

    // returns a vec of all children of a directory, recursively
    // directories are suffixed with a '/'
    fn all_children(dir: &Path) -> Vec<String> {
        let mut result = vec![];
        for entry in std::fs::read_dir(dir).unwrap() {
            let entry = entry.unwrap();
            let path = entry.path();
            let filename = path
                .file_name()
                .unwrap()
                .to_os_string()
                .into_string()
                .unwrap();

            if path.is_dir() {
                result.push(filename.clone() + "/");
                let all_children = all_children(&path);
                let all_children: Vec<_> = all_children
                    .iter()
                    .map(|c| format!("{}/{}", &filename, c))
                    .collect();
                result.extend(all_children);
            } else {
                result.push(filename);
            }
        }
        result
    }
}


-----------------------

/src/dfx-core/src/interface/dfx.rs:
-----------------------

use crate::config::model::dfinity::{Config, NetworksConfig};
use crate::config::model::network_descriptor::NetworkDescriptor;
use crate::error::builder::BuildDfxInterfaceError;
use crate::DfxInterfaceBuilder;
use ic_agent::{Agent, Identity};
use std::sync::Arc;

pub struct DfxInterface {
    pub(crate) config: Option<Arc<Config>>,
    pub(crate) agent: Agent,
    pub(crate) identity: Arc<dyn Identity>,
    pub(crate) networks_config: NetworksConfig,
    pub(crate) network_descriptor: NetworkDescriptor,
}

impl DfxInterface {
    pub fn builder() -> DfxInterfaceBuilder {
        DfxInterfaceBuilder::new()
    }

    pub async fn anonymous() -> Result<DfxInterface, BuildDfxInterfaceError> {
        DfxInterfaceBuilder::new().anonymous().build().await
    }

    pub fn config(&self) -> Option<Arc<Config>> {
        self.config.clone()
    }

    pub fn agent(&self) -> &Agent {
        &self.agent
    }

    pub fn identity(&self) -> Arc<dyn Identity> {
        self.identity.clone()
    }

    pub fn networks_config(&self) -> &NetworksConfig {
        &self.networks_config
    }

    pub fn network_descriptor(&self) -> &NetworkDescriptor {
        &self.network_descriptor
    }
}


-----------------------

/src/dfx-core/src/interface/mod.rs:
-----------------------

pub mod builder;
pub mod dfx;

pub use builder::DfxInterfaceBuilder;


-----------------------

/src/dfx-core/src/json/mod.rs:
-----------------------

pub mod structure;
use crate::error::structured_file::StructuredFileError;
use crate::error::structured_file::StructuredFileError::ReadJsonFileFailed;
use crate::error::structured_file::StructuredFileError::{
    DeserializeJsonFileFailed, SerializeJsonFileFailed,
};
use serde::Serialize;
use std::path::Path;

pub fn load_json_file<T: for<'a> serde::de::Deserialize<'a>>(
    path: &Path,
) -> Result<T, StructuredFileError> {
    let content = crate::fs::read(path).map_err(ReadJsonFileFailed)?;

    serde_json::from_slice(content.as_ref())
        .map_err(|err| DeserializeJsonFileFailed(Box::new(path.to_path_buf()), err))
}

pub fn save_json_file<T: Serialize>(path: &Path, value: &T) -> Result<(), StructuredFileError> {
    let content = serde_json::to_string_pretty(&value)
        .map_err(|err| SerializeJsonFileFailed(Box::new(path.to_path_buf()), err))?;
    crate::fs::write(path, content)?;
    Ok(())
}


-----------------------

/src/dfx-core/src/json/structure.rs:
-----------------------

use candid::Deserialize;
use schemars::JsonSchema;
use semver::{Version, VersionReq};
use serde::Serialize;
use std::fmt::Display;
use std::ops::{Deref, DerefMut};
use std::str::FromStr;
use url::Url;

#[derive(Serialize, Deserialize, Debug, Clone, JsonSchema, PartialEq, Eq)]
#[serde(untagged)]
pub enum SerdeVec<T> {
    One(T),
    Many(Vec<T>),
}

impl<T> SerdeVec<T> {
    pub fn into_vec(self) -> Vec<T> {
        match self {
            Self::One(t) => vec![t],
            Self::Many(ts) => ts,
        }
    }
}

impl<T> Default for SerdeVec<T> {
    fn default() -> Self {
        Self::Many(vec![])
    }
}

#[derive(Serialize, serde::Deserialize)]
#[serde(untagged)]
enum PossiblyStrInner<T> {
    NotStr(T),
    Str(String),
}

#[derive(Serialize, Deserialize, Default, Copy, Clone, Debug, JsonSchema)]
#[serde(try_from = "PossiblyStrInner<T>")]
pub struct PossiblyStr<T>(pub T)
where
    T: FromStr,
    T::Err: Display;

impl<T> TryFrom<PossiblyStrInner<T>> for PossiblyStr<T>
where
    T: FromStr,
    T::Err: Display,
{
    type Error = T::Err;
    fn try_from(inner: PossiblyStrInner<T>) -> Result<Self, Self::Error> {
        match inner {
            PossiblyStrInner::NotStr(t) => Ok(Self(t)),
            PossiblyStrInner::Str(str) => T::from_str(&str).map(Self),
        }
    }
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
#[serde(transparent)]
pub struct VersionReqWithJsonSchema(#[schemars(with = "String")] pub VersionReq);

impl Deref for VersionReqWithJsonSchema {
    type Target = VersionReq;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl DerefMut for VersionReqWithJsonSchema {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.0
    }
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
#[serde(transparent)]
pub struct VersionWithJsonSchema(#[schemars(with = "String")] pub Version);

impl Deref for VersionWithJsonSchema {
    type Target = Version;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl DerefMut for VersionWithJsonSchema {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.0
    }
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
#[serde(transparent)]
pub struct UrlWithJsonSchema(#[schemars(with = "String")] pub Url);

impl Deref for UrlWithJsonSchema {
    type Target = Url;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl DerefMut for UrlWithJsonSchema {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.0
    }
}


-----------------------

/src/dfx-core/src/lib.rs:
-----------------------

pub mod canister;
pub mod cli;
pub mod config;
pub mod error;
pub mod extension;
pub mod foundation;
pub mod fs;
pub mod http;
pub mod identity;
pub mod interface;
pub mod json;
pub mod network;
pub mod process;
pub mod util;

pub use interface::builder::DfxInterfaceBuilder;
pub use interface::dfx::DfxInterface;


-----------------------

/src/dfx-core/src/network/directory.rs:
-----------------------

use crate::config::model::local_server_descriptor::LocalNetworkScopeDescriptor;
use crate::config::model::network_descriptor::NetworkDescriptor;
use crate::error::canister_id_store::EnsureCohesiveNetworkDirectoryError;
use std::path::Path;

/// A cohesive network directory is one in which the directory in question contains
/// a file `network-id`, which contains the same contents as the `network-id` file
/// in the network data directory.  In this way, after `dfx start --clean`, we
/// can later clean up data in project directories.
pub fn ensure_cohesive_network_directory(
    network_descriptor: &NetworkDescriptor,
    directory: &Path,
) -> Result<(), EnsureCohesiveNetworkDirectoryError> {
    let scope = network_descriptor
        .local_server_descriptor
        .as_ref()
        .map(|d| &d.scope);

    if let Some(LocalNetworkScopeDescriptor::Shared { network_id_path }) = &scope {
        if network_id_path.is_file() {
            let network_id = crate::fs::read_to_string(network_id_path)?;
            let project_network_id_path = directory.join("network-id");
            let reset = directory.is_dir()
                && (!project_network_id_path.exists()
                    || crate::fs::read_to_string(&project_network_id_path)? != network_id);

            if reset {
                crate::fs::remove_dir_all(directory)?;
            };

            if !directory.exists() {
                crate::fs::create_dir_all(directory)?;
                crate::fs::write(&project_network_id_path, &network_id)?;
            }
        }
    }

    Ok(())
}


-----------------------

/src/dfx-core/src/network/mod.rs:
-----------------------

pub mod directory;
pub mod provider;
pub mod root_key;


-----------------------

/src/dfx-core/src/network/provider.rs:
-----------------------

use crate::config::directories::get_shared_network_data_directory;
use crate::config::model::dfinity::{
    Config, ConfigDefaults, ConfigLocalProvider, ConfigNetwork, NetworkType, NetworksConfig,
    DEFAULT_PROJECT_LOCAL_BIND, DEFAULT_SHARED_LOCAL_BIND,
};
use crate::config::model::local_server_descriptor::{
    LocalNetworkScopeDescriptor, LocalServerDescriptor,
};
use crate::config::model::network_descriptor::{
    NetworkDescriptor, NetworkTypeDescriptor, PLAYGROUND_NETWORK_NAME,
};
use crate::error::network_config::NetworkConfigError::{
    self, NetworkNotFound, NoNetworkContext, NoProvidersForNetwork, ParsePortValueFailed,
    ParseProviderUrlFailed, ReadWebserverPortFailed,
};
use crate::identity::WALLET_CONFIG_FILENAME;
use crate::util;
use lazy_static::lazy_static;
use serde_json::json;
use slog::{debug, info, warn, Logger};
use std::path::{Display, Path, PathBuf};
use std::sync::{Arc, RwLock};
use url::Url;

lazy_static! {
    static ref NETWORK_CONTEXT: Arc<RwLock<Option<String>>> = Arc::new(RwLock::new(None));
}

fn set_network_context(network: Option<String>) {
    let name = network.unwrap_or_else(|| "local".to_string());

    let mut n = NETWORK_CONTEXT.write().unwrap();
    *n = Some(name);
}

pub fn get_network_context() -> Result<String, NetworkConfigError> {
    NETWORK_CONTEXT
        .read()
        .unwrap()
        .clone()
        .ok_or(NoNetworkContext())
}

pub enum LocalBindDetermination {
    /// Use value from configuration
    AsConfigured,

    /// Get port of running server from webserver-port file
    ApplyRunningWebserverPort,
}

fn config_network_to_network_descriptor(
    network_name: &str,
    config_network: &ConfigNetwork,
    project_defaults: Option<&ConfigDefaults>,
    data_directory: PathBuf,
    local_scope: LocalNetworkScopeDescriptor,
    ephemeral_wallet_config_path: &Path,
    local_bind_determination: &LocalBindDetermination,
    default_local_bind: &str,
    legacy_pid_path: Option<PathBuf>,
) -> Result<NetworkDescriptor, NetworkConfigError> {
    match config_network {
        ConfigNetwork::ConfigNetworkProvider(network_provider) => {
            let providers = if !network_provider.providers.is_empty() {
                network_provider
                    .providers
                    .iter()
                    .map(|provider| parse_provider_url(provider))
                    .collect::<Result<_, NetworkConfigError>>()
            } else {
                Err(NoProvidersForNetwork(network_name.to_string()))
            }?;
            let playground = network_provider.playground.clone();
            let is_ic = NetworkDescriptor::is_ic(network_name, &providers);
            Ok(NetworkDescriptor {
                name: network_name.to_string(),
                providers,
                r#type: NetworkTypeDescriptor::new(
                    network_provider.r#type,
                    ephemeral_wallet_config_path,
                    playground,
                )?,
                is_ic,
                local_server_descriptor: None,
            })
        }
        ConfigNetwork::ConfigLocalProvider(local_provider) => {
            let bitcoin = local_provider
                .bitcoin
                .clone()
                .or_else(|| project_defaults.and_then(|x| x.bitcoin.clone()))
                .unwrap_or_default();
            let canister_http = local_provider
                .canister_http
                .clone()
                .or_else(|| project_defaults.and_then(|x| x.canister_http.clone()))
                .unwrap_or_default();
            let proxy = local_provider
                .proxy
                .clone()
                .or_else(|| project_defaults.and_then(|x| x.proxy.clone()))
                .unwrap_or_default();
            let replica = local_provider
                .replica
                .clone()
                .or_else(|| project_defaults.and_then(|x| x.replica.clone()))
                .unwrap_or_default();
            let playground = local_provider.playground.clone();

            let network_type = NetworkTypeDescriptor::new(
                local_provider.r#type,
                ephemeral_wallet_config_path,
                playground,
            )?;
            let bind_address = get_local_bind_address(
                local_provider,
                local_bind_determination,
                &data_directory,
                default_local_bind,
            )?;
            let provider_url = format!("http://{}", bind_address);
            let providers = vec![parse_provider_url(&provider_url)?];
            let local_server_descriptor = LocalServerDescriptor::new(
                data_directory,
                bind_address,
                bitcoin,
                canister_http,
                proxy,
                replica,
                local_scope,
                legacy_pid_path,
            )?;
            Ok(NetworkDescriptor {
                name: network_name.to_string(),
                providers,
                r#type: network_type,
                is_ic: false,
                local_server_descriptor: Some(local_server_descriptor),
            })
        }
    }
}

pub fn create_network_descriptor(
    project_config: Option<Arc<Config>>,
    shared_config: Arc<NetworksConfig>,
    network: Option<String>,
    logger: Option<Logger>,
    local_bind_determination: LocalBindDetermination,
) -> Result<NetworkDescriptor, NetworkConfigError> {
    let logger = (logger.clone()).unwrap_or_else(|| Logger::root(slog::Discard, slog::o!()));

    set_network_context(network);
    let network_name = get_network_context()?;

    create_mainnet_network_descriptor(&network_name, &logger)
        .or_else(|| {
            create_project_network_descriptor(
                &network_name,
                project_config.clone(),
                &local_bind_determination,
                &logger,
            )
        })
        .or_else(|| {
            let project_config_for_warnings_only = project_config;
            create_shared_network_descriptor(
                &network_name,
                shared_config,
                project_config_for_warnings_only,
                &local_bind_determination,
                &logger,
            )
        })
        .or_else(|| create_default_network_from_name(&network_name, &logger).map(Ok))
        .or_else(|| create_url_based_network_descriptor(&network_name))
        .unwrap_or(Err(NetworkNotFound(network_name)))
}

fn create_mainnet_network_descriptor(
    network_name: &str,
    logger: &Logger,
) -> Option<Result<NetworkDescriptor, NetworkConfigError>> {
    if network_name == "ic" {
        info!(
            logger,
            "Using built-in definition for network 'ic' (mainnet)"
        );
        Some(Ok(NetworkDescriptor::ic()))
    } else {
        None
    }
}

fn create_url_based_network_descriptor(
    network_name: &str,
) -> Option<Result<NetworkDescriptor, NetworkConfigError>> {
    parse_provider_url(network_name).ok().map(|url| {
        // Replace any non-ascii-alphanumeric characters with `_`, to create an
        // OS-friendly directory name for it.
        let name = util::network_to_pathcompat(network_name);
        let is_ic = NetworkDescriptor::is_ic(&name, &vec![url.to_string()]);
        let data_directory = get_shared_network_data_directory(network_name)?;
        let network_type = NetworkTypeDescriptor::new(
            NetworkType::Ephemeral,
            &data_directory.join(WALLET_CONFIG_FILENAME),
            None,
        )?;
        Ok(NetworkDescriptor {
            name,
            providers: vec![url],
            r#type: network_type,
            is_ic,
            local_server_descriptor: None,
        })
    })
}

fn create_shared_network_descriptor(
    network_name: &str,
    shared_config: Arc<NetworksConfig>,
    project_config_for_warnings_only: Option<Arc<Config>>,
    local_bind_determination: &LocalBindDetermination,
    logger: &Logger,
) -> Option<Result<NetworkDescriptor, NetworkConfigError>> {
    let shared_config_file_exists = shared_config.get_path().is_file();
    let shared_config_display_path = shared_config.get_path().display();
    let network = shared_config.get_interface().get_network(network_name);
    let network = match (network_name, network) {
        ("local", None) => {
            info!(
                logger,
                "Using the default configuration for the local shared network."
            );

            Some(ConfigNetwork::ConfigLocalProvider(ConfigLocalProvider {
                bind: Some(String::from(DEFAULT_SHARED_LOCAL_BIND)),
                r#type: NetworkType::Ephemeral,
                bitcoin: None,
                bootstrap: None,
                canister_http: None,
                replica: None,
                playground: None,
                proxy: None,
            }))
        }
        (network_name, None) => {
            if shared_config_file_exists {
                debug!(
                    logger,
                    "There is no shared network '{}' defined in {}",
                    &shared_config_display_path,
                    network_name
                );
            } else {
                debug!(
                    logger,
                    "There is no shared network '{}' because {} does not exist.",
                    network_name,
                    &shared_config_display_path
                );
            }
            None
        }
        (network_name, Some(network)) => {
            info!(
                logger,
                "Using shared network '{}' defined in {}",
                network_name,
                shared_config.get_path().display()
            );
            Some(network.clone())
        }
    };

    network.as_ref().map(|config_network| {
        warn_if_ignoring_project_defaults_in_shared_network(
            network_name,
            &shared_config_display_path,
            project_config_for_warnings_only,
            logger,
        );

        let data_directory = get_shared_network_data_directory(network_name)?;

        let ephemeral_wallet_config_path = data_directory.join(WALLET_CONFIG_FILENAME);

        let local_scope = LocalNetworkScopeDescriptor::shared(&data_directory);
        config_network_to_network_descriptor(
            network_name,
            config_network,
            None,
            data_directory,
            local_scope,
            &ephemeral_wallet_config_path,
            local_bind_determination,
            DEFAULT_SHARED_LOCAL_BIND,
            None,
        )
    })
}

fn warn_if_ignoring_project_defaults_in_shared_network(
    network_name: &str,
    shared_config_display_path: &Display,
    project_config_for_warnings_only: Option<Arc<Config>>,
    logger: &Logger,
) {
    if let Some(project_config_for_warnings_only) = project_config_for_warnings_only {
        let defaults = project_config_for_warnings_only.get_json().get("defaults");
        let bitcoin = defaults.and_then(|x| x.get("bitcoin")).cloned();
        let replica = defaults.and_then(|x| x.get("replica")).cloned();
        let canister_http = defaults.and_then(|x| x.get("canister_http")).cloned();

        if bitcoin.is_some() || replica.is_some() || canister_http.is_some() {
            let mut example_network_json = json!({});
            if let Some(bitcoin) = bitcoin {
                example_network_json["bitcoin"] = bitcoin;
            }
            if let Some(replica) = replica {
                example_network_json["replica"] = replica;
            }
            if let Some(canister_http) = canister_http {
                example_network_json["canister_http"] = canister_http;
            }
            let example_networks_json = json!({
                network_name: example_network_json
            });

            let example_networks_json =
                serde_json::to_string_pretty(&example_networks_json).unwrap();
            warn!(logger, "Ignoring the 'defaults' field in dfx.json because project settings never apply to the shared network.\n\
                    To apply these settings to the shared network, define them in {shared_config_display_path} like so:\n\
                    {example_networks_json}",
                    shared_config_display_path = &shared_config_display_path,
                    example_networks_json = &example_networks_json);
        }
    }
}

fn create_project_network_descriptor(
    network_name: &str,
    project_config: Option<Arc<Config>>,
    local_bind_determination: &LocalBindDetermination,
    logger: &Logger,
) -> Option<Result<NetworkDescriptor, NetworkConfigError>> {
    if let Some(config) = project_config {
        if let Some(config_network) = config.get_config().get_network(network_name) {
            info!(
                logger,
                "Using project-specific network '{}' defined in {}",
                network_name,
                config.get_path().display(),
            );

            let temp_path = match config.get_temp_path() {
                Ok(temp_path) => temp_path,
                Err(e) => {
                    return Some(Err(NetworkConfigError::GetTempPath(e)));
                }
            };
            let data_directory = temp_path.join("network").join(network_name);
            let legacy_pid_path = Some(temp_path.join("pid"));
            let ephemeral_wallet_config_path = temp_path.join("local").join(WALLET_CONFIG_FILENAME);
            Some(config_network_to_network_descriptor(
                network_name,
                config_network,
                Some(config.get_config().get_defaults()),
                data_directory,
                LocalNetworkScopeDescriptor::Project,
                &ephemeral_wallet_config_path,
                local_bind_determination,
                DEFAULT_PROJECT_LOCAL_BIND,
                legacy_pid_path,
            ))
        } else {
            debug!(
                logger,
                "There is no project-specific network '{}' defined in {}",
                network_name,
                config.get_path().display()
            );
            None
        }
    } else {
        debug!(
            logger,
            "There is no project-specific network '{}' because there is no project (no dfx.json).",
            network_name
        );
        None
    }
}

fn create_default_network_from_name(
    network_name: &str,
    logger: &Logger,
) -> Option<NetworkDescriptor> {
    match network_name {
        PLAYGROUND_NETWORK_NAME => {
            debug!(logger, "Using default definition for network 'playground'.");
            Some(NetworkDescriptor::default_playground_network())
        }
        _ => None,
    }
}

fn get_local_bind_address(
    local_provider: &ConfigLocalProvider,
    local_bind_determination: &LocalBindDetermination,
    data_directory: &Path,
    default_local_bind: &str,
) -> Result<String, NetworkConfigError> {
    match local_bind_determination {
        LocalBindDetermination::AsConfigured => Ok(local_provider
            .bind
            .clone()
            .unwrap_or_else(|| default_local_bind.to_string())),
        LocalBindDetermination::ApplyRunningWebserverPort => {
            get_running_webserver_bind_address(data_directory, local_provider, default_local_bind)
        }
    }
}

fn get_running_webserver_bind_address(
    data_directory: &Path,
    local_provider: &ConfigLocalProvider,
    default_local_bind: &str,
) -> Result<String, NetworkConfigError> {
    let local_bind = local_provider
        .bind
        .clone()
        .unwrap_or_else(|| default_local_bind.to_string());
    let path = data_directory.join("webserver-port");
    if path.exists() {
        let s = crate::fs::read_to_string(&path).map_err(ReadWebserverPortFailed)?;
        let s = s.trim();
        if s.is_empty() {
            Ok(local_bind)
        } else {
            let port = s
                .parse::<u16>()
                .map_err(|e| ParsePortValueFailed(Box::new(path), Box::new(e)))?;
            // converting to a socket address, and then setting the port,
            // will unfortunately transform "localhost:port" to "[::1]:{port}",
            // which the agent fails to connect with.
            let host = match local_bind.rfind(':') {
                None => local_bind.clone(),
                Some(index) => local_bind[0..index].to_string(),
            };
            Ok(format!("{}:{}", host, port))
        }
    } else {
        Ok(local_bind)
    }
}

pub fn command_line_provider_to_url(s: &str) -> Result<String, NetworkConfigError> {
    match parse_provider_url(s) {
        Ok(url) => Ok(url),
        Err(original_error) => {
            let prefixed_with_http = format!("http://{}", s);
            parse_provider_url(&prefixed_with_http).or(Err(original_error))
        }
    }
}

pub fn parse_provider_url(url: &str) -> Result<String, NetworkConfigError> {
    Url::parse(url)
        .map(|_| String::from(url))
        .map_err(|e| ParseProviderUrlFailed(Box::new(url.to_string()), e))
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::model::bitcoin_adapter::BitcoinAdapterLogLevel;
    use crate::config::model::canister_http_adapter::HttpAdapterLogLevel;
    use crate::config::model::dfinity::ReplicaSubnetType::{System, VerifiedApplication};
    use crate::config::model::dfinity::{
        to_socket_addr, ConfigDefaultsBitcoin, ConfigDefaultsCanisterHttp, ConfigDefaultsReplica,
        ReplicaLogLevel,
    };
    use std::fs;
    use std::net::SocketAddr;
    use std::str::FromStr;

    #[test]
    fn use_default_if_no_webserver_port_file() {
        // no file - use default
        test_with_webserver_port_file_contents(
            LocalBindDetermination::ApplyRunningWebserverPort,
            None,
            "localhost:8000",
        );
    }

    #[test]
    fn ignore_running_webserver_port_if_not_requested() {
        // port file present and populated, but not asked for: ignored
        test_with_webserver_port_file_contents(
            LocalBindDetermination::AsConfigured,
            Some("1234"),
            "localhost:8000",
        );
    }

    #[test]
    fn use_port_if_have_file() {
        // port file present and populated: reflected in socket address
        test_with_webserver_port_file_contents(
            LocalBindDetermination::ApplyRunningWebserverPort,
            Some("1234"),
            "localhost:1234",
        );
    }

    #[test]
    fn ignore_port_if_not_requested() {
        // port file present and populated, but not asked for: ignored
        test_with_webserver_port_file_contents(
            LocalBindDetermination::AsConfigured,
            Some("1234"),
            "localhost:8000",
        );
    }

    #[test]
    fn extra_whitespace_in_webserver_port_is_ok() {
        // trailing newline is ok
        test_with_webserver_port_file_contents(
            LocalBindDetermination::ApplyRunningWebserverPort,
            Some("  \n3456 \n"),
            "localhost:3456",
        );
    }

    #[test]
    fn use_running_webserver_address() {
        // no file - use default
        test_with_webserver_port_file_contents(
            LocalBindDetermination::ApplyRunningWebserverPort,
            None,
            "localhost:8000",
        );
    }

    #[test]
    fn ignore_empty_webserver_port_file() {
        // empty is ok: ignore
        test_with_webserver_port_file_contents(
            LocalBindDetermination::ApplyRunningWebserverPort,
            Some(""),
            "localhost:8000",
        );
    }
    #[test]
    fn ignore_whitespace_only_webserver_port_file() {
        // just whitespace is ok: ignore
        test_with_webserver_port_file_contents(
            LocalBindDetermination::ApplyRunningWebserverPort,
            Some("\n"),
            "localhost:8000",
        );
    }

    fn test_with_webserver_port_file_contents(
        local_bind_determination: LocalBindDetermination,
        webserver_port_contents: Option<&str>,
        expected_socket_addr: &str,
    ) {
        let temp_dir = tempfile::tempdir().unwrap();
        let project_dir = temp_dir.path().join("project");
        fs::create_dir_all(&project_dir).unwrap();
        let project_dfx_json = project_dir.join("dfx.json");
        std::fs::write(
            project_dfx_json,
            r#"{
            "networks": {
                "local": {
                    "bind": "localhost:8000"
                }
            }
        }"#,
        )
        .unwrap();

        if let Some(webserver_port_contents) = webserver_port_contents {
            let dot_dfx_dir = project_dir.join(".dfx");
            let network_data_dir = dot_dfx_dir.join("network").join("local");
            fs::create_dir_all(&network_data_dir).unwrap();
            std::fs::write(
                network_data_dir.join("webserver-port"),
                webserver_port_contents,
            )
            .unwrap();
        }

        let config = Config::from_dir(&project_dir, None).unwrap().unwrap();
        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            local_bind_determination,
        )
        .unwrap();

        assert_eq!(
            network_descriptor
                .local_server_descriptor()
                .unwrap()
                .bind_address,
            to_socket_addr(expected_socket_addr).unwrap()
        );
    }

    #[test]
    fn config_with_local_bind_addr() {
        let config = Config::from_str(
            r#"{
            "networks": {
                "local": {
                    "bind": "localhost:8000"
                }
            }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();

        assert_eq!(
            network_descriptor
                .local_server_descriptor()
                .unwrap()
                .bind_address,
            to_socket_addr("localhost:8000").unwrap()
        );
    }

    #[test]
    fn config_with_invalid_local_bind_addr() {
        let config = Config::from_str(
            r#"{
            "networks": {
                "local": {
                    "bind": "not a valid bind address"
                }
            }
        }"#,
        )
        .unwrap();

        let result = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        );
        assert!(result.is_err());
    }

    #[test]
    fn config_returns_local_bind_address_if_no_local_network() {
        let config = Config::from_str(
            r#"{
            "networks": {
            }
        }"#,
        )
        .unwrap();
        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();

        assert_eq!(
            network_descriptor
                .local_server_descriptor()
                .unwrap()
                .bind_address,
            to_socket_addr("127.0.0.1:4943").unwrap()
        );
    }

    #[test]
    fn config_returns_local_bind_address_if_no_networks() {
        let config = Config::from_str(
            r#"{
        }"#,
        )
        .unwrap();
        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();

        assert_eq!(
            network_descriptor
                .local_server_descriptor()
                .unwrap()
                .bind_address,
            to_socket_addr("127.0.0.1:4943").unwrap()
        );
    }

    #[test]
    fn get_bitcoin_config() {
        let config = Config::from_str(
            r#"{
              "defaults": {
                "bitcoin": {
                  "enabled": true,
                  "nodes": ["127.0.0.1:18444"],
                  "log_level": "info"
                }
              },
              "networks": {
                "local": {
                    "bind": "localhost:8000"
                }
              }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();
        let bitcoin_config = &network_descriptor
            .local_server_descriptor()
            .unwrap()
            .bitcoin;

        assert_eq!(
            bitcoin_config,
            &ConfigDefaultsBitcoin {
                enabled: true,
                nodes: Some(vec![SocketAddr::from_str("127.0.0.1:18444").unwrap()]),
                log_level: BitcoinAdapterLogLevel::Info,
                ..ConfigDefaultsBitcoin::default()
            }
        );
    }

    #[test]
    fn get_bitcoin_config_default_log_level() {
        let config = Config::from_str(
            r#"{
              "defaults": {
                "bitcoin": {
                  "enabled": true,
                  "nodes": ["127.0.0.1:18444"]
                }
              },
              "networks": {
                "local": {
                    "bind": "localhost:8000"
                }
              }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();
        let bitcoin_config = &network_descriptor
            .local_server_descriptor()
            .unwrap()
            .bitcoin;

        assert_eq!(
            bitcoin_config,
            &ConfigDefaultsBitcoin {
                enabled: true,
                nodes: Some(vec![SocketAddr::from_str("127.0.0.1:18444").unwrap()]),
                log_level: BitcoinAdapterLogLevel::Info, // A default log level of "info" is assumed
                ..ConfigDefaultsBitcoin::default()
            }
        );
    }

    #[test]
    fn get_bitcoin_config_debug_log_level() {
        let config = Config::from_str(
            r#"{
              "defaults": {
                "bitcoin": {
                  "enabled": true,
                  "log_level": "debug"
                }
              },
              "networks": {
                "local": {
                    "bind": "localhost:8000"
                }
              }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();
        let bitcoin_config = &network_descriptor
            .local_server_descriptor()
            .unwrap()
            .bitcoin;

        assert_eq!(
            bitcoin_config,
            &ConfigDefaultsBitcoin {
                enabled: true,
                log_level: BitcoinAdapterLogLevel::Debug,
                ..ConfigDefaultsBitcoin::default()
            }
        );
    }

    #[test]
    fn bitcoin_config_on_local_network() {
        let config = Config::from_str(
            r#"{
              "networks": {
                "local": {
                  "bind": "127.0.0.1:8000",
                  "bitcoin": {
                    "enabled": true,
                    "nodes": ["127.0.0.1:18444"],
                    "log_level": "info"
                  }
                }
              }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();
        let bitcoin_config = &network_descriptor
            .local_server_descriptor()
            .unwrap()
            .bitcoin;

        assert_eq!(
            bitcoin_config,
            &ConfigDefaultsBitcoin {
                enabled: true,
                nodes: Some(vec![SocketAddr::from_str("127.0.0.1:18444").unwrap()]),
                log_level: BitcoinAdapterLogLevel::Info,
                ..ConfigDefaultsBitcoin::default()
            }
        );
    }

    #[test]
    fn replica_config_on_local_network() {
        let config = Config::from_str(
            r#"{
              "networks": {
                "local": {
                  "bind": "127.0.0.1:8000",
                  "replica": {
                    "subnet_type": "verifiedapplication",
                    "port": 17001,
                    "log_level": "trace"
                  }
                }
              }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();
        let replica_config = &network_descriptor
            .local_server_descriptor()
            .unwrap()
            .replica;

        assert_eq!(
            replica_config,
            &ConfigDefaultsReplica {
                subnet_type: Some(VerifiedApplication),
                port: Some(17001),
                log_level: Some(ReplicaLogLevel::Trace)
            }
        );
    }

    #[test]
    fn replica_config_on_local_network_overrides_default() {
        // Defaults are not combined.
        // Here the 'default' level specifies a port, but it's ignored due to the
        // network-level setting.
        let config = Config::from_str(
            r#"{
              "defaults": {
                "replica": {
                  "port": 13131
                }
              },
              "networks": {
                "local": {
                  "bind": "127.0.0.1:8000",
                  "replica": {
                    "subnet_type": "system"
                  }
                }
              }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();
        let replica_config = &network_descriptor
            .local_server_descriptor()
            .unwrap()
            .replica;

        assert_eq!(
            replica_config,
            &ConfigDefaultsReplica {
                subnet_type: Some(System),
                port: None,
                log_level: None
            }
        );
    }

    #[test]
    fn canister_http_config_on_local_network() {
        let config = Config::from_str(
            r#"{
              "networks": {
                "local": {
                  "bind": "127.0.0.1:8000",
                  "canister_http": {
                    "enabled": true,
                    "log_level": "debug"
                  }
                }
              }
        }"#,
        )
        .unwrap();

        let network_descriptor = create_network_descriptor(
            Some(Arc::new(config)),
            Arc::new(NetworksConfig::new().unwrap()),
            None,
            None,
            LocalBindDetermination::AsConfigured,
        )
        .unwrap();
        let canister_http_config = &network_descriptor
            .local_server_descriptor()
            .unwrap()
            .canister_http;

        assert_eq!(
            canister_http_config,
            &ConfigDefaultsCanisterHttp {
                enabled: true,
                log_level: HttpAdapterLogLevel::Debug
            }
        );
    }

    #[test]
    fn url_is_url() {
        assert_eq!(
            command_line_provider_to_url("http://127.0.0.1:8000").unwrap(),
            "http://127.0.0.1:8000"
        );
    }

    #[test]
    fn addr_and_port_to_url() {
        assert_eq!(
            command_line_provider_to_url("127.0.0.1:8000").unwrap(),
            "http://127.0.0.1:8000"
        );
    }
}


-----------------------

/src/dfx-core/src/network/root_key.rs:
-----------------------

use crate::{
    config::model::network_descriptor::NetworkDescriptor, error::root_key::FetchRootKeyError,
};
use ic_agent::Agent;

#[deprecated(note = "use fetch_root_key_when_non_mainnet() instead")]
pub async fn fetch_root_key_when_local(
    agent: &Agent,
    network: &NetworkDescriptor,
) -> Result<(), FetchRootKeyError> {
    fetch_root_key_when_non_mainnet(agent, network).await
}

pub async fn fetch_root_key_when_non_mainnet(
    agent: &Agent,
    network: &NetworkDescriptor,
) -> Result<(), FetchRootKeyError> {
    if !network.is_ic {
        agent
            .fetch_root_key()
            .await
            .map_err(FetchRootKeyError::AgentError)?;
    }
    Ok(())
}

#[deprecated(note = "use fetch_root_key_when_non_mainnet_or_error() instead")]
pub async fn fetch_root_key_when_local_or_error(
    agent: &Agent,
    network: &NetworkDescriptor,
) -> Result<(), FetchRootKeyError> {
    fetch_root_key_when_non_mainnet_or_error(agent, network).await
}

pub async fn fetch_root_key_when_non_mainnet_or_error(
    agent: &Agent,
    network: &NetworkDescriptor,
) -> Result<(), FetchRootKeyError> {
    if !network.is_ic {
        agent
            .fetch_root_key()
            .await
            .map_err(FetchRootKeyError::AgentError)
    } else {
        Err(FetchRootKeyError::MustNotFetchRootKeyOnMainnet)
    }
}


-----------------------

/src/dfx-core/src/process/mod.rs:
-----------------------

use crate::error::process::ProcessError;
use std::process::{Command, ExitStatus};

pub fn execute_process(cmd: &mut Command) -> Result<ExitStatus, ProcessError> {
    cmd.status()
        .map_err(|e| ProcessError::ExecutionFailed(cmd.get_program().to_owned(), e))
}


-----------------------

/src/dfx-core/src/util/mod.rs:
-----------------------

use std::{borrow::Cow, time::Duration};

use schemars::{
    gen::SchemaGenerator,
    schema::{InstanceType, Metadata, Schema, SchemaObject, StringValidation},
    JsonSchema,
};

pub fn network_to_pathcompat(network_name: &str) -> String {
    network_name.replace(|c: char| !c.is_ascii_alphanumeric(), "_")
}

pub fn expiry_duration() -> Duration {
    // 5 minutes is max ingress timeout
    // 4 minutes accounts for possible replica drift
    Duration::from_secs(60 * 4)
}

pub struct ByteSchema;

impl JsonSchema for ByteSchema {
    fn schema_name() -> String {
        "Byte".to_string()
    }
    fn schema_id() -> Cow<'static, str> {
        Cow::Borrowed("byte_unit::Byte")
    }
    fn json_schema(_: &mut SchemaGenerator) -> Schema {
        Schema::Object(SchemaObject {
            instance_type: Some(vec![InstanceType::Integer, InstanceType::String].into()),
            number: None,
            string: Some(Box::new(StringValidation {
                pattern: Some("^[0-9]+( *([KkMmGgTtPpEeZzYy]i?)?[Bb])?$".to_string()),
                ..Default::default()
            })),
            metadata: Some(Box::new(Metadata {
                title: Some("Byte Count".to_string()),
                description: Some("A quantity of bytes. Representable either as an integer, or as an SI unit string".to_string()),
                examples: vec![72.into(), "2KB".into(), "4 MiB".into()],
                ..Default::default()
            })),
            ..Default::default()
        })
    }
}


-----------------------

/src/dfx/Cargo.toml:
-----------------------

[package]
name = "dfx"
version = "0.24.0"
authors.workspace = true
edition.workspace = true
repository.workspace = true
license.workspace = true
rust-version.workspace = true
build = "assets/build.rs"

[[bin]]
name = "dfx"
path = "src/main.rs"

[build-dependencies]
backoff = { version = "0.4.0", features = ["futures", "tokio"] }
bytes = "1"
flate2 = { version = "1.0.11", default-features = false, features = [
    "zlib-ng",
] }
hex = "0.4.3"
reqwest.workspace = true
serde = { version = "1.0", features = ["derive"] }
sha2 = "0.10.6"
tar = "0.4.26"
tokio = { version = "1.24.2", features = ["full"] }
toml = "0.7.3"
walkdir = "2.3.2"

[dependencies]
actix = "0.13.0"
aes-gcm.workspace = true
anstyle.workspace = true
anyhow.workspace = true
apply-patch.path = "../lib/apply-patch"
argon2.workspace = true
backoff.workspace = true
base64.workspace = true
byte-unit = { workspace = true, features = ["serde"] }
bytes.workspace = true
candid = { workspace = true }
candid_parser = { workspace = true, features = ["random", "assist"] }
cargo_metadata = "0.18.1"
clap = { workspace = true, features = [
    "derive",
    "env",
    "unstable-styles",
    "wrap_help",
] }
clap_complete = { workspace = true }
console = "0.15.0"
crc32fast = "1.3.2"
crossbeam = "0.8.1"
ctrlc = { version = "3.2.1", features = ["termination"] }
dfx-core.workspace = true
dialoguer = { workspace = true, features = ["fuzzy-select"] }
directories-next.workspace = true
flate2 = { workspace = true, default-features = false, features = ["zlib-ng"] }
fn-error-context = "0.2.0"
futures-util = "0.3.21"
futures.workspace = true
handlebars.workspace = true
hex = { workspace = true, features = ["serde"] }
humantime.workspace = true
hyper-rustls = { version = "0.24.1", default-features = false, features = [
    "webpki-roots",
    "http2",
] }
ic-agent.workspace = true
ic-asset.workspace = true
ic-cdk.workspace = true
ic-identity-hsm.workspace = true
ic-utils.workspace = true
ic-wasm = "0.8.0"
icrc-ledger-types = "0.1.5"
idl2json = "0.10.1"
indicatif = "0.16.0"
itertools.workspace = true
json-patch = "1.0.0"
keyring.workspace = true
lazy_static.workspace = true
mime.workspace = true
mime_guess.workspace = true
num-traits.workspace = true
os_str_bytes = { version = "6.3.0", features = ["conversions"] }
patch = "0.7.0"
pem.workspace = true
petgraph = "0.6.0"
rand = "0.8.5"
regex = "1.5.5"
reqwest = { workspace = true, features = ["blocking", "json"] }
ring.workspace = true
rust_decimal = "1.22.0"
rustls-webpki = "0.101.4"
schemars.workspace = true
sec1 = { workspace = true, features = ["std"] }
semver = { workspace = true }
serde.workspace = true
serde_bytes.workspace = true
serde_cbor.workspace = true
serde_json.workspace = true
sha2.workspace = true
shell-words = "1.1.0"
slog = { workspace = true, features = ["max_level_trace"] }
slog-async.workspace = true
slog-term.workspace = true
socket2 = "0.5.5"
supports-color = "2.1.0"
sysinfo = "0.28.4"
tar.workspace = true
tempfile.workspace = true
term = "0.7.0"
thiserror.workspace = true
time = { workspace = true, features = [
    "serde",
    "macros",
    "serde-human-readable",
] }
tokio = { workspace = true, features = ["full"] }
url.workspace = true
walkdir.workspace = true
walrus = "0.21.1"
which = "4.2.5"
ci_info = "0.14"

[target.'cfg(windows)'.dependencies]
junction = "1.0.0"

[target.'cfg(unix)'.dependencies]
pocket-ic = { git = "https://github.com/dfinity/ic", rev = "179973553248415fc85679d853b48b0e0ec231c6" }

[dev-dependencies]
env_logger = "0.10"
proptest = "1.0"
mockito = "0.31.0"
tempfile = "3.1.0"


-----------------------

= Assets

The `new_project_*/` directories contain all the files of a new project. These are tar gzipped at build time
and injected into the binary.

The following strings are replaced:

- `{project_name}` / `+__project_name__+` => the project name.
- `{dfx_version}` => the DFX version used to create the project.

Also, files that start with `+++__dot__+++` will be replaced with `.`.


-----------------------

/src/dfx/assets/build.rs:
-----------------------

use flate2::write::GzEncoder;
use flate2::Compression;
use serde::Deserialize;
use sha2::{Digest, Sha256};
use std::collections::HashMap;
use std::fs::{read_to_string, File};
use std::io::{Read, Write};
use std::path::{Path, PathBuf};
use std::process::{Command, Stdio};
use std::{env, fs};
use walkdir::WalkDir;

mod prepare_assets;

const INPUTS: &[&str] = &[
    "nix/sources.json",
    "src/dfx/assets/prepare_assets.rs",
    "src/dfx/assets/build.rs",
    "src/distributed/assetstorage.did",
    "src/distributed/assetstorage.wasm.gz",
    "src/distributed/ui.did",
    "src/distributed/ui.wasm",
    "src/distributed/wallet.did",
    "src/distributed/wallet.wasm.gz",
];

fn calculate_hash_of_inputs(project_root_path: &Path) -> String {
    let mut sha256 = Sha256::new();

    for input in INPUTS {
        let pathname = project_root_path.join(input);
        let mut f = File::open(pathname).expect("unable to open input file");
        let mut buffer = Vec::new();
        f.read_to_end(&mut buffer)
            .expect("unable to read input file");
        sha256.update(&buffer);
    }

    hex::encode(sha256.finalize())
}

fn get_project_root_path() -> PathBuf {
    let project_root_dir = format!("{}/../..", env!("CARGO_MANIFEST_DIR"));
    PathBuf::from(project_root_dir)
        .canonicalize()
        .expect("Unable to determine project root")
}

#[derive(Deserialize, Clone)]
struct Source {
    url: String,
    sha256: String,
}

impl Source {
    fn sha256(&self) -> Vec<u8> {
        hex::decode(&self.sha256).expect("Invalid SHA-256")
    }
}

#[derive(Deserialize)]
struct Sources {
    #[serde(rename = "x86_64-linux")]
    x86_64_linux: HashMap<String, Source>,
    #[serde(rename = "x86_64-darwin")]
    x86_64_darwin: HashMap<String, Source>,
    #[serde(rename = "replica-rev")]
    replica_rev: String,
}

fn find_assets(sources: Sources) -> PathBuf {
    println!("cargo:rerun-if-env-changed=DFX_ASSETS");
    if let Ok(a) = env::var("DFX_ASSETS") {
        PathBuf::from(a)
    } else {
        let project_root_path = get_project_root_path();
        for input in INPUTS {
            println!(
                "cargo:rerun-if-changed={}",
                project_root_path.join(input).display()
            );
        }
        let hash_of_inputs = calculate_hash_of_inputs(&project_root_path);

        let out_dir = PathBuf::from(env::var("OUT_DIR").unwrap());
        let dfx_assets_path = out_dir.join("dfx-assets");
        let last_hash_of_inputs_path = out_dir.join("dfx-assets-inputs-hash");

        if dfx_assets_path.exists() && last_hash_of_inputs_path.exists() {
            let last_hash_of_inputs = read_to_string(&last_hash_of_inputs_path)
                .expect("unable to read last hash of inputs");
            if last_hash_of_inputs == hash_of_inputs {
                return dfx_assets_path;
            }
        }

        let source_set = match (
            &*env::var("CARGO_CFG_TARGET_ARCH").unwrap(),
            &*env::var("CARGO_CFG_TARGET_OS").unwrap(),
        ) {
            ("x86_64" | "aarch64", "macos") => sources.x86_64_darwin, // rosetta
            ("x86_64", "linux" | "windows") => sources.x86_64_linux,
            (arch, os) => panic!("Unsupported OS type {arch}-{os}"),
        };
        prepare_assets::prepare(&dfx_assets_path, source_set);

        fs::write(last_hash_of_inputs_path, hash_of_inputs)
            .expect("unable to write last hash of inputs");
        dfx_assets_path
    }
}

fn add_asset_archive(fn_name: &str, f: &mut File, assets_path: &Path) {
    let filename_tgz = format!("{}.tgz", fn_name);

    let prebuilt_file = assets_path.join(&filename_tgz);
    println!("cargo:rerun-if-changed={}", prebuilt_file.display());

    let out_dir = env::var("OUT_DIR").unwrap();
    let tgz_path = Path::new(&out_dir).join(&filename_tgz);

    if tgz_path.exists() {
        // This avoids PermissionDenied errors.
        std::fs::remove_file(&tgz_path).unwrap();
    }
    std::fs::copy(prebuilt_file, tgz_path).unwrap();

    write_archive_accessor(fn_name, f);
}

fn add_assets_from_directory(fn_name: &str, f: &mut File, path: &str) {
    for file in WalkDir::new(path).into_iter().filter_map(|x| x.ok()) {
        println!("cargo:rerun-if-changed={}", file.path().display())
    }
    let out_dir = env::var("OUT_DIR").unwrap();
    let tgz_path = Path::new(&out_dir).join(format!("{}.tgz", fn_name));

    let tar_gz = File::create(tgz_path).unwrap();
    let enc = GzEncoder::new(tar_gz, Compression::default());
    let mut tar = tar::Builder::new(enc);
    tar.append_dir_all("", path).unwrap();

    write_archive_accessor(fn_name, f);
}

fn write_archive_accessor(fn_name: &str, f: &mut File) {
    f.write_all(
        format!(
            "
        pub fn {fn_name}() -> Result<Archive<GzDecoder<&'static [u8]>>> {{
            let tar = GzDecoder::new(&include_bytes!(\"{fn_name}.tgz\")[..]);
            let archive = Archive::new(tar);
            Ok(archive)
        }}
    ",
            fn_name = fn_name,
        )
        .as_bytes(),
    )
    .unwrap();
}

/// Gets a git tag with the least number of revs between HEAD of current branch and the tag,
/// and combines is with SHA of the HEAD commit. Example of expected output: `0.12.0-beta.1-b9ace030`
fn get_git_hash() -> Result<String, std::io::Error> {
    let mut latest_tag = String::from("0");
    let mut latest_distance = u128::MAX;
    let tags = Command::new("git")
        .arg("tag")
        .stdout(Stdio::piped())
        .spawn()?
        .wait_with_output()?
        .stdout;
    for tag in String::from_utf8_lossy(&tags).split_whitespace() {
        let output = Command::new("git")
            .arg("rev-list")
            .arg("--count")
            .arg(format!("{}..HEAD", tag))
            .arg(tag)
            .stdout(Stdio::piped())
            .spawn()?
            .wait_with_output()?
            .stdout;
        if let Some(count) = String::from_utf8_lossy(&output)
            .split_whitespace()
            .next()
            .and_then(|v| v.parse::<u128>().ok())
        {
            if count < latest_distance {
                latest_tag = String::from(tag);
                latest_distance = count;
            }
        }
    }
    let head_commit_sha = Command::new("git")
        .arg("rev-parse")
        .arg("--short")
        .arg("HEAD")
        .output()?
        .stdout;
    let head_commit_sha = String::from_utf8_lossy(&head_commit_sha);
    let is_dirty = !Command::new("git")
        .arg("status")
        .arg("--porcelain")
        .output()?
        .stdout
        .is_empty();

    Ok(format!(
        "{latest_tag}+rev{count}.{head_status}{head_commit_sha}",
        count = latest_distance,
        head_status = if is_dirty { "dirty-" } else { "" }
    ))
}

fn add_assets(sources: Sources) {
    let out_dir = env::var("OUT_DIR").unwrap();
    let loader_path = Path::new(&out_dir).join("load_assets.rs");
    let mut f = File::create(loader_path).unwrap();

    f.write_all(
        b"
        use flate2::read::GzDecoder;
        use std::io::Result;
        use std::vec::Vec;
        use tar::Archive;

    ",
    )
    .unwrap();

    let dfx_assets = find_assets(sources);
    add_asset_archive("binary_cache", &mut f, &dfx_assets);
    add_asset_archive("assetstorage_canister", &mut f, &dfx_assets);
    add_asset_archive("wallet_canister", &mut f, &dfx_assets);
    add_asset_archive("ui_canister", &mut f, &dfx_assets);
    add_asset_archive("btc_canister", &mut f, &dfx_assets);
    add_assets_from_directory("language_bindings", &mut f, "assets/language_bindings");
    add_assets_from_directory(
        "new_project_motoko_files",
        &mut f,
        "assets/project_templates/motoko",
    );
    add_assets_from_directory(
        "new_project_rust_files",
        &mut f,
        "assets/project_templates/rust",
    );
    add_assets_from_directory(
        "new_project_base_files",
        &mut f,
        "assets/project_templates/base",
    );
    add_assets_from_directory(
        "new_project_js_files",
        &mut f,
        "assets/project_templates/any_js",
    );
    add_assets_from_directory(
        "new_project_kybra_files",
        &mut f,
        "assets/project_templates/kybra",
    );
    add_assets_from_directory(
        "new_project_azle_files",
        &mut f,
        "assets/project_templates/azle",
    );
    add_assets_from_directory(
        "new_project_vanillajs_files",
        &mut f,
        "assets/project_templates/vanilla_js",
    );
    add_assets_from_directory(
        "new_project_vanillajs_test_files",
        &mut f,
        "assets/project_templates/vanilla_js_tests",
    );
    add_assets_from_directory(
        "new_project_react_files",
        &mut f,
        "assets/project_templates/react",
    );
    add_assets_from_directory(
        "new_project_react_test_files",
        &mut f,
        "assets/project_templates/react_tests",
    );
    add_assets_from_directory(
        "new_project_svelte_files",
        &mut f,
        "assets/project_templates/svelte",
    );
    add_assets_from_directory(
        "new_project_svelte_test_files",
        &mut f,
        "assets/project_templates/svelte_tests",
    );
    add_assets_from_directory(
        "new_project_vue_files",
        &mut f,
        "assets/project_templates/vue",
    );
    add_assets_from_directory(
        "new_project_vue_test_files",
        &mut f,
        "assets/project_templates/vue_tests",
    );
    add_assets_from_directory(
        "new_project_assets_files",
        &mut f,
        "assets/project_templates/simple_assets",
    );
    add_assets_from_directory(
        "new_project_internet_identity_files",
        &mut f,
        "assets/project_templates/internet_identity",
    );
    add_assets_from_directory(
        "new_project_bitcoin_files",
        &mut f,
        "assets/project_templates/bitcoin",
    );
}

/// Use a verion based on environment variable,
/// or the latest git tag plus sha of current git HEAD at time of build,
/// or let the cargo.toml version.
fn define_dfx_version() {
    if let Ok(v) = std::env::var("DFX_VERSION") {
        // If the version is passed in the environment, use that.
        // Used by the release process in .github/workflows/publish.yml
        println!("cargo:rustc-env=CARGO_PKG_VERSION={}", v);
    } else if let Ok(git) = get_git_hash() {
        // If the version isn't passed in the environment, use the git describe version.
        // Used when building from source.
        println!("cargo:rustc-env=CARGO_PKG_VERSION={}", git);
    } else {
        // Nothing to do here, as there is no GIT. We keep the CARGO_PKG_VERSION.
    }
}

fn define_replica_rev(replica_rev: &str) {
    println!("cargo:rustc-env=DFX_ASSET_REPLICA_REV={}", replica_rev);
}

fn main() {
    let sources: Sources = toml::from_str(
        &fs::read_to_string("assets/dfx-asset-sources.toml")
            .expect("unable to read dfx-asset-sources.toml"),
    )
    .expect("unable to parse dfx-asset-sources.toml");
    define_replica_rev(&sources.replica_rev);
    add_assets(sources);
    define_dfx_version();
}


-----------------------


                 8HG5yoXWQ                            Q#DXZG$b%Q
            Rf^_.--..----.'=fR                    0ol!~~~~~~~~~:~>fb
         Qz:---------..------._)W              R{;~~~~~~~~~~~:,____'_]%
       Q)'._~!++=+;!!~_--------..~y          X^~~~~~~~~~~!+;~___'''''.-r&
      H,~>)TTv)f6&     Q2*_--.-----_}Q    Qy;~~~~~~~~*40      go^'..-````y
     k=)TTTv)5Q            bi'.------'{  y!~~~~~~~^2Q            Q/-```   c
    Q)]TT))/b                Q{_-------_!~~~~~~~>q                 $-      K
    ATvv)/iS                    C_------.'_~~~<W                    a      ~
    y)//il?%                      C'-------':h                             `
    C/l??**#                      5:'..------!W                     Q      -
    $?***<>z                    Z!,,,_'-------.=b                   ]      ^
     ]*<>>rry                Qy!,,,,,,,^<.-------!$                v      `%
     Qi>rr^^=<A            d)~,,,,,,,=D  %>.-------_vb           y'      .H
      Q]^^=+++;=JZRQ   %X]!,,,,,,,,=X      R>.-------._*S% Q&Df=`     ``~R
        q<+;!!!!~~~~~~:,,,,,,,,,:<q          Q/'.------.---`````````--_C
          WJ!!~~~~~::,,,,,,,,:r5Q               k>'--.-------------.;G
             Wyi+~:,,,,,,~>zqQ                     bz^_.------._=c$Q
                  QQ8#8Q                                Q#WRBQ



-----------------------

/src/dfx/assets/dfx-asset-sources.toml:
-----------------------

# generated by write-dfx-asset-sources.sh
replica-rev = 'c43a4880199c00135c8415957851e823b3fb769e'

[x86_64-darwin.ic-admin]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-admin.gz'
sha256 = 'e1a3dae787f694deea59e0f906ee08d44bf5fe1b394b7b92218a1077670f76d5'

[x86_64-darwin.ic-btc-adapter]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-btc-adapter.gz'
sha256 = '9eff8b53ba92445419b43329dfd3fa11c726abced42517836878628c3ea8d219'

[x86_64-darwin.ic-https-outcalls-adapter]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-https-outcalls-adapter.gz'
sha256 = 'eae8a95f7ec1a3dc6176d4726c9eebf57b2756fa2fb5feaa0280c725c15f27ce'

[x86_64-darwin.ic-nns-init]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-nns-init.gz'
sha256 = '8f53cf4248ef82b5948b02160653ef85968034a1e52c91fff978dfb352026f31'

[x86_64-darwin.ic-starter]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/ic-starter.gz'
sha256 = 'b55fd9caaa875499c33cb0603d7ecccb386073702eb21d0e68b88ba8f376a3c9'

[x86_64-darwin.motoko]
url = 'https://github.com/dfinity/motoko/releases/download/0.13.0/motoko-Darwin-x86_64-0.13.0.tar.gz'
sha256 = '8c61abbd0fd14b4e1140dd582fca29652151ba29a88fd7436eda1302e1e676ae'
# The replica, canister_sandbox and compiler_sandbox binaries must have the same revision.

[x86_64-darwin.replica]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/replica.gz'
sha256 = 'dffedf2fd8862697f81c5eb77f6b95e3fa97e1a4cc8e7f83cde1bbd3c72aa531'
# The replica, canister_sandbox and compiler_sandbox binaries must have the same revision.

[x86_64-darwin.canister_sandbox]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/canister_sandbox.gz'
sha256 = '052086910ab55984e4a0689dcefe36879455883b7fe476ef83de40eb7f21c6f5'
# The replica, canister_sandbox and compiler_sandbox binaries must have the same revision.

[x86_64-darwin.compiler_sandbox]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/compiler_sandbox.gz'
sha256 = '405b282a4f86cd4182f132f4c05529b36ed2b55bc9df660289d648b12cd169af'

[x86_64-darwin.sandbox_launcher]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/sandbox_launcher.gz'
sha256 = '80e005f67bf724bea316056191e0cd25487941c61050f91525fb39a691c8b7bb'

[x86_64-darwin.sns]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/sns.gz'
sha256 = '8e978efc38af9595fef4b807b4815f78a9632d2ae07d44f4f73b64dabea82851'

[x86_64-darwin.pocket-ic]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-darwin/pocket-ic.gz'
sha256 = 'e03ef7f87409fbb8f8295d11a06fbd90f5cd15505054ddc4909e1b50921eda67'

[x86_64-darwin.motoko-base]
url = 'https://github.com/dfinity/motoko/releases/download/0.13.0/motoko-base-library.tar.gz'
sha256 = 'fcefaf998b0ddb75100f46cc23ac8a384ef4d7b26374a38f353055b2c8fa3fb6'

[x86_64-darwin.ic-btc-canister]
url = 'https://github.com/dfinity/bitcoin-canister/releases/download/release%2F2023-10-13/ic-btc-canister.wasm.gz'
sha256 = '09f5647a45ff6d5d05b2b0ed48613fb2365b5fe6573ba0e901509c39fb9564ac'

[x86_64-linux.ic-admin]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-admin.gz'
sha256 = '0d283dcb68e9e3ed08c5ff865afd6395d8ec792cf9000eb726e2afaf93f2678c'

[x86_64-linux.ic-btc-adapter]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-btc-adapter.gz'
sha256 = 'c79e348060a30194d5f7fc44946010881daa2ff45cd3acd63089c50a544c8a84'

[x86_64-linux.ic-https-outcalls-adapter]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-https-outcalls-adapter.gz'
sha256 = 'f47b1e90a4485a0da8956e429a0cdbbd445c5d04927cb985aa257be948d75661'

[x86_64-linux.ic-nns-init]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-nns-init.gz'
sha256 = '9ccbb24197498fe8be5fbf78442ac074c370ad9995c92ce87d436607b7ec4f98'

[x86_64-linux.ic-starter]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/ic-starter.gz'
sha256 = '1246c3d4fde929db43d6d3562d51631968babe446fc18a7bf540adf7d08e3c43'

[x86_64-linux.motoko]
url = 'https://github.com/dfinity/motoko/releases/download/0.13.0/motoko-Linux-x86_64-0.13.0.tar.gz'
sha256 = 'ae82aba73b868ac43547abccc99ada0e5c7cfb5d8b2eed6b1c433eaa3c4b56da'
# The replica, canister_sandbox and compiler_sandbox binaries must have the same revision.

[x86_64-linux.replica]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/replica.gz'
sha256 = '6b8afa61767e8e779585b9571a7b5939a6e60b726950076b8f512d9d7d418b4a'
# The replica, canister_sandbox and compiler_sandbox binaries must have the same revision.

[x86_64-linux.canister_sandbox]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/canister_sandbox.gz'
sha256 = '094f313b964d3b6006b14d5e192c7ae901877347ac57cecbadaf65e8919eb575'
# The replica, canister_sandbox and compiler_sandbox binaries must have the same revision.

[x86_64-linux.compiler_sandbox]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/compiler_sandbox.gz'
sha256 = '7de7542ad9b6af4a3ab42efb363a5b461b3c8189603e02be8b8b7120a388384a'

[x86_64-linux.sandbox_launcher]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/sandbox_launcher.gz'
sha256 = '3d3cb844cd13ea5958d392d7a2a69e25cfc6c55edbef3ead7a647dfaf69f28f7'

[x86_64-linux.sns]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/sns.gz'
sha256 = 'd77a61ccf1274a442b7c4c2c3ed70a9ef452a41ed62508c2fafa9fd6ca2b3bec'

[x86_64-linux.pocket-ic]
url = 'https://download.dfinity.systems/ic/c43a4880199c00135c8415957851e823b3fb769e/binaries/x86_64-linux/pocket-ic.gz'
sha256 = 'bd7eb010b5ac4ad88d45a5b439d1dd4dca2ddf60c8aec3dc35fb94155120941a'

[x86_64-linux.motoko-base]
url = 'https://github.com/dfinity/motoko/releases/download/0.13.0/motoko-base-library.tar.gz'
sha256 = 'fcefaf998b0ddb75100f46cc23ac8a384ef4d7b26374a38f353055b2c8fa3fb6'

[x86_64-linux.ic-btc-canister]
url = 'https://github.com/dfinity/bitcoin-canister/releases/download/release%2F2023-10-13/ic-btc-canister.wasm.gz'
sha256 = '09f5647a45ff6d5d05b2b0ed48613fb2365b5fe6573ba0e901509c39fb9564ac'


-----------------------

/src/dfx/assets/language_bindings/canister.js:
-----------------------

import { Actor, HttpAgent } from "@dfinity/agent";

// Imports and re-exports candid interface
import { idlFactory } from './{canister_name}.did.js';
export { idlFactory } from './{canister_name}.did.js';
// CANISTER_ID is replaced by webpack based on node environment
export const canisterId = process.env.CANISTER_ID_{canister_name_ident_uppercase};

/**
 * @deprecated since dfx 0.11.1
 * Do not import from `.dfx`, instead switch to using `dfx generate` to generate your JS interface.
 * @param {string | import("@dfinity/principal").Principal} canisterId Canister ID of Agent
 * @param {{agentOptions?: import("@dfinity/agent").HttpAgentOptions; actorOptions?: import("@dfinity/agent").ActorConfig} | { agent?: import("@dfinity/agent").Agent; actorOptions?: import("@dfinity/agent").ActorConfig }} [options]
 * @return {import("@dfinity/agent").ActorSubclass<import("./{canister_name}.did.js")._SERVICE>}
 */
export const createActor = (canisterId, options = {}) => {
  console.warn(`Deprecation warning: you are currently importing code from .dfx. Going forward, refactor to use the dfx generate command for JavaScript bindings.

See https://internetcomputer.org/docs/current/developer-docs/updates/release-notes/ for migration instructions`);
  const agent = options.agent || new HttpAgent({ ...options.agentOptions });
  
  // Fetch root key for certificate validation during development
  if (process.env.DFX_NETWORK !== "ic") {
    agent.fetchRootKey().catch(err => {
      console.warn("Unable to fetch root key. Check to ensure that your local replica is running");
      console.error(err);
    });
  }

  // Creates an actor with using the candid interface and the HttpAgent
  return Actor.createActor(idlFactory, {
    agent,
    canisterId,
    ...(options ? options.actorOptions : {}),
  });
};
  
/**
 * A ready-to-use agent for the {canister_name} canister
 * @type {import("@dfinity/agent").ActorSubclass<import("./{canister_name}.did.js")._SERVICE>}
 */
export const {canister_name_ident} = createActor(canisterId);


-----------------------

/src/dfx/assets/language_bindings/index.d.ts.hbs:
-----------------------

import type {
  ActorSubclass,
  HttpAgentOptions,
  ActorConfig,
  Agent,
} from "@dfinity/agent";
import type { Principal } from "@dfinity/principal";
import type { IDL } from "@dfinity/candid";

import { _SERVICE } from './{{canister_name}}.did';

export declare const idlFactory: IDL.InterfaceFactory;
export declare const canisterId: string;

export declare interface CreateActorOptions {
  /**
   * @see {@link Agent}
   */
  agent?: Agent;
  /**
   * @see {@link HttpAgentOptions}
   */
  agentOptions?: HttpAgentOptions;
  /**
   * @see {@link ActorConfig}
   */
  actorOptions?: ActorConfig;
}

/**
 * Intializes an {@link ActorSubclass}, configured with the provided SERVICE interface of a canister.
 * @constructs {@link ActorSubClass}
 * @param {string | Principal} canisterId - ID of the canister the {@link Actor} will talk to
 * @param {CreateActorOptions} options - see {@link CreateActorOptions}
 * @param {CreateActorOptions["agent"]} options.agent - a pre-configured agent you'd like to use. Supercedes agentOptions
 * @param {CreateActorOptions["agentOptions"]} options.agentOptions - options to set up a new agent
 * @see {@link HttpAgentOptions}
 * @param {CreateActorOptions["actorOptions"]} options.actorOptions - options for the Actor
 * @see {@link ActorConfig}
 */
export declare const createActor: (
  canisterId: string | Principal,
  options?: CreateActorOptions
) => ActorSubclass<_SERVICE>;

/**
 * Intialized Actor using default settings, ready to talk to a canister using its candid interface
 * @constructs {@link ActorSubClass}
 */
export declare const {{canister_name_ident}}: ActorSubclass<_SERVICE>;


-----------------------

/src/dfx/assets/language_bindings/index.js.hbs:
-----------------------

import { Actor, HttpAgent } from "@dfinity/agent";

// Imports and re-exports candid interface
import { idlFactory } from "./{{canister_name}}.did.js";
export { idlFactory } from "./{{canister_name}}.did.js";

/* CANISTER_ID is replaced by webpack based on node environment
 * Note: canister environment variable will be standardized as
 * process.env.CANISTER_ID_<CANISTER_NAME_UPPERCASE>
 * beginning in dfx 0.15.0
 */
export const canisterId =
  {{{canister_name_process_env}}};

export const createActor = (canisterId, options = {}) => {
  const agent = options.agent || new HttpAgent({ ...options.agentOptions });

  if (options.agent && options.agentOptions) {
    console.warn(
      "Detected both agent and agentOptions passed to createActor. Ignoring agentOptions and proceeding with the provided agent."
    );
  }

  // Fetch root key for certificate validation during development
  if (process.env.DFX_NETWORK !== "ic") {
    agent.fetchRootKey().catch((err) => {
      console.warn(
        "Unable to fetch root key. Check to ensure that your local replica is running"
      );
      console.error(err);
    });
  }

  // Creates an actor with using the candid interface and the HttpAgent
  return Actor.createActor(idlFactory, {
    agent,
    canisterId,
    ...options.actorOptions,
  });
};{{{actor_export}}}


-----------------------

/src/dfx/assets/prepare_assets.rs:
-----------------------

use crate::Source;
use backoff::future::retry;
use backoff::ExponentialBackoffBuilder;
use bytes::{Buf, BufMut, Bytes, BytesMut};
use flate2::{bufread::GzDecoder, write::GzEncoder, Compression};
use reqwest::Client;
use sha2::{Digest, Sha256};
use std::{
    collections::HashMap,
    fs::File,
    io::{self, BufWriter},
    path::{Path, PathBuf},
    sync::Arc,
    time::Duration,
};
use tar::{Archive, Builder, EntryType, Header};
use tokio::task::{spawn, spawn_blocking, JoinSet};

#[tokio::main]
pub(crate) async fn prepare(out_dir: &Path, source_set: HashMap<String, Source>) {
    std::fs::create_dir_all(out_dir).expect("error creating output directory");
    let out_dir_ = out_dir.to_owned();
    let copy_join = spawn_blocking(|| copy_canisters(out_dir_));
    let out_dir = out_dir.to_owned();
    make_binary_cache(out_dir, source_set).await;
    copy_join.await.unwrap();
}

fn copy_canisters(out_dir: PathBuf) {
    let distributed = Path::new("../distributed");
    for can in ["assetstorage", "wallet", "ui"] {
        let mut tar = Builder::new(GzEncoder::new(
            BufWriter::new(File::create(out_dir.join(format!("{can}_canister.tgz"))).unwrap()),
            Compression::new(6),
        ));
        for ext in [
            ".did",
            if can == "assetstorage" || can == "wallet" {
                ".wasm.gz"
            } else {
                ".wasm"
            },
        ] {
            let filename = format!("{can}{ext}");
            let input_file = File::open(distributed.join(&filename)).unwrap();
            let metadata = input_file.metadata().unwrap();
            let mut header = Header::new_gnu();
            header.set_mode(0o644);
            header.set_size(metadata.len());
            tar.append_data(&mut header, &filename, input_file).unwrap();
        }
        tar.finish().unwrap();
    }
}

async fn download_canisters(
    client: Client,
    sources: Arc<HashMap<String, Source>>,
    out_dir: PathBuf,
) {
    // replace with joinset setup from download_binaries if another gets added
    let source = sources["ic-btc-canister"].clone();
    let btc_canister = download_and_check_sha(client, source).await;
    spawn_blocking(move || {
        let mut tar = Builder::new(GzEncoder::new(
            BufWriter::new(File::create(out_dir.join("btc_canister.tgz")).unwrap()),
            Compression::new(6),
        ));
        let mut header = Header::new_gnu();
        header.set_mode(0o644);
        header.set_size(btc_canister.len() as u64);
        tar.append_data(
            &mut header,
            "ic-btc-canister.wasm.gz",
            btc_canister.reader(),
        )
        .unwrap();
        tar.finish().unwrap();
    })
    .await
    .unwrap();
}

async fn make_binary_cache(out_dir: PathBuf, sources: HashMap<String, Source>) {
    let sources = Arc::new(sources);
    let client = Client::builder()
        .timeout(Duration::from_secs(300))
        .build()
        .unwrap();
    let mo_base = spawn(download_mo_base(client.clone(), sources.clone()));
    let bins = spawn(download_binaries(client.clone(), sources.clone()));
    let bin_tars = spawn(download_bin_tarballs(client.clone(), sources.clone()));
    let canisters = spawn(download_canisters(
        client.clone(),
        sources.clone(),
        out_dir.clone(),
    ));
    let (mo_base, bins, bin_tars, _) =
        tokio::try_join!(mo_base, bins, bin_tars, canisters).unwrap();
    spawn_blocking(|| write_binary_cache(out_dir, mo_base, bins, bin_tars))
        .await
        .unwrap();
}

fn write_binary_cache(
    out_dir: PathBuf,
    mo_base: HashMap<PathBuf, Bytes>,
    bins: HashMap<PathBuf, Bytes>,
    mut bin_tars: HashMap<PathBuf, Bytes>,
) {
    let mut tar = Builder::new(GzEncoder::new(
        BufWriter::new(File::create(out_dir.join("binary_cache.tgz")).unwrap()),
        Compression::new(6),
    ));
    for (path, bin) in bins.into_iter().chain(
        ["moc", "mo-doc", "mo-ide"]
            .map(|bin| (bin.into(), bin_tars.remove(Path::new(bin)).unwrap())),
    ) {
        let mut header = Header::new_gnu();
        header.set_size(bin.len() as u64);
        header.set_mode(0o500);
        tar.append_data(&mut header, path, bin.reader()).unwrap();
    }

    for (path, file) in bin_tars {
        let mut header = Header::new_gnu();
        header.set_size(file.len() as u64);
        header.set_mode(0o644);
        tar.append_data(&mut header, path, file.reader()).unwrap();
    }
    let mut base_hdr = Header::new_gnu();
    base_hdr.set_entry_type(EntryType::dir());
    base_hdr.set_mode(0o755);
    base_hdr.set_size(0);
    tar.append_data(&mut base_hdr, "base", io::empty()).unwrap();
    for (path, file) in mo_base {
        let mut header = Header::new_gnu();
        header.set_mode(0o644);
        header.set_size(file.len() as u64);
        tar.append_data(&mut header, Path::new("base").join(path), file.reader())
            .unwrap();
    }
    tar.finish().unwrap();
}

async fn download_and_check_sha(client: Client, source: Source) -> Bytes {
    let retry_policy = ExponentialBackoffBuilder::new()
        .with_initial_interval(Duration::from_secs(1))
        .with_max_interval(Duration::from_secs(16))
        .with_multiplier(2.0)
        .with_max_elapsed_time(Some(Duration::from_secs(300)))
        .build();

    let response = retry(retry_policy, || async {
        match client.get(&source.url).send().await {
            Ok(response) => Ok(response),
            Err(err) => Err(backoff::Error::transient(err)),
        }
    })
    .await
    .unwrap();

    response.error_for_status_ref().unwrap();
    let content = response.bytes().await.unwrap();
    let sha = Sha256::digest(&content);
    assert_eq!(
        sha[..],
        source.sha256()[..],
        "sha256 hash for {} did not match",
        source.url
    );
    content
}

async fn download_binaries(
    client: Client,
    sources: Arc<HashMap<String, Source>>,
) -> HashMap<PathBuf, Bytes> {
    let mut joinset = JoinSet::new();
    for bin in [
        "canister_sandbox",
        "compiler_sandbox",
        "ic-admin",
        "ic-btc-adapter",
        "ic-https-outcalls-adapter",
        "ic-nns-init",
        "ic-starter",
        "pocket-ic",
        "replica",
        "sandbox_launcher",
        "sns",
    ] {
        let source = sources
            .get(bin)
            .unwrap_or_else(|| panic!("Cannot find source for {bin}"))
            .clone();
        let client_ = client.clone();
        joinset.spawn(async move { (bin, download_and_check_sha(client_, source).await) });
    }
    let mut map = HashMap::new();
    while let Some(res) = joinset.join_next().await {
        let (bin, content) = res.unwrap();
        let decompressed = spawn_blocking(|| {
            let mut buf = BytesMut::new();
            io::copy(
                &mut GzDecoder::new(content.reader()),
                &mut (&mut buf).writer(),
            )
            .unwrap();
            buf.freeze()
        })
        .await
        .unwrap();
        map.insert(bin.into(), decompressed);
    }
    map
}

async fn download_bin_tarballs(
    client: Client,
    sources: Arc<HashMap<String, Source>>,
) -> HashMap<PathBuf, Bytes> {
    let mut map = HashMap::new();
    let [motoko] = ["motoko"].map(|pkg| {
        let client = client.clone();
        let source = sources[pkg].clone();
        spawn(download_and_check_sha(client, source))
    });
    let (motoko,) = tokio::try_join!(motoko,).unwrap();
    {
        let tar = motoko;
        tar_xzf(&tar, |path, content| {
            map.insert(path, content);
        });
    }
    map
}

async fn download_mo_base(
    client: Client,
    sources: Arc<HashMap<String, Source>>,
) -> HashMap<PathBuf, Bytes> {
    let source = sources["motoko-base"].clone();
    let mo_base = download_and_check_sha(client, source).await;
    let mut map = HashMap::new();
    tar_xzf(&mo_base, |path, content| {
        let path = path.strip_prefix(".").unwrap_or(&path); // normalize ./x to x
        if let Ok(file) = path.strip_prefix("src") {
            map.insert(file.to_owned(), content);
        }
    });
    map
}

fn tar_xzf(gz: &[u8], mut each: impl FnMut(PathBuf, Bytes)) {
    let mut tar = Archive::new(GzDecoder::new(gz));
    for entry in tar.entries().unwrap() {
        let mut entry = entry.unwrap();
        if !entry.header().entry_type().is_file() {
            continue;
        }
        let path = entry.path().unwrap_or_else(|e| {
            panic!(
                "Malformed file path {}: {e}",
                String::from_utf8_lossy(&entry.path_bytes())
            )
        });
        let path = path.strip_prefix(".").unwrap_or(&path).to_owned();
        let mut content = BytesMut::with_capacity(entry.header().size().unwrap() as usize);
        io::copy(&mut entry, &mut (&mut content).writer()).unwrap();
        each(path, content.freeze());
    }
}


-----------------------

/src/dfx/assets/project_templates/any_js/package.json:
-----------------------

{
    "name": "__project_name__",
    "type": "module",
    "scripts": {
        "start": "npm start --workspaces --if-present",
        "prebuild": "npm run prebuild --workspaces --if-present",
        "build": "npm run build --workspaces --if-present",
        "pretest": "npm run prebuild --workspaces --if-present",
        "test": "npm test --workspaces --if-present"
    },
    "engines": {
        "node": ">=16.0.0",
        "npm": ">=7.0.0"
    },
    "workspaces": [ ]
}


-----------------------

/src/dfx/assets/project_templates/any_js/tsconfig.json:
-----------------------

{
    "compilerOptions": {
        "strict": true,
        "target": "ES2020",
        "experimentalDecorators": true,
        "strictPropertyInitialization": false,
        "moduleResolution": "node",
        "allowJs": true,
        "outDir": "HACK_BECAUSE_OF_ALLOW_JS"
    }
}


-----------------------

/src/dfx/assets/project_templates/azle/dfx.json-patch:
-----------------------

[
    {
        "path": "/canisters/__backend_name__",
        "op": "add",
        "value": {
            "type": "custom",
            "main": "src/__backend_name__/src/index.ts",
            "candid": "src/__backend_name__/__backend_name__.did",
            "build": "npx azle __backend_name__",
            "wasm": ".azle/__backend_name__/__backend_name__.wasm",
            "gzip": true,
            "tech_stack": {
                "language": {
                    "javascript": {},
                    "typescript": {}
                },
                "cdk": {
                    "azle": {}
                }
            }
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/azle/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/workspaces/-",
        "value": "src/__backend_name__"
    }
]

-----------------------

/src/dfx/assets/project_templates/azle/src/__backend_name__/__backend_name__.did:
-----------------------

service : {
    "greet" : (text) -> (text) query;
}


-----------------------

/src/dfx/assets/project_templates/azle/src/__backend_name__/package.json:
-----------------------

{
    "name": "__backend_name__",
    "version": "0.0.0",
    "private": true,
    "type": "module",
    "dependencies": {
        "azle": "^0.19.0"
    }
}

-----------------------

/src/dfx/assets/project_templates/azle/src/__backend_name__/src/index.ts:
-----------------------

import { Canister, query, text } from 'azle';

export default Canister({
    greet: query([text], text, (name) => {
        return `Hello, ${name}!`;
    })
})


-----------------------

/src/dfx/assets/project_templates/base/README.md:
-----------------------

# `__project_name__`

Welcome to your new `__project_name__` project and to the Internet Computer development community. By default, creating a new project adds this README and some template files to your project directory. You can edit these template files to customize your project and to include your own code to speed up the development cycle.

To get started, you might want to explore the project directory structure and the default configuration file. Working with this project in your development environment will not affect any production deployment or identity tokens.

To learn more before you start working with `__project_name__`, see the following documentation available online:

- [Quick Start](https://internetcomputer.org/docs/current/developer-docs/setup/deploy-locally)
- [SDK Developer Tools](https://internetcomputer.org/docs/current/developer-docs/setup/install)

If you want to start working on your project right away, you might want to try the following commands:

```bash
cd __project_name__/
dfx help
dfx canister --help
```

## Running the project locally

If you want to test your project locally, you can use the following commands:

```bash
# Starts the replica, running in the background
dfx start --background

# Deploys your canisters to the replica and generates your candid interface
dfx deploy
```

Once the job completes, your application will be available at `http://localhost:4943?canisterId={asset_canister_id}`.

If you have made changes to your backend canister, you can generate a new candid interface with

```bash
npm run generate
```

at any time. This is recommended before starting the frontend development server, and will be run automatically any time you run `dfx deploy`.

If you are making frontend changes, you can start a development server with

```bash
npm start
```

Which will start a server at `http://localhost:8080`, proxying API requests to the replica at port 4943.

### Note on frontend environment variables

If you are hosting frontend code somewhere without using DFX, you may need to make one of the following adjustments to ensure your project does not fetch the root key in production:

- set`DFX_NETWORK` to `ic` if you are using Webpack
- use your own preferred method to replace `process.env.DFX_NETWORK` in the autogenerated declarations
  - Setting `canisters -> {asset_canister_id} -> declarations -> env_override to a string` in `dfx.json` will replace `process.env.DFX_NETWORK` with the string in the autogenerated declarations
- Write your own `createActor` constructor


-----------------------

/src/dfx/assets/project_templates/base/__dot__gitignore:
-----------------------

# Various IDEs and Editors
.vscode/
.idea/
**/*~

# Mac OSX temporary files
.DS_Store
**/.DS_Store

# dfx temporary files
.dfx/

# generated files
**/declarations/

# rust
target/

# frontend code
node_modules/
dist/
.svelte-kit/

# environment variables
.env


-----------------------

/src/dfx/assets/project_templates/base/dfx.json:
-----------------------

{
    "version": 1,
    "canisters": { },
    "defaults": {
        "build": {
            "packtool": "",
            "args": ""
        }
    },
    "output_env_file": ".env"
}


-----------------------

/src/dfx/assets/project_templates/bitcoin/dfx.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/defaults/bitcoin",
        "value": {
            "enabled": true,
            "nodes": [
                "127.0.0.1:18444"
            ],
            "log_level": "info"
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/internet_identity/dfx.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/canisters/internet_identity",
        "value": {
            "type": "custom",
            "candid": "https://github.com/dfinity/internet-identity/releases/latest/download/internet_identity.did",
            "wasm": "https://github.com/dfinity/internet-identity/releases/latest/download/internet_identity_dev.wasm.gz",
            "remote": {
                "id": {
                    "ic": "rdmx6-jaaaa-aaaaa-aaadq-cai"
                }
            },
            "frontend": { }
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/kybra/dfx.json-patch:
-----------------------

[
    {
        "path": "/canisters/__backend_name__",
        "op": "add",
        "value": {
            "type": "custom",
            "build": "python -m kybra __backend_name__ src/__backend_name__/src/main.py src/__backend_name__/__backend_name__.did",
            "post_install": ".kybra/__backend_name__/post_install.sh",
            "candid": "src/__backend_name__/__backend_name__.did",
            "wasm": ".kybra/__backend_name__/__backend_name__.wasm",
            "gzip": true,
            "tech_stack": {
                "language": {
                    "python": {}
                },
                "cdk": {
                    "kybra": {}
                }
            }
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/kybra/requirements.in:
-----------------------

kybra~=0.5.0


-----------------------

/src/dfx/assets/project_templates/kybra/requirements.txt:
-----------------------

#
# This file is autogenerated by pip-compile with Python 3.11
# by the following command:
#
#    pip-compile requirements.in
#
altgraph==0.17.4
    # via modulegraph
kybra==0.5.2
    # via -r requirements.in
modulegraph==0.19.3
    # via kybra

# The following packages are considered to be unsafe in a requirements file:
# setuptools


-----------------------

/src/dfx/assets/project_templates/kybra/src/__backend_name__/__backend_name__.did:
-----------------------

service : {
    "greet": (text) -> (text) query;
}


-----------------------

/src/dfx/assets/project_templates/kybra/src/__backend_name__/src/main.py:
-----------------------

from kybra import query

@query
def greet(name: str) -> str:
    return f"Hello, {name}!"


-----------------------

/src/dfx/assets/project_templates/motoko/README.md.patch:
-----------------------

--- a/README.md
+++ b/README.md
@@ -9,2 +9,4 @@
 - [Quick Start](https://internetcomputer.org/docs/current/developer-docs/setup/deploy-locally)
 - [SDK Developer Tools](https://internetcomputer.org/docs/current/developer-docs/setup/install)
+- [Motoko Programming Language Guide](https://internetcomputer.org/docs/current/motoko/main/motoko)
+- [Motoko Language Quick Reference](https://internetcomputer.org/docs/current/motoko/main/language-manual)


-----------------------

/src/dfx/assets/project_templates/motoko/dfx.json-patch:
-----------------------

[
  {
    "op": "add",
    "path": "/canisters/__backend_name__",
    "value": {
      "type": "motoko",
      "main": "src/__backend_name__/main.mo"
    }
  }
]


-----------------------

/src/dfx/assets/project_templates/motoko/src/__backend_name__/main.mo:
-----------------------

actor {
  public query func greet(name : Text) : async Text {
    return "Hello, " # name # "!";
  };
};


-----------------------

/src/dfx/assets/project_templates/react/dfx.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/canisters/__frontend_name__",
        "value": {
            "type": "assets",
            "source": [
                "src/__frontend_name__/dist"
            ],
            "dependencies": [
                "__backend_name__"
            ],
            "workspace": "__frontend_name__"
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/react/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/workspaces/-",
        "value": "src/__frontend_name__"
    }
]


-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/index.html:
-----------------------

<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width" />
  <title>IC Hello Starter</title>
  <base href="/" />
  <link rel="icon" href="/favicon.ico" />
</head>

<body>
  <div id="root"></div>
  <script type="module" src="/src/main.jsx"></script>
</body>

</html>

-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/package.json:
-----------------------

{
  "name": "__frontend_name__",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "setup": "npm i && dfx canister create __backend_name__ && dfx generate __backend_name__ && dfx deploy",
    "start": "vite --port 3000",
    "prebuild": "dfx generate",
    "build": "tsc && vite build",
    "format": "prettier --write \"src/**/*.{json,js,jsx,ts,tsx,css,scss}\""
  },
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "@dfinity/agent": "^1.4.0",
    "@dfinity/candid": "^1.4.0",
    "@dfinity/principal": "^1.4.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.14",
    "@types/react-dom": "^18.2.6",
    "@vitejs/plugin-react": "^4.0.1",
    "dotenv": "^16.3.1",
    "sass": "^1.63.6",
    "typescript": "^5.1.3",
    "vite": "^4.3.9",
    "vite-plugin-environment": "^1.1.3"
  }
}

-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/public/.ic-assets.json5:
-----------------------

[
    {
        "match": "**/*",
        "headers": {
            // Security: The Content Security Policy (CSP) given below aims at working with many apps rather than providing maximal security.
            // We recommend tightening the CSP for your specific application. Some recommendations are as follows:
            // - Use the CSP Evaluator (https://csp-evaluator.withgoogle.com/) to validate the CSP you define.
            // - Follow the â€œStrict CSPâ€ recommendations (https://csp.withgoogle.com/docs/strict-csp.html). However, note that in the context of the IC,
            //   nonces cannot be used because the response bodies must be static to work well with HTTP asset certification.
            //   Thus, we recommend to include script hashes (in combination with strict-dynamic) in the CSP as described
            //   in https://csp.withgoogle.com/docs/faq.html in section â€œWhat if my site is static and I can't add nonces to scripts?â€.
            //   See for example the II CSP (https://github.com/dfinity/internet-identity/blob/main/src/internet_identity/src/http.rs).
            // - It is recommended to tighten the connect-src directive. With the current CSP configuration the browser can
            //   make requests to https://*.icp0.io, hence being able to call any canister via https://icp0.io/api/v2/canister/{canister-ID}.
            //   This could potentially be used in combination with another vulnerability (e.g. XSS) to exfiltrate private data.
            //   The developer can configure this policy to only allow requests to their specific canisters,
            //   e.g: connect-src 'self' https://icp-api.io/api/v2/canister/{my-canister-ID}, where {my-canister-ID} has the following format: aaaaa-aaaaa-aaaaa-aaaaa-aaa
            // - It is recommended to configure style-src, style-src-elem and font-src directives with the resources your canister is going to use
            //   instead of using the wild card (*) option. Normally this will include 'self' but also other third party styles or fonts resources (e.g: https://fonts.googleapis.com or other CDNs)

            // Notes about the CSP below:
            // - We added img-src data: because data: images are used often.
            // - frame-ancestors: none mitigates clickjacking attacks. See https://owasp.org/www-community/attacks/Clickjacking.
            "Content-Security-Policy": "default-src 'self';script-src 'self';connect-src 'self' http://localhost:* https://icp0.io https://*.icp0.io https://icp-api.io;img-src 'self' data:;style-src * 'unsafe-inline';style-src-elem * 'unsafe-inline';font-src *;object-src 'none';base-uri 'self';frame-ancestors 'none';form-action 'self';upgrade-insecure-requests;",

            // Security: The permissions policy disables all features for security reasons. If your site needs such permissions, activate them.
            // To configure permissions go here https://www.permissionspolicy.com/
            "Permissions-Policy": "accelerometer=(), ambient-light-sensor=(), autoplay=(), battery=(), camera=(), cross-origin-isolated=(), display-capture=(), document-domain=(), encrypted-media=(), execution-while-not-rendered=(), execution-while-out-of-viewport=(), fullscreen=(), geolocation=(), gyroscope=(), keyboard-map=(), magnetometer=(), microphone=(), midi=(), navigation-override=(), payment=(), picture-in-picture=(), publickey-credentials-get=(), screen-wake-lock=(), sync-xhr=(), usb=(), web-share=(), xr-spatial-tracking=(), clipboard-read=(), clipboard-write=(), gamepad=(), speaker-selection=(), conversion-measurement=(), focus-without-user-activation=(), hid=(), idle-detection=(), interest-cohort=(), serial=(), sync-script=(), trust-token-redemption=(), window-placement=(), vertical-scroll=()",

            // Security: Mitigates clickjacking attacks.
            // See: https://owasp.org/www-community/attacks/Clickjacking.
            "X-Frame-Options": "DENY",

            // Security: Avoids forwarding referrer information to other origins.
            // See: https://owasp.org/www-project-secure-headers/#referrer-policy.
            "Referrer-Policy": "same-origin",

            // Security: Tells the userâ€™s browser that it must always use HTTPS with your site.
            // See: https://owasp.org/www-project-secure-headers/#http-strict-transport-security
            "Strict-Transport-Security": "max-age=31536000; includeSubDomains",

            // Security: Prevents the browser from interpreting files as a different MIME type to what is specified in the Content-Type header.
            // See: https://owasp.org/www-project-secure-headers/#x-content-type-options
            "X-Content-Type-Options": "nosniff",

            // Security: Enables browser features to mitigate some of the XSS attacks. Note that it has to be in mode=block.
            // See: https://owasp.org/www-community/attacks/xss/
            "X-XSS-Protection": "1; mode=block"
        },
        // Uncomment to redirect all requests from .raw.icp0.io to .icp0.io
        // "allow_raw_access": false
    },
]


-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/dfx/assets/project_templates/react/src/__frontend_name__/public/favicon.ico

-----------------------

<svg width="684" height="147" viewBox="0 0 228 49" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="0.5" y="0.5" width="227" height="48" rx="3.5" fill="white" stroke="url(#paint0_linear_1810_46)"/>
<path d="M51.3703 12C48.4634 12 45.2947 13.5165 41.9454 16.5037C40.3565 17.9191 38.9842 19.4357 37.9551 20.6489C37.9551 20.6489 37.9551 20.6489 37.9641 20.6581V20.6489C37.9641 20.6489 39.5891 22.4504 41.3856 24.3805C42.3516 23.2133 43.7419 21.6232 45.3398 20.1894C48.319 17.5331 50.2599 16.9724 51.3703 16.9724C55.5502 16.9724 58.9446 20.3456 58.9446 24.4908C58.9446 28.6085 55.5412 31.9816 51.3703 32.0092C51.1808 32.0092 50.937 31.9816 50.6301 31.9173C51.8488 32.4504 53.1578 32.8364 54.4037 32.8364C62.0592 32.8364 63.5578 27.7537 63.6571 27.3861C63.8828 26.4577 64.0002 25.4835 64.0002 24.4816C64.0002 17.6066 58.3307 12 51.3703 12Z" fill="url(#paint1_linear_1810_46)"/>
<path d="M24.6298 36.9999C27.5368 36.9999 30.7055 35.4834 34.0548 32.4962C35.6437 31.0808 37.0159 29.5642 38.0451 28.351C38.0451 28.351 38.0451 28.351 38.0361 28.3418V28.351C38.0361 28.351 36.4111 26.5495 34.6145 24.6194C33.6486 25.7867 32.2583 27.3767 30.6604 28.8106C27.6812 31.4668 25.7403 32.0275 24.6298 32.0275C20.45 32.0183 17.0555 28.6451 17.0555 24.4999C17.0555 20.3822 20.459 17.0091 24.6298 16.9815C24.8194 16.9815 25.0632 17.0091 25.3701 17.0734C24.1514 16.5403 22.8423 16.1543 21.5965 16.1543C13.941 16.1543 12.4514 21.237 12.3431 21.5955C12.1174 22.533 12 23.4981 12 24.4999C12 31.3933 17.6694 36.9999 24.6298 36.9999Z" fill="url(#paint2_linear_1810_46)"/>
<path d="M54.3856 32.7261C50.4675 32.625 46.396 29.4816 45.5654 28.7004C43.4168 26.6783 38.4606 21.2096 38.0724 20.7776C34.4432 16.6324 29.5231 12 24.63 12H24.621H24.612C18.6717 12.0276 13.6794 16.1268 12.3433 21.5956C12.4426 21.2371 14.4016 16.0533 21.5877 16.2371C25.5057 16.3382 29.5953 19.5276 30.4349 20.3088C32.5835 22.3309 37.5398 27.7997 37.9279 28.2316C41.5571 32.3677 46.4772 37 51.3703 37H51.3793H51.3883C57.3286 36.9725 62.33 32.8732 63.6571 27.4044C63.5487 27.7629 61.5807 32.9008 54.3856 32.7261Z" fill="#29ABE2"/>
<path d="M73.0301 33.827C73.3063 33.827 73.5301 33.6032 73.5301 33.327V26.6622C73.5301 26.386 73.3063 26.1622 73.0301 26.1622H72.5C72.2239 26.1622 72 26.386 72 26.6622V33.327C72 33.6032 72.2239 33.827 72.5 33.827H73.0301Z" fill="#161617"/>
<path d="M83.2018 33.827C83.478 33.827 83.7018 33.6032 83.7018 33.327V26.6622C83.7018 26.386 83.478 26.1622 83.2018 26.1622H82.6936C82.4174 26.1622 82.1936 26.386 82.1936 26.6622V31.2324L79.264 26.6256C79.0805 26.337 78.7622 26.1622 78.4202 26.1622H77.5895C77.3133 26.1622 77.0895 26.386 77.0895 26.6622V33.327C77.0895 33.6032 77.3133 33.827 77.5895 33.827H78.0978C78.3739 33.827 78.5978 33.6032 78.5978 33.327V28.3892L81.9693 33.5987C82.0614 33.7411 82.2195 33.827 82.389 33.827H83.2018Z" fill="#161617"/>
<path d="M92.4535 27.5784C92.7297 27.5784 92.9535 27.3545 92.9535 27.0784V26.6622C92.9535 26.386 92.7297 26.1622 92.4535 26.1622H87.0379C86.7617 26.1622 86.5379 26.386 86.5379 26.6622V27.0784C86.5379 27.3545 86.7617 27.5784 87.0379 27.5784H88.9861V33.327C88.9861 33.6032 89.21 33.827 89.4861 33.827H90.0053C90.2814 33.827 90.5053 33.6032 90.5053 33.327V27.5784H92.4535Z" fill="#161617"/>
<path d="M100.138 33.827C100.415 33.827 100.638 33.6032 100.638 33.327V32.9216C100.638 32.6455 100.415 32.4216 100.138 32.4216H97.2941V30.6486H99.8215C100.098 30.6486 100.322 30.4248 100.322 30.1486V29.8189C100.322 29.5428 100.098 29.3189 99.8215 29.3189H97.2941V27.5676H100.138C100.415 27.5676 100.638 27.3437 100.638 27.0676V26.6622C100.638 26.386 100.415 26.1622 100.138 26.1622H96.2858C96.0096 26.1622 95.7858 26.386 95.7858 26.6622V33.327C95.7858 33.6032 96.0096 33.827 96.2858 33.827H100.138Z" fill="#161617"/>
<path d="M107.55 33.5559C107.635 33.7224 107.807 33.827 107.994 33.827H108.541C108.918 33.827 109.16 33.4259 108.983 33.0929L107.711 30.6919C108.727 30.4 109.361 29.5892 109.361 28.5189C109.361 27.1892 108.4 26.1622 106.891 26.1622H104.364C104.088 26.1622 103.864 26.386 103.864 26.6622V33.327C103.864 33.6032 104.088 33.827 104.364 33.827H104.883C105.159 33.827 105.383 33.6032 105.383 33.327V30.8757H106.17L107.55 33.5559ZM105.383 29.5892V27.4595H106.607C107.372 27.4595 107.82 27.8811 107.82 28.5297C107.82 29.1568 107.372 29.5892 106.607 29.5892H105.383Z" fill="#161617"/>
<path d="M118.609 33.827C118.885 33.827 119.109 33.6032 119.109 33.327V26.6622C119.109 26.386 118.885 26.1622 118.609 26.1622H118.101C117.825 26.1622 117.601 26.386 117.601 26.6622V31.2324L114.671 26.6256C114.488 26.337 114.17 26.1622 113.828 26.1622H112.997C112.721 26.1622 112.497 26.386 112.497 26.6622V33.327C112.497 33.6032 112.721 33.827 112.997 33.827H113.505C113.781 33.827 114.005 33.6032 114.005 33.327V28.3892L117.377 33.5987C117.469 33.7411 117.627 33.827 117.796 33.827H118.609Z" fill="#161617"/>
<path d="M127.03 33.827C127.306 33.827 127.53 33.6032 127.53 33.327V32.9216C127.53 32.6455 127.306 32.4216 127.03 32.4216H124.186V30.6486H126.713C126.989 30.6486 127.213 30.4248 127.213 30.1486V29.8189C127.213 29.5428 126.989 29.3189 126.713 29.3189H124.186V27.5676H127.03C127.306 27.5676 127.53 27.3437 127.53 27.0676V26.6622C127.53 26.386 127.306 26.1622 127.03 26.1622H123.178C122.901 26.1622 122.678 26.386 122.678 26.6622V33.327C122.678 33.6032 122.901 33.827 123.178 33.827H127.03Z" fill="#161617"/>
<path d="M135.939 27.5784C136.215 27.5784 136.439 27.3545 136.439 27.0784V26.6622C136.439 26.386 136.215 26.1622 135.939 26.1622H130.523C130.247 26.1622 130.023 26.386 130.023 26.6622V27.0784C130.023 27.3545 130.247 27.5784 130.523 27.5784H132.471V33.327C132.471 33.6032 132.695 33.827 132.971 33.827H133.491C133.767 33.827 133.991 33.6032 133.991 33.327V27.5784H135.939Z" fill="#161617"/>
<path d="M146.823 33.9892C148.998 33.9892 150.113 32.5622 150.408 31.3838L148.998 30.9622C148.79 31.6757 148.146 32.5297 146.823 32.5297C145.577 32.5297 144.419 31.6324 144.419 30C144.419 28.2595 145.643 27.4378 146.801 27.4378C148.146 27.4378 148.747 28.2486 148.932 28.9838L150.353 28.5405C150.047 27.2973 148.943 26 146.801 26C144.725 26 142.856 27.5568 142.856 30C142.856 32.4432 144.659 33.9892 146.823 33.9892Z" fill="#161617"/>
<path d="M154.428 29.9892C154.428 28.2595 155.653 27.4378 156.844 27.4378C158.046 27.4378 159.27 28.2595 159.27 29.9892C159.27 31.7189 158.046 32.5405 156.844 32.5405C155.653 32.5405 154.428 31.7189 154.428 29.9892ZM152.866 30C152.866 32.4649 154.745 33.9892 156.844 33.9892C158.953 33.9892 160.833 32.4649 160.833 30C160.833 27.5243 158.953 26 156.844 26C154.745 26 152.866 27.5243 152.866 30Z" fill="#161617"/>
<path d="M172.077 33.827C172.354 33.827 172.577 33.6032 172.577 33.327V26.6622C172.577 26.386 172.354 26.1622 172.077 26.1622H170.858C170.655 26.1622 170.472 26.2846 170.395 26.4722L168.249 31.6973L166.05 26.4683C165.972 26.2828 165.79 26.1622 165.589 26.1622H164.432C164.156 26.1622 163.932 26.386 163.932 26.6622V33.327C163.932 33.6032 164.156 33.827 164.432 33.827H164.875C165.151 33.827 165.375 33.6032 165.375 33.327V28.4973L167.476 33.52C167.554 33.706 167.736 33.827 167.937 33.827H168.528C168.73 33.827 168.912 33.7053 168.989 33.5187L171.091 28.4541V33.327C171.091 33.6032 171.315 33.827 171.591 33.827H172.077Z" fill="#161617"/>
<path d="M177.66 29.6541V27.4595H178.851C179.605 27.4595 180.065 27.8811 180.065 28.5622C180.065 29.2216 179.605 29.6541 178.851 29.6541H177.66ZM179.037 30.9405C180.567 30.9405 181.584 29.9459 181.584 28.5514C181.584 27.1676 180.567 26.1622 179.037 26.1622H176.641C176.365 26.1622 176.141 26.386 176.141 26.6622V33.327C176.141 33.6032 176.365 33.827 176.641 33.827H177.149C177.425 33.827 177.649 33.6032 177.649 33.327V30.9405H179.037Z" fill="#161617"/>
<path d="M187.414 34C189.076 34 190.398 32.9946 190.398 31.1135V26.6622C190.398 26.386 190.174 26.1622 189.898 26.1622H189.39C189.114 26.1622 188.89 26.386 188.89 26.6622V31.0054C188.89 32.0108 188.332 32.5405 187.414 32.5405C186.518 32.5405 185.95 32.0108 185.95 31.0054V26.6622C185.95 26.386 185.726 26.1622 185.45 26.1622H184.941C184.665 26.1622 184.441 26.386 184.441 26.6622V31.1135C184.441 32.9946 185.764 34 187.414 34Z" fill="#161617"/>
<path d="M199.081 27.5784C199.357 27.5784 199.581 27.3545 199.581 27.0784V26.6622C199.581 26.386 199.357 26.1622 199.081 26.1622H193.666C193.389 26.1622 193.166 26.386 193.166 26.6622V27.0784C193.166 27.3545 193.389 27.5784 193.666 27.5784H195.614V33.327C195.614 33.6032 195.838 33.827 196.114 33.827H196.633C196.909 33.827 197.133 33.6032 197.133 33.327V27.5784H199.081Z" fill="#161617"/>
<path d="M206.766 33.827C207.042 33.827 207.266 33.6032 207.266 33.327V32.9216C207.266 32.6455 207.042 32.4216 206.766 32.4216H203.922V30.6486H206.449C206.725 30.6486 206.949 30.4248 206.949 30.1486V29.8189C206.949 29.5428 206.725 29.3189 206.449 29.3189H203.922V27.5676H206.766C207.042 27.5676 207.266 27.3437 207.266 27.0676V26.6622C207.266 26.386 207.042 26.1622 206.766 26.1622H202.913C202.637 26.1622 202.413 26.386 202.413 26.6622V33.327C202.413 33.6032 202.637 33.827 202.913 33.827H206.766Z" fill="#161617"/>
<path d="M214.177 33.5559C214.263 33.7224 214.435 33.827 214.622 33.827H215.169C215.546 33.827 215.787 33.4259 215.611 33.0929L214.339 30.6919C215.355 30.4 215.989 29.5892 215.989 28.5189C215.989 27.1892 215.027 26.1622 213.519 26.1622H210.492V33.327C210.492 33.6032 210.715 33.827 210.992 33.827H211.511C211.787 33.827 212.011 33.6032 212.011 33.327V30.8757H212.798L214.177 33.5559ZM212.011 29.5892V27.4595H213.235C214 27.4595 214.448 27.8811 214.448 28.5297C214.448 29.1568 214 29.5892 213.235 29.5892H212.011Z" fill="#161617"/>
<path d="M73.937 21V13.872H73.123C72.947 14.642 72.276 15.225 71.286 15.269V15.797H73.035V21H73.937ZM78.1258 20.34C77.5978 20.34 77.2128 20.142 76.9268 19.834C76.4538 19.339 76.2558 18.514 76.2558 17.436C76.2558 16.358 76.4538 15.533 76.9268 15.038C77.2128 14.73 77.5978 14.532 78.1258 14.532C78.6538 14.532 79.0388 14.73 79.3248 15.038C79.7978 15.533 79.9958 16.358 79.9958 17.436C79.9958 18.514 79.7978 19.339 79.3248 19.834C79.0388 20.142 78.6538 20.34 78.1258 20.34ZM78.1258 21.154C79.0938 21.154 79.7868 20.725 80.2378 20.065C80.7328 19.35 80.8978 18.404 80.8978 17.436C80.8978 16.468 80.7328 15.522 80.2378 14.807C79.7868 14.147 79.0938 13.718 78.1258 13.718C77.1578 13.718 76.4648 14.147 76.0138 14.807C75.5188 15.522 75.3538 16.468 75.3538 17.436C75.3538 18.404 75.5188 19.35 76.0138 20.065C76.4648 20.725 77.1578 21.154 78.1258 21.154ZM84.8611 20.34C84.3331 20.34 83.9481 20.142 83.6621 19.834C83.1891 19.339 82.9911 18.514 82.9911 17.436C82.9911 16.358 83.1891 15.533 83.6621 15.038C83.9481 14.73 84.3331 14.532 84.8611 14.532C85.3891 14.532 85.7741 14.73 86.0601 15.038C86.5331 15.533 86.7311 16.358 86.7311 17.436C86.7311 18.514 86.5331 19.339 86.0601 19.834C85.7741 20.142 85.3891 20.34 84.8611 20.34ZM84.8611 21.154C85.8291 21.154 86.5221 20.725 86.9731 20.065C87.4681 19.35 87.6331 18.404 87.6331 17.436C87.6331 16.468 87.4681 15.522 86.9731 14.807C86.5221 14.147 85.8291 13.718 84.8611 13.718C83.8931 13.718 83.2001 14.147 82.7491 14.807C82.2541 15.522 82.0891 16.468 82.0891 17.436C82.0891 18.404 82.2541 19.35 82.7491 20.065C83.2001 20.725 83.8931 21.154 84.8611 21.154ZM89.3635 15.478C89.3635 14.917 89.7815 14.499 90.3095 14.499C90.8595 14.499 91.2665 14.917 91.2665 15.478C91.2665 16.039 90.8595 16.457 90.3095 16.457C89.7815 16.457 89.3635 16.039 89.3635 15.478ZM88.5935 15.478C88.5935 16.435 89.3745 17.194 90.3095 17.194C91.2555 17.194 92.0475 16.435 92.0475 15.478C92.0475 14.521 91.2555 13.762 90.3095 13.762C89.3745 13.762 88.5935 14.521 88.5935 15.478ZM93.7195 19.339C93.7195 18.778 94.1375 18.36 94.6655 18.36C95.2155 18.36 95.6225 18.778 95.6225 19.339C95.6225 19.9 95.2155 20.318 94.6655 20.318C94.1375 20.318 93.7195 19.9 93.7195 19.339ZM92.9495 19.339C92.9495 20.296 93.7305 21.055 94.6655 21.055C95.6115 21.055 96.4035 20.296 96.4035 19.339C96.4035 18.382 95.6115 17.623 94.6655 17.623C93.7305 17.623 92.9495 18.382 92.9495 19.339ZM90.3425 21L95.4795 13.872H94.6325L89.4735 21H90.3425ZM102.709 15.577C101.191 15.577 100.069 16.743 100.069 18.36C100.069 19.988 101.191 21.154 102.709 21.154C104.227 21.154 105.349 19.988 105.349 18.36C105.349 16.743 104.227 15.577 102.709 15.577ZM102.709 16.369C103.655 16.369 104.469 17.084 104.469 18.36C104.469 19.647 103.655 20.362 102.709 20.362C101.763 20.362 100.949 19.647 100.949 18.36C100.949 17.084 101.763 16.369 102.709 16.369ZM107.586 17.898C107.586 17.073 108.059 16.369 108.906 16.369C109.863 16.369 110.215 16.985 110.215 17.755V21H111.095V17.623C111.095 16.479 110.479 15.577 109.214 15.577C108.521 15.577 107.905 15.907 107.564 16.523V15.731H106.706V21H107.586V17.898ZM115.57 18.228V17.458H112.633V18.228H115.57ZM119.423 16.369C120.369 16.369 120.798 17.018 120.941 17.579L121.733 17.238C121.513 16.402 120.776 15.577 119.423 15.577C117.938 15.577 116.816 16.721 116.816 18.36C116.816 19.966 117.916 21.154 119.434 21.154C120.787 21.154 121.524 20.296 121.777 19.515L121.007 19.174C120.853 19.658 120.435 20.362 119.434 20.362C118.499 20.362 117.696 19.625 117.696 18.36C117.696 17.084 118.499 16.369 119.423 16.369ZM123.914 17.854C123.914 17.04 124.398 16.369 125.234 16.369C126.191 16.369 126.543 16.985 126.543 17.755V21H127.423V17.623C127.423 16.479 126.807 15.577 125.542 15.577C124.86 15.577 124.255 15.885 123.914 16.435V13.047H123.034V21H123.914V17.854ZM128.697 19.603C128.697 20.406 129.346 21.154 130.435 21.154C131.381 21.154 131.953 20.637 132.184 20.208C132.184 20.615 132.228 20.879 132.25 21H133.108C133.075 20.835 133.042 20.56 133.042 20.12V17.458C133.042 16.435 132.415 15.577 130.919 15.577C129.786 15.577 128.906 16.259 128.796 17.26L129.632 17.458C129.709 16.787 130.171 16.325 130.941 16.325C131.766 16.325 132.162 16.776 132.162 17.37C132.162 17.601 132.074 17.766 131.689 17.821L130.27 18.03C129.346 18.162 128.697 18.679 128.697 19.603ZM130.545 20.406C129.94 20.406 129.577 19.988 129.577 19.559C129.577 19.042 129.94 18.756 130.446 18.679L132.162 18.426V18.778C132.162 19.9 131.458 20.406 130.545 20.406ZM135.58 21V15.731H134.7V21H135.58ZM134.48 13.652C134.48 14.015 134.777 14.312 135.14 14.312C135.503 14.312 135.8 14.015 135.8 13.652C135.8 13.289 135.503 12.992 135.14 12.992C134.777 12.992 134.48 13.289 134.48 13.652ZM138.179 17.898C138.179 17.073 138.652 16.369 139.499 16.369C140.456 16.369 140.808 16.985 140.808 17.755V21H141.688V17.623C141.688 16.479 141.072 15.577 139.807 15.577C139.114 15.577 138.498 15.907 138.157 16.523V15.731H137.299V21H138.179V17.898Z" fill="black"/>
<defs>
<linearGradient id="paint0_linear_1810_46" x1="33.5294" y1="7.20588" x2="47.888" y2="74.0171" gradientUnits="userSpaceOnUse">
<stop stop-color="#ED1E79"/>
<stop offset="1" stop-color="#522785"/>
</linearGradient>
<linearGradient id="paint1_linear_1810_46" x1="44.7958" y1="13.6484" x2="62.2866" y2="31.4386" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#F15A24"/>
<stop offset="0.6841" stop-color="#FBB03B"/>
</linearGradient>
<linearGradient id="paint2_linear_1810_46" x1="31.2044" y1="35.3515" x2="13.7136" y2="17.5614" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#ED1E79"/>
<stop offset="0.8929" stop-color="#522785"/>
</linearGradient>
</defs>
</svg>


-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/src/App.jsx:
-----------------------

import { useState } from 'react';
import { __backend_name_ident__ } from 'declarations/__backend_name__';

function App() {
  const [greeting, setGreeting] = useState('');

  function handleSubmit(event) {
    event.preventDefault();
    const name = event.target.elements.name.value;
    __backend_name_ident__.greet(name).then((greeting) => {
      setGreeting(greeting);
    });
    return false;
  }

  return (
    <main>
      <img src="/logo2.svg" alt="DFINITY logo" />
      <br />
      <br />
      <form action="#" onSubmit={handleSubmit}>
        <label htmlFor="name">Enter your name: &nbsp;</label>
        <input id="name" alt="Name" type="text" />
        <button type="submit">Click Me!</button>
      </form>
      <section id="greeting">{greeting}</section>
    </main>
  );
}

export default App;


-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/src/index.scss:
-----------------------

body {
  font-family: sans-serif;
  font-size: 1.5rem;
}

img {
  max-width: 50vw;
  max-height: 25vw;
  display: block;
  margin: auto;
}

form {
  display: flex;
  justify-content: center;
  gap: 0.5em;
  flex-flow: row wrap;
  max-width: 40vw;
  margin: auto;
  align-items: baseline;
}

button[type="submit"] {
  padding: 5px 20px;
  margin: 10px auto;
  float: right;
}

#greeting {
  margin: 10px auto;
  padding: 10px 60px;
  border: 1px solid #222;
}

#greeting:empty {
  display: none;
}


-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/src/main.jsx:
-----------------------

import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';
import './index.scss';

ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);


-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/src/vite-env.d.ts:
-----------------------

/// <reference types="vite/client" />


-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/tsconfig.json:
-----------------------

{
  "compilerOptions": {
    "target": "ESNext",
    "useDefineForClassFields": true,
    "lib": ["DOM", "DOM.Iterable", "ESNext"],
    "allowJs": false,
    "skipLibCheck": true,
    "esModuleInterop": false,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "module": "ESNext",
    "moduleResolution": "Node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "types": ["vite/client"]
  },
  "include": ["src"]
}


-----------------------

/src/dfx/assets/project_templates/react/src/__frontend_name__/vite.config.js:
-----------------------

import { fileURLToPath, URL } from 'url';
import react from '@vitejs/plugin-react';
import { defineConfig } from 'vite';
import environment from 'vite-plugin-environment';
import dotenv from 'dotenv';

dotenv.config({ path: '../../.env' });

export default defineConfig({
  build: {
    emptyOutDir: true,
  },
  optimizeDeps: {
    esbuildOptions: {
      define: {
        global: "globalThis",
      },
    },
  },
  server: {
    proxy: {
      "/api": {
        target: "http://127.0.0.1:4943",
        changeOrigin: true,
      },
    },
  },
  plugins: [
    react(),
    environment("all", { prefix: "CANISTER_" }),
    environment("all", { prefix: "DFX_" }),
  ],
  resolve: {
    alias: [
      {
        find: "declarations",
        replacement: fileURLToPath(
          new URL("../declarations", import.meta.url)
        ),
      },
    ],
  },
});


-----------------------

/src/dfx/assets/project_templates/react_tests/src/__frontend_name__/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/devDependencies/@testing-library~1jest-dom",
        "value": "^5.16.5"
    },
    {
        "op": "add",
        "path": "/devDependencies/@testing-library~1react",
        "value": "^14.0.0"
    },
    {
        "op": "add",
        "path": "/devDependencies/cross-fetch",
        "value": "^3.1.6"
    },
    {
        "op": "add",
        "path": "/devDependencies/vitest",
        "value": "^2.0.5"
    },
    {
        "op": "add",
        "path": "/devDependencies/jsdom",
        "value": "^22.1.0"
    },
    {
        "op": "add",
        "path": "/scripts/test",
        "value": "vitest run"
    }
]


-----------------------

/src/dfx/assets/project_templates/react_tests/src/__frontend_name__/src/setupTests.js:
-----------------------

import matchers from '@testing-library/jest-dom/matchers';
import 'cross-fetch/polyfill';
import { expect } from 'vitest';

expect.extend(matchers);


-----------------------

/src/dfx/assets/project_templates/react_tests/src/__frontend_name__/src/tests/App.test.jsx:
-----------------------

import { render, screen } from '@testing-library/react';
import { describe, expect, it } from 'vitest';
import App from '../App';
import { StrictMode } from 'react';

describe('App', () => {
  it('renders as expected', () => {
    render(
      <StrictMode>
        <App />
      </StrictMode>,
    );
    expect(document.body.innerHTML).toMatchInlineSnapshot('"<div><main><img src="/logo2.svg" alt="DFINITY logo"><br><br><form action="#"><label for="name">Enter your name: &nbsp;</label><input id="name" alt="Name" type="text"><button type="submit">Click Me!</button></form><section id="greeting"></section></main></div>"');
    expect(1).toEqual(1);
  });
});


-----------------------

/src/dfx/assets/project_templates/react_tests/src/__frontend_name__/tsconfig.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/compilerOptions/types/-",
        "value": "@testing-library/jest-dom"
    }
]


-----------------------

/src/dfx/assets/project_templates/react_tests/src/__frontend_name__/vite.config.js.patch:
-----------------------

--- a/src/__frontend_name__/vite.config.js
+++ b/src/__frontend_name__/vite.config.js
@@ -1,1 +1,2 @@
+/// <reference types="vitest" />
 import { fileURLToPath, URL } from 'url';
@@ -33,1 +33,5 @@
+  test: {
+    environment: 'jsdom',
+    setupFiles: 'src/setupTests.js',
+  },
   resolve: {


-----------------------

/src/dfx/assets/project_templates/rust/Cargo.toml:
-----------------------

[workspace]
members = [
    "src/__backend_name__"
]
resolver = "2"


-----------------------

/src/dfx/assets/project_templates/rust/README.md.patch:
-----------------------

--- a/README.md
+++ b/README.md
@@ -9,2 +9,6 @@
 - [Quick Start](https://internetcomputer.org/docs/current/developer-docs/setup/deploy-locally)
 - [SDK Developer Tools](https://internetcomputer.org/docs/current/developer-docs/setup/install)
+- [Rust Canister Development Guide](https://internetcomputer.org/docs/current/developer-docs/backend/rust/)
+- [ic-cdk](https://docs.rs/ic-cdk)
+- [ic-cdk-macros](https://docs.rs/ic-cdk-macros)
+- [Candid Introduction](https://internetcomputer.org/docs/current/developer-docs/backend/candid/)


-----------------------

/src/dfx/assets/project_templates/rust/dfx.json-patch:
-----------------------

[
  {
    "op": "add",
    "path": "/canisters/__backend_name__",
    "value": {
      "type": "rust",
      "package": "__backend_name__",
      "candid": "src/__backend_name__/__backend_name__.did"
    }
  }
]


-----------------------

/src/dfx/assets/project_templates/rust/src/__backend_name__/Cargo.toml:
-----------------------

[package]
name = "__backend_name__"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
crate-type = ["cdylib"]

[dependencies]
candid = "0.10"
ic-cdk = "0.16"
ic-cdk-timers = "0.10" # Feel free to remove this dependency if you don't need timers


-----------------------

/src/dfx/assets/project_templates/rust/src/__backend_name__/__backend_name__.did:
-----------------------

service : {
    "greet": (text) -> (text) query;
}


-----------------------

/src/dfx/assets/project_templates/rust/src/__backend_name__/src/lib.rs:
-----------------------

#[ic_cdk::query]
fn greet(name: String) -> String {
    format!("Hello, {}!", name)
}


-----------------------

/src/dfx/assets/project_templates/simple_assets/dfx.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/canisters/__frontend_name__",
        "value": {
            "type": "assets",
            "source": [
                "src/__frontend_name__/assets"
            ],
            "dependencies": [
                "__backend_name__"
            ]
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/simple_assets/src/__frontend_name__/assets/.ic-assets.json5:
-----------------------

[
    {
        "match": "**/*",

        // Provides a base set of security headers that will work for most dapps.
        // Any headers you manually specify will override the headers provided by the policy.
        // See 'dfx info security-policy' to see the policy and for advice on how to harden the headers.
        // Once you improved the headers for your dapp, set the security policy to "hardened" to disable the warning.
        // Options are: "hardened" | "standard" | "disabled".
        "security_policy": "standard",

        // Uncomment to disable the warning about using the
        // standard security policy, if you understand the risk
        // "disable_security_policy_warning": true,

        // Uncomment to redirect all requests from .raw.icp0.io to .icp0.io
        // "allow_raw_access": false
    },
]


-----------------------

/src/dfx/assets/project_templates/simple_assets/src/__frontend_name__/assets/sample-asset.txt:
-----------------------

This is a sample asset!


-----------------------

/src/dfx/assets/project_templates/svelte/dfx.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/canisters/__frontend_name__",
        "value": {
            "type": "assets",
            "source": [
                "src/__frontend_name__/dist"
            ],
            "dependencies": [
                "__backend_name__"
            ],
            "workspace": "__frontend_name__"
        }
    },
    {
        "op": "add",
        "path": "/canisters/__backend_name__/declarations",
        "value": {
            "node_compatibility": true
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/svelte/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/workspaces/-",
        "value": "src/__frontend_name__"
    }
]


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/package.json:
-----------------------

{
  "name": "__frontend_name__",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "setup": "npm i && dfx canister create __backend_name__ && dfx generate __backend_name__ && dfx deploy",
    "start": "vite --port 3000",
    "prebuild": "dfx generate",
    "build": "tsc && vite build",
    "format": "prettier --write \"src/**/*.{json,js,jsx,ts,tsx,css,scss}\""
  },
  "dependencies": {
    "@dfinity/agent": "^1.4.0",
    "@dfinity/candid": "^1.4.0",
    "@dfinity/principal": "^1.4.0"
  },
  "devDependencies": {
    "@sveltejs/adapter-static": "^3.0.4",
    "@sveltejs/kit": "^2.5.26",
    "@sveltejs/vite-plugin-svelte": "^3.1.2",
    "dotenv": "^16.3.1",
    "sass": "^1.63.6",
    "svelte": "^4.2.19",
    "svelte-check": "^4.0.1",
    "typescript": "^5.6.2",
    "vite": "^5.4.3",
    "vite-plugin-environment": "^1.1.3"
  }
}


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/src/app.d.ts:
-----------------------

// See https://kit.svelte.dev/docs/types#app
// for information about these interfaces
declare global {
  namespace App {
    // interface Error {}
    // interface Locals {}
    // interface PageData {}
    // interface Platform {}
  }
}

export {};


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/src/app.html:
-----------------------

<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="icon" href="%sveltekit.assets%/favicon.ico" />
	<meta name="viewport" content="width=device-width" />
	<title>IC Hello Starter</title>
	%sveltekit.head%
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">%sveltekit.body%</div>
</body>

</html>

-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/src/index.scss:
-----------------------

body {
  font-family: sans-serif;
  font-size: 1.5rem;
}

img {
  max-width: 50vw;
  max-height: 25vw;
  display: block;
  margin: auto;
}

form {
  display: flex;
  justify-content: center;
  gap: 0.5em;
  flex-flow: row wrap;
  max-width: 40vw;
  margin: auto;
  align-items: baseline;
}

button[type="submit"] {
  padding: 5px 20px;
  margin: 10px auto;
  float: right;
}

#greeting {
  margin: 10px auto;
  padding: 10px 60px;
  border: 1px solid #222;
}

#greeting:empty {
  display: none;
}


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/src/lib/canisters.js:
-----------------------

import { createActor, canisterId } from 'declarations/__backend_name__';
import { building } from '$app/environment';

function dummyActor() {
    return new Proxy({}, { get() { throw new Error("Canister invoked while building"); } });
}

const buildingOrTesting = building || process.env.NODE_ENV === "test";

export const backend = buildingOrTesting
    ? dummyActor()
    : createActor(canisterId);


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/src/routes/+layout.js:
-----------------------

export const prerender = true;

-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/src/routes/+page.svelte:
-----------------------

<script>
  import "../index.scss";
  import { backend } from "$lib/canisters";

  let greeting = "";

  function onSubmit(event) {
    const name = event.target.name.value;
    backend.greet(name).then((response) => {
      greeting = response;
    });
    return false;
  }
</script>

<main>
  <img src="/logo2.svg" alt="DFINITY logo" />
  <br />
  <br />
  <form action="#" on:submit|preventDefault={onSubmit}>
    <label for="name">Enter your name: &nbsp;</label>
    <input id="name" alt="Name" type="text" />
    <button type="submit">Click Me!</button>
  </form>
  <section id="greeting">{greeting}</section>
</main>


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/src/vite-env.d.ts:
-----------------------

/// <reference types="vite/client" />


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/static/.ic-assets.json5:
-----------------------

[
    {
        "match": "**/*",

        // Provides a base set of security headers that will work for most dapps.
        // Any headers you manually specify will override the headers provided by the policy.
        // See 'dfx info security-policy' to see the policy and for advice on how to harden the headers.
        // Once you improved the headers for your dapp, set the security policy to "hardened" to disable the warning.
        // Options are: "hardened" | "standard" | "disabled".
        "security_policy": "standard",

        "headers": {
            "Content-Security-Policy": "default-src 'self';script-src 'self' 'unsafe-eval' 'unsafe-inline';connect-src 'self' http://localhost:* https://icp0.io https://*.icp0.io https://icp-api.io;img-src 'self' data:;style-src * 'unsafe-inline';style-src-elem * 'unsafe-inline';font-src *;object-src 'none';base-uri 'self';frame-ancestors 'none';form-action 'self';upgrade-insecure-requests;",
        },

        // Uncomment to disable the warning about using the
        // standard security policy, if you understand the risk
        // "disable_security_policy_warning": true,

        // Uncomment to redirect all requests from .raw.icp0.io to .icp0.io
        // "allow_raw_access": false
    },
]


-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/dfx/assets/project_templates/svelte/src/__frontend_name__/static/favicon.ico

-----------------------

<svg width="684" height="147" viewBox="0 0 228 49" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="0.5" y="0.5" width="227" height="48" rx="3.5" fill="white" stroke="url(#paint0_linear_1810_46)"/>
<path d="M51.3703 12C48.4634 12 45.2947 13.5165 41.9454 16.5037C40.3565 17.9191 38.9842 19.4357 37.9551 20.6489C37.9551 20.6489 37.9551 20.6489 37.9641 20.6581V20.6489C37.9641 20.6489 39.5891 22.4504 41.3856 24.3805C42.3516 23.2133 43.7419 21.6232 45.3398 20.1894C48.319 17.5331 50.2599 16.9724 51.3703 16.9724C55.5502 16.9724 58.9446 20.3456 58.9446 24.4908C58.9446 28.6085 55.5412 31.9816 51.3703 32.0092C51.1808 32.0092 50.937 31.9816 50.6301 31.9173C51.8488 32.4504 53.1578 32.8364 54.4037 32.8364C62.0592 32.8364 63.5578 27.7537 63.6571 27.3861C63.8828 26.4577 64.0002 25.4835 64.0002 24.4816C64.0002 17.6066 58.3307 12 51.3703 12Z" fill="url(#paint1_linear_1810_46)"/>
<path d="M24.6298 36.9999C27.5368 36.9999 30.7055 35.4834 34.0548 32.4962C35.6437 31.0808 37.0159 29.5642 38.0451 28.351C38.0451 28.351 38.0451 28.351 38.0361 28.3418V28.351C38.0361 28.351 36.4111 26.5495 34.6145 24.6194C33.6486 25.7867 32.2583 27.3767 30.6604 28.8106C27.6812 31.4668 25.7403 32.0275 24.6298 32.0275C20.45 32.0183 17.0555 28.6451 17.0555 24.4999C17.0555 20.3822 20.459 17.0091 24.6298 16.9815C24.8194 16.9815 25.0632 17.0091 25.3701 17.0734C24.1514 16.5403 22.8423 16.1543 21.5965 16.1543C13.941 16.1543 12.4514 21.237 12.3431 21.5955C12.1174 22.533 12 23.4981 12 24.4999C12 31.3933 17.6694 36.9999 24.6298 36.9999Z" fill="url(#paint2_linear_1810_46)"/>
<path d="M54.3856 32.7261C50.4675 32.625 46.396 29.4816 45.5654 28.7004C43.4168 26.6783 38.4606 21.2096 38.0724 20.7776C34.4432 16.6324 29.5231 12 24.63 12H24.621H24.612C18.6717 12.0276 13.6794 16.1268 12.3433 21.5956C12.4426 21.2371 14.4016 16.0533 21.5877 16.2371C25.5057 16.3382 29.5953 19.5276 30.4349 20.3088C32.5835 22.3309 37.5398 27.7997 37.9279 28.2316C41.5571 32.3677 46.4772 37 51.3703 37H51.3793H51.3883C57.3286 36.9725 62.33 32.8732 63.6571 27.4044C63.5487 27.7629 61.5807 32.9008 54.3856 32.7261Z" fill="#29ABE2"/>
<path d="M73.0301 33.827C73.3063 33.827 73.5301 33.6032 73.5301 33.327V26.6622C73.5301 26.386 73.3063 26.1622 73.0301 26.1622H72.5C72.2239 26.1622 72 26.386 72 26.6622V33.327C72 33.6032 72.2239 33.827 72.5 33.827H73.0301Z" fill="#161617"/>
<path d="M83.2018 33.827C83.478 33.827 83.7018 33.6032 83.7018 33.327V26.6622C83.7018 26.386 83.478 26.1622 83.2018 26.1622H82.6936C82.4174 26.1622 82.1936 26.386 82.1936 26.6622V31.2324L79.264 26.6256C79.0805 26.337 78.7622 26.1622 78.4202 26.1622H77.5895C77.3133 26.1622 77.0895 26.386 77.0895 26.6622V33.327C77.0895 33.6032 77.3133 33.827 77.5895 33.827H78.0978C78.3739 33.827 78.5978 33.6032 78.5978 33.327V28.3892L81.9693 33.5987C82.0614 33.7411 82.2195 33.827 82.389 33.827H83.2018Z" fill="#161617"/>
<path d="M92.4535 27.5784C92.7297 27.5784 92.9535 27.3545 92.9535 27.0784V26.6622C92.9535 26.386 92.7297 26.1622 92.4535 26.1622H87.0379C86.7617 26.1622 86.5379 26.386 86.5379 26.6622V27.0784C86.5379 27.3545 86.7617 27.5784 87.0379 27.5784H88.9861V33.327C88.9861 33.6032 89.21 33.827 89.4861 33.827H90.0053C90.2814 33.827 90.5053 33.6032 90.5053 33.327V27.5784H92.4535Z" fill="#161617"/>
<path d="M100.138 33.827C100.415 33.827 100.638 33.6032 100.638 33.327V32.9216C100.638 32.6455 100.415 32.4216 100.138 32.4216H97.2941V30.6486H99.8215C100.098 30.6486 100.322 30.4248 100.322 30.1486V29.8189C100.322 29.5428 100.098 29.3189 99.8215 29.3189H97.2941V27.5676H100.138C100.415 27.5676 100.638 27.3437 100.638 27.0676V26.6622C100.638 26.386 100.415 26.1622 100.138 26.1622H96.2858C96.0096 26.1622 95.7858 26.386 95.7858 26.6622V33.327C95.7858 33.6032 96.0096 33.827 96.2858 33.827H100.138Z" fill="#161617"/>
<path d="M107.55 33.5559C107.635 33.7224 107.807 33.827 107.994 33.827H108.541C108.918 33.827 109.16 33.4259 108.983 33.0929L107.711 30.6919C108.727 30.4 109.361 29.5892 109.361 28.5189C109.361 27.1892 108.4 26.1622 106.891 26.1622H104.364C104.088 26.1622 103.864 26.386 103.864 26.6622V33.327C103.864 33.6032 104.088 33.827 104.364 33.827H104.883C105.159 33.827 105.383 33.6032 105.383 33.327V30.8757H106.17L107.55 33.5559ZM105.383 29.5892V27.4595H106.607C107.372 27.4595 107.82 27.8811 107.82 28.5297C107.82 29.1568 107.372 29.5892 106.607 29.5892H105.383Z" fill="#161617"/>
<path d="M118.609 33.827C118.885 33.827 119.109 33.6032 119.109 33.327V26.6622C119.109 26.386 118.885 26.1622 118.609 26.1622H118.101C117.825 26.1622 117.601 26.386 117.601 26.6622V31.2324L114.671 26.6256C114.488 26.337 114.17 26.1622 113.828 26.1622H112.997C112.721 26.1622 112.497 26.386 112.497 26.6622V33.327C112.497 33.6032 112.721 33.827 112.997 33.827H113.505C113.781 33.827 114.005 33.6032 114.005 33.327V28.3892L117.377 33.5987C117.469 33.7411 117.627 33.827 117.796 33.827H118.609Z" fill="#161617"/>
<path d="M127.03 33.827C127.306 33.827 127.53 33.6032 127.53 33.327V32.9216C127.53 32.6455 127.306 32.4216 127.03 32.4216H124.186V30.6486H126.713C126.989 30.6486 127.213 30.4248 127.213 30.1486V29.8189C127.213 29.5428 126.989 29.3189 126.713 29.3189H124.186V27.5676H127.03C127.306 27.5676 127.53 27.3437 127.53 27.0676V26.6622C127.53 26.386 127.306 26.1622 127.03 26.1622H123.178C122.901 26.1622 122.678 26.386 122.678 26.6622V33.327C122.678 33.6032 122.901 33.827 123.178 33.827H127.03Z" fill="#161617"/>
<path d="M135.939 27.5784C136.215 27.5784 136.439 27.3545 136.439 27.0784V26.6622C136.439 26.386 136.215 26.1622 135.939 26.1622H130.523C130.247 26.1622 130.023 26.386 130.023 26.6622V27.0784C130.023 27.3545 130.247 27.5784 130.523 27.5784H132.471V33.327C132.471 33.6032 132.695 33.827 132.971 33.827H133.491C133.767 33.827 133.991 33.6032 133.991 33.327V27.5784H135.939Z" fill="#161617"/>
<path d="M146.823 33.9892C148.998 33.9892 150.113 32.5622 150.408 31.3838L148.998 30.9622C148.79 31.6757 148.146 32.5297 146.823 32.5297C145.577 32.5297 144.419 31.6324 144.419 30C144.419 28.2595 145.643 27.4378 146.801 27.4378C148.146 27.4378 148.747 28.2486 148.932 28.9838L150.353 28.5405C150.047 27.2973 148.943 26 146.801 26C144.725 26 142.856 27.5568 142.856 30C142.856 32.4432 144.659 33.9892 146.823 33.9892Z" fill="#161617"/>
<path d="M154.428 29.9892C154.428 28.2595 155.653 27.4378 156.844 27.4378C158.046 27.4378 159.27 28.2595 159.27 29.9892C159.27 31.7189 158.046 32.5405 156.844 32.5405C155.653 32.5405 154.428 31.7189 154.428 29.9892ZM152.866 30C152.866 32.4649 154.745 33.9892 156.844 33.9892C158.953 33.9892 160.833 32.4649 160.833 30C160.833 27.5243 158.953 26 156.844 26C154.745 26 152.866 27.5243 152.866 30Z" fill="#161617"/>
<path d="M172.077 33.827C172.354 33.827 172.577 33.6032 172.577 33.327V26.6622C172.577 26.386 172.354 26.1622 172.077 26.1622H170.858C170.655 26.1622 170.472 26.2846 170.395 26.4722L168.249 31.6973L166.05 26.4683C165.972 26.2828 165.79 26.1622 165.589 26.1622H164.432C164.156 26.1622 163.932 26.386 163.932 26.6622V33.327C163.932 33.6032 164.156 33.827 164.432 33.827H164.875C165.151 33.827 165.375 33.6032 165.375 33.327V28.4973L167.476 33.52C167.554 33.706 167.736 33.827 167.937 33.827H168.528C168.73 33.827 168.912 33.7053 168.989 33.5187L171.091 28.4541V33.327C171.091 33.6032 171.315 33.827 171.591 33.827H172.077Z" fill="#161617"/>
<path d="M177.66 29.6541V27.4595H178.851C179.605 27.4595 180.065 27.8811 180.065 28.5622C180.065 29.2216 179.605 29.6541 178.851 29.6541H177.66ZM179.037 30.9405C180.567 30.9405 181.584 29.9459 181.584 28.5514C181.584 27.1676 180.567 26.1622 179.037 26.1622H176.641C176.365 26.1622 176.141 26.386 176.141 26.6622V33.327C176.141 33.6032 176.365 33.827 176.641 33.827H177.149C177.425 33.827 177.649 33.6032 177.649 33.327V30.9405H179.037Z" fill="#161617"/>
<path d="M187.414 34C189.076 34 190.398 32.9946 190.398 31.1135V26.6622C190.398 26.386 190.174 26.1622 189.898 26.1622H189.39C189.114 26.1622 188.89 26.386 188.89 26.6622V31.0054C188.89 32.0108 188.332 32.5405 187.414 32.5405C186.518 32.5405 185.95 32.0108 185.95 31.0054V26.6622C185.95 26.386 185.726 26.1622 185.45 26.1622H184.941C184.665 26.1622 184.441 26.386 184.441 26.6622V31.1135C184.441 32.9946 185.764 34 187.414 34Z" fill="#161617"/>
<path d="M199.081 27.5784C199.357 27.5784 199.581 27.3545 199.581 27.0784V26.6622C199.581 26.386 199.357 26.1622 199.081 26.1622H193.666C193.389 26.1622 193.166 26.386 193.166 26.6622V27.0784C193.166 27.3545 193.389 27.5784 193.666 27.5784H195.614V33.327C195.614 33.6032 195.838 33.827 196.114 33.827H196.633C196.909 33.827 197.133 33.6032 197.133 33.327V27.5784H199.081Z" fill="#161617"/>
<path d="M206.766 33.827C207.042 33.827 207.266 33.6032 207.266 33.327V32.9216C207.266 32.6455 207.042 32.4216 206.766 32.4216H203.922V30.6486H206.449C206.725 30.6486 206.949 30.4248 206.949 30.1486V29.8189C206.949 29.5428 206.725 29.3189 206.449 29.3189H203.922V27.5676H206.766C207.042 27.5676 207.266 27.3437 207.266 27.0676V26.6622C207.266 26.386 207.042 26.1622 206.766 26.1622H202.913C202.637 26.1622 202.413 26.386 202.413 26.6622V33.327C202.413 33.6032 202.637 33.827 202.913 33.827H206.766Z" fill="#161617"/>
<path d="M214.177 33.5559C214.263 33.7224 214.435 33.827 214.622 33.827H215.169C215.546 33.827 215.787 33.4259 215.611 33.0929L214.339 30.6919C215.355 30.4 215.989 29.5892 215.989 28.5189C215.989 27.1892 215.027 26.1622 213.519 26.1622H210.492V33.327C210.492 33.6032 210.715 33.827 210.992 33.827H211.511C211.787 33.827 212.011 33.6032 212.011 33.327V30.8757H212.798L214.177 33.5559ZM212.011 29.5892V27.4595H213.235C214 27.4595 214.448 27.8811 214.448 28.5297C214.448 29.1568 214 29.5892 213.235 29.5892H212.011Z" fill="#161617"/>
<path d="M73.937 21V13.872H73.123C72.947 14.642 72.276 15.225 71.286 15.269V15.797H73.035V21H73.937ZM78.1258 20.34C77.5978 20.34 77.2128 20.142 76.9268 19.834C76.4538 19.339 76.2558 18.514 76.2558 17.436C76.2558 16.358 76.4538 15.533 76.9268 15.038C77.2128 14.73 77.5978 14.532 78.1258 14.532C78.6538 14.532 79.0388 14.73 79.3248 15.038C79.7978 15.533 79.9958 16.358 79.9958 17.436C79.9958 18.514 79.7978 19.339 79.3248 19.834C79.0388 20.142 78.6538 20.34 78.1258 20.34ZM78.1258 21.154C79.0938 21.154 79.7868 20.725 80.2378 20.065C80.7328 19.35 80.8978 18.404 80.8978 17.436C80.8978 16.468 80.7328 15.522 80.2378 14.807C79.7868 14.147 79.0938 13.718 78.1258 13.718C77.1578 13.718 76.4648 14.147 76.0138 14.807C75.5188 15.522 75.3538 16.468 75.3538 17.436C75.3538 18.404 75.5188 19.35 76.0138 20.065C76.4648 20.725 77.1578 21.154 78.1258 21.154ZM84.8611 20.34C84.3331 20.34 83.9481 20.142 83.6621 19.834C83.1891 19.339 82.9911 18.514 82.9911 17.436C82.9911 16.358 83.1891 15.533 83.6621 15.038C83.9481 14.73 84.3331 14.532 84.8611 14.532C85.3891 14.532 85.7741 14.73 86.0601 15.038C86.5331 15.533 86.7311 16.358 86.7311 17.436C86.7311 18.514 86.5331 19.339 86.0601 19.834C85.7741 20.142 85.3891 20.34 84.8611 20.34ZM84.8611 21.154C85.8291 21.154 86.5221 20.725 86.9731 20.065C87.4681 19.35 87.6331 18.404 87.6331 17.436C87.6331 16.468 87.4681 15.522 86.9731 14.807C86.5221 14.147 85.8291 13.718 84.8611 13.718C83.8931 13.718 83.2001 14.147 82.7491 14.807C82.2541 15.522 82.0891 16.468 82.0891 17.436C82.0891 18.404 82.2541 19.35 82.7491 20.065C83.2001 20.725 83.8931 21.154 84.8611 21.154ZM89.3635 15.478C89.3635 14.917 89.7815 14.499 90.3095 14.499C90.8595 14.499 91.2665 14.917 91.2665 15.478C91.2665 16.039 90.8595 16.457 90.3095 16.457C89.7815 16.457 89.3635 16.039 89.3635 15.478ZM88.5935 15.478C88.5935 16.435 89.3745 17.194 90.3095 17.194C91.2555 17.194 92.0475 16.435 92.0475 15.478C92.0475 14.521 91.2555 13.762 90.3095 13.762C89.3745 13.762 88.5935 14.521 88.5935 15.478ZM93.7195 19.339C93.7195 18.778 94.1375 18.36 94.6655 18.36C95.2155 18.36 95.6225 18.778 95.6225 19.339C95.6225 19.9 95.2155 20.318 94.6655 20.318C94.1375 20.318 93.7195 19.9 93.7195 19.339ZM92.9495 19.339C92.9495 20.296 93.7305 21.055 94.6655 21.055C95.6115 21.055 96.4035 20.296 96.4035 19.339C96.4035 18.382 95.6115 17.623 94.6655 17.623C93.7305 17.623 92.9495 18.382 92.9495 19.339ZM90.3425 21L95.4795 13.872H94.6325L89.4735 21H90.3425ZM102.709 15.577C101.191 15.577 100.069 16.743 100.069 18.36C100.069 19.988 101.191 21.154 102.709 21.154C104.227 21.154 105.349 19.988 105.349 18.36C105.349 16.743 104.227 15.577 102.709 15.577ZM102.709 16.369C103.655 16.369 104.469 17.084 104.469 18.36C104.469 19.647 103.655 20.362 102.709 20.362C101.763 20.362 100.949 19.647 100.949 18.36C100.949 17.084 101.763 16.369 102.709 16.369ZM107.586 17.898C107.586 17.073 108.059 16.369 108.906 16.369C109.863 16.369 110.215 16.985 110.215 17.755V21H111.095V17.623C111.095 16.479 110.479 15.577 109.214 15.577C108.521 15.577 107.905 15.907 107.564 16.523V15.731H106.706V21H107.586V17.898ZM115.57 18.228V17.458H112.633V18.228H115.57ZM119.423 16.369C120.369 16.369 120.798 17.018 120.941 17.579L121.733 17.238C121.513 16.402 120.776 15.577 119.423 15.577C117.938 15.577 116.816 16.721 116.816 18.36C116.816 19.966 117.916 21.154 119.434 21.154C120.787 21.154 121.524 20.296 121.777 19.515L121.007 19.174C120.853 19.658 120.435 20.362 119.434 20.362C118.499 20.362 117.696 19.625 117.696 18.36C117.696 17.084 118.499 16.369 119.423 16.369ZM123.914 17.854C123.914 17.04 124.398 16.369 125.234 16.369C126.191 16.369 126.543 16.985 126.543 17.755V21H127.423V17.623C127.423 16.479 126.807 15.577 125.542 15.577C124.86 15.577 124.255 15.885 123.914 16.435V13.047H123.034V21H123.914V17.854ZM128.697 19.603C128.697 20.406 129.346 21.154 130.435 21.154C131.381 21.154 131.953 20.637 132.184 20.208C132.184 20.615 132.228 20.879 132.25 21H133.108C133.075 20.835 133.042 20.56 133.042 20.12V17.458C133.042 16.435 132.415 15.577 130.919 15.577C129.786 15.577 128.906 16.259 128.796 17.26L129.632 17.458C129.709 16.787 130.171 16.325 130.941 16.325C131.766 16.325 132.162 16.776 132.162 17.37C132.162 17.601 132.074 17.766 131.689 17.821L130.27 18.03C129.346 18.162 128.697 18.679 128.697 19.603ZM130.545 20.406C129.94 20.406 129.577 19.988 129.577 19.559C129.577 19.042 129.94 18.756 130.446 18.679L132.162 18.426V18.778C132.162 19.9 131.458 20.406 130.545 20.406ZM135.58 21V15.731H134.7V21H135.58ZM134.48 13.652C134.48 14.015 134.777 14.312 135.14 14.312C135.503 14.312 135.8 14.015 135.8 13.652C135.8 13.289 135.503 12.992 135.14 12.992C134.777 12.992 134.48 13.289 134.48 13.652ZM138.179 17.898C138.179 17.073 138.652 16.369 139.499 16.369C140.456 16.369 140.808 16.985 140.808 17.755V21H141.688V17.623C141.688 16.479 141.072 15.577 139.807 15.577C139.114 15.577 138.498 15.907 138.157 16.523V15.731H137.299V21H138.179V17.898Z" fill="black"/>
<defs>
<linearGradient id="paint0_linear_1810_46" x1="33.5294" y1="7.20588" x2="47.888" y2="74.0171" gradientUnits="userSpaceOnUse">
<stop stop-color="#ED1E79"/>
<stop offset="1" stop-color="#522785"/>
</linearGradient>
<linearGradient id="paint1_linear_1810_46" x1="44.7958" y1="13.6484" x2="62.2866" y2="31.4386" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#F15A24"/>
<stop offset="0.6841" stop-color="#FBB03B"/>
</linearGradient>
<linearGradient id="paint2_linear_1810_46" x1="31.2044" y1="35.3515" x2="13.7136" y2="17.5614" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#ED1E79"/>
<stop offset="0.8929" stop-color="#522785"/>
</linearGradient>
</defs>
</svg>


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/svelte.config.js:
-----------------------

import adapter from '@sveltejs/adapter-static';

/** @type {import('@sveltejs/kit').Config} */
const config = {
  kit: {
    // adapter-auto only supports some environments, see https://kit.svelte.dev/docs/adapter-auto for a list.
    // If your environment is not supported or you settled on a specific environment, switch out the adapter.
    // See https://kit.svelte.dev/docs/adapters for more information about adapters.
    adapter: adapter({
      pages: 'dist',
      assets: 'dist',
      fallback: undefined,
      precompress: false,
      strict: true,
    }),
  },
};

export default config;


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/tsconfig.json:
-----------------------

{
  "extends": "./.svelte-kit/tsconfig.json",
  "compilerOptions": {
    "target": "ESNext",
    "useDefineForClassFields": true,
    "lib": ["DOM", "DOM.Iterable", "ESNext"],
    "allowJs": false,
    "skipLibCheck": true,
    "esModuleInterop": false,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "module": "ESNext",
    "moduleResolution": "Node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "types": ["vite/client"]
  },
  "include": ["src"]
}


-----------------------

/src/dfx/assets/project_templates/svelte/src/__frontend_name__/vite.config.js:
-----------------------

import { fileURLToPath, URL } from 'url';
import { sveltekit } from '@sveltejs/kit/vite';
import { defineConfig } from 'vite';
import environment from 'vite-plugin-environment';
import dotenv from 'dotenv';

dotenv.config({ path: '../../.env' });

export default defineConfig({
  build: {
    emptyOutDir: true,
  },
  optimizeDeps: {
    esbuildOptions: {
      define: {
        global: "globalThis",
      },
    },
  },
  server: {
    proxy: {
      "/api": {
        target: "http://127.0.0.1:4943",
        changeOrigin: true,
      },
    },
  },
  plugins: [
    sveltekit(),
    environment("all", { prefix: "CANISTER_" }),
    environment("all", { prefix: "DFX_" }),
  ],
  resolve: {
    alias: [
      {
        find: "declarations",
        replacement: fileURLToPath(
          new URL("../declarations", import.meta.url)
        ),
      },
    ],
  },
});


-----------------------

/src/dfx/assets/project_templates/svelte_tests/src/__frontend_name__/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/devDependencies/@testing-library~1jest-dom",
        "value": "^5.16.5"
    },
    {
        "op": "add",
        "path": "/devDependencies/cross-fetch",
        "value": "^3.1.6"
    },
    {
        "op": "add",
        "path": "/devDependencies/jsdom",
        "value": "^22.1.0"
    },
    {
        "op": "add",
        "path": "/devDependencies/vitest",
        "value": "^2.0.5"
    },
    {
        "op": "add",
        "path": "/scripts/test",
        "value": "vitest run"
    }
]


-----------------------

/src/dfx/assets/project_templates/svelte_tests/src/__frontend_name__/src/setupTests.js:
-----------------------

import matchers from '@testing-library/jest-dom/matchers';
import 'cross-fetch/polyfill';
import { expect } from 'vitest';

expect.extend(matchers);


-----------------------

/src/dfx/assets/project_templates/svelte_tests/src/__frontend_name__/src/tests/App.test.js:
-----------------------

import { test, expect, afterEach } from 'vitest';
import App from '../routes/+page.svelte';

let host;

afterEach(() => {
  host.remove();
});

test('mount component', async () => {
  host = document.createElement('div');
  host.setAttribute('id', 'host');
  document.body.appendChild(host);
  const instance = new App({ target: host, props: {} });
  expect(instance).toBeTruthy();
  expect(host.innerHTML).toMatchInlineSnapshot(
    '"<main><img src="/logo2.svg" alt="DFINITY logo"> <br> <br> <form action="#"><label for="name">Enter your name: &nbsp;</label> <input id="name" alt="Name" type="text"> <button type="submit">Click Me!</button></form> <section id="greeting"></section></main>"'
  );
});


-----------------------

/src/dfx/assets/project_templates/svelte_tests/src/__frontend_name__/tsconfig.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/compilerOptions/types/-",
        "value": "@testing-library/jest-dom"
    }
]


-----------------------

/src/dfx/assets/project_templates/svelte_tests/src/__frontend_name__/vite.config.js.patch:
-----------------------

--- a/src/__frontend_name__/vite.config.js
+++ b/src/__frontend_name__/vite.config.js
@@ -1,1 +1,2 @@
+/// <reference types="vitest" />
 import { fileURLToPath, URL } from 'url';
@@ -33,1 +33,5 @@
+  test: {
+    environment: 'jsdom',
+    setupFiles: 'src/setupTests.js',
+  },
   resolve: {


-----------------------

/src/dfx/assets/project_templates/vanilla_js/dfx.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/canisters/__frontend_name__",
        "value": {
            "type": "assets",
            "source": [
                "src/__frontend_name__/dist"
            ],
            "dependencies": [
                "__backend_name__"
            ],
            "workspace": "__frontend_name__"
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/vanilla_js/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/workspaces/-",
        "value": "src/__frontend_name__"
    }
]


-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/assets/.ic-assets.json5:
-----------------------

[
    {
        "match": "**/*",

        // Provides a base set of security headers that will work for most dapps.
        // Any headers you manually specify will override the headers provided by the policy.
        // See 'dfx info security-policy' to see the policy and for advice on how to harden the headers.
        // Once you improved the headers for your dapp, set the security policy to "hardened" to disable the warning.
        // Options are: "hardened" | "standard" | "disabled".
        "security_policy": "standard",

        // Uncomment to disable the warning about using the
        // standard security policy, if you understand the risk
        // "disable_security_policy_warning": true,

        // Uncomment to redirect all requests from .raw.icp0.io to .icp0.io
        // "allow_raw_access": false
    },
]


-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/assets/favicon.ico

-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/index.html:
-----------------------

<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <link rel="icon" type="image/svg+xml" href="/favicon.ico" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>IC Hello Starter</title>
</head>

<body>
  <div id="root"></div>
  <script type="module" src="/src/main.js"></script>
</body>

</html>

-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/package.json:
-----------------------

{
  "name": "__frontend_name__",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "setup": "npm i && dfx canister create __backend_name__ && dfx generate __backend_name__ && dfx deploy",
    "start": "vite --port 3000",
    "prebuild": "dfx generate",
    "build": "tsc && vite build",
    "format": "prettier --write \"src/**/*.{json,js,jsx,ts,tsx,css,scss}\""
  },
  "devDependencies": {
    "@testing-library/jest-dom": "^5.16.5",
    "cross-fetch": "^3.1.6",
    "dotenv": "^16.3.1",
    "sass": "^1.63.6",
    "typescript": "^5.1.3",
    "vite": "^4.3.9",
    "vite-plugin-environment": "^1.1.3",
    "vitest": "^2.0.5"
  },
  "dependencies": {
    "@dfinity/agent": "^1.4.0",
    "@dfinity/candid": "^1.4.0",
    "@dfinity/principal": "^1.4.0",
    "lit-html": "^2.8.0"
  }
}


-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/src/App.js:
-----------------------

import { html, render } from 'lit-html';
import { __backend_name_ident__ } from 'declarations/__backend_name__';
import logo from './logo2.svg';

class App {
  greeting = '';

  constructor() {
    this.#render();
  }

  #handleSubmit = async (e) => {
    e.preventDefault();
    const name = document.getElementById('name').value;
    this.greeting = await __backend_name_ident__.greet(name);
    this.#render();
  };

  #render() {
    let body = html`
      <main>
        <img src="${logo}" alt="DFINITY logo" />
        <br />
        <br />
        <form action="#">
          <label for="name">Enter your name: &nbsp;</label>
          <input id="name" alt="Name" type="text" />
          <button type="submit">Click Me!</button>
        </form>
        <section id="greeting">${this.greeting}</section>
      </main>
    `;
    render(body, document.getElementById('root'));
    document
      .querySelector('form')
      .addEventListener('submit', this.#handleSubmit);
  }
}

export default App;


-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/src/index.scss:
-----------------------

body {
  font-family: sans-serif;
  font-size: 1.5rem;
}

img {
  max-width: 50vw;
  max-height: 25vw;
  display: block;
  margin: auto;
}

form {
  display: flex;
  justify-content: center;
  gap: 0.5em;
  flex-flow: row wrap;
  max-width: 40vw;
  margin: auto;
  align-items: baseline;
}

button[type="submit"] {
  padding: 5px 20px;
  margin: 10px auto;
  float: right;
}

#greeting {
  margin: 10px auto;
  padding: 10px 60px;
  border: 1px solid #222;
}

#greeting:empty {
  display: none;
}


-----------------------

<svg width="684" height="147" viewBox="0 0 228 49" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="0.5" y="0.5" width="227" height="48" rx="3.5" fill="white" stroke="url(#paint0_linear_1810_46)"/>
<path d="M51.3703 12C48.4634 12 45.2947 13.5165 41.9454 16.5037C40.3565 17.9191 38.9842 19.4357 37.9551 20.6489C37.9551 20.6489 37.9551 20.6489 37.9641 20.6581V20.6489C37.9641 20.6489 39.5891 22.4504 41.3856 24.3805C42.3516 23.2133 43.7419 21.6232 45.3398 20.1894C48.319 17.5331 50.2599 16.9724 51.3703 16.9724C55.5502 16.9724 58.9446 20.3456 58.9446 24.4908C58.9446 28.6085 55.5412 31.9816 51.3703 32.0092C51.1808 32.0092 50.937 31.9816 50.6301 31.9173C51.8488 32.4504 53.1578 32.8364 54.4037 32.8364C62.0592 32.8364 63.5578 27.7537 63.6571 27.3861C63.8828 26.4577 64.0002 25.4835 64.0002 24.4816C64.0002 17.6066 58.3307 12 51.3703 12Z" fill="url(#paint1_linear_1810_46)"/>
<path d="M24.6298 36.9999C27.5368 36.9999 30.7055 35.4834 34.0548 32.4962C35.6437 31.0808 37.0159 29.5642 38.0451 28.351C38.0451 28.351 38.0451 28.351 38.0361 28.3418V28.351C38.0361 28.351 36.4111 26.5495 34.6145 24.6194C33.6486 25.7867 32.2583 27.3767 30.6604 28.8106C27.6812 31.4668 25.7403 32.0275 24.6298 32.0275C20.45 32.0183 17.0555 28.6451 17.0555 24.4999C17.0555 20.3822 20.459 17.0091 24.6298 16.9815C24.8194 16.9815 25.0632 17.0091 25.3701 17.0734C24.1514 16.5403 22.8423 16.1543 21.5965 16.1543C13.941 16.1543 12.4514 21.237 12.3431 21.5955C12.1174 22.533 12 23.4981 12 24.4999C12 31.3933 17.6694 36.9999 24.6298 36.9999Z" fill="url(#paint2_linear_1810_46)"/>
<path d="M54.3856 32.7261C50.4675 32.625 46.396 29.4816 45.5654 28.7004C43.4168 26.6783 38.4606 21.2096 38.0724 20.7776C34.4432 16.6324 29.5231 12 24.63 12H24.621H24.612C18.6717 12.0276 13.6794 16.1268 12.3433 21.5956C12.4426 21.2371 14.4016 16.0533 21.5877 16.2371C25.5057 16.3382 29.5953 19.5276 30.4349 20.3088C32.5835 22.3309 37.5398 27.7997 37.9279 28.2316C41.5571 32.3677 46.4772 37 51.3703 37H51.3793H51.3883C57.3286 36.9725 62.33 32.8732 63.6571 27.4044C63.5487 27.7629 61.5807 32.9008 54.3856 32.7261Z" fill="#29ABE2"/>
<path d="M73.0301 33.827C73.3063 33.827 73.5301 33.6032 73.5301 33.327V26.6622C73.5301 26.386 73.3063 26.1622 73.0301 26.1622H72.5C72.2239 26.1622 72 26.386 72 26.6622V33.327C72 33.6032 72.2239 33.827 72.5 33.827H73.0301Z" fill="#161617"/>
<path d="M83.2018 33.827C83.478 33.827 83.7018 33.6032 83.7018 33.327V26.6622C83.7018 26.386 83.478 26.1622 83.2018 26.1622H82.6936C82.4174 26.1622 82.1936 26.386 82.1936 26.6622V31.2324L79.264 26.6256C79.0805 26.337 78.7622 26.1622 78.4202 26.1622H77.5895C77.3133 26.1622 77.0895 26.386 77.0895 26.6622V33.327C77.0895 33.6032 77.3133 33.827 77.5895 33.827H78.0978C78.3739 33.827 78.5978 33.6032 78.5978 33.327V28.3892L81.9693 33.5987C82.0614 33.7411 82.2195 33.827 82.389 33.827H83.2018Z" fill="#161617"/>
<path d="M92.4535 27.5784C92.7297 27.5784 92.9535 27.3545 92.9535 27.0784V26.6622C92.9535 26.386 92.7297 26.1622 92.4535 26.1622H87.0379C86.7617 26.1622 86.5379 26.386 86.5379 26.6622V27.0784C86.5379 27.3545 86.7617 27.5784 87.0379 27.5784H88.9861V33.327C88.9861 33.6032 89.21 33.827 89.4861 33.827H90.0053C90.2814 33.827 90.5053 33.6032 90.5053 33.327V27.5784H92.4535Z" fill="#161617"/>
<path d="M100.138 33.827C100.415 33.827 100.638 33.6032 100.638 33.327V32.9216C100.638 32.6455 100.415 32.4216 100.138 32.4216H97.2941V30.6486H99.8215C100.098 30.6486 100.322 30.4248 100.322 30.1486V29.8189C100.322 29.5428 100.098 29.3189 99.8215 29.3189H97.2941V27.5676H100.138C100.415 27.5676 100.638 27.3437 100.638 27.0676V26.6622C100.638 26.386 100.415 26.1622 100.138 26.1622H96.2858C96.0096 26.1622 95.7858 26.386 95.7858 26.6622V33.327C95.7858 33.6032 96.0096 33.827 96.2858 33.827H100.138Z" fill="#161617"/>
<path d="M107.55 33.5559C107.635 33.7224 107.807 33.827 107.994 33.827H108.541C108.918 33.827 109.16 33.4259 108.983 33.0929L107.711 30.6919C108.727 30.4 109.361 29.5892 109.361 28.5189C109.361 27.1892 108.4 26.1622 106.891 26.1622H104.364C104.088 26.1622 103.864 26.386 103.864 26.6622V33.327C103.864 33.6032 104.088 33.827 104.364 33.827H104.883C105.159 33.827 105.383 33.6032 105.383 33.327V30.8757H106.17L107.55 33.5559ZM105.383 29.5892V27.4595H106.607C107.372 27.4595 107.82 27.8811 107.82 28.5297C107.82 29.1568 107.372 29.5892 106.607 29.5892H105.383Z" fill="#161617"/>
<path d="M118.609 33.827C118.885 33.827 119.109 33.6032 119.109 33.327V26.6622C119.109 26.386 118.885 26.1622 118.609 26.1622H118.101C117.825 26.1622 117.601 26.386 117.601 26.6622V31.2324L114.671 26.6256C114.488 26.337 114.17 26.1622 113.828 26.1622H112.997C112.721 26.1622 112.497 26.386 112.497 26.6622V33.327C112.497 33.6032 112.721 33.827 112.997 33.827H113.505C113.781 33.827 114.005 33.6032 114.005 33.327V28.3892L117.377 33.5987C117.469 33.7411 117.627 33.827 117.796 33.827H118.609Z" fill="#161617"/>
<path d="M127.03 33.827C127.306 33.827 127.53 33.6032 127.53 33.327V32.9216C127.53 32.6455 127.306 32.4216 127.03 32.4216H124.186V30.6486H126.713C126.989 30.6486 127.213 30.4248 127.213 30.1486V29.8189C127.213 29.5428 126.989 29.3189 126.713 29.3189H124.186V27.5676H127.03C127.306 27.5676 127.53 27.3437 127.53 27.0676V26.6622C127.53 26.386 127.306 26.1622 127.03 26.1622H123.178C122.901 26.1622 122.678 26.386 122.678 26.6622V33.327C122.678 33.6032 122.901 33.827 123.178 33.827H127.03Z" fill="#161617"/>
<path d="M135.939 27.5784C136.215 27.5784 136.439 27.3545 136.439 27.0784V26.6622C136.439 26.386 136.215 26.1622 135.939 26.1622H130.523C130.247 26.1622 130.023 26.386 130.023 26.6622V27.0784C130.023 27.3545 130.247 27.5784 130.523 27.5784H132.471V33.327C132.471 33.6032 132.695 33.827 132.971 33.827H133.491C133.767 33.827 133.991 33.6032 133.991 33.327V27.5784H135.939Z" fill="#161617"/>
<path d="M146.823 33.9892C148.998 33.9892 150.113 32.5622 150.408 31.3838L148.998 30.9622C148.79 31.6757 148.146 32.5297 146.823 32.5297C145.577 32.5297 144.419 31.6324 144.419 30C144.419 28.2595 145.643 27.4378 146.801 27.4378C148.146 27.4378 148.747 28.2486 148.932 28.9838L150.353 28.5405C150.047 27.2973 148.943 26 146.801 26C144.725 26 142.856 27.5568 142.856 30C142.856 32.4432 144.659 33.9892 146.823 33.9892Z" fill="#161617"/>
<path d="M154.428 29.9892C154.428 28.2595 155.653 27.4378 156.844 27.4378C158.046 27.4378 159.27 28.2595 159.27 29.9892C159.27 31.7189 158.046 32.5405 156.844 32.5405C155.653 32.5405 154.428 31.7189 154.428 29.9892ZM152.866 30C152.866 32.4649 154.745 33.9892 156.844 33.9892C158.953 33.9892 160.833 32.4649 160.833 30C160.833 27.5243 158.953 26 156.844 26C154.745 26 152.866 27.5243 152.866 30Z" fill="#161617"/>
<path d="M172.077 33.827C172.354 33.827 172.577 33.6032 172.577 33.327V26.6622C172.577 26.386 172.354 26.1622 172.077 26.1622H170.858C170.655 26.1622 170.472 26.2846 170.395 26.4722L168.249 31.6973L166.05 26.4683C165.972 26.2828 165.79 26.1622 165.589 26.1622H164.432C164.156 26.1622 163.932 26.386 163.932 26.6622V33.327C163.932 33.6032 164.156 33.827 164.432 33.827H164.875C165.151 33.827 165.375 33.6032 165.375 33.327V28.4973L167.476 33.52C167.554 33.706 167.736 33.827 167.937 33.827H168.528C168.73 33.827 168.912 33.7053 168.989 33.5187L171.091 28.4541V33.327C171.091 33.6032 171.315 33.827 171.591 33.827H172.077Z" fill="#161617"/>
<path d="M177.66 29.6541V27.4595H178.851C179.605 27.4595 180.065 27.8811 180.065 28.5622C180.065 29.2216 179.605 29.6541 178.851 29.6541H177.66ZM179.037 30.9405C180.567 30.9405 181.584 29.9459 181.584 28.5514C181.584 27.1676 180.567 26.1622 179.037 26.1622H176.641C176.365 26.1622 176.141 26.386 176.141 26.6622V33.327C176.141 33.6032 176.365 33.827 176.641 33.827H177.149C177.425 33.827 177.649 33.6032 177.649 33.327V30.9405H179.037Z" fill="#161617"/>
<path d="M187.414 34C189.076 34 190.398 32.9946 190.398 31.1135V26.6622C190.398 26.386 190.174 26.1622 189.898 26.1622H189.39C189.114 26.1622 188.89 26.386 188.89 26.6622V31.0054C188.89 32.0108 188.332 32.5405 187.414 32.5405C186.518 32.5405 185.95 32.0108 185.95 31.0054V26.6622C185.95 26.386 185.726 26.1622 185.45 26.1622H184.941C184.665 26.1622 184.441 26.386 184.441 26.6622V31.1135C184.441 32.9946 185.764 34 187.414 34Z" fill="#161617"/>
<path d="M199.081 27.5784C199.357 27.5784 199.581 27.3545 199.581 27.0784V26.6622C199.581 26.386 199.357 26.1622 199.081 26.1622H193.666C193.389 26.1622 193.166 26.386 193.166 26.6622V27.0784C193.166 27.3545 193.389 27.5784 193.666 27.5784H195.614V33.327C195.614 33.6032 195.838 33.827 196.114 33.827H196.633C196.909 33.827 197.133 33.6032 197.133 33.327V27.5784H199.081Z" fill="#161617"/>
<path d="M206.766 33.827C207.042 33.827 207.266 33.6032 207.266 33.327V32.9216C207.266 32.6455 207.042 32.4216 206.766 32.4216H203.922V30.6486H206.449C206.725 30.6486 206.949 30.4248 206.949 30.1486V29.8189C206.949 29.5428 206.725 29.3189 206.449 29.3189H203.922V27.5676H206.766C207.042 27.5676 207.266 27.3437 207.266 27.0676V26.6622C207.266 26.386 207.042 26.1622 206.766 26.1622H202.913C202.637 26.1622 202.413 26.386 202.413 26.6622V33.327C202.413 33.6032 202.637 33.827 202.913 33.827H206.766Z" fill="#161617"/>
<path d="M214.177 33.5559C214.263 33.7224 214.435 33.827 214.622 33.827H215.169C215.546 33.827 215.787 33.4259 215.611 33.0929L214.339 30.6919C215.355 30.4 215.989 29.5892 215.989 28.5189C215.989 27.1892 215.027 26.1622 213.519 26.1622H210.492V33.327C210.492 33.6032 210.715 33.827 210.992 33.827H211.511C211.787 33.827 212.011 33.6032 212.011 33.327V30.8757H212.798L214.177 33.5559ZM212.011 29.5892V27.4595H213.235C214 27.4595 214.448 27.8811 214.448 28.5297C214.448 29.1568 214 29.5892 213.235 29.5892H212.011Z" fill="#161617"/>
<path d="M73.937 21V13.872H73.123C72.947 14.642 72.276 15.225 71.286 15.269V15.797H73.035V21H73.937ZM78.1258 20.34C77.5978 20.34 77.2128 20.142 76.9268 19.834C76.4538 19.339 76.2558 18.514 76.2558 17.436C76.2558 16.358 76.4538 15.533 76.9268 15.038C77.2128 14.73 77.5978 14.532 78.1258 14.532C78.6538 14.532 79.0388 14.73 79.3248 15.038C79.7978 15.533 79.9958 16.358 79.9958 17.436C79.9958 18.514 79.7978 19.339 79.3248 19.834C79.0388 20.142 78.6538 20.34 78.1258 20.34ZM78.1258 21.154C79.0938 21.154 79.7868 20.725 80.2378 20.065C80.7328 19.35 80.8978 18.404 80.8978 17.436C80.8978 16.468 80.7328 15.522 80.2378 14.807C79.7868 14.147 79.0938 13.718 78.1258 13.718C77.1578 13.718 76.4648 14.147 76.0138 14.807C75.5188 15.522 75.3538 16.468 75.3538 17.436C75.3538 18.404 75.5188 19.35 76.0138 20.065C76.4648 20.725 77.1578 21.154 78.1258 21.154ZM84.8611 20.34C84.3331 20.34 83.9481 20.142 83.6621 19.834C83.1891 19.339 82.9911 18.514 82.9911 17.436C82.9911 16.358 83.1891 15.533 83.6621 15.038C83.9481 14.73 84.3331 14.532 84.8611 14.532C85.3891 14.532 85.7741 14.73 86.0601 15.038C86.5331 15.533 86.7311 16.358 86.7311 17.436C86.7311 18.514 86.5331 19.339 86.0601 19.834C85.7741 20.142 85.3891 20.34 84.8611 20.34ZM84.8611 21.154C85.8291 21.154 86.5221 20.725 86.9731 20.065C87.4681 19.35 87.6331 18.404 87.6331 17.436C87.6331 16.468 87.4681 15.522 86.9731 14.807C86.5221 14.147 85.8291 13.718 84.8611 13.718C83.8931 13.718 83.2001 14.147 82.7491 14.807C82.2541 15.522 82.0891 16.468 82.0891 17.436C82.0891 18.404 82.2541 19.35 82.7491 20.065C83.2001 20.725 83.8931 21.154 84.8611 21.154ZM89.3635 15.478C89.3635 14.917 89.7815 14.499 90.3095 14.499C90.8595 14.499 91.2665 14.917 91.2665 15.478C91.2665 16.039 90.8595 16.457 90.3095 16.457C89.7815 16.457 89.3635 16.039 89.3635 15.478ZM88.5935 15.478C88.5935 16.435 89.3745 17.194 90.3095 17.194C91.2555 17.194 92.0475 16.435 92.0475 15.478C92.0475 14.521 91.2555 13.762 90.3095 13.762C89.3745 13.762 88.5935 14.521 88.5935 15.478ZM93.7195 19.339C93.7195 18.778 94.1375 18.36 94.6655 18.36C95.2155 18.36 95.6225 18.778 95.6225 19.339C95.6225 19.9 95.2155 20.318 94.6655 20.318C94.1375 20.318 93.7195 19.9 93.7195 19.339ZM92.9495 19.339C92.9495 20.296 93.7305 21.055 94.6655 21.055C95.6115 21.055 96.4035 20.296 96.4035 19.339C96.4035 18.382 95.6115 17.623 94.6655 17.623C93.7305 17.623 92.9495 18.382 92.9495 19.339ZM90.3425 21L95.4795 13.872H94.6325L89.4735 21H90.3425ZM102.709 15.577C101.191 15.577 100.069 16.743 100.069 18.36C100.069 19.988 101.191 21.154 102.709 21.154C104.227 21.154 105.349 19.988 105.349 18.36C105.349 16.743 104.227 15.577 102.709 15.577ZM102.709 16.369C103.655 16.369 104.469 17.084 104.469 18.36C104.469 19.647 103.655 20.362 102.709 20.362C101.763 20.362 100.949 19.647 100.949 18.36C100.949 17.084 101.763 16.369 102.709 16.369ZM107.586 17.898C107.586 17.073 108.059 16.369 108.906 16.369C109.863 16.369 110.215 16.985 110.215 17.755V21H111.095V17.623C111.095 16.479 110.479 15.577 109.214 15.577C108.521 15.577 107.905 15.907 107.564 16.523V15.731H106.706V21H107.586V17.898ZM115.57 18.228V17.458H112.633V18.228H115.57ZM119.423 16.369C120.369 16.369 120.798 17.018 120.941 17.579L121.733 17.238C121.513 16.402 120.776 15.577 119.423 15.577C117.938 15.577 116.816 16.721 116.816 18.36C116.816 19.966 117.916 21.154 119.434 21.154C120.787 21.154 121.524 20.296 121.777 19.515L121.007 19.174C120.853 19.658 120.435 20.362 119.434 20.362C118.499 20.362 117.696 19.625 117.696 18.36C117.696 17.084 118.499 16.369 119.423 16.369ZM123.914 17.854C123.914 17.04 124.398 16.369 125.234 16.369C126.191 16.369 126.543 16.985 126.543 17.755V21H127.423V17.623C127.423 16.479 126.807 15.577 125.542 15.577C124.86 15.577 124.255 15.885 123.914 16.435V13.047H123.034V21H123.914V17.854ZM128.697 19.603C128.697 20.406 129.346 21.154 130.435 21.154C131.381 21.154 131.953 20.637 132.184 20.208C132.184 20.615 132.228 20.879 132.25 21H133.108C133.075 20.835 133.042 20.56 133.042 20.12V17.458C133.042 16.435 132.415 15.577 130.919 15.577C129.786 15.577 128.906 16.259 128.796 17.26L129.632 17.458C129.709 16.787 130.171 16.325 130.941 16.325C131.766 16.325 132.162 16.776 132.162 17.37C132.162 17.601 132.074 17.766 131.689 17.821L130.27 18.03C129.346 18.162 128.697 18.679 128.697 19.603ZM130.545 20.406C129.94 20.406 129.577 19.988 129.577 19.559C129.577 19.042 129.94 18.756 130.446 18.679L132.162 18.426V18.778C132.162 19.9 131.458 20.406 130.545 20.406ZM135.58 21V15.731H134.7V21H135.58ZM134.48 13.652C134.48 14.015 134.777 14.312 135.14 14.312C135.503 14.312 135.8 14.015 135.8 13.652C135.8 13.289 135.503 12.992 135.14 12.992C134.777 12.992 134.48 13.289 134.48 13.652ZM138.179 17.898C138.179 17.073 138.652 16.369 139.499 16.369C140.456 16.369 140.808 16.985 140.808 17.755V21H141.688V17.623C141.688 16.479 141.072 15.577 139.807 15.577C139.114 15.577 138.498 15.907 138.157 16.523V15.731H137.299V21H138.179V17.898Z" fill="black"/>
<defs>
<linearGradient id="paint0_linear_1810_46" x1="33.5294" y1="7.20588" x2="47.888" y2="74.0171" gradientUnits="userSpaceOnUse">
<stop stop-color="#ED1E79"/>
<stop offset="1" stop-color="#522785"/>
</linearGradient>
<linearGradient id="paint1_linear_1810_46" x1="44.7958" y1="13.6484" x2="62.2866" y2="31.4386" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#F15A24"/>
<stop offset="0.6841" stop-color="#FBB03B"/>
</linearGradient>
<linearGradient id="paint2_linear_1810_46" x1="31.2044" y1="35.3515" x2="13.7136" y2="17.5614" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#ED1E79"/>
<stop offset="0.8929" stop-color="#522785"/>
</linearGradient>
</defs>
</svg>


-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/src/main.js:
-----------------------

import App from './App';
import './index.scss';

const app = new App();


-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/src/vite-env.d.ts:
-----------------------

/// <reference types="vite/client" />


-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/tsconfig.json:
-----------------------

{
  "compilerOptions": {
    "target": "ESNext",
    "useDefineForClassFields": true,
    "lib": ["DOM", "DOM.Iterable", "ESNext"],
    "allowJs": false,
    "skipLibCheck": true,
    "esModuleInterop": false,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "module": "ESNext",
    "moduleResolution": "Node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "types": ["vite/client"]
  },
  "include": ["src"]
}


-----------------------

/src/dfx/assets/project_templates/vanilla_js/src/__frontend_name__/vite.config.js:
-----------------------

import { defineConfig } from 'vite';
import { fileURLToPath, URL } from 'url';
import environment from 'vite-plugin-environment';
import dotenv from 'dotenv';

dotenv.config({ path: '../../.env' });

export default defineConfig({
  build: {
    emptyOutDir: true,
  },
  optimizeDeps: {
    esbuildOptions: {
      define: {
        global: "globalThis",
      },
    },
  },
  server: {
    proxy: {
      "/api": {
        target: "http://127.0.0.1:4943",
        changeOrigin: true,
      },
    },
  },
  publicDir: "assets",
  plugins: [
    environment("all", { prefix: "CANISTER_" }),
    environment("all", { prefix: "DFX_" }),
  ],
  resolve: {
    alias: [
      {
        find: "declarations",
        replacement: fileURLToPath(
          new URL("../declarations", import.meta.url)
        ),
      },
    ],
  },
});


-----------------------

/src/dfx/assets/project_templates/vanilla_js_tests/src/__frontend_name__/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/devDependencies/@testing-library~1jest-dom",
        "value": "^5.16.5"
    },
    {
        "op": "add",
        "path": "/devDependencies/cross-fetch",
        "value": "^3.1.6"
    },
    {
        "op": "add",
        "path": "/devDependencies/jsdom",
        "value": "^22.1.0"
    },
    {
        "op": "add",
        "path": "/devDependencies/vitest",
        "value": "^2.0.5"
    },
    {
        "op": "add",
        "path": "/scripts/test",
        "value": "vitest run"
    }
]


-----------------------

/src/dfx/assets/project_templates/vanilla_js_tests/src/__frontend_name__/src/setupTests.js:
-----------------------

import matchers from '@testing-library/jest-dom/matchers';
import 'cross-fetch/polyfill';
import { expect } from 'vitest';

expect.extend(matchers);


-----------------------

/src/dfx/assets/project_templates/vanilla_js_tests/src/__frontend_name__/src/tests/App.test.js:
-----------------------

import { describe, expect, it } from 'vitest';
import App from '../App';

describe('App', () => {
  it('renders as expected', () => {
    const root = document.createElement('div');
    root.id = 'root';
    document.body.appendChild(root);
    new App();

    expect(root.querySelector('main')).toBeTruthy();
  });
});


-----------------------

/src/dfx/assets/project_templates/vanilla_js_tests/src/__frontend_name__/tsconfig.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/compilerOptions/types/-",
        "value": "@testing-library/jest-dom"
    }
]


-----------------------

/src/dfx/assets/project_templates/vanilla_js_tests/src/__frontend_name__/vite.config.js.patch:
-----------------------

--- a/src/__frontend_name__/vite.config.js
+++ b/src/__frontend_name__/vite.config.js
@@ -1,1 +1,2 @@
+/// <reference types="vitest" />
 import { defineConfig } from 'vite';
@@ -32,1 +32,5 @@
+  test: {
+    environment: 'jsdom',
+    setupFiles: 'src/setupTests.js',
+  },
   resolve: {


-----------------------

/src/dfx/assets/project_templates/vue/dfx.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/canisters/__frontend_name__",
        "value": {
            "type": "assets",
            "dependencies": [
                "__backend_name__"
            ],
            "source": [
                "src/__frontend_name__/dist"
            ],
            "workspace": "__frontend_name__"
        }
    }
]


-----------------------

/src/dfx/assets/project_templates/vue/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/workspaces/-",
        "value": "src/__frontend_name__"
    }
]


-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/index.html:
-----------------------

<!DOCTYPE html>
<html lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width,initial-scale=1.0" />
  <link rel="icon" href="/favicon.ico" />
  <title>IC Hello Starter</title>
</head>

<body>
  <noscript>
    <strong>We're sorry but this application doesn't work properly without
      JavaScript enabled. Please enable it to continue.</strong>
  </noscript>
  <div id="app"></div>
  <script src="/src/main.js" type="module"></script>
</body>

</html>

-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/package.json:
-----------------------

{
  "name": "__frontend_name__",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "setup": "npm i && dfx canister create __backend_name__ && dfx generate __backend_name__ && dfx deploy",
    "start": "vite --port 3000",
    "prebuild": "dfx generate",
    "build": "tsc && vite build",
    "format": "prettier --write \"src/**/*.{json,js,jsx,ts,tsx,css,scss}\""
  },
  "devDependencies": {
    "@vitejs/plugin-vue": "^4.2.3",
    "@vue/tsconfig": "^0.4.0",
    "dotenv": "^16.3.1",
    "prettier": "^2.8.8",
    "sass": "^1.63.6",
    "typescript": "^5.1.3",
    "vite": "^4.3.9",
    "vite-plugin-environment": "^1.1.3"
  },
  "dependencies": {
    "pinia": "^2.1.6",
    "vue": "^3.3.4",
    "@dfinity/agent": "^1.4.0",
    "@dfinity/candid": "^1.4.0",
    "@dfinity/principal": "^1.4.0"
  }
}

-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/public/.ic-assets.json5:
-----------------------

[
    {
        "match": "**/*",

        // Provides a base set of security headers that will work for most dapps.
        // Any headers you manually specify will override the headers provided by the policy.
        // See 'dfx info security-policy' to see the policy and for advice on how to harden the headers.
        // Once you improved the headers for your dapp, set the security policy to "hardened" to disable the warning.
        // Options are: "hardened" | "standard" | "disabled".
        "security_policy": "standard",

        // Uncomment to disable the warning about using the
        // standard security policy, if you understand the risk
        // "disable_security_policy_warning": true,

        // Uncomment to redirect all requests from .raw.icp0.io to .icp0.io
        // "allow_raw_access": false
    },
]


-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/dfx/assets/project_templates/vue/src/__frontend_name__/public/favicon.ico

-----------------------

<svg width="684" height="147" viewBox="0 0 228 49" fill="none" xmlns="http://www.w3.org/2000/svg">
<rect x="0.5" y="0.5" width="227" height="48" rx="3.5" fill="white" stroke="url(#paint0_linear_1810_46)"/>
<path d="M51.3703 12C48.4634 12 45.2947 13.5165 41.9454 16.5037C40.3565 17.9191 38.9842 19.4357 37.9551 20.6489C37.9551 20.6489 37.9551 20.6489 37.9641 20.6581V20.6489C37.9641 20.6489 39.5891 22.4504 41.3856 24.3805C42.3516 23.2133 43.7419 21.6232 45.3398 20.1894C48.319 17.5331 50.2599 16.9724 51.3703 16.9724C55.5502 16.9724 58.9446 20.3456 58.9446 24.4908C58.9446 28.6085 55.5412 31.9816 51.3703 32.0092C51.1808 32.0092 50.937 31.9816 50.6301 31.9173C51.8488 32.4504 53.1578 32.8364 54.4037 32.8364C62.0592 32.8364 63.5578 27.7537 63.6571 27.3861C63.8828 26.4577 64.0002 25.4835 64.0002 24.4816C64.0002 17.6066 58.3307 12 51.3703 12Z" fill="url(#paint1_linear_1810_46)"/>
<path d="M24.6298 36.9999C27.5368 36.9999 30.7055 35.4834 34.0548 32.4962C35.6437 31.0808 37.0159 29.5642 38.0451 28.351C38.0451 28.351 38.0451 28.351 38.0361 28.3418V28.351C38.0361 28.351 36.4111 26.5495 34.6145 24.6194C33.6486 25.7867 32.2583 27.3767 30.6604 28.8106C27.6812 31.4668 25.7403 32.0275 24.6298 32.0275C20.45 32.0183 17.0555 28.6451 17.0555 24.4999C17.0555 20.3822 20.459 17.0091 24.6298 16.9815C24.8194 16.9815 25.0632 17.0091 25.3701 17.0734C24.1514 16.5403 22.8423 16.1543 21.5965 16.1543C13.941 16.1543 12.4514 21.237 12.3431 21.5955C12.1174 22.533 12 23.4981 12 24.4999C12 31.3933 17.6694 36.9999 24.6298 36.9999Z" fill="url(#paint2_linear_1810_46)"/>
<path d="M54.3856 32.7261C50.4675 32.625 46.396 29.4816 45.5654 28.7004C43.4168 26.6783 38.4606 21.2096 38.0724 20.7776C34.4432 16.6324 29.5231 12 24.63 12H24.621H24.612C18.6717 12.0276 13.6794 16.1268 12.3433 21.5956C12.4426 21.2371 14.4016 16.0533 21.5877 16.2371C25.5057 16.3382 29.5953 19.5276 30.4349 20.3088C32.5835 22.3309 37.5398 27.7997 37.9279 28.2316C41.5571 32.3677 46.4772 37 51.3703 37H51.3793H51.3883C57.3286 36.9725 62.33 32.8732 63.6571 27.4044C63.5487 27.7629 61.5807 32.9008 54.3856 32.7261Z" fill="#29ABE2"/>
<path d="M73.0301 33.827C73.3063 33.827 73.5301 33.6032 73.5301 33.327V26.6622C73.5301 26.386 73.3063 26.1622 73.0301 26.1622H72.5C72.2239 26.1622 72 26.386 72 26.6622V33.327C72 33.6032 72.2239 33.827 72.5 33.827H73.0301Z" fill="#161617"/>
<path d="M83.2018 33.827C83.478 33.827 83.7018 33.6032 83.7018 33.327V26.6622C83.7018 26.386 83.478 26.1622 83.2018 26.1622H82.6936C82.4174 26.1622 82.1936 26.386 82.1936 26.6622V31.2324L79.264 26.6256C79.0805 26.337 78.7622 26.1622 78.4202 26.1622H77.5895C77.3133 26.1622 77.0895 26.386 77.0895 26.6622V33.327C77.0895 33.6032 77.3133 33.827 77.5895 33.827H78.0978C78.3739 33.827 78.5978 33.6032 78.5978 33.327V28.3892L81.9693 33.5987C82.0614 33.7411 82.2195 33.827 82.389 33.827H83.2018Z" fill="#161617"/>
<path d="M92.4535 27.5784C92.7297 27.5784 92.9535 27.3545 92.9535 27.0784V26.6622C92.9535 26.386 92.7297 26.1622 92.4535 26.1622H87.0379C86.7617 26.1622 86.5379 26.386 86.5379 26.6622V27.0784C86.5379 27.3545 86.7617 27.5784 87.0379 27.5784H88.9861V33.327C88.9861 33.6032 89.21 33.827 89.4861 33.827H90.0053C90.2814 33.827 90.5053 33.6032 90.5053 33.327V27.5784H92.4535Z" fill="#161617"/>
<path d="M100.138 33.827C100.415 33.827 100.638 33.6032 100.638 33.327V32.9216C100.638 32.6455 100.415 32.4216 100.138 32.4216H97.2941V30.6486H99.8215C100.098 30.6486 100.322 30.4248 100.322 30.1486V29.8189C100.322 29.5428 100.098 29.3189 99.8215 29.3189H97.2941V27.5676H100.138C100.415 27.5676 100.638 27.3437 100.638 27.0676V26.6622C100.638 26.386 100.415 26.1622 100.138 26.1622H96.2858C96.0096 26.1622 95.7858 26.386 95.7858 26.6622V33.327C95.7858 33.6032 96.0096 33.827 96.2858 33.827H100.138Z" fill="#161617"/>
<path d="M107.55 33.5559C107.635 33.7224 107.807 33.827 107.994 33.827H108.541C108.918 33.827 109.16 33.4259 108.983 33.0929L107.711 30.6919C108.727 30.4 109.361 29.5892 109.361 28.5189C109.361 27.1892 108.4 26.1622 106.891 26.1622H104.364C104.088 26.1622 103.864 26.386 103.864 26.6622V33.327C103.864 33.6032 104.088 33.827 104.364 33.827H104.883C105.159 33.827 105.383 33.6032 105.383 33.327V30.8757H106.17L107.55 33.5559ZM105.383 29.5892V27.4595H106.607C107.372 27.4595 107.82 27.8811 107.82 28.5297C107.82 29.1568 107.372 29.5892 106.607 29.5892H105.383Z" fill="#161617"/>
<path d="M118.609 33.827C118.885 33.827 119.109 33.6032 119.109 33.327V26.6622C119.109 26.386 118.885 26.1622 118.609 26.1622H118.101C117.825 26.1622 117.601 26.386 117.601 26.6622V31.2324L114.671 26.6256C114.488 26.337 114.17 26.1622 113.828 26.1622H112.997C112.721 26.1622 112.497 26.386 112.497 26.6622V33.327C112.497 33.6032 112.721 33.827 112.997 33.827H113.505C113.781 33.827 114.005 33.6032 114.005 33.327V28.3892L117.377 33.5987C117.469 33.7411 117.627 33.827 117.796 33.827H118.609Z" fill="#161617"/>
<path d="M127.03 33.827C127.306 33.827 127.53 33.6032 127.53 33.327V32.9216C127.53 32.6455 127.306 32.4216 127.03 32.4216H124.186V30.6486H126.713C126.989 30.6486 127.213 30.4248 127.213 30.1486V29.8189C127.213 29.5428 126.989 29.3189 126.713 29.3189H124.186V27.5676H127.03C127.306 27.5676 127.53 27.3437 127.53 27.0676V26.6622C127.53 26.386 127.306 26.1622 127.03 26.1622H123.178C122.901 26.1622 122.678 26.386 122.678 26.6622V33.327C122.678 33.6032 122.901 33.827 123.178 33.827H127.03Z" fill="#161617"/>
<path d="M135.939 27.5784C136.215 27.5784 136.439 27.3545 136.439 27.0784V26.6622C136.439 26.386 136.215 26.1622 135.939 26.1622H130.523C130.247 26.1622 130.023 26.386 130.023 26.6622V27.0784C130.023 27.3545 130.247 27.5784 130.523 27.5784H132.471V33.327C132.471 33.6032 132.695 33.827 132.971 33.827H133.491C133.767 33.827 133.991 33.6032 133.991 33.327V27.5784H135.939Z" fill="#161617"/>
<path d="M146.823 33.9892C148.998 33.9892 150.113 32.5622 150.408 31.3838L148.998 30.9622C148.79 31.6757 148.146 32.5297 146.823 32.5297C145.577 32.5297 144.419 31.6324 144.419 30C144.419 28.2595 145.643 27.4378 146.801 27.4378C148.146 27.4378 148.747 28.2486 148.932 28.9838L150.353 28.5405C150.047 27.2973 148.943 26 146.801 26C144.725 26 142.856 27.5568 142.856 30C142.856 32.4432 144.659 33.9892 146.823 33.9892Z" fill="#161617"/>
<path d="M154.428 29.9892C154.428 28.2595 155.653 27.4378 156.844 27.4378C158.046 27.4378 159.27 28.2595 159.27 29.9892C159.27 31.7189 158.046 32.5405 156.844 32.5405C155.653 32.5405 154.428 31.7189 154.428 29.9892ZM152.866 30C152.866 32.4649 154.745 33.9892 156.844 33.9892C158.953 33.9892 160.833 32.4649 160.833 30C160.833 27.5243 158.953 26 156.844 26C154.745 26 152.866 27.5243 152.866 30Z" fill="#161617"/>
<path d="M172.077 33.827C172.354 33.827 172.577 33.6032 172.577 33.327V26.6622C172.577 26.386 172.354 26.1622 172.077 26.1622H170.858C170.655 26.1622 170.472 26.2846 170.395 26.4722L168.249 31.6973L166.05 26.4683C165.972 26.2828 165.79 26.1622 165.589 26.1622H164.432C164.156 26.1622 163.932 26.386 163.932 26.6622V33.327C163.932 33.6032 164.156 33.827 164.432 33.827H164.875C165.151 33.827 165.375 33.6032 165.375 33.327V28.4973L167.476 33.52C167.554 33.706 167.736 33.827 167.937 33.827H168.528C168.73 33.827 168.912 33.7053 168.989 33.5187L171.091 28.4541V33.327C171.091 33.6032 171.315 33.827 171.591 33.827H172.077Z" fill="#161617"/>
<path d="M177.66 29.6541V27.4595H178.851C179.605 27.4595 180.065 27.8811 180.065 28.5622C180.065 29.2216 179.605 29.6541 178.851 29.6541H177.66ZM179.037 30.9405C180.567 30.9405 181.584 29.9459 181.584 28.5514C181.584 27.1676 180.567 26.1622 179.037 26.1622H176.641C176.365 26.1622 176.141 26.386 176.141 26.6622V33.327C176.141 33.6032 176.365 33.827 176.641 33.827H177.149C177.425 33.827 177.649 33.6032 177.649 33.327V30.9405H179.037Z" fill="#161617"/>
<path d="M187.414 34C189.076 34 190.398 32.9946 190.398 31.1135V26.6622C190.398 26.386 190.174 26.1622 189.898 26.1622H189.39C189.114 26.1622 188.89 26.386 188.89 26.6622V31.0054C188.89 32.0108 188.332 32.5405 187.414 32.5405C186.518 32.5405 185.95 32.0108 185.95 31.0054V26.6622C185.95 26.386 185.726 26.1622 185.45 26.1622H184.941C184.665 26.1622 184.441 26.386 184.441 26.6622V31.1135C184.441 32.9946 185.764 34 187.414 34Z" fill="#161617"/>
<path d="M199.081 27.5784C199.357 27.5784 199.581 27.3545 199.581 27.0784V26.6622C199.581 26.386 199.357 26.1622 199.081 26.1622H193.666C193.389 26.1622 193.166 26.386 193.166 26.6622V27.0784C193.166 27.3545 193.389 27.5784 193.666 27.5784H195.614V33.327C195.614 33.6032 195.838 33.827 196.114 33.827H196.633C196.909 33.827 197.133 33.6032 197.133 33.327V27.5784H199.081Z" fill="#161617"/>
<path d="M206.766 33.827C207.042 33.827 207.266 33.6032 207.266 33.327V32.9216C207.266 32.6455 207.042 32.4216 206.766 32.4216H203.922V30.6486H206.449C206.725 30.6486 206.949 30.4248 206.949 30.1486V29.8189C206.949 29.5428 206.725 29.3189 206.449 29.3189H203.922V27.5676H206.766C207.042 27.5676 207.266 27.3437 207.266 27.0676V26.6622C207.266 26.386 207.042 26.1622 206.766 26.1622H202.913C202.637 26.1622 202.413 26.386 202.413 26.6622V33.327C202.413 33.6032 202.637 33.827 202.913 33.827H206.766Z" fill="#161617"/>
<path d="M214.177 33.5559C214.263 33.7224 214.435 33.827 214.622 33.827H215.169C215.546 33.827 215.787 33.4259 215.611 33.0929L214.339 30.6919C215.355 30.4 215.989 29.5892 215.989 28.5189C215.989 27.1892 215.027 26.1622 213.519 26.1622H210.492V33.327C210.492 33.6032 210.715 33.827 210.992 33.827H211.511C211.787 33.827 212.011 33.6032 212.011 33.327V30.8757H212.798L214.177 33.5559ZM212.011 29.5892V27.4595H213.235C214 27.4595 214.448 27.8811 214.448 28.5297C214.448 29.1568 214 29.5892 213.235 29.5892H212.011Z" fill="#161617"/>
<path d="M73.937 21V13.872H73.123C72.947 14.642 72.276 15.225 71.286 15.269V15.797H73.035V21H73.937ZM78.1258 20.34C77.5978 20.34 77.2128 20.142 76.9268 19.834C76.4538 19.339 76.2558 18.514 76.2558 17.436C76.2558 16.358 76.4538 15.533 76.9268 15.038C77.2128 14.73 77.5978 14.532 78.1258 14.532C78.6538 14.532 79.0388 14.73 79.3248 15.038C79.7978 15.533 79.9958 16.358 79.9958 17.436C79.9958 18.514 79.7978 19.339 79.3248 19.834C79.0388 20.142 78.6538 20.34 78.1258 20.34ZM78.1258 21.154C79.0938 21.154 79.7868 20.725 80.2378 20.065C80.7328 19.35 80.8978 18.404 80.8978 17.436C80.8978 16.468 80.7328 15.522 80.2378 14.807C79.7868 14.147 79.0938 13.718 78.1258 13.718C77.1578 13.718 76.4648 14.147 76.0138 14.807C75.5188 15.522 75.3538 16.468 75.3538 17.436C75.3538 18.404 75.5188 19.35 76.0138 20.065C76.4648 20.725 77.1578 21.154 78.1258 21.154ZM84.8611 20.34C84.3331 20.34 83.9481 20.142 83.6621 19.834C83.1891 19.339 82.9911 18.514 82.9911 17.436C82.9911 16.358 83.1891 15.533 83.6621 15.038C83.9481 14.73 84.3331 14.532 84.8611 14.532C85.3891 14.532 85.7741 14.73 86.0601 15.038C86.5331 15.533 86.7311 16.358 86.7311 17.436C86.7311 18.514 86.5331 19.339 86.0601 19.834C85.7741 20.142 85.3891 20.34 84.8611 20.34ZM84.8611 21.154C85.8291 21.154 86.5221 20.725 86.9731 20.065C87.4681 19.35 87.6331 18.404 87.6331 17.436C87.6331 16.468 87.4681 15.522 86.9731 14.807C86.5221 14.147 85.8291 13.718 84.8611 13.718C83.8931 13.718 83.2001 14.147 82.7491 14.807C82.2541 15.522 82.0891 16.468 82.0891 17.436C82.0891 18.404 82.2541 19.35 82.7491 20.065C83.2001 20.725 83.8931 21.154 84.8611 21.154ZM89.3635 15.478C89.3635 14.917 89.7815 14.499 90.3095 14.499C90.8595 14.499 91.2665 14.917 91.2665 15.478C91.2665 16.039 90.8595 16.457 90.3095 16.457C89.7815 16.457 89.3635 16.039 89.3635 15.478ZM88.5935 15.478C88.5935 16.435 89.3745 17.194 90.3095 17.194C91.2555 17.194 92.0475 16.435 92.0475 15.478C92.0475 14.521 91.2555 13.762 90.3095 13.762C89.3745 13.762 88.5935 14.521 88.5935 15.478ZM93.7195 19.339C93.7195 18.778 94.1375 18.36 94.6655 18.36C95.2155 18.36 95.6225 18.778 95.6225 19.339C95.6225 19.9 95.2155 20.318 94.6655 20.318C94.1375 20.318 93.7195 19.9 93.7195 19.339ZM92.9495 19.339C92.9495 20.296 93.7305 21.055 94.6655 21.055C95.6115 21.055 96.4035 20.296 96.4035 19.339C96.4035 18.382 95.6115 17.623 94.6655 17.623C93.7305 17.623 92.9495 18.382 92.9495 19.339ZM90.3425 21L95.4795 13.872H94.6325L89.4735 21H90.3425ZM102.709 15.577C101.191 15.577 100.069 16.743 100.069 18.36C100.069 19.988 101.191 21.154 102.709 21.154C104.227 21.154 105.349 19.988 105.349 18.36C105.349 16.743 104.227 15.577 102.709 15.577ZM102.709 16.369C103.655 16.369 104.469 17.084 104.469 18.36C104.469 19.647 103.655 20.362 102.709 20.362C101.763 20.362 100.949 19.647 100.949 18.36C100.949 17.084 101.763 16.369 102.709 16.369ZM107.586 17.898C107.586 17.073 108.059 16.369 108.906 16.369C109.863 16.369 110.215 16.985 110.215 17.755V21H111.095V17.623C111.095 16.479 110.479 15.577 109.214 15.577C108.521 15.577 107.905 15.907 107.564 16.523V15.731H106.706V21H107.586V17.898ZM115.57 18.228V17.458H112.633V18.228H115.57ZM119.423 16.369C120.369 16.369 120.798 17.018 120.941 17.579L121.733 17.238C121.513 16.402 120.776 15.577 119.423 15.577C117.938 15.577 116.816 16.721 116.816 18.36C116.816 19.966 117.916 21.154 119.434 21.154C120.787 21.154 121.524 20.296 121.777 19.515L121.007 19.174C120.853 19.658 120.435 20.362 119.434 20.362C118.499 20.362 117.696 19.625 117.696 18.36C117.696 17.084 118.499 16.369 119.423 16.369ZM123.914 17.854C123.914 17.04 124.398 16.369 125.234 16.369C126.191 16.369 126.543 16.985 126.543 17.755V21H127.423V17.623C127.423 16.479 126.807 15.577 125.542 15.577C124.86 15.577 124.255 15.885 123.914 16.435V13.047H123.034V21H123.914V17.854ZM128.697 19.603C128.697 20.406 129.346 21.154 130.435 21.154C131.381 21.154 131.953 20.637 132.184 20.208C132.184 20.615 132.228 20.879 132.25 21H133.108C133.075 20.835 133.042 20.56 133.042 20.12V17.458C133.042 16.435 132.415 15.577 130.919 15.577C129.786 15.577 128.906 16.259 128.796 17.26L129.632 17.458C129.709 16.787 130.171 16.325 130.941 16.325C131.766 16.325 132.162 16.776 132.162 17.37C132.162 17.601 132.074 17.766 131.689 17.821L130.27 18.03C129.346 18.162 128.697 18.679 128.697 19.603ZM130.545 20.406C129.94 20.406 129.577 19.988 129.577 19.559C129.577 19.042 129.94 18.756 130.446 18.679L132.162 18.426V18.778C132.162 19.9 131.458 20.406 130.545 20.406ZM135.58 21V15.731H134.7V21H135.58ZM134.48 13.652C134.48 14.015 134.777 14.312 135.14 14.312C135.503 14.312 135.8 14.015 135.8 13.652C135.8 13.289 135.503 12.992 135.14 12.992C134.777 12.992 134.48 13.289 134.48 13.652ZM138.179 17.898C138.179 17.073 138.652 16.369 139.499 16.369C140.456 16.369 140.808 16.985 140.808 17.755V21H141.688V17.623C141.688 16.479 141.072 15.577 139.807 15.577C139.114 15.577 138.498 15.907 138.157 16.523V15.731H137.299V21H138.179V17.898Z" fill="black"/>
<defs>
<linearGradient id="paint0_linear_1810_46" x1="33.5294" y1="7.20588" x2="47.888" y2="74.0171" gradientUnits="userSpaceOnUse">
<stop stop-color="#ED1E79"/>
<stop offset="1" stop-color="#522785"/>
</linearGradient>
<linearGradient id="paint1_linear_1810_46" x1="44.7958" y1="13.6484" x2="62.2866" y2="31.4386" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#F15A24"/>
<stop offset="0.6841" stop-color="#FBB03B"/>
</linearGradient>
<linearGradient id="paint2_linear_1810_46" x1="31.2044" y1="35.3515" x2="13.7136" y2="17.5614" gradientUnits="userSpaceOnUse">
<stop offset="0.21" stop-color="#ED1E79"/>
<stop offset="0.8929" stop-color="#522785"/>
</linearGradient>
</defs>
</svg>


-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/src/App.vue:
-----------------------

<script setup>
import { ref } from 'vue';
import { __backend_name_ident__ } from 'declarations/__backend_name__/index';
let greeting = ref('');

async function handleSubmit(e) {
  e.preventDefault();
  const target = e.target;
  const name = target.querySelector('#name').value;
  await __backend_name_ident__.greet(name).then((response) => {
    greeting.value = response;
  });
}
</script>

<template>
  <main>
    <img src="/logo2.svg" alt="DFINITY logo" />
    <br />
    <br />
    <form action="#" @submit="handleSubmit">
      <label for="name">Enter your name: &nbsp;</label>
      <input id="name" alt="Name" type="text" />
      <button type="submit">Click Me!</button>
    </form>
    <section id="greeting">{{ greeting }}</section>
  </main>
</template>


-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/src/index.scss:
-----------------------

body {
  font-family: sans-serif;
  font-size: 1.5rem;
}

img {
  max-width: 50vw;
  max-height: 25vw;
  display: block;
  margin: auto;
}

form {
  display: flex;
  justify-content: center;
  gap: 0.5em;
  flex-flow: row wrap;
  max-width: 40vw;
  margin: auto;
  align-items: baseline;
}

button[type="submit"] {
  padding: 5px 20px;
  margin: 10px auto;
  float: right;
}

#greeting {
  margin: 10px auto;
  padding: 10px 60px;
  border: 1px solid #222;
}

#greeting:empty {
  display: none;
}


-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/src/main.js:
-----------------------

import { createPinia } from 'pinia';
import { createApp } from 'vue';
import './index.scss';
import App from './App.vue';

createApp(App).use(createPinia()).mount('#app');


-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/src/vite-env.d.ts:
-----------------------

/// <reference types="vite/client" />


-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/tsconfig.json:
-----------------------

{
  "extends": "@vue/tsconfig/tsconfig.json",
  "compilerOptions": {
    "target": "ESNext",
    "useDefineForClassFields": true,
    "lib": ["DOM", "DOM.Iterable", "ESNext"],
    "allowJs": true,
    "skipLibCheck": true,
    "esModuleInterop": false,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true,
    "module": "ESNext",
    "moduleResolution": "Node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "types": ["vite/client"]
  },
  "include": ["src"]
}


-----------------------

/src/dfx/assets/project_templates/vue/src/__frontend_name__/vite.config.js:
-----------------------

import { fileURLToPath, URL } from 'url';
import { defineConfig } from 'vite';
import environment from 'vite-plugin-environment';
import vue from '@vitejs/plugin-vue';
import dotenv from 'dotenv';

dotenv.config({ path: '../../.env' });

export default defineConfig({
  build: {
    emptyOutDir: true,
  },
  optimizeDeps: {
    esbuildOptions: {
      define: {
        global: 'globalThis',
      },
    },
  },
  server: {
    proxy: {
      '/api': {
        target: 'http://127.0.0.1:4943',
        changeOrigin: true,
      },
    },
  },
  plugins: [
    vue(),
    environment('all', { prefix: 'CANISTER_' }),
    environment('all', { prefix: 'DFX_' }),
  ],
  resolve: {
    alias: [
      { find: 'declarations', replacement: fileURLToPath(new URL('../declarations', import.meta.url)) },
      { find: '@', replacement: fileURLToPath(new URL('./src', import.meta.url)) },
    ]
  }
});


-----------------------

/src/dfx/assets/project_templates/vue_tests/src/__frontend_name__/package.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/devDependencies/@testing-library~1jest-dom",
        "value": "^5.16.5"
    },
    {
        "op": "add",
        "path": "/devDependencies/@vue~1test-utils",
        "value": "^2.4.1"
    },
    {
        "op": "add",
        "path": "/devDependencies/cross-fetch",
        "value": "^3.1.6"
    },
    {
        "op": "add",
        "path": "/devDependencies/jsdom",
        "value": "^22.1.0"
    },
    {
        "op": "add",
        "path": "/devDependencies/vitest",
        "value": "^2.0.5"
    },
    {
        "op": "add",
        "path": "/scripts/test",
        "value": "vitest run"
    }
]


-----------------------

/src/dfx/assets/project_templates/vue_tests/src/__frontend_name__/src/setupTests.js:
-----------------------

import matchers from '@testing-library/jest-dom/matchers';
import 'cross-fetch/polyfill';
import { expect } from 'vitest';

expect.extend(matchers);


-----------------------

/src/dfx/assets/project_templates/vue_tests/src/__frontend_name__/src/tests/App.test.js:
-----------------------

import { describe, expect, it } from 'vitest';
import App from '../App.vue';
import { mount } from '@vue/test-utils';

describe('App', () => {
  it('renders as expected', () => {
    const root = document.createElement('div');
    root.id = 'root';
    document.body.appendChild(root);
    mount(App, { attachTo: root });

    expect(root.querySelector('main')).toBeTruthy();
  });
});


-----------------------

/src/dfx/assets/project_templates/vue_tests/src/__frontend_name__/tsconfig.json-patch:
-----------------------

[
    {
        "op": "add",
        "path": "/compilerOptions/types/-",
        "value": "@testing-library/jest-dom"
    }
]


-----------------------

/src/dfx/assets/project_templates/vue_tests/src/__frontend_name__/vite.config.js.patch:
-----------------------

--- a/src/__frontend_name__/vite.config.js
+++ b/src/__frontend_name__/vite.config.js
@@ -1,1 +1,2 @@
+/// <reference types="vitest" />
 import { fileURLToPath, URL } from 'url';
@@ -33,1 +33,5 @@
+  test: {
+    environment: 'jsdom',
+    setupFiles: 'src/setupTests.js',
+  },
   resolve: {


-----------------------

/src/dfx/assets/welcome.txt:
-----------------------


===============================================================================
        Welcome to the internet computer developer community!
                        You're using dfx {0}

{1}

To learn more before you start coding, see the documentation available online:

- Quick Start: https://internetcomputer.org/docs/current/tutorials/deploy_sample_app
- SDK Developer Tools: https://internetcomputer.org/docs/current/developer-docs/setup/install/
- Motoko Language Guide: https://internetcomputer.org/docs/current/motoko/main/about-this-guide
- Motoko Quick Reference: https://internetcomputer.org/docs/current/motoko/main/language-manual
- Rust CDK Guide: https://internetcomputer.org/docs/current/developer-docs/backend/rust/

If you want to work on programs right away, try the following commands to get started:

    cd {2}
    dfx help
    dfx new --help

===============================================================================


-----------------------

/src/dfx/src/actors/btc_adapter.rs:
-----------------------

use crate::actors::btc_adapter::signals::{BtcAdapterReady, BtcAdapterReadySubscribe};
use crate::actors::shutdown::{wait_for_child_or_receiver, ChildOrReceiver};
use crate::actors::shutdown_controller::signals::outbound::Shutdown;
use crate::actors::shutdown_controller::signals::ShutdownSubscribe;
use crate::actors::shutdown_controller::ShutdownController;
use crate::lib::error::{DfxError, DfxResult};
use actix::{
    Actor, ActorContext, ActorFutureExt, Addr, AsyncContext, Context, Handler, Recipient,
    ResponseActFuture, Running, WrapFuture,
};
use anyhow::bail;
use crossbeam::channel::{unbounded, Receiver, Sender};
use slog::{debug, info, Logger};
use std::path::{Path, PathBuf};
use std::thread::JoinHandle;
use std::time::Duration;

pub mod signals {
    use actix::prelude::*;

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct BtcAdapterReady {}

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct BtcAdapterReadySubscribe(pub Recipient<BtcAdapterReady>);
}

#[derive(Clone)]
pub struct Config {
    pub btc_adapter_path: PathBuf,

    pub config_path: PathBuf,
    pub socket_path: Option<PathBuf>,
    pub shutdown_controller: Addr<ShutdownController>,
    pub btc_adapter_pid_file_path: PathBuf,

    pub logger: Option<Logger>,
}

/// An actor for the ic-btc-adapter process.  Publishes information about
/// the process starting or restarting, so that other processes can reconnect.
pub struct BtcAdapter {
    config: Config,

    stop_sender: Option<Sender<()>>,
    thread_join: Option<JoinHandle<()>>,

    ready: bool,
    ready_subscribers: Vec<Recipient<BtcAdapterReady>>,

    logger: Logger,
}

impl BtcAdapter {
    pub fn new(config: Config) -> Self {
        let logger =
            (config.logger.clone()).unwrap_or_else(|| Logger::root(slog::Discard, slog::o!()));
        BtcAdapter {
            config,
            stop_sender: None,
            thread_join: None,
            ready: false,
            ready_subscribers: Vec::new(),
            logger,
        }
    }

    /// Wait for BTC adapter creating the socket file.
    /// Retry every 0.1s for 5 minutes.
    /// Will break out of the loop if receive stop signal.
    ///
    /// Returns
    /// - Ok(Some()) if succeed;
    /// - Ok(None) if receive stop signal (`dfx start` then Ctrl-C immediately);
    /// - Err if time out;
    fn wait_for_socket(socket_path: &Path, stop_receiver: &Receiver<()>) -> DfxResult<Option<()>> {
        let mut retries = 0;
        while !socket_path.exists() {
            if stop_receiver.try_recv().is_ok() {
                return Ok(None);
            }
            if retries >= 3000 {
                bail!("Cannot start btc-adapter: timed out");
            }
            std::thread::sleep(Duration::from_millis(100));
            retries += 1;
        }
        Ok(Some(()))
    }

    fn start_btc_adapter(&mut self, addr: Addr<Self>) -> DfxResult {
        let logger = self.logger.clone();

        let (sender, receiver) = unbounded();

        let handle = anyhow::Context::context(
            btc_adapter_start_thread(logger, self.config.clone(), addr, receiver),
            "Failed to start BTC adapter thread.",
        )?;

        self.thread_join = Some(handle);
        self.stop_sender = Some(sender);
        Ok(())
    }

    fn send_ready_signal(&self) {
        for sub in &self.ready_subscribers {
            sub.do_send(BtcAdapterReady {});
        }
    }
}

impl Actor for BtcAdapter {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        self.start_btc_adapter(ctx.address())
            .expect("Could not start btc-adapter");

        self.config
            .shutdown_controller
            .do_send(ShutdownSubscribe(ctx.address().recipient::<Shutdown>()));
    }

    fn stopping(&mut self, _ctx: &mut Self::Context) -> Running {
        info!(self.logger, "Stopping btc-adapter...");
        if let Some(sender) = self.stop_sender.take() {
            let _ = sender.send(());
        }

        if let Some(join) = self.thread_join.take() {
            let _ = join.join();
        }

        info!(self.logger, "Stopped.");
        Running::Stop
    }
}

impl Handler<signals::BtcAdapterReady> for BtcAdapter {
    type Result = ();

    fn handle(&mut self, _msg: signals::BtcAdapterReady, _ctx: &mut Self::Context) -> Self::Result {
        self.ready = true;
        self.send_ready_signal();
    }
}

impl Handler<BtcAdapterReadySubscribe> for BtcAdapter {
    type Result = ();

    fn handle(&mut self, msg: BtcAdapterReadySubscribe, _: &mut Self::Context) {
        // If the adapter is already ready, let the new subscriber know! Yeah!
        if self.ready {
            msg.0.do_send(BtcAdapterReady {});
        }

        self.ready_subscribers.push(msg.0);
    }
}

impl Handler<Shutdown> for BtcAdapter {
    type Result = ResponseActFuture<Self, Result<(), ()>>;

    fn handle(&mut self, _msg: Shutdown, _ctx: &mut Self::Context) -> Self::Result {
        Box::pin(
            async {}
                .into_actor(self) // converts future to ActorFuture
                .map(|_, _act, ctx| {
                    ctx.stop();
                    Ok(())
                }),
        )
    }
}

fn btc_adapter_start_thread(
    logger: Logger,
    config: Config,
    addr: Addr<BtcAdapter>,
    receiver: Receiver<()>,
) -> DfxResult<std::thread::JoinHandle<()>> {
    let thread_handler = move || {
        let btc_adapter_path = config.btc_adapter_path.as_os_str();
        let mut cmd = std::process::Command::new(btc_adapter_path);
        cmd.arg(&config.config_path.to_string_lossy().to_string());

        cmd.stdout(std::process::Stdio::inherit());
        cmd.stderr(std::process::Stdio::inherit());

        loop {
            if let Some(socket_path) = &config.socket_path {
                if socket_path.exists() {
                    std::fs::remove_file(socket_path).expect("Could not remove btc-adapter socket");
                }
            }
            let last_start = std::time::Instant::now();
            debug!(logger, "Starting ic-btc-adapter...");
            let mut child = cmd.spawn().expect("Could not start ic-btc-adapter.");

            std::fs::write(&config.btc_adapter_pid_file_path, "")
                .expect("Could not write to btc-adapter-pid file.");
            std::fs::write(&config.btc_adapter_pid_file_path, child.id().to_string())
                .expect("Could not write to btc-adapter-pid file.");

            if let Some(socket_path) = &config.socket_path {
                // If Ctrl-C right after `dfx start`, the adapter child process will be killed already.
                // And the socket file will never be ready.
                // So we let `wait_for_socket` method to break out from the waiting,
                // finish this actor starting ASAP and let the system stop the actor.
                if BtcAdapter::wait_for_socket(socket_path, &receiver)
                    .expect("btc adapter socket was not created")
                    .is_none()
                {
                    break;
                }
            }
            addr.do_send(signals::BtcAdapterReady {});

            // This waits for the child to stop, or the receiver to receive a message.
            // We don't restart the adapter if done = true.
            match wait_for_child_or_receiver(&mut child, &receiver) {
                ChildOrReceiver::Receiver => {
                    debug!(logger, "Got signal to stop. Killing btc-adapter process...");
                    let _ = child.kill();
                    let _ = child.wait();
                    break;
                }
                ChildOrReceiver::Child => {
                    debug!(logger, "ic-btc-adapter process failed.");
                    // If it took less than two seconds to exit, wait a bit before trying again.
                    if std::time::Instant::now().duration_since(last_start) < Duration::from_secs(2)
                    {
                        std::thread::sleep(Duration::from_secs(2));
                    } else {
                        debug!(
                            logger,
                            "Last ic-btc-adapter seemed to have been healthy, not waiting..."
                        );
                    }
                }
            }
        }
    };

    std::thread::Builder::new()
        .name("btc-adapter-actor".to_owned())
        .spawn(thread_handler)
        .map_err(DfxError::from)
}


-----------------------

/src/dfx/src/actors/canister_http_adapter.rs:
-----------------------

use crate::actors::canister_http_adapter::signals::{
    CanisterHttpAdapterReady, CanisterHttpAdapterReadySubscribe,
};
use crate::actors::shutdown::{wait_for_child_or_receiver, ChildOrReceiver};
use crate::actors::shutdown_controller::signals::outbound::Shutdown;
use crate::actors::shutdown_controller::signals::ShutdownSubscribe;
use crate::actors::shutdown_controller::ShutdownController;
use crate::lib::error::{DfxError, DfxResult};
use actix::{
    Actor, ActorContext, ActorFutureExt, Addr, AsyncContext, Context, Handler, Recipient,
    ResponseActFuture, Running, WrapFuture,
};
use anyhow::bail;
use crossbeam::channel::{unbounded, Receiver, Sender};
use slog::{debug, info, Logger};
use std::path::{Path, PathBuf};
use std::thread::JoinHandle;
use std::time::Duration;

pub mod signals {
    use actix::prelude::*;

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct CanisterHttpAdapterReady {}

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct CanisterHttpAdapterReadySubscribe(pub Recipient<CanisterHttpAdapterReady>);
}

#[derive(Clone)]
pub struct Config {
    pub adapter_path: PathBuf,

    pub config_path: PathBuf,
    pub socket_path: Option<PathBuf>,
    pub shutdown_controller: Addr<ShutdownController>,
    pub pid_file_path: PathBuf,

    pub logger: Option<Logger>,
}

/// An actor for the ic-https-outcalls-adapter process.  Publishes information about
/// the process starting or restarting, so that other processes can reconnect.
pub struct CanisterHttpAdapter {
    config: Config,

    stop_sender: Option<Sender<()>>,
    thread_join: Option<JoinHandle<()>>,

    ready: bool,
    ready_subscribers: Vec<Recipient<CanisterHttpAdapterReady>>,

    logger: Logger,
}

impl CanisterHttpAdapter {
    pub fn new(config: Config) -> Self {
        let logger =
            (config.logger.clone()).unwrap_or_else(|| Logger::root(slog::Discard, slog::o!()));
        CanisterHttpAdapter {
            config,
            stop_sender: None,
            thread_join: None,
            ready: false,
            ready_subscribers: Vec::new(),
            logger,
        }
    }

    /// Wait for canister http adapter creating the socket file.
    /// Retry every 0.1s for 5 minutes.
    /// Will break out of the loop if receive stop signal.
    ///
    /// Returns
    /// - Ok(Some()) if succeed;
    /// - Ok(None) if receive stop signal (`dfx start` then Ctrl-C immediately);
    /// - Err if time out;
    fn wait_for_socket(socket_path: &Path, stop_receiver: &Receiver<()>) -> DfxResult<Option<()>> {
        let mut retries = 0;
        while !socket_path.exists() {
            if stop_receiver.try_recv().is_ok() {
                return Ok(None);
            }
            if retries >= 3000 {
                bail!("Cannot start canister-http-adapter: timed out");
            }
            std::thread::sleep(Duration::from_millis(100));
            retries += 1;
        }

        Ok(Some(()))
    }

    fn start_adapter(&mut self, addr: Addr<Self>) -> DfxResult {
        let logger = self.logger.clone();

        let (sender, receiver) = unbounded();

        let handle =
            canister_http_adapter_start_thread(logger, self.config.clone(), addr, receiver)?;

        self.thread_join = Some(handle);
        self.stop_sender = Some(sender);
        Ok(())
    }

    fn send_ready_signal(&self) {
        for sub in &self.ready_subscribers {
            sub.do_send(CanisterHttpAdapterReady {});
        }
    }
}

impl Actor for CanisterHttpAdapter {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        self.start_adapter(ctx.address())
            .expect("Could not start canister http adapter");

        self.config
            .shutdown_controller
            .do_send(ShutdownSubscribe(ctx.address().recipient::<Shutdown>()));
    }

    fn stopping(&mut self, _ctx: &mut Self::Context) -> Running {
        info!(self.logger, "Stopping canister http adapter...");
        if let Some(sender) = self.stop_sender.take() {
            let _ = sender.send(());
        }

        if let Some(join) = self.thread_join.take() {
            let _ = join.join();
        }

        info!(self.logger, "Stopped.");
        Running::Stop
    }
}

impl Handler<signals::CanisterHttpAdapterReady> for CanisterHttpAdapter {
    type Result = ();

    fn handle(
        &mut self,
        _msg: signals::CanisterHttpAdapterReady,
        _ctx: &mut Self::Context,
    ) -> Self::Result {
        self.ready = true;
        self.send_ready_signal();
    }
}

impl Handler<CanisterHttpAdapterReadySubscribe> for CanisterHttpAdapter {
    type Result = ();

    fn handle(&mut self, msg: CanisterHttpAdapterReadySubscribe, _: &mut Self::Context) {
        // If the adapter is already ready, let the new subscriber know! Yeah!
        if self.ready {
            msg.0.do_send(CanisterHttpAdapterReady {});
        }

        self.ready_subscribers.push(msg.0);
    }
}

impl Handler<Shutdown> for CanisterHttpAdapter {
    type Result = ResponseActFuture<Self, Result<(), ()>>;

    fn handle(&mut self, _msg: Shutdown, _ctx: &mut Self::Context) -> Self::Result {
        Box::pin(
            async {}
                .into_actor(self) // converts future to ActorFuture
                .map(|_, _act, ctx| {
                    ctx.stop();
                    Ok(())
                }),
        )
    }
}

fn canister_http_adapter_start_thread(
    logger: Logger,
    config: Config,
    addr: Addr<CanisterHttpAdapter>,
    receiver: Receiver<()>,
) -> DfxResult<std::thread::JoinHandle<()>> {
    let thread_handler = move || {
        let adapter_path = config.adapter_path.as_os_str();
        let mut cmd = std::process::Command::new(adapter_path);
        cmd.arg(&config.config_path.to_string_lossy().to_string());

        cmd.stdout(std::process::Stdio::inherit());
        cmd.stderr(std::process::Stdio::inherit());

        loop {
            if let Some(socket_path) = &config.socket_path {
                if socket_path.exists() {
                    std::fs::remove_file(socket_path)
                        .expect("Could not remove ic-https-outcalls-adapter socket");
                }
            }
            let last_start = std::time::Instant::now();
            debug!(logger, "Starting canister http adapter...");
            let mut child = cmd.spawn().expect("Could not start canister http adapter.");

            std::fs::write(&config.pid_file_path, "")
                .expect("Could not write to canister http adapter pid file.");
            std::fs::write(&config.pid_file_path, child.id().to_string())
                .expect("Could not write to canister http adapter pid file.");

            if let Some(socket_path) = &config.socket_path {
                // If Ctrl-C right after `dfx start`, the adapter child process will be killed already.
                // And the socket file will never be ready.
                // So we let `wait_for_socket` method to break out from the waiting,
                // finish this actor starting ASAP and let the system stop the actor.
                if CanisterHttpAdapter::wait_for_socket(socket_path, &receiver)
                    .expect("canister http adapter socket was not created")
                    .is_none()
                {
                    break;
                }
            }
            addr.do_send(signals::CanisterHttpAdapterReady {});

            // This waits for the child to stop, or the receiver to receive a message.
            // We don't restart the adapter if done = true.
            match wait_for_child_or_receiver(&mut child, &receiver) {
                ChildOrReceiver::Receiver => {
                    debug!(
                        logger,
                        "Got signal to stop. Killing ic-https-outcalls-adapter process..."
                    );
                    let _ = child.kill();
                    let _ = child.wait();
                    break;
                }
                ChildOrReceiver::Child => {
                    debug!(logger, "ic-https-outcalls-adapter process failed.");
                    // If it took less than two seconds to exit, wait a bit before trying again.
                    if std::time::Instant::now().duration_since(last_start) < Duration::from_secs(2)
                    {
                        std::thread::sleep(Duration::from_secs(2));
                    } else {
                        debug!(
                            logger,
                            "Last ic-https-outcalls-adapter seemed to have been healthy, not waiting..."
                        );
                    }
                }
            }
        }
    };

    std::thread::Builder::new()
        .name("canister-http-adapter-actor".to_owned())
        .spawn(thread_handler)
        .map_err(DfxError::from)
}


-----------------------

/src/dfx/src/actors/mod.rs:
-----------------------

use crate::actors::btc_adapter::signals::BtcAdapterReadySubscribe;
use crate::actors::btc_adapter::BtcAdapter;
use crate::actors::canister_http_adapter::signals::CanisterHttpAdapterReadySubscribe;
use crate::actors::canister_http_adapter::CanisterHttpAdapter;
use crate::actors::replica::{BitcoinIntegrationConfig, Replica};
use crate::actors::shutdown_controller::ShutdownController;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use actix::{Actor, Addr, Recipient};
use anyhow::Context;
use dfx_core::config::model::local_server_descriptor::LocalServerDescriptor;
use dfx_core::config::model::replica_config::ReplicaConfig;
use fn_error_context::context;
use pocketic_proxy::signals::PortReadySubscribe;
use pocketic_proxy::{PocketIcProxy, PocketIcProxyConfig};
use std::fs;
use std::path::PathBuf;

use self::pocketic::PocketIc;

pub mod btc_adapter;
pub mod canister_http_adapter;
pub mod pocketic;
pub mod pocketic_proxy;
pub mod replica;
mod shutdown;
pub mod shutdown_controller;

#[context("Failed to start shutdown controller.")]
pub fn start_shutdown_controller(env: &dyn Environment) -> DfxResult<Addr<ShutdownController>> {
    let actor_config = shutdown_controller::Config {
        logger: Some(env.get_logger().clone()),
    };
    Ok(ShutdownController::new(actor_config).start())
}

#[context("Failed to start btc adapter.")]
pub fn start_btc_adapter_actor(
    env: &dyn Environment,
    config_path: PathBuf,
    socket_path: Option<PathBuf>,
    shutdown_controller: Addr<ShutdownController>,
    btc_adapter_pid_file_path: PathBuf,
) -> DfxResult<Recipient<BtcAdapterReadySubscribe>> {
    let btc_adapter_path = env.get_cache().get_binary_command_path("ic-btc-adapter")?;

    let actor_config = btc_adapter::Config {
        btc_adapter_path,

        config_path,
        socket_path,

        shutdown_controller,
        btc_adapter_pid_file_path,
        logger: Some(env.get_logger().clone()),
    };
    Ok(BtcAdapter::new(actor_config).start().recipient())
}

#[context("Failed to start canister http adapter actor.")]
pub fn start_canister_http_adapter_actor(
    env: &dyn Environment,
    config_path: PathBuf,
    socket_path: Option<PathBuf>,
    shutdown_controller: Addr<ShutdownController>,
    pid_file_path: PathBuf,
) -> DfxResult<Recipient<CanisterHttpAdapterReadySubscribe>> {
    let adapter_path = env
        .get_cache()
        .get_binary_command_path("ic-https-outcalls-adapter")?;

    let actor_config = canister_http_adapter::Config {
        adapter_path,

        config_path,
        socket_path,

        shutdown_controller,
        pid_file_path,
        logger: Some(env.get_logger().clone()),
    };
    Ok(CanisterHttpAdapter::new(actor_config).start().recipient())
}

#[context("Failed to setup replica environment.")]
fn setup_replica_env(
    local_server_descriptor: &LocalServerDescriptor,
    replica_config: &ReplicaConfig,
) -> DfxResult {
    // create replica config dir
    let replica_configuration_dir = local_server_descriptor.replica_configuration_dir();
    fs::create_dir_all(&replica_configuration_dir).with_context(|| {
        format!(
            "Failed to create replica config directory {}.",
            replica_configuration_dir.to_string_lossy()
        )
    })?;

    if let Some(replica_port_path) = &replica_config.http_handler.write_port_to {
        // Touch the replica port file. This ensures it is empty prior to
        // handing it over to the replica. If we read the file and it has
        // contents we shall assume it is due to our spawned replica
        // process.
        std::fs::write(replica_port_path, "").with_context(|| {
            format!(
                "Failed to write/clear replica port file {}.",
                replica_port_path.to_string_lossy()
            )
        })?;
    }

    // create replica state dir
    let state_dir = local_server_descriptor.replicated_state_dir();
    fs::create_dir_all(&state_dir).with_context(|| {
        format!(
            "Failed to create replica state directory {}.",
            state_dir.to_string_lossy()
        )
    })?;

    Ok(())
}

#[context("Failed to start replica actor.")]
pub fn start_replica_actor(
    env: &dyn Environment,
    replica_config: ReplicaConfig,
    local_server_descriptor: &LocalServerDescriptor,
    shutdown_controller: Addr<ShutdownController>,
    btc_adapter_ready_subscribe: Option<Recipient<BtcAdapterReadySubscribe>>,
    canister_http_adapter_ready_subscribe: Option<Recipient<CanisterHttpAdapterReadySubscribe>>,
) -> DfxResult<Addr<Replica>> {
    // get binary path
    let replica_path = env.get_cache().get_binary_command_path("replica")?;
    let ic_starter_path = env.get_cache().get_binary_command_path("ic-starter")?;

    setup_replica_env(local_server_descriptor, &replica_config)?;
    let replica_pid_path = local_server_descriptor.replica_pid_path();

    let bitcoin_integration_config = if local_server_descriptor.bitcoin.enabled {
        let canister_init_arg = local_server_descriptor.bitcoin.canister_init_arg.clone();
        Some(BitcoinIntegrationConfig { canister_init_arg })
    } else {
        None
    };

    let actor_config = replica::Config {
        ic_starter_path,
        replica_config,
        bitcoin_integration_config,
        replica_path,
        shutdown_controller,
        logger: Some(env.get_logger().clone()),
        replica_pid_path,
        btc_adapter_ready_subscribe,
        canister_http_adapter_ready_subscribe,
    };
    Ok(Replica::new(actor_config).start())
}

#[context("Failed to start HTTP gateway actor.")]
pub fn start_pocketic_proxy_actor(
    env: &dyn Environment,
    pocketic_proxy_config: PocketIcProxyConfig,
    port_ready_subscribe: Option<Recipient<PortReadySubscribe>>,
    shutdown_controller: Addr<ShutdownController>,
    pocketic_proxy_pid_path: PathBuf,
    pocketic_proxy_port_path: PathBuf,
) -> DfxResult<Addr<PocketIcProxy>> {
    let pocketic_proxy_path = env.get_cache().get_binary_command_path("pocket-ic")?;
    let actor_config = pocketic_proxy::Config {
        logger: Some(env.get_logger().clone()),
        port_ready_subscribe,
        shutdown_controller,
        pocketic_proxy_config,
        pocketic_proxy_path,
        pocketic_proxy_pid_path,
        pocketic_proxy_port_path,
    };
    Ok(PocketIcProxy::new(actor_config).start())
}

#[context("Failed to start PocketIC actor.")]
pub fn start_pocketic_actor(
    env: &dyn Environment,
    replica_config: ReplicaConfig,
    local_server_descriptor: &LocalServerDescriptor,
    shutdown_controller: Addr<ShutdownController>,
    pocketic_port_path: PathBuf,
) -> DfxResult<Addr<PocketIc>> {
    let pocketic_path = env.get_cache().get_binary_command_path("pocket-ic")?;

    // Touch the port file. This ensures it is empty prior to
    // handing it over to PocketIC. If we read the file and it has
    // contents we shall assume it is due to our spawned pocket-ic
    // process.
    std::fs::write(&pocketic_port_path, "").with_context(|| {
        format!(
            "Failed to write/clear PocketIC port file {}.",
            pocketic_port_path.to_string_lossy()
        )
    })?;

    let actor_config = pocketic::Config {
        pocketic_path,
        effective_config_path: local_server_descriptor.effective_config_path(),
        replica_config,
        port: local_server_descriptor.replica.port,
        port_file: pocketic_port_path,
        pid_file: local_server_descriptor.pocketic_pid_path(),
        shutdown_controller,
        logger: Some(env.get_logger().clone()),
        verbose: env.get_verbose_level() > 0,
    };
    Ok(pocketic::PocketIc::new(actor_config).start())
}


-----------------------

/src/dfx/src/actors/pocketic.rs:
-----------------------

use crate::actors::pocketic_proxy::signals::{PortReadySignal, PortReadySubscribe};
use crate::actors::shutdown::{wait_for_child_or_receiver, ChildOrReceiver};
use crate::actors::shutdown_controller::signals::outbound::Shutdown;
use crate::actors::shutdown_controller::signals::ShutdownSubscribe;
use crate::actors::shutdown_controller::ShutdownController;
use crate::lib::error::{DfxError, DfxResult};
#[cfg(unix)]
use crate::lib::info::replica_rev;
use actix::{
    Actor, ActorContext, ActorFutureExt, Addr, AsyncContext, Context, Handler, Recipient,
    ResponseActFuture, Running, WrapFuture,
};
use anyhow::{anyhow, bail};
#[cfg(unix)]
use candid::Principal;
use crossbeam::channel::{unbounded, Receiver, Sender};
#[cfg(unix)]
use dfx_core::config::model::replica_config::CachedConfig;
use dfx_core::config::model::replica_config::ReplicaConfig;
#[cfg(unix)]
use dfx_core::json::save_json_file;
use slog::{debug, error, info, warn, Logger};
use std::ops::ControlFlow::{self, *};
use std::path::{Path, PathBuf};
use std::thread::JoinHandle;
use std::time::{Duration, Instant};

pub mod signals {
    use actix::prelude::*;

    /// A message sent to the PocketIc when the process is restarted. Since we're
    /// restarting inside our own actor, this message should not be exposed.
    #[derive(Message)]
    #[rtype(result = "()")]
    pub(super) struct PocketIcRestarted {
        pub port: u16,
    }
}

/// The configuration for the PocketIC actor.
#[derive(Clone)]
pub struct Config {
    pub pocketic_path: PathBuf,
    pub effective_config_path: PathBuf,
    pub replica_config: ReplicaConfig,
    pub port: Option<u16>,
    pub port_file: PathBuf,
    pub pid_file: PathBuf,
    pub shutdown_controller: Addr<ShutdownController>,
    pub logger: Option<Logger>,
    pub verbose: bool,
}

/// A PocketIC actor. Starts the server, can subscribe to a Ready signal and a
/// Killed signal.
/// This starts a thread that monitors the process and send signals to any subscriber
/// listening for restarts. The message contains the port the server is listening to.
///
/// Signals
///   - PortReadySubscribe
///     Subscribe a recipient (address) to receive a PocketIcReadySignal message when
///     the server is ready to listen to a port. The message can be sent multiple
///     times (e.g. if the server crashes).
///     If a server is already started and another actor sends this message, a
///     PocketIcReadySignal will be sent free of charge in the same thread.
pub struct PocketIc {
    logger: Logger,
    config: Config,

    // We keep the port to send to subscribers on subscription.
    port: Option<u16>,
    stop_sender: Option<Sender<()>>,
    thread_join: Option<JoinHandle<()>>,

    /// Ready Signal subscribers.
    ready_subscribers: Vec<Recipient<PortReadySignal>>,
}

impl PocketIc {
    pub fn new(config: Config) -> Self {
        let logger =
            (config.logger.clone()).unwrap_or_else(|| Logger::root(slog::Discard, slog::o!()));
        Self {
            config,
            port: None,
            stop_sender: None,
            thread_join: None,
            ready_subscribers: Vec::new(),
            logger,
        }
    }

    fn wait_for_ready(
        port_file_path: &Path,
        shutdown_signal: Receiver<()>,
    ) -> Result<u16, ControlFlow<(), DfxError>> {
        let mut retries = 0;
        loop {
            if let Ok(content) = std::fs::read_to_string(port_file_path) {
                if content.ends_with('\n') {
                    if let Ok(port) = content.trim().parse::<u16>() {
                        return Ok(port);
                    }
                }
            }
            if shutdown_signal.try_recv().is_ok() {
                return Err(Break(()));
            }
            if retries >= 3000 {
                return Err(Continue(anyhow!("Timed out")));
            }
            std::thread::sleep(Duration::from_millis(100));
            retries += 1;
        }
    }

    fn start_pocketic(&mut self, addr: Addr<Self>) -> DfxResult {
        let logger = self.logger.clone();

        let (sender, receiver) = unbounded();

        let handle = anyhow::Context::context(
            pocketic_start_thread(logger, self.config.clone(), addr, receiver),
            "Failed to start PocketIC thread.",
        )?;

        self.thread_join = Some(handle);
        self.stop_sender = Some(sender);
        Ok(())
    }

    fn send_ready_signal(&self, port: u16) {
        for sub in &self.ready_subscribers {
            sub.do_send(PortReadySignal {
                url: format!("http://localhost:{port}/instances/0/"),
            });
        }
    }
}

impl Actor for PocketIc {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        self.start_pocketic(ctx.address())
            .expect("Could not start PocketIC");

        self.config
            .shutdown_controller
            .do_send(ShutdownSubscribe(ctx.address().recipient::<Shutdown>()));
    }

    fn stopping(&mut self, _ctx: &mut Self::Context) -> Running {
        info!(self.logger, "Stopping PocketIC...");
        if let Some(sender) = self.stop_sender.take() {
            let _ = sender.send(());
        }

        if let Some(join) = self.thread_join.take() {
            let _ = join.join();
        }

        info!(self.logger, "Stopped.");
        Running::Stop
    }
}

impl Handler<PortReadySubscribe> for PocketIc {
    type Result = ();

    fn handle(&mut self, msg: PortReadySubscribe, _: &mut Self::Context) {
        // If we have a port, send that we're already ready! Yeah!
        if let Some(port) = self.port {
            msg.0.do_send(PortReadySignal {
                url: format!("http://localhost:{port}/instances/0/"),
            });
        }

        self.ready_subscribers.push(msg.0);
    }
}

impl Handler<signals::PocketIcRestarted> for PocketIc {
    type Result = ();

    fn handle(
        &mut self,
        msg: signals::PocketIcRestarted,
        _ctx: &mut Self::Context,
    ) -> Self::Result {
        self.port = Some(msg.port);
        self.send_ready_signal(msg.port);
    }
}

impl Handler<Shutdown> for PocketIc {
    type Result = ResponseActFuture<Self, Result<(), ()>>;

    fn handle(&mut self, _msg: Shutdown, _ctx: &mut Self::Context) -> Self::Result {
        // This is just the example for ResponseActFuture but stopping the context
        Box::pin(
            async {}
                .into_actor(self) // converts future to ActorFuture
                .map(|_, _act, ctx| {
                    ctx.stop();
                    Ok(())
                }),
        )
    }
}

fn pocketic_start_thread(
    logger: Logger,
    config: Config,
    addr: Addr<PocketIc>,
    receiver: Receiver<()>,
) -> DfxResult<std::thread::JoinHandle<()>> {
    let thread_handler = move || {
        loop {
            // Start the process, then wait for the file.
            let pocketic_path = config.pocketic_path.as_os_str();

            // form the pocket-ic command here similar to the ic-starter command
            let mut cmd = std::process::Command::new(pocketic_path);
            if let Some(port) = config.port {
                cmd.args(["--port", &port.to_string()]);
            };
            cmd.args([
                "--port-file",
                &config.port_file.to_string_lossy(),
                "--ttl",
                "2592000",
            ]);
            if !config.verbose {
                cmd.env("RUST_LOG", "error");
            }
            cmd.stdout(std::process::Stdio::inherit());
            cmd.stderr(std::process::Stdio::inherit());
            let _ = std::fs::remove_file(&config.port_file);
            let last_start = std::time::Instant::now();
            debug!(logger, "Starting PocketIC...");
            let mut child = cmd.spawn().expect("Could not start PocketIC.");
            if let Err(e) = std::fs::write(&config.pid_file, child.id().to_string()) {
                warn!(
                    logger,
                    "Failed to write PocketIC PID to {}: {e}",
                    config.pid_file.display()
                );
            }

            let port = match PocketIc::wait_for_ready(&config.port_file, receiver.clone()) {
                Ok(p) => p,
                Err(e) => {
                    let _ = child.kill();
                    let _ = child.wait();
                    if let Continue(e) = e {
                        error!(logger, "Failed to start pocket-ic: {e:#}");
                        continue;
                    } else {
                        debug!(logger, "Got signal to stop");
                        break;
                    }
                }
            };
            let instance = match initialize_pocketic(
                port,
                &config.effective_config_path,
                &config.replica_config,
                logger.clone(),
            ) {
                Err(e) => {
                    error!(logger, "Failed to initialize PocketIC: {e:#}");

                    let _ = child.kill();
                    let _ = child.wait();
                    if receiver.try_recv().is_ok() {
                        debug!(logger, "Got signal to stop.");
                        break;
                    } else {
                        continue;
                    }
                }
                Ok(i) => i,
            };
            addr.do_send(signals::PocketIcRestarted { port });
            // This waits for the child to stop, or the receiver to receive a message.
            // We don't restart the server if done = true.
            match wait_for_child_or_receiver(&mut child, &receiver) {
                ChildOrReceiver::Receiver => {
                    debug!(logger, "Got signal to stop. Killing PocketIC process...");
                    if let Err(e) = shutdown_pocketic(port, instance, logger.clone()) {
                        error!(logger, "Error shutting down PocketIC gracefully: {e}");
                    }
                    let _ = child.kill();
                    let _ = child.wait();
                    break;
                }
                ChildOrReceiver::Child => {
                    debug!(logger, "PocketIC process failed.");
                    // If it took less than two seconds to exit, wait a bit before trying again.
                    if Instant::now().duration_since(last_start) < Duration::from_secs(2) {
                        std::thread::sleep(Duration::from_secs(2));
                    } else {
                        debug!(
                            logger,
                            "Last PocketIC seemed to have been healthy, not waiting..."
                        );
                    }
                }
            }
        }
    };

    std::thread::Builder::new()
        .name("pocketic-actor".to_owned())
        .spawn(thread_handler)
        .map_err(DfxError::from)
}

#[cfg(unix)]
#[tokio::main(flavor = "current_thread")]
async fn initialize_pocketic(
    port: u16,
    effective_config_path: &Path,
    replica_config: &ReplicaConfig,
    logger: Logger,
) -> DfxResult<usize> {
    use dfx_core::config::model::dfinity::ReplicaSubnetType;
    use pocket_ic::common::rest::{
        AutoProgressConfig, CreateInstanceResponse, ExtendedSubnetConfigSet, InstanceConfig,
        RawTime, SubnetSpec,
    };
    use reqwest::Client;
    use time::OffsetDateTime;
    let init_client = Client::new();
    debug!(logger, "Configuring PocketIC server");
    let mut subnet_config_set = ExtendedSubnetConfigSet {
        nns: Some(SubnetSpec::default()),
        sns: Some(SubnetSpec::default()),
        ii: Some(SubnetSpec::default()),
        fiduciary: None,
        bitcoin: None,
        system: vec![],
        verified_application: vec![],
        application: vec![],
    };
    match replica_config.subnet_type {
        ReplicaSubnetType::Application => subnet_config_set.application.push(<_>::default()),
        ReplicaSubnetType::System => subnet_config_set.system.push(<_>::default()),
        ReplicaSubnetType::VerifiedApplication => {
            subnet_config_set.verified_application.push(<_>::default())
        }
    }
    let resp = init_client
        .post(format!("http://localhost:{port}/instances"))
        .json(&InstanceConfig {
            subnet_config_set,
            state_dir: Some(replica_config.state_manager.state_root.clone()),
            nonmainnet_features: true,
            log_level: Some(replica_config.log_level.to_ic_starter_string()),
        })
        .send()
        .await?
        .error_for_status()?
        .json::<CreateInstanceResponse>()
        .await?;
    let instance = match resp {
        CreateInstanceResponse::Error { message } => {
            bail!("PocketIC init error: {message}");
        }
        CreateInstanceResponse::Created {
            instance_id,
            topology,
        } => {
            let subnets = match replica_config.subnet_type {
                ReplicaSubnetType::Application => topology.get_app_subnets(),
                ReplicaSubnetType::System => topology.get_system_subnets(),
                ReplicaSubnetType::VerifiedApplication => topology.get_verified_app_subnets(),
            };
            if subnets.len() != 1 {
                return Err(anyhow!("Internal error: PocketIC topology contains multiple subnets of the same subnet kind."));
            }
            let subnet_id = subnets[0];
            let subnet_config = topology.0.get(&subnet_id).ok_or(anyhow!(
                "Internal error: subnet id {} not found in PocketIC topology",
                subnet_id
            ))?;
            let effective_canister_id =
                Principal::from_slice(&subnet_config.canister_ranges[0].start.canister_id);
            let effective_config = CachedConfig::pocketic(
                replica_config,
                replica_rev().into(),
                Some(effective_canister_id),
            );
            save_json_file(effective_config_path, &effective_config)?;
            instance_id
        }
    };
    init_client
        .post(format!(
            "http://localhost:{port}/instances/{instance}/update/set_time"
        ))
        .json(&RawTime {
            nanos_since_epoch: OffsetDateTime::now_utc()
                .unix_timestamp_nanos()
                .try_into()
                .unwrap(),
        })
        .send()
        .await?
        .error_for_status()?;
    init_client
        .post(format!(
            "http://localhost:{port}/instances/{instance}/auto_progress"
        ))
        .json(&AutoProgressConfig {
            artificial_delay_ms: Some(replica_config.artificial_delay as u64),
        })
        .send()
        .await?
        .error_for_status()?;
    info!(logger, "Initialized PocketIC.");
    Ok(instance)
}

#[cfg(not(unix))]
fn initialize_pocketic(_: u16, _: &Path, _: &ReplicaConfig, _: Logger) -> DfxResult<usize> {
    bail!("PocketIC not supported on this platform")
}

#[cfg(unix)]
#[tokio::main(flavor = "current_thread")]
async fn shutdown_pocketic(port: u16, instance: usize, logger: Logger) -> DfxResult {
    use reqwest::Client;
    let shutdown_client = Client::new();
    debug!(logger, "Sending shutdown request to PocketIC server");
    shutdown_client
        .delete(format!("http://localhost:{port}/instances/{instance}"))
        .send()
        .await?
        .error_for_status()?;
    Ok(())
}

#[cfg(not(unix))]
fn shutdown_pocketic(_: u16, _: usize, _: Logger) -> DfxResult {
    bail!("PocketIC not supported on this platform")
}


-----------------------

/src/dfx/src/actors/pocketic_proxy.rs:
-----------------------

use crate::actors::pocketic_proxy::signals::{PortReadySignal, PortReadySubscribe};
use crate::actors::shutdown::{wait_for_child_or_receiver, ChildOrReceiver};
use crate::actors::shutdown_controller::signals::outbound::Shutdown;
use crate::actors::shutdown_controller::signals::ShutdownSubscribe;
use crate::actors::shutdown_controller::ShutdownController;
use crate::lib::error::{DfxError, DfxResult};
use actix::{
    Actor, ActorContext, ActorFutureExt, Addr, AsyncContext, Context, Handler, Recipient,
    ResponseActFuture, Running, WrapFuture,
};
use anyhow::{anyhow, bail};
use crossbeam::channel::{unbounded, Receiver, Sender};
use slog::{debug, error, info, Logger};
use std::net::SocketAddr;
use std::ops::ControlFlow::{self, *};
use std::path::{Path, PathBuf};
use std::thread::JoinHandle;
use std::time::Duration;
use url::Url;

pub mod signals {
    use actix::prelude::*;

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct PortReadySignal {
        pub url: String,
    }

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct PortReadySubscribe(pub Recipient<PortReadySignal>);
}

pub struct PocketIcProxyConfig {
    /// where to listen.  Becomes argument like --address 127.0.0.1:3000
    pub bind: SocketAddr,

    /// fixed replica address
    pub replica_url: Option<Url>,

    /// does the proxy need to fetch the root key
    pub fetch_root_key: bool,

    /// run pocket-ic in non-quiet mode
    pub verbose: bool,

    /// list of domains that can be served (localhost if none specified)
    pub domains: Option<Vec<String>>,
}

/// The configuration for the pocketic_proxy actor.
pub struct Config {
    pub logger: Option<Logger>,

    pub port_ready_subscribe: Option<Recipient<PortReadySubscribe>>,
    pub shutdown_controller: Addr<ShutdownController>,

    pub pocketic_proxy_config: PocketIcProxyConfig,
    pub pocketic_proxy_path: PathBuf,
    pub pocketic_proxy_pid_path: PathBuf,
    pub pocketic_proxy_port_path: PathBuf,
}

/// An actor for the PocketIC proxy webserver.  Starts/restarts pocket-ic when the replica
/// restarts (because the replica changes ports when it restarts).
pub struct PocketIcProxy {
    logger: Logger,
    config: Config,

    stop_sender: Option<Sender<()>>,
    thread_join: Option<JoinHandle<()>>,
}

impl PocketIcProxy {
    pub fn new(config: Config) -> Self {
        let logger =
            (config.logger.clone()).unwrap_or_else(|| Logger::root(slog::Discard, slog::o!()));
        Self {
            config,
            stop_sender: None,
            thread_join: None,
            logger,
        }
    }

    fn start_pocketic_proxy(&mut self, replica_url: Url) -> DfxResult {
        let logger = self.logger.clone();
        let config = &self.config.pocketic_proxy_config;
        let pocketic_proxy_path = self.config.pocketic_proxy_path.clone();
        let pocketic_proxy_pid_path = self.config.pocketic_proxy_pid_path.clone();
        let pocketic_proxy_port_path = self.config.pocketic_proxy_port_path.clone();
        let (sender, receiver) = unbounded();

        let handle = anyhow::Context::context(
            pocketic_proxy_start_thread(
                logger,
                config.bind,
                replica_url,
                pocketic_proxy_path,
                pocketic_proxy_pid_path,
                pocketic_proxy_port_path,
                receiver,
                config.verbose,
                config.domains.clone(),
            ),
            "Failed to start PocketIC proxy thread.",
        )?;

        self.thread_join = Some(handle);
        self.stop_sender = Some(sender);
        Ok(())
    }

    fn stop_pocketic_proxy(&mut self) {
        if self.stop_sender.is_some() || self.thread_join.is_some() {
            info!(self.logger, "Stopping HTTP gateway...");
            if let Some(sender) = self.stop_sender.take() {
                let _ = sender.send(());
            }

            if let Some(join) = self.thread_join.take() {
                let _ = join.join();
            }
            info!(self.logger, "Stopped.");
        }
    }

    fn wait_for_ready(
        port_file_path: &Path,
        shutdown_signal: Receiver<()>,
    ) -> Result<u16, ControlFlow<(), DfxError>> {
        let mut retries = 0;
        loop {
            if let Ok(content) = std::fs::read_to_string(port_file_path) {
                if content.ends_with('\n') {
                    if let Ok(port) = content.trim().parse::<u16>() {
                        return Ok(port);
                    }
                }
            }
            if shutdown_signal.try_recv().is_ok() {
                return Err(Break(()));
            }
            if retries >= 3000 {
                return Err(Continue(anyhow!("Timed out")));
            }
            std::thread::sleep(Duration::from_millis(100));
            retries += 1;
        }
    }
}

impl Actor for PocketIcProxy {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        if let Some(port_ready_subscribe) = &self.config.port_ready_subscribe {
            port_ready_subscribe.do_send(PortReadySubscribe(ctx.address().recipient()));
        }

        self.config
            .shutdown_controller
            .do_send(ShutdownSubscribe(ctx.address().recipient::<Shutdown>()));

        if let Some(replica_url) = &self.config.pocketic_proxy_config.replica_url {
            self.start_pocketic_proxy(replica_url.clone())
                .expect("Could not start PocketIC HTTP gateway");
        }
    }

    fn stopping(&mut self, _ctx: &mut Self::Context) -> Running {
        self.stop_pocketic_proxy();

        Running::Stop
    }
}

impl Handler<PortReadySignal> for PocketIcProxy {
    type Result = ();

    fn handle(&mut self, msg: PortReadySignal, _ctx: &mut Self::Context) {
        debug!(
            self.logger,
            "replica ready on {}, so re/starting HTTP gateway", msg.url
        );

        self.stop_pocketic_proxy();

        let replica_url = Url::parse(&msg.url).unwrap();

        self.start_pocketic_proxy(replica_url)
            .expect("Could not start PocketIC HTTP gateway");
    }
}

impl Handler<Shutdown> for PocketIcProxy {
    type Result = ResponseActFuture<Self, Result<(), ()>>;

    fn handle(&mut self, _msg: Shutdown, _ctx: &mut Self::Context) -> Self::Result {
        // This is just the example for ResponseActFuture but stopping the context
        Box::pin(
            async {}
                .into_actor(self) // converts future to ActorFuture
                .map(|_, _act, ctx| {
                    ctx.stop();
                    Ok(())
                }),
        )
    }
}

fn pocketic_proxy_start_thread(
    logger: Logger,
    address: SocketAddr,
    replica_url: Url,
    pocketic_proxy_path: PathBuf,
    pocketic_proxy_pid_path: PathBuf,
    pocketic_proxy_port_path: PathBuf,
    receiver: Receiver<()>,
    verbose: bool,
    domains: Option<Vec<String>>,
) -> DfxResult<std::thread::JoinHandle<()>> {
    let thread_handler = move || {
        loop {
            // Start the process, then wait for the file.

            // form the pocket-ic command here similar to replica command
            let mut cmd = std::process::Command::new(&pocketic_proxy_path);
            if !verbose {
                cmd.env("RUST_LOG", "error");
            }

            cmd.args(["--ttl", "2592000"]);
            cmd.args(["--port-file".as_ref(), pocketic_proxy_port_path.as_os_str()]);
            cmd.stdout(std::process::Stdio::inherit());
            cmd.stderr(std::process::Stdio::inherit());
            let last_start = std::time::Instant::now();
            debug!(logger, "Starting pocket-ic gateway...");
            let mut child = cmd.spawn().expect("Could not start pocket-ic gateway.");

            std::fs::write(&pocketic_proxy_pid_path, "")
                .expect("Could not write to pocketic-proxy-pid file.");
            std::fs::write(&pocketic_proxy_pid_path, child.id().to_string())
                .expect("Could not write to pocketic-proxy-pid file.");
            let port =
                match PocketIcProxy::wait_for_ready(&pocketic_proxy_port_path, receiver.clone()) {
                    Ok(p) => p,
                    Err(e) => {
                        let _ = child.kill();
                        let _ = child.wait();
                        if let Continue(e) = e {
                            error!(logger, "Failed to start HTTP gateway: {e:#}");
                            continue;
                        } else {
                            debug!(logger, "Got signal to stop");
                            break;
                        }
                    }
                };
            if let Err(e) = initialize_gateway(
                format!("http://localhost:{port}").parse().unwrap(),
                replica_url.clone(),
                domains.clone(),
                address,
                logger.clone(),
            ) {
                error!(logger, "Failed to initialize HTTP gateway: {e:#}");
                let _ = child.kill();
                let _ = child.wait();
                if receiver.try_recv().is_ok() {
                    debug!(logger, "Got signal to stop.");
                    break;
                } else {
                    continue;
                }
            }
            info!(logger, "Replica API running on {address}");

            // This waits for the child to stop, or the receiver to receive a message.
            // We don't restart pocket-ic if done = true.
            match wait_for_child_or_receiver(&mut child, &receiver) {
                ChildOrReceiver::Receiver => {
                    debug!(
                        logger,
                        "Got signal to stop. Killing pocket-ic gateway process..."
                    );
                    let _ = child.kill();
                    let _ = child.wait();
                    break;
                }
                ChildOrReceiver::Child => {
                    debug!(logger, "pocket-ic gateway process failed.");
                    // If it took less than two seconds to exit, wait a bit before trying again.
                    if std::time::Instant::now().duration_since(last_start) < Duration::from_secs(2)
                    {
                        std::thread::sleep(Duration::from_secs(2));
                    } else {
                        debug!(
                            logger,
                            "Last pocket-ic gateway seemed to have been healthy, not waiting..."
                        );
                    }
                }
            }
        }
    };

    std::thread::Builder::new()
        .name("pocketic-proxy-actor".to_owned())
        .spawn(thread_handler)
        .map_err(DfxError::from)
}

#[cfg(unix)]
#[tokio::main(flavor = "current_thread")]
async fn initialize_gateway(
    pocketic_url: Url,
    replica_url: Url,
    domains: Option<Vec<String>>,
    addr: SocketAddr,
    logger: Logger,
) -> DfxResult {
    use pocket_ic::common::rest::{
        CreateHttpGatewayResponse, HttpGatewayBackend, HttpGatewayConfig,
    };
    use reqwest::Client;
    let init_client = Client::new();
    debug!(logger, "Configuring PocketIC gateway");
    let resp = init_client
        .post(pocketic_url.join("http_gateway").unwrap())
        .json(&HttpGatewayConfig {
            forward_to: HttpGatewayBackend::Replica(replica_url.to_string()),
            ip_addr: Some(addr.ip().to_string()),
            port: Some(addr.port()),
            domains,
            https_config: None,
        })
        .send()
        .await?
        .error_for_status()?;
    let resp = resp.json::<CreateHttpGatewayResponse>().await?;
    if let CreateHttpGatewayResponse::Error { message } = resp {
        bail!("Gateway init error: {message}")
    }
    info!(logger, "Initialized HTTP gateway.");
    Ok(())
}

#[cfg(not(unix))]
fn initialize_gateway(
    _: Url,
    _: Url,
    _: Option<Vec<String>>,
    _: SocketAddr,
    _: Logger,
) -> DfxResult {
    bail!("PocketIC gateway not supported on this platform")
}


-----------------------

/src/dfx/src/actors/replica.rs:
-----------------------

use crate::actors::btc_adapter::signals::{BtcAdapterReady, BtcAdapterReadySubscribe};
use crate::actors::canister_http_adapter::signals::{
    CanisterHttpAdapterReady, CanisterHttpAdapterReadySubscribe,
};
use crate::actors::pocketic_proxy::signals::{PortReadySignal, PortReadySubscribe};
use crate::actors::replica::signals::ReplicaRestarted;
use crate::actors::shutdown::{wait_for_child_or_receiver, ChildOrReceiver};
use crate::actors::shutdown_controller::signals::outbound::Shutdown;
use crate::actors::shutdown_controller::signals::ShutdownSubscribe;
use crate::actors::shutdown_controller::ShutdownController;
use crate::lib::error::{DfxError, DfxResult};
use crate::lib::integrations::bitcoin::initialize_bitcoin_canister;
use crate::lib::integrations::create_integrations_agent;
use actix::{
    Actor, ActorContext, ActorFutureExt, Addr, AsyncContext, Context, Handler, Recipient,
    ResponseActFuture, Running, WrapFuture,
};
use anyhow::bail;
use crossbeam::channel::{unbounded, Receiver, Sender};
use dfx_core::config::model::replica_config::ReplicaConfig;
use slog::{debug, error, info, Logger};
use std::path::{Path, PathBuf};
use std::thread::JoinHandle;
use std::time::Duration;

pub mod signals {
    use actix::prelude::*;

    /// A message sent to the Replica when the process is restarted. Since we're
    /// restarting inside our own actor, this message should not be exposed.
    #[derive(Message)]
    #[rtype(result = "()")]
    pub(super) struct ReplicaRestarted {
        pub port: u16,
    }
}

#[derive(Clone)]
pub struct BitcoinIntegrationConfig {
    pub canister_init_arg: String,
}

/// The configuration for the replica actor.
pub struct Config {
    pub ic_starter_path: PathBuf,
    pub replica_config: ReplicaConfig,
    pub bitcoin_integration_config: Option<BitcoinIntegrationConfig>,
    pub replica_path: PathBuf,
    pub replica_pid_path: PathBuf,
    pub shutdown_controller: Addr<ShutdownController>,
    pub logger: Option<Logger>,
    pub btc_adapter_ready_subscribe: Option<Recipient<BtcAdapterReadySubscribe>>,
    pub canister_http_adapter_ready_subscribe: Option<Recipient<CanisterHttpAdapterReadySubscribe>>,
}

/// A replica actor. Starts the replica, can subscribe to a Ready signal and a
/// Killed signal.
/// This starts a thread that monitors the process and send signals to any subscriber
/// listening for restarts. The message contains the port the replica is listening to.
///
/// Signals
///   - PortReadySubscribe
///     Subscribe a recipient (address) to receive a PortReadySignal message when
///     the replica is ready to listen to a port. The message can be sent multiple
///     times (e.g. if the replica crashes).
///     If a replica is already started and another actor sends this message, a
///     PortReadySignal will be sent free of charge in the same thread.
pub struct Replica {
    logger: Logger,
    config: Config,

    // We keep the port to send to subscribers on subscription.
    port: Option<u16>,
    stop_sender: Option<Sender<()>>,
    thread_join: Option<JoinHandle<()>>,

    /// Ready Signal subscribers.
    ready_subscribers: Vec<Recipient<PortReadySignal>>,

    // We must wait until certain other actors are ready, if they are enabled
    awaiting_btc_adapter_ready: bool,
    awaiting_canister_http_adapter_ready: bool,
}

impl Replica {
    pub fn new(config: Config) -> Self {
        let logger =
            (config.logger.clone()).unwrap_or_else(|| Logger::root(slog::Discard, slog::o!()));
        Replica {
            config,
            port: None,
            stop_sender: None,
            thread_join: None,
            ready_subscribers: Vec::new(),
            awaiting_btc_adapter_ready: false,
            awaiting_canister_http_adapter_ready: false,
            logger,
        }
    }

    /// Wait for `ic-starter` process writing the http port file.
    /// Retry every 0.1s for 2 minutes.
    /// Will break out of the loop if receive stop signal.
    ///
    /// Returns
    /// - Ok(Some(port)) if succeed;
    /// - Ok(None) if receive stop signal (`dfx start` then Ctrl-C immediately);
    /// - Err if time out;
    fn wait_for_port_file(
        file_path: &Path,
        stop_receiver: &Receiver<()>,
    ) -> DfxResult<Option<u16>> {
        let mut retries = 0;
        loop {
            if stop_receiver.try_recv().is_ok() {
                return Ok(None);
            }
            if let Ok(content) = std::fs::read_to_string(file_path) {
                if let Ok(port) = content.parse::<u16>() {
                    return Ok(Some(port));
                }
            }
            if retries >= 1200 {
                bail!("Cannot start the replica: timed out");
            }
            std::thread::sleep(Duration::from_millis(100));
            retries += 1;
        }
    }

    fn restart_replica_if_all_ready(&mut self, addr: Addr<Self>) {
        let done_waiting =
            !self.awaiting_canister_http_adapter_ready && !self.awaiting_btc_adapter_ready;
        if done_waiting {
            self.stop_replica();
            self.start_replica(addr)
                .expect("unable to start the replica");
        }
    }

    fn start_replica(&mut self, addr: Addr<Self>) -> DfxResult {
        let logger = self.logger.clone();

        // Create a replica config.
        let config = &self.config.replica_config;
        let replica_pid_path = self.config.replica_pid_path.to_path_buf();

        let port = config.http_handler.port;
        let write_port_to = config.http_handler.write_port_to.clone();
        let artificial_delay = config.artificial_delay;
        let replica_path = self.config.replica_path.to_path_buf();
        let ic_starter_path = self.config.ic_starter_path.to_path_buf();

        let (sender, receiver) = unbounded();

        let handle = anyhow::Context::context(
            replica_start_thread(
                logger,
                config.clone(),
                self.config.bitcoin_integration_config.clone(),
                port,
                write_port_to,
                ic_starter_path,
                replica_path,
                replica_pid_path,
                artificial_delay,
                addr,
                receiver,
            ),
            "Failed to start replica thread.",
        )?;

        self.thread_join = Some(handle);
        self.stop_sender = Some(sender);
        Ok(())
    }

    fn stop_replica(&mut self) {
        if self.stop_sender.is_some() || self.thread_join.is_some() {
            debug!(self.logger, "stopping replica");
        }

        if let Some(sender) = self.stop_sender.take() {
            let _ = sender.send(());
        }

        if let Some(join) = self.thread_join.take() {
            let _ = join.join();
        }
    }

    fn send_ready_signal(&self, port: u16) {
        for sub in &self.ready_subscribers {
            sub.do_send(PortReadySignal {
                url: format!("http://localhost:{port}"),
            });
        }
    }
}

impl Actor for Replica {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        if let Some(btc_adapter_ready_subscribe) = &self.config.btc_adapter_ready_subscribe {
            btc_adapter_ready_subscribe
                .do_send(BtcAdapterReadySubscribe(ctx.address().recipient()));
            self.awaiting_btc_adapter_ready = true;
        }
        if let Some(subscribe) = &self.config.canister_http_adapter_ready_subscribe {
            subscribe.do_send(CanisterHttpAdapterReadySubscribe(ctx.address().recipient()));
            self.awaiting_canister_http_adapter_ready = true;
        }

        self.restart_replica_if_all_ready(ctx.address());

        self.config
            .shutdown_controller
            .do_send(ShutdownSubscribe(ctx.address().recipient::<Shutdown>()));
    }

    fn stopping(&mut self, _ctx: &mut Self::Context) -> Running {
        info!(self.logger, "Stopping the replica...");
        self.stop_replica();

        info!(self.logger, "Stopped.");
        Running::Stop
    }
}

impl Handler<PortReadySubscribe> for Replica {
    type Result = ();

    fn handle(&mut self, msg: PortReadySubscribe, _: &mut Self::Context) {
        // If we have a port, send that we're already ready! Yeah!
        if let Some(port) = self.port {
            msg.0.do_send(PortReadySignal {
                url: format!("http://localhost:{port}"),
            });
        }

        self.ready_subscribers.push(msg.0);
    }
}

impl Handler<signals::ReplicaRestarted> for Replica {
    type Result = ();

    fn handle(&mut self, msg: ReplicaRestarted, _ctx: &mut Self::Context) -> Self::Result {
        self.port = Some(msg.port);
        self.send_ready_signal(msg.port);
    }
}

impl Handler<BtcAdapterReady> for Replica {
    type Result = ();

    fn handle(&mut self, _msg: BtcAdapterReady, ctx: &mut Self::Context) {
        debug!(self.logger, "btc adapter ready");
        self.awaiting_btc_adapter_ready = false;

        self.restart_replica_if_all_ready(ctx.address());
    }
}

impl Handler<CanisterHttpAdapterReady> for Replica {
    type Result = ();

    fn handle(&mut self, _msg: CanisterHttpAdapterReady, ctx: &mut Self::Context) {
        debug!(self.logger, "canister http adapter ready");
        self.awaiting_canister_http_adapter_ready = false;

        self.restart_replica_if_all_ready(ctx.address());
    }
}

impl Handler<Shutdown> for Replica {
    type Result = ResponseActFuture<Self, Result<(), ()>>;

    fn handle(&mut self, _msg: Shutdown, _ctx: &mut Self::Context) -> Self::Result {
        // This is just the example for ResponseActFuture but stopping the context
        Box::pin(
            async {}
                .into_actor(self) // converts future to ActorFuture
                .map(|_, _act, ctx| {
                    ctx.stop();
                    Ok(())
                }),
        )
    }
}

fn replica_start_thread(
    logger: Logger,
    config: ReplicaConfig,
    bitcoin_integration_config: Option<BitcoinIntegrationConfig>,
    port: Option<u16>,
    write_port_to: Option<PathBuf>,
    ic_starter_path: PathBuf,
    replica_path: PathBuf,
    replica_pid_path: PathBuf,
    artificial_delay: u32,
    addr: Addr<Replica>,
    receiver: Receiver<()>,
) -> DfxResult<std::thread::JoinHandle<()>> {
    let thread_handler = move || {
        // Start the process, then wait for the file.
        let ic_starter_path = ic_starter_path.as_os_str();

        // form the ic-start command here similar to replica command
        let mut cmd = std::process::Command::new(ic_starter_path);
        cmd.args([
            "--replica-path",
            replica_path.to_str().unwrap_or_default(),
            "--state-dir",
            config.state_manager.state_root.to_str().unwrap_or_default(),
            "--create-funds-whitelist",
            "*",
            "--subnet-type",
            &config.subnet_type.as_ic_starter_string(),
            "--chain-key-ids",
            "ecdsa:Secp256k1:dfx_test_key",
            "--chain-key-ids",
            "schnorr:Bip340Secp256k1:dfx_test_key",
            "--chain-key-ids",
            "schnorr:Ed25519:dfx_test_key",
            "--log-level",
            &config.log_level.to_ic_starter_string(),
            "--use-specified-ids-allocation-range",
            "--metrics-addr=[100::]:0",
        ]);
        #[cfg(target_os = "macos")]
        cmd.args(["--consensus-pool-backend", "rocksdb"]);
        if let Some(port) = port {
            cmd.args(["--http-port", &port.to_string()]);
        }
        // Enable canister sandboxing to be consistent with the mainnet.
        // The flag will be removed on the `ic-starter` side once this
        // change is rolled out without any issues.
        cmd.args(["--subnet-features", "canister_sandboxing"]);
        if config.btc_adapter.enabled {
            if let Some(socket_path) = config.btc_adapter.socket_path {
                cmd.args([
                    "--bitcoin-testnet-uds-path",
                    socket_path.to_str().unwrap_or_default(),
                ]);
            }
        }
        if config.canister_http_adapter.enabled {
            cmd.args(["--subnet-features", "http_requests"]);
            if let Some(socket_path) = config.canister_http_adapter.socket_path {
                cmd.args([
                    "--canister-http-uds-path",
                    socket_path.to_str().unwrap_or_default(),
                ]);
            }
        }

        if let Some(write_port_to) = &write_port_to {
            cmd.args(["--http-port-file", &write_port_to.to_string_lossy()]);
        }
        cmd.args([
            "--initial-notary-delay-millis",
            // The initial notary delay is set to 2500ms in the replica's
            // default subnet configuration to help running tests.
            // For our production network, we actually set them to 600ms.
            &format!("{artificial_delay}"),
        ]);

        // This should agree with the value at
        // at https://gitlab.com/dfinity-lab/core/ic/-/blob/master/ic-os/guestos/rootfs/etc/systemd/system/ic-replica.service
        cmd.env("RUST_MIN_STACK", "8192000");

        cmd.stdout(std::process::Stdio::inherit());
        cmd.stderr(std::process::Stdio::inherit());

        loop {
            if let Some(port_path) = write_port_to.as_ref() {
                let _ = std::fs::remove_file(port_path);
            }
            let last_start = std::time::Instant::now();
            debug!(logger, "Starting replica...");
            let mut child = cmd.spawn().expect("Could not start replica.");

            std::fs::write(&replica_pid_path, "").expect("Could not write to replica-pid file.");
            std::fs::write(&replica_pid_path, child.id().to_string())
                .expect("Could not write to replica-pid file.");

            let port = if let Some(p) = port {
                p
            } else {
                match Replica::wait_for_port_file(write_port_to.as_ref().unwrap(), &receiver)
                    .unwrap()
                {
                    Some(p) => p,
                    // If Ctrl-C right after `dfx start`, the `ic-starter` child process will be killed already.
                    // And the `write_port_to` file will never be ready.
                    // So we let `wait_for_port_file` method to break out from the waiting,
                    // finish this actor starting ASAP and let the system stop the actor.
                    None => break,
                }
            };

            if let Err(e) =
                initialize_replica(port, logger.clone(), bitcoin_integration_config.clone())
            {
                error!(logger, "Failed to initialize replica: {:#}", e);
                let _ = child.kill();
                let _ = child.wait();
                if receiver.try_recv().is_ok() {
                    debug!(logger, "Got signal to stop.");
                    break;
                } else {
                    continue;
                }
            }
            addr.do_send(signals::ReplicaRestarted { port });
            let log_clone = logger.clone();
            debug!(log_clone, "Dashboard: http://localhost:{port}/_/dashboard");

            // This waits for the child to stop, or the receiver to receive a message.
            // We don't restart the replica if done = true.
            match wait_for_child_or_receiver(&mut child, &receiver) {
                ChildOrReceiver::Receiver => {
                    debug!(logger, "Got signal to stop. Killing replica process...");
                    let _ = child.kill();
                    let _ = child.wait();
                    break;
                }
                ChildOrReceiver::Child => {
                    debug!(logger, "Replica process failed.");
                    // If it took less than two seconds to exit, wait a bit before trying again.
                    if std::time::Instant::now().duration_since(last_start) < Duration::from_secs(2)
                    {
                        std::thread::sleep(Duration::from_secs(2));
                    } else {
                        debug!(
                            logger,
                            "Last ic-btc-adapter seemed to have been healthy, not waiting..."
                        );
                    }
                }
            }
        }
    };

    std::thread::Builder::new()
        .name("replica-actor".to_owned())
        .spawn(thread_handler)
        .map_err(DfxError::from)
}

#[tokio::main(flavor = "current_thread")]
async fn initialize_replica(
    port: u16,
    logger: Logger,
    bitcoin_integration_config: Option<BitcoinIntegrationConfig>,
) -> DfxResult {
    let agent_url = format!("http://localhost:{port}");

    debug!(logger, "Waiting for replica to report healthy status");
    crate::lib::replica::status::ping_and_wait(&agent_url).await?;

    let agent = create_integrations_agent(&agent_url, &logger).await?;

    if let Some(bitcoin_integration_config) = bitcoin_integration_config {
        initialize_bitcoin_canister(&agent, &logger, bitcoin_integration_config).await?;
    }

    info!(logger, "Initialized replica.");

    Ok(())
}


-----------------------

/src/dfx/src/actors/shutdown.rs:
-----------------------

use crossbeam::channel::Receiver;

/// Differentiate between:
///   - the process exited (Child)
///   - shutdown was requested (Receiver)
pub enum ChildOrReceiver {
    Child,
    Receiver,
}

/// Function that waits for a child or a receiver to stop. This encapsulate the polling so
/// it is easier to maintain.
pub fn wait_for_child_or_receiver(
    child: &mut std::process::Child,
    receiver: &Receiver<()>,
) -> ChildOrReceiver {
    loop {
        // Check if either the child exited or a shutdown has been requested.
        // These can happen in either order in response to Ctrl-C, so increase the chance
        // to notice a shutdown request even if the replica exited quickly.
        let child_try_wait = child.try_wait();
        let receiver_signalled = receiver.recv_timeout(std::time::Duration::from_millis(100));

        match (receiver_signalled, child_try_wait) {
            (Ok(()), _) => {
                // Prefer to indicate the shutdown request
                return ChildOrReceiver::Receiver;
            }
            (Err(_), Ok(Some(_))) => {
                return ChildOrReceiver::Child;
            }
            _ => {}
        };
    }
}


-----------------------

/src/dfx/src/actors/shutdown_controller.rs:
-----------------------

use crate::actors::shutdown_controller::signals::outbound::Shutdown;
use crate::actors::shutdown_controller::signals::ShutdownTrigger;
use actix::{Actor, Addr, AsyncContext, Context, Handler, Recipient};
use slog::Logger;
use std::time::Duration;

pub mod signals {
    use actix::prelude::*;

    pub mod outbound {
        use super::*;

        #[derive(Message)]
        #[rtype(result = "Result<(), ()>")]
        pub struct Shutdown {}
    }

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct ShutdownSubscribe(pub Recipient<outbound::Shutdown>);

    #[derive(Message)]
    #[rtype(result = "()")]
    pub struct ShutdownTrigger();
}

pub struct Config {
    pub logger: Option<Logger>,
}

pub struct ShutdownController {
    _logger: Logger,

    shutdown_subscribers: Vec<Recipient<signals::outbound::Shutdown>>,
}

impl ShutdownController {
    pub fn new(config: Config) -> Self {
        let logger =
            (config.logger.clone()).unwrap_or_else(|| Logger::root(slog::Discard, slog::o!()));
        ShutdownController {
            _logger: logger,
            shutdown_subscribers: Vec::new(),
        }
    }

    // This is copied with minor changes from
    //   https://github.com/getsentry/relay/blob/master/relay-server/src/actors/controller.rs
    pub fn shutdown(&mut self, ctx: &mut Context<Self>) {
        use actix::prelude::*;
        use futures::prelude::*;

        let futures: Vec<_> = self
            .shutdown_subscribers
            .iter()
            .map(|recipient| recipient.send(Shutdown {}))
            .map(|response| response.then(|_| future::ok::<(), ()>(())))
            .collect();

        futures::future::join_all(futures)
            .into_actor(self)
            .then(|_, _, ctx| {
                // Once all shutdowns have completed, we can schedule a stop of the actix system. It is
                // performed with a slight delay to give pending synced futures a chance to perform their
                // error handlers.
                //
                // Delay the shutdown for 100ms to allow synchronized futures to execute their error
                // handlers. Once `System::stop` is called, futures won't be polled anymore and we will not
                // be able to print error messages.
                let when = Duration::from_secs(0) + Duration::from_millis(100);

                ctx.run_later(when, |_, _| {
                    System::current().stop();
                });

                fut::wrap_future(async {})
            })
            .spawn(ctx)
    }

    fn install_ctrlc_handler(&self, shutdown_controller: Addr<ShutdownController>) {
        ctrlc::set_handler(move || {
            shutdown_controller.do_send(ShutdownTrigger());
        })
        .expect("Error setting Ctrl-C handler");
    }
}

impl Actor for ShutdownController {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        self.install_ctrlc_handler(ctx.address());
    }
}

impl Handler<signals::ShutdownSubscribe> for ShutdownController {
    type Result = ();

    fn handle(&mut self, msg: signals::ShutdownSubscribe, _: &mut Self::Context) {
        self.shutdown_subscribers.push(msg.0);
    }
}

impl Handler<signals::ShutdownTrigger> for ShutdownController {
    type Result = ();

    fn handle(&mut self, _msg: signals::ShutdownTrigger, ctx: &mut Self::Context) {
        self.shutdown(ctx);
    }
}


-----------------------

/src/dfx/src/commands/beta/mod.rs:
-----------------------

use crate::lib::error::DfxResult;
use crate::Environment;
use clap::Parser;

mod project;

/// Beta commands.
#[derive(Parser)]
#[command(name = "beta")]
pub struct BetaOpts {
    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
enum SubCommand {
    Project(project::ProjectOpts),
}

pub fn exec(env: &dyn Environment, cmd: BetaOpts) -> DfxResult {
    match cmd.subcmd {
        SubCommand::Project(v) => project::exec(env, v),
    }
}


-----------------------

/src/dfx/src/commands/beta/project/import.rs:
-----------------------

use crate::lib::error::DfxResult;
use crate::lib::project::import::import_canister_definitions;
use crate::lib::project::network_mappings::get_network_mappings;
use crate::Environment;
use clap::{ArgAction, Parser};
use tokio::runtime::Runtime;

/// Imports canister definitions from another project, as remote canisters
#[derive(Parser)]
pub struct ImportOpts {
    /// Path to dfx.json (local file path or url)
    location: String,

    /// Specifies the canister name. Either this or the --all flag are required.
    canister_name: Option<String>,

    /// Imports all canisters found in the other project.
    #[arg(long, required_unless_present("canister_name"))]
    all: bool,

    /// An optional prefix for canisters names to add to the project
    #[arg(long)]
    prefix: Option<String>,

    /// Networks to import canisters ids for.
    ///   --network-mapping <network name in both places>
    ///   --network-mapping <network name here>=<network name in project being imported>
    /// Examples:
    ///   --network-mapping ic
    ///   --network-mapping ic=mainnet
    #[arg(long, default_value = "ic", action = ArgAction::Append)]
    network_mapping: Vec<String>,
}

pub fn exec(env: &dyn Environment, opts: ImportOpts) -> DfxResult {
    let config = env.get_config_or_anyhow()?;
    let mut config = config.as_ref().clone();

    let network_mappings = get_network_mappings(&opts.network_mapping)?;

    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async {
        import_canister_definitions(
            env.get_logger(),
            &mut config,
            &opts.location,
            opts.prefix.as_deref(),
            opts.canister_name,
            &network_mappings,
        )
        .await
    })?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/beta/project/mod.rs:
-----------------------

use crate::lib::error::DfxResult;
use crate::Environment;
use clap::Parser;

mod import;

/// Project commands.
#[derive(Parser)]
#[command(name = "project")]
pub struct ProjectOpts {
    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
enum SubCommand {
    Import(import::ImportOpts),
}

pub fn exec(env: &dyn Environment, cmd: ProjectOpts) -> DfxResult {
    match cmd.subcmd {
        SubCommand::Import(v) => import::exec(env, v),
    }
}


-----------------------

/src/dfx/src/commands/build.rs:
-----------------------

use crate::config::cache::DiskBasedCache;
use crate::lib::agent::create_anonymous_agent_environment;
use crate::lib::builders::BuildConfig;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::models::canister::CanisterPool;
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::operations::canister::add_canisters_with_ids;
use clap::Parser;
use std::path::PathBuf;
use tokio::runtime::Runtime;

/// Builds all or specific canisters from the code in your project. By default, all canisters are built.
#[derive(Parser)]
pub struct CanisterBuildOpts {
    /// Specifies the name of the canister to build.
    /// You must specify either a canister name or the --all option.
    canister_name: Option<String>,

    /// Builds all canisters configured in the dfx.json file.
    #[arg(long, conflicts_with("canister_name"))]
    all: bool,

    /// Build canisters without creating them. This can be used to check that canisters build ok.
    #[arg(long)]
    check: bool,

    /// Output environment variables to a file in dotenv format (without overwriting any user-defined variables, if the file already exists).
    #[arg(long)]
    output_env_file: Option<PathBuf>,

    #[command(flatten)]
    network: NetworkOpt,
}

pub fn exec(env: &dyn Environment, opts: CanisterBuildOpts) -> DfxResult {
    let env = create_anonymous_agent_environment(env, opts.network.to_network_name())?;

    let logger = env.get_logger();

    // Read the config.
    let config = env.get_config_or_anyhow()?;
    let env_file = config.get_output_env_file(opts.output_env_file)?;

    // Check the cache. This will only install the cache if there isn't one installed
    // already.
    DiskBasedCache::install(&env.get_cache().version_str())?;

    let build_mode_check = opts.check;

    // Option can be None in which case --all was specified
    let required_canisters = config
        .get_config()
        .get_canister_names_with_dependencies(opts.canister_name.as_deref())?;
    let canisters_to_load = add_canisters_with_ids(&required_canisters, &env, &config);

    let canisters_to_build = required_canisters
        .into_iter()
        .filter(|canister_name| {
            !config
                .get_config()
                .is_remote_canister(canister_name, &env.get_network_descriptor().name)
                .unwrap_or(false)
        })
        .collect();

    let canister_pool = CanisterPool::load(&env, build_mode_check, &canisters_to_load)?;

    // Create canisters on the replica and associate canister ids locally.
    if build_mode_check {
        slog::warn!(
            logger,
            "Building canisters to check they build ok. Canister IDs might be hard coded."
        );
    } else {
        // CanisterIds would have been set in CanisterPool::load, if available.
        // This is just to display an error if trying to build before creating the canister.
        let store = env.get_canister_id_store()?;
        for canister in canister_pool.get_canister_list() {
            let canister_name = canister.get_name();
            store.get(canister_name)?;
        }
    }

    slog::info!(logger, "Building canisters...");

    let runtime = Runtime::new().expect("Unable to create a runtime");
    let build_config =
        BuildConfig::from_config(&config, env.get_network_descriptor().is_playground())?
            .with_build_mode_check(build_mode_check)
            .with_canisters_to_build(canisters_to_build)
            .with_env_file(env_file);
    runtime.block_on(canister_pool.build_or_fail(logger, &build_config))?;

    Ok(())
}


-----------------------

/src/dfx/src/commands/cache/delete.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use clap::Parser;
use dfx_core::config::cache::delete_version;

/// Deletes a specific versioned cache of dfx.
#[derive(Parser)]
#[command(name = "delete")]
pub struct CacheDeleteOpts {
    #[arg(long)]
    version: Option<String>,
}

pub fn exec(env: &dyn Environment, opts: CacheDeleteOpts) -> DfxResult {
    match opts.version {
        Some(v) => delete_version(v.as_str()).map(|_| {}),
        _ => env.get_cache().delete(),
    }
    .map_err(DfxError::new)
}


-----------------------

/src/dfx/src/commands/cache/install.rs:
-----------------------

use crate::config::cache::DiskBasedCache;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use clap::Parser;

/// Forces unpacking the cache from this dfx version.
#[derive(Parser)]
#[command(name = "install")]
pub struct CacheInstall {}

pub fn exec(env: &dyn Environment, _opts: CacheInstall) -> DfxResult {
    DiskBasedCache::force_install(&env.get_cache().version_str()).map_err(DfxError::from)?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/cache/list.rs:
-----------------------

use crate::config::dfx_version;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;
use dfx_core::config::cache::list_versions;
use std::io::Write;

/// Lists installed and used version.
#[derive(Parser)]
#[command(name = "list")]
pub struct CacheListOpts {}

pub fn exec(env: &dyn Environment, _opts: CacheListOpts) -> DfxResult {
    let mut current_printed = false;
    let current_version = env.get_version();
    let mut all_versions = list_versions()?;
    all_versions.sort();
    for version in all_versions {
        if current_version == &version {
            current_printed = true;
            // Same version, prefix with `*`.
            std::io::stderr().flush()?;
            print!("{}", version);
            std::io::stdout().flush()?;
            eprintln!(" *");
        } else {
            eprintln!("{}", version);
        }
    }

    if !current_printed {
        // The current version wasn't printed, so it's not in the cache.
        std::io::stderr().flush()?;
        print!("{}", dfx_version());
        std::io::stdout().flush()?;
        eprintln!(" [missing]");
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/cache/mod.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;

mod delete;
mod install;
mod list;
mod show;

/// Manages the dfx version cache.
#[derive(Parser)]
#[command(name = "cache")]
pub struct CacheOpts {
    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
pub enum SubCommand {
    Delete(delete::CacheDeleteOpts),
    Install(install::CacheInstall),
    List(list::CacheListOpts),
    Show(show::CacheShowOpts),
}

pub fn exec(env: &dyn Environment, opts: CacheOpts) -> DfxResult {
    match opts.subcmd {
        SubCommand::Delete(v) => delete::exec(env, v),
        SubCommand::Install(v) => install::exec(env, v),
        SubCommand::List(v) => list::exec(env, v),
        SubCommand::Show(v) => show::exec(env, v),
    }
}


-----------------------

/src/dfx/src/commands/cache/show.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;

/// Shows the path of the cache used by this version.
#[derive(Parser)]
#[command(name = "show")]
pub struct CacheShowOpts {}

pub fn exec(env: &dyn Environment, _opts: CacheShowOpts) -> DfxResult {
    let v = format!("{}", env.get_version());
    println!(
        "{}",
        dfx_core::config::cache::get_bin_cache(&v)?
            .as_path()
            .display()
    );
    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/call.rs:
-----------------------

use crate::lib::diagnosis::DiagnosedError;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister::get_canister_id_and_candid_path;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::argument_from_cli::ArgumentFromCliPositionalOpt;
use crate::util::clap::parsers::cycle_amount_parser;
use crate::util::{blob_from_arguments, fetch_remote_did_file, get_candid_type, print_idl_blob};
use anyhow::{anyhow, Context};
use candid::Principal as CanisterId;
use candid::{CandidType, Decode, Deserialize, Principal};
use candid_parser::utils::CandidSource;
use clap::Parser;
use dfx_core::canister::build_wallet_canister;
use dfx_core::identity::CallSender;
use ic_agent::agent::CallResponse;
use ic_utils::canister::Argument;
use ic_utils::interfaces::management_canister::builders::{CanisterInstall, CanisterSettings};
use ic_utils::interfaces::management_canister::MgmtMethod;
use ic_utils::interfaces::wallet::{CallForwarder, CallResult};
use ic_utils::interfaces::WalletCanister;
use slog::warn;
use std::option::Option;
use std::path::PathBuf;
use std::str::FromStr;

/// Calls a method on a deployed canister.
#[derive(Parser)]
pub struct CanisterCallOpts {
    /// Specifies the name/id of the canister to call.
    /// You must specify either a canister or the --all option.
    canister_name: String,

    /// Specifies the method name to call on the canister.
    method_name: String,

    #[command(flatten)]
    argument_from_cli: ArgumentFromCliPositionalOpt,

    /// Specifies not to wait for the result of the call to be returned by polling the replica.
    /// Instead return a response ID.
    #[arg(long)]
    r#async: bool,

    /// Sends a query request to a canister instead of an update request.
    #[arg(long, conflicts_with("async"))]
    query: bool,

    /// Sends an update request to a canister. This is the default if the method is not a query method.
    #[arg(long, conflicts_with("async"), conflicts_with("query"))]
    update: bool,

    /// Specifies the config for generating random argument.
    #[arg(
        long,
        conflicts_with("argument"),
        conflicts_with("argument_file"),
        conflicts_with("always_assist")
    )]
    random: Option<String>,

    /// Specifies the format for displaying the method's return result.
    #[arg(long, conflicts_with("async"),
        value_parser = ["idl", "raw", "pp", "json"])]
    output: Option<String>,

    /// Specifies the amount of cycles to send on the call.
    /// Deducted from the wallet.
    /// Requires --wallet as a flag to `dfx canister`.
    #[arg(long, value_parser = cycle_amount_parser, requires("wallet"))]
    with_cycles: Option<u128>,

    /// Provide the .did file with which to decode the response.  Overrides value from dfx.json
    /// for project canisters.
    #[arg(long)]
    candid: Option<PathBuf>,

    /// Always use Candid assist when the argument types are all optional.
    #[arg(
        long,
        conflicts_with("argument"),
        conflicts_with("argument_file"),
        conflicts_with("random")
    )]
    always_assist: bool,
}

#[derive(Clone, CandidType, Deserialize, Debug)]
struct CallIn<TCycles = u128> {
    canister: CanisterId,
    method_name: String,
    #[serde(with = "serde_bytes")]
    args: Vec<u8>,
    cycles: TCycles,
}

async fn do_wallet_call(wallet: &WalletCanister<'_>, args: &CallIn) -> DfxResult<Vec<u8>> {
    // todo change to wallet.call when IDLValue implements ArgumentDecoder
    let builder = if wallet.version_supports_u128_cycles() {
        wallet.update("wallet_call128").with_arg(args)
    } else {
        let CallIn {
            canister,
            method_name,
            args,
            cycles,
        } = args.clone();
        let args64 = CallIn {
            canister,
            method_name,
            args,
            cycles: cycles as u64,
        };
        wallet.update("wallet_call").with_arg(args64)
    };
    let (result,): (Result<CallResult, String>,) =
        builder.build().await.context("Failed wallet call.")?;
    Ok(result.map_err(|err| anyhow!(err))?.r#return)
}

async fn request_id_via_wallet_call(
    wallet: &WalletCanister<'_>,
    canister: Principal,
    method_name: &str,
    args: Argument,
    cycles: u128,
) -> DfxResult<CallResponse<(CallResult,)>> {
    let call_forwarder: CallForwarder<'_, '_, (CallResult,)> =
        wallet.call(canister, method_name, args, cycles);
    call_forwarder
        .call()
        .await
        .map_err(|err| anyhow!("Agent error {}", err))
}

// TODO: move to ic_utils? SDKTG-302
pub fn get_effective_canister_id(
    method_name: &MgmtMethod,
    arg_value: &[u8],
) -> DfxResult<CanisterId> {
    match method_name {
        MgmtMethod::CreateCanister
        | MgmtMethod::RawRand
        | MgmtMethod::BitcoinGetBalance
        | MgmtMethod::BitcoinGetUtxos
        | MgmtMethod::BitcoinSendTransaction
        | MgmtMethod::BitcoinGetCurrentFeePercentiles
        | MgmtMethod::EcdsaPublicKey
        | MgmtMethod::SignWithEcdsa
        | MgmtMethod::NodeMetricsHistory => Ok(CanisterId::management_canister()),
        MgmtMethod::InstallCode => {
            // TODO: Maybe this case can be merged with the following one.
            let install_args = candid::Decode!(arg_value, CanisterInstall)
                .context("Failed to decode arguments.")?;
            Ok(install_args.canister_id)
        }
        MgmtMethod::UpdateSettings => {
            // TODO: Maybe this case can be merged with the following one.
            #[derive(CandidType, Deserialize)]
            struct In {
                canister_id: CanisterId,
                settings: CanisterSettings,
            }
            let in_args = candid::Decode!(arg_value, In).context("Failed to decode arguments.")?;
            Ok(in_args.canister_id)
        }
        MgmtMethod::StartCanister
        | MgmtMethod::StopCanister
        | MgmtMethod::CanisterStatus
        | MgmtMethod::DeleteCanister
        | MgmtMethod::DepositCycles
        | MgmtMethod::UninstallCode
        | MgmtMethod::ProvisionalTopUpCanister
        | MgmtMethod::UploadChunk
        | MgmtMethod::ClearChunkStore
        | MgmtMethod::StoredChunks
        | MgmtMethod::FetchCanisterLogs
        | MgmtMethod::TakeCanisterSnapshot
        | MgmtMethod::LoadCanisterSnapshot
        | MgmtMethod::ListCanisterSnapshots
        | MgmtMethod::DeleteCanisterSnapshot => {
            #[derive(CandidType, Deserialize)]
            struct In {
                canister_id: CanisterId,
            }
            let in_args = candid::Decode!(arg_value, In).context("Failed to decode arguments.")?;
            Ok(in_args.canister_id)
        }
        MgmtMethod::ProvisionalCreateCanisterWithCycles => {
            // TODO: Should we use the provisional_create_canister_effective_canister_id option from main.rs?
            Ok(CanisterId::management_canister())
        }
        MgmtMethod::InstallChunkedCode => {
            #[derive(CandidType, Deserialize)]
            struct In {
                target_canister: Principal,
            }
            let in_args =
                Decode!(arg_value, In).context("Argument is not valid for InstallChunkedCode")?;
            Ok(in_args.target_canister)
        }
    }
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterCallOpts,
    call_sender: &CallSender,
) -> DfxResult {
    let agent = env.get_agent();
    fetch_root_key_if_needed(env).await?;

    let method_name = opts.method_name.as_str();

    let (canister_id, maybe_local_candid_path) =
        get_canister_id_and_candid_path(env, opts.canister_name.as_str())?;

    let method_type = if let Some(path) = opts.candid {
        get_candid_type(CandidSource::File(&path), method_name)
    } else if let Some(did) = fetch_remote_did_file(agent, canister_id).await {
        get_candid_type(CandidSource::Text(&did), method_name)
    } else if let Some(path) = maybe_local_candid_path {
        warn!(env.get_logger(), "DEPRECATION WARNING: Cannot fetch Candid interface from canister metadata, reading Candid interface from the local build artifact. In a future dfx release, we will only read candid interface from canister metadata.");
        warn!(
            env.get_logger(),
            r#"Please add the following to dfx.json to store local candid file into metadata:
"metadata": [
   {{
     "name": "candid:service"
   }}
]"#
        );
        get_candid_type(CandidSource::File(&path), method_name)
    } else {
        None
    };
    if method_type.is_none() {
        warn!(env.get_logger(), "Cannot fetch Candid interface for {method_name}, sending arguments with inferred types.");
    }

    let (argument_from_cli, argument_type) = opts.argument_from_cli.get_argument_and_type()?;

    // Get the argument, get the type, convert the argument to the type and return
    // an error if any of it doesn't work.
    let arg_value = blob_from_arguments(
        Some(env),
        argument_from_cli.as_deref(),
        opts.random.as_deref(),
        argument_type.as_deref(),
        &method_type,
        false,
        opts.always_assist,
    )?;

    let effective_canister_id = if canister_id == CanisterId::management_canister() {
        let management_method = MgmtMethod::from_str(method_name).map_err(|_| {
            anyhow!(
                "Attempted to call an unsupported management canister method: {}",
                method_name
            )
        })?;

        if matches!(call_sender, CallSender::SelectedId)
            && matches!(
                management_method,
                MgmtMethod::CreateCanister
                    | MgmtMethod::RawRand
                    | MgmtMethod::BitcoinGetBalance
                    | MgmtMethod::BitcoinGetUtxos
                    | MgmtMethod::BitcoinSendTransaction
                    | MgmtMethod::BitcoinGetCurrentFeePercentiles
                    | MgmtMethod::EcdsaPublicKey
                    | MgmtMethod::SignWithEcdsa
                    | MgmtMethod::NodeMetricsHistory
            )
        {
            return Err(DiagnosedError::new(
                format!(
                    "{} can only be called by a canister, not by an external user.",
                    method_name
                ),
                format!(
                    "The easiest way to call {} externally is to proxy this call through a wallet.
Try calling this with 'dfx canister call <other arguments> (--network ic) --wallet <wallet id>'.
To figure out the id of your wallet, run 'dfx identity get-wallet (--network ic)'.",
                    method_name
                ),
            ))
            .context("Method only callable by a canister.");
        }

        get_effective_canister_id(&management_method, &arg_value)?
    } else {
        canister_id
    };

    let is_query_method = method_type.as_ref().map(|(_, f)| f.is_query());

    let output_type = opts.output.as_deref();
    let is_query = if opts.r#async {
        false
    } else {
        match is_query_method {
            Some(true) => !opts.update,
            Some(false) => {
                if opts.query {
                    return Err(DiagnosedError::new(
                        format!("{} is an update method, not a query method.", method_name),
                        "Run the command without '--query'.".to_string(),
                    ))
                    .context("Not a query method.");
                } else {
                    false
                }
            }
            None => opts.query,
        }
    };

    // amount has been validated by cycle_amount_validator
    let cycles = opts.with_cycles.unwrap_or(0);

    if call_sender == &CallSender::SelectedId && cycles != 0 {
        return Err(DiagnosedError::new("It is only possible to send cycles from a canister.".to_string(), "To send the same function call from your wallet (a canister), run the command using 'dfx canister call <other arguments> (--network ic) --wallet <wallet id>'.\n\
        To figure out the id of your wallet, run 'dfx identity get-wallet (--network ic)'.".to_string())).context("Function caller is not a canister.");
    }

    if is_query {
        let blob = match call_sender {
            CallSender::SelectedId => {
                let query_builder = agent
                    .query(&canister_id, method_name)
                    .with_effective_canister_id(effective_canister_id)
                    .with_arg(arg_value);
                query_builder.call().await.context("Failed query call.")?
            }
            CallSender::Wallet(wallet_id) => {
                let wallet = build_wallet_canister(*wallet_id, agent).await?;
                do_wallet_call(
                    &wallet,
                    &CallIn {
                        canister: canister_id,
                        method_name: method_name.to_string(),
                        args: arg_value,
                        cycles,
                    },
                )
                .await
                .context("Failed wallet call.")?
            }
        };
        print_idl_blob(&blob, output_type, &method_type)?;
    } else if opts.r#async {
        let call_response = match call_sender {
            CallSender::SelectedId => agent
                .update(&canister_id, method_name)
                .with_effective_canister_id(effective_canister_id)
                .with_arg(arg_value)
                .call()
                .await
                .context("Failed update call.")?,
            CallSender::Wallet(wallet_id) => {
                let wallet = build_wallet_canister(*wallet_id, agent).await?;
                let mut args = Argument::default();
                args.set_raw_arg(arg_value);

                request_id_via_wallet_call(&wallet, canister_id, method_name, args, cycles)
                    .await
                    .context("Failed request via wallet.")?
                    .map(|(res,)| res.r#return)
            }
        };
        match call_response {
            CallResponse::Poll(request_id) => {
                eprint!("Request ID: ");
                println!("0x{}", String::from(request_id));
            }
            CallResponse::Response(response) => {
                print_idl_blob(&response, output_type, &method_type)?;
            }
        }
    } else {
        let blob = match call_sender {
            CallSender::SelectedId => agent
                .update(&canister_id, method_name)
                .with_effective_canister_id(effective_canister_id)
                .with_arg(arg_value)
                .await
                .context("Failed update call.")?,
            CallSender::Wallet(wallet_id) => {
                let wallet = build_wallet_canister(*wallet_id, agent).await?;
                do_wallet_call(
                    &wallet,
                    &CallIn {
                        canister: canister_id,
                        method_name: method_name.to_string(),
                        args: arg_value,
                        cycles,
                    },
                )
                .await
                .context("Failed to do wallet call.")?
            }
        };
        print_idl_blob(&blob, output_type, &method_type)?;
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/create.rs:
-----------------------

use crate::lib::canister_logs::log_visibility::LogVisibilityOpt;
use crate::lib::deps::get_pull_canisters_in_config;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use crate::lib::ic_attributes::{
    get_compute_allocation, get_freezing_threshold, get_log_visibility, get_memory_allocation,
    get_reserved_cycles_limit, get_wasm_memory_limit, CanisterSettings,
};
use crate::lib::operations::canister::create_canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::{
    compute_allocation_parser, freezing_threshold_parser, log_visibility_parser,
    memory_allocation_parser, principal_parser, reserved_cycles_limit_parser,
    wasm_memory_limit_parser,
};
use crate::util::clap::parsers::{cycle_amount_parser, icrc_subaccount_parser};
use crate::util::clap::subnet_selection_opt::SubnetSelectionOpt;
use anyhow::{bail, Context};
use byte_unit::Byte;
use candid::Principal as CanisterId;
use clap::{ArgAction, Parser};
use dfx_core::error::identity::InstantiateIdentityFromNameError::GetIdentityPrincipalFailed;
use dfx_core::identity::CallSender;
use ic_agent::Identity as _;
use ic_utils::interfaces::management_canister::LogVisibility;
use icrc_ledger_types::icrc1::account::Subaccount;
use slog::info;

/// Creates an empty canister and associates the assigned Canister ID to the canister name.
#[derive(Parser)]
pub struct CanisterCreateOpts {
    /// Specifies the canister name. Either this or the --all flag are required.
    canister_name: Option<String>,

    /// Creates all canisters configured in dfx.json.
    #[arg(long, required_unless_present("canister_name"))]
    all: bool,

    /// Specifies the initial cycle balance to deposit into the newly created canister.
    /// The specified amount needs to take the canister create fee into account.
    /// This amount is deducted from the wallet's cycle balance.
    #[arg(long, value_parser = cycle_amount_parser)]
    with_cycles: Option<u128>,

    /// Attempts to create the canister with this Canister ID.
    ///
    /// This option only works with non-mainnet replica.
    /// This option implies the --no-wallet flag.
    /// This option takes precedence over the specified_id field in dfx.json.
    #[arg(long, value_name = "PRINCIPAL", conflicts_with = "all")]
    specified_id: Option<CanisterId>,

    /// Specifies the identity name or the principal of the new controller.
    #[arg(long, action = ArgAction::Append)]
    controller: Option<Vec<String>>,

    /// Specifies the canister's compute allocation. This should be a percent in the range [0..100]
    #[arg(long, short('c'), value_parser = compute_allocation_parser)]
    compute_allocation: Option<u64>,

    /// Specifies how much memory the canister is allowed to use in total.
    /// This should be a value in the range [0..12 GiB]. Can include units, e.g. "4KiB".
    /// A setting of 0 means the canister will have access to memory on a â€œbest-effortâ€ basis:
    /// It will only be charged for the memory it uses, but at any point in time may stop running
    /// if it tries to allocate more memory when there isnâ€™t space available on the subnet.
    #[arg(long, value_parser = memory_allocation_parser)]
    memory_allocation: Option<Byte>,

    #[arg(long, value_parser = freezing_threshold_parser, hide = true)]
    freezing_threshold: Option<u64>,

    /// Specifies the upper limit of the canister's reserved cycles balance.
    ///
    /// Reserved cycles are cycles that the system sets aside for future use by the canister.
    /// If a subnet's storage exceeds 450 GiB, then every time a canister allocates new storage bytes,
    /// the system sets aside some amount of cycles from the main balance of the canister.
    /// These reserved cycles will be used to cover future payments for the newly allocated bytes.
    /// The reserved cycles are not transferable and the amount of reserved cycles depends on how full the subnet is.
    ///
    /// A setting of 0 means that the canister will trap if it tries to allocate new storage while the subnet's memory usage exceeds 450 GiB.
    #[arg(long, value_parser = reserved_cycles_limit_parser, hide = true)]
    reserved_cycles_limit: Option<u128>,

    /// Specifies a soft limit on the Wasm memory usage of the canister.
    ///
    /// Update calls, timers, heartbeats, installs, and post-upgrades fail if the
    /// Wasm memory usage exceeds this limit. The main purpose of this setting is
    /// to protect against the case when the canister reaches the hard 4GiB
    /// limit.
    ///
    /// Must be a number between 0 B and 256 TiB, inclusive. Can include units, e.g. "4KiB".
    #[arg(long, value_parser = wasm_memory_limit_parser, hide = true)]
    wasm_memory_limit: Option<Byte>,

    /// Specifies who is allowed to read the canister's logs.
    /// Can be either "controllers" or "public".
    #[arg(long, value_parser = log_visibility_parser, conflicts_with("log_viewer"))]
    log_visibility: Option<LogVisibility>,

    /// Specifies the the principal of the log viewer of the canister.
    /// Can be specified more than once.
    #[arg(long, action = ArgAction::Append, value_parser = principal_parser, conflicts_with("log_visibility"))]
    log_viewer: Option<Vec<candid::Principal>>,

    /// Performs the call with the user Identity as the Sender of messages.
    /// Bypasses the Wallet canister.
    #[arg(long)]
    no_wallet: bool,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction deduplication, default is system time.
    /// https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long, conflicts_with = "all")]
    created_at_time: Option<u64>,

    /// Subaccount of the selected identity to spend cycles from.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    from_subaccount: Option<Subaccount>,

    #[command(flatten)]
    subnet_selection: SubnetSelectionOpt,
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterCreateOpts,
    call_sender: &CallSender,
) -> DfxResult {
    let config = env.get_config_or_anyhow()?;

    fetch_root_key_if_needed(env).await?;

    let with_cycles = opts.with_cycles;

    let config_interface = config.get_config();
    let network = env.get_network_descriptor();

    let controllers: Option<Vec<_>> = opts
        .controller
        .clone()
        .map(|controllers| {
            controllers
                .iter()
                .map(
                    |controller| match CanisterId::from_text(controller.clone()) {
                        Ok(principal) => Ok(principal),
                        Err(_) => {
                            let current_id = env.get_selected_identity().unwrap();
                            if current_id == controller {
                                Ok(env.get_selected_identity_principal().unwrap())
                            } else {
                                let identity_name = controller;
                                env.new_identity_manager()?
                                    .instantiate_identity_from_name(identity_name, env.get_logger())
                                    .and_then(|identity| {
                                        identity.sender().map_err(GetIdentityPrincipalFailed)
                                    })
                                    .map_err(DfxError::new)
                            }
                        }
                    },
                )
                .collect::<Result<Vec<_>, DfxError>>()
        })
        .transpose()
        .context("Failed to determine controllers.")?;
    let mut subnet_selection = opts
        .subnet_selection
        .into_subnet_selection_type(env)
        .await?;

    let pull_canisters_in_config = get_pull_canisters_in_config(env)?;
    if let Some(canister_name) = opts.canister_name.as_deref() {
        if pull_canisters_in_config.contains_key(canister_name) {
            bail!("{canister_name} is a pull dependency. Please deploy it using `dfx deps deploy {canister_name}`");
        }
        let canister_is_remote =
            config_interface.is_remote_canister(canister_name, &network.name)?;
        if canister_is_remote {
            bail!("Canister '{canister_name}' is a remote canister on network '{}', and cannot be created from here.", &network.name)
        }
        let compute_allocation = get_compute_allocation(
            opts.compute_allocation,
            Some(config_interface),
            Some(canister_name),
        )
        .with_context(|| format!("Failed to read compute allocation of {canister_name}."))?;
        let memory_allocation = get_memory_allocation(
            opts.memory_allocation,
            Some(config_interface),
            Some(canister_name),
        )
        .with_context(|| format!("Failed to read memory allocation of {canister_name}."))?;
        let freezing_threshold = get_freezing_threshold(
            opts.freezing_threshold,
            Some(config_interface),
            Some(canister_name),
        )
        .with_context(|| format!("Failed to read freezing threshold of {canister_name}."))?;
        let reserved_cycles_limit = get_reserved_cycles_limit(
            opts.reserved_cycles_limit,
            Some(config_interface),
            Some(canister_name),
        )
        .with_context(|| format!("Failed to read reserved cycles limit of {canister_name}."))?;
        let wasm_memory_limit = get_wasm_memory_limit(
            opts.wasm_memory_limit,
            Some(config_interface),
            Some(canister_name),
        )
        .with_context(|| format!("Failed to read Wasm memory limit of {canister_name}."))?;
        let log_visibility = get_log_visibility(
            env,
            LogVisibilityOpt::from(&opts.log_visibility, &opts.log_viewer).as_ref(),
            None,
            Some(config_interface),
            Some(canister_name),
        )
        .with_context(|| format!("Failed to read log visibility of {canister_name}."))?;
        create_canister(
            env,
            canister_name,
            with_cycles,
            opts.specified_id,
            call_sender,
            opts.no_wallet,
            opts.from_subaccount,
            CanisterSettings {
                controllers,
                compute_allocation,
                memory_allocation,
                freezing_threshold,
                reserved_cycles_limit,
                wasm_memory_limit,
                log_visibility,
            },
            opts.created_at_time,
            &mut subnet_selection,
        )
        .await?;
        Ok(())
    } else if opts.all {
        // Create all canisters.
        if let Some(canisters) = &config_interface.canisters {
            for canister_name in canisters.keys() {
                if pull_canisters_in_config.contains_key(canister_name) {
                    continue;
                }
                let canister_is_remote =
                    config_interface.is_remote_canister(canister_name, &network.name)?;
                if canister_is_remote {
                    info!(
                        env.get_logger(),
                        "Skipping canister '{canister_name}' because it is remote for network '{}'",
                        &network.name,
                    );

                    continue;
                }
                let specified_id = config_interface.get_specified_id(canister_name)?;
                let compute_allocation = get_compute_allocation(
                    opts.compute_allocation,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| {
                    format!("Failed to read compute allocation of {canister_name}.")
                })?;
                let memory_allocation = get_memory_allocation(
                    opts.memory_allocation,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| format!("Failed to read memory allocation of {canister_name}."))?;
                let freezing_threshold = get_freezing_threshold(
                    opts.freezing_threshold,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| {
                    format!("Failed to read freezing threshold of {canister_name}.")
                })?;
                let reserved_cycles_limit = get_reserved_cycles_limit(
                    opts.reserved_cycles_limit,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| {
                    format!("Failed to read reserved cycles limit of {canister_name}.")
                })?;
                let wasm_memory_limit = get_wasm_memory_limit(
                    opts.wasm_memory_limit,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| format!("Failed to read Wasm memory limit of {canister_name}."))?;
                let log_visibility = get_log_visibility(
                    env,
                    LogVisibilityOpt::from(&opts.log_visibility, &opts.log_viewer).as_ref(),
                    None,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| format!("Failed to read log visibility of {canister_name}."))?;
                create_canister(
                    env,
                    canister_name,
                    with_cycles,
                    specified_id,
                    call_sender,
                    opts.no_wallet,
                    opts.from_subaccount,
                    CanisterSettings {
                        controllers: controllers.clone(),
                        compute_allocation,
                        memory_allocation,
                        freezing_threshold,
                        reserved_cycles_limit,
                        wasm_memory_limit,
                        log_visibility,
                    },
                    opts.created_at_time,
                    &mut subnet_selection,
                )
                .await?;
            }
            if !pull_canisters_in_config.is_empty() {
                info!(env.get_logger(), "There are pull dependencies defined in dfx.json. Please deploy them using `dfx deps deploy`.");
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}


-----------------------

/src/dfx/src/commands/canister/delete.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::ic_attributes::CanisterSettings;
use crate::lib::operations::canister;
use crate::lib::operations::canister::{
    deposit_cycles, start_canister, stop_canister, update_settings,
};
use crate::lib::operations::cycles_ledger::wallet_deposit_to_cycles_ledger;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::assets::wallet_wasm;
use crate::util::blob_from_arguments;
use crate::util::clap::parsers::{cycle_amount_parser, icrc_subaccount_parser};
use anyhow::{bail, Context};
use candid::Principal;
use clap::Parser;
use dfx_core::canister::build_wallet_canister;
use dfx_core::cli::ask_for_consent;
use dfx_core::identity::wallet::wallet_canister_id;
use dfx_core::identity::CallSender;
use fn_error_context::context;
use ic_utils::interfaces::management_canister::attributes::FreezingThreshold;
use ic_utils::interfaces::management_canister::builders::InstallMode;
use ic_utils::interfaces::management_canister::CanisterStatus;
use ic_utils::interfaces::ManagementCanister;
use ic_utils::Argument;
use icrc_ledger_types::icrc1::account::{Account, Subaccount};
use num_traits::cast::ToPrimitive;
use slog::{debug, info};
use std::convert::TryFrom;

const DANK_PRINCIPAL: Principal =
    Principal::from_slice(&[0, 0, 0, 0, 0, 0xe0, 1, 0x11, 0x01, 0x01]); // Principal: aanaa-xaaaa-aaaah-aaeiq-cai

// "Couldn't send message" when deleting a canister: increase WITHDRAWAL_COST
const WITHDRAWAL_COST: u128 = 30_000_000_000; // conservative estimate based on mainnet observation

/// Deletes a currently stopped canister.
#[derive(Parser)]
pub struct CanisterDeleteOpts {
    /// Specifies the name of the canister to delete.
    /// You must specify either a canister name/id or the --all flag.
    canister: Option<String>,

    /// Deletes all of the canisters configured in the dfx.json file.
    #[arg(long, required_unless_present("canister"))]
    all: bool,

    /// Do not withdrawal cycles, just delete the canister.
    #[arg(long)]
    no_withdrawal: bool,

    /// Withdraw cycles from canister(s) to the specified canister/wallet before deleting.
    #[arg(long, conflicts_with("no_withdrawal"))]
    withdraw_cycles_to_canister: Option<String>,

    /// Withdraw cycles to dank with the current principal.
    #[arg(
        long,
        conflicts_with("withdraw_cycles_to_canister"),
        conflicts_with("no_withdrawal")
    )]
    withdraw_cycles_to_dank: bool,

    /// Withdraw cycles to dank with the given principal.
    #[arg(
        long,
        conflicts_with("withdraw_cycles_to_canister"),
        conflicts_with("no_withdrawal")
    )]
    withdraw_cycles_to_dank_principal: Option<String>,

    /// Leave this many cycles in the canister when withdrawing cycles.
    #[arg(long, value_parser = cycle_amount_parser, conflicts_with("no_withdrawal"))]
    initial_margin: Option<u128>,

    /// Auto-confirm deletion for a non-stopped canister.
    #[arg(long, short)]
    yes: bool,

    /// Subaccount of the selected identity to deposit cycles to.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    to_subaccount: Option<Subaccount>,
}

#[context("Failed to delete canister '{}'.", canister)]
async fn delete_canister(
    env: &dyn Environment,
    canister: &str,
    call_sender: &CallSender,
    no_withdrawal: bool,
    skip_confirmation: bool,
    withdraw_cycles_to_canister: Option<String>,
    withdraw_cycles_to_dank: bool,
    withdraw_cycles_to_dank_principal: Option<String>,
    to_cycles_ledger_subaccount: Option<Subaccount>,
    initial_margin: Option<u128>,
) -> DfxResult {
    let log = env.get_logger();
    let mut canister_id_store = env.get_canister_id_store()?;
    let (canister_id, canister_name_to_delete) = match Principal::from_text(canister) {
        Ok(canister_id) => (
            canister_id,
            canister_id_store.get_name_in_project(canister).cloned(),
        ),
        Err(_) => (canister_id_store.get(canister)?, Some(canister.to_string())),
    };

    if !env.get_network_descriptor().is_playground() {
        let mut call_sender = call_sender;
        let to_dank = withdraw_cycles_to_dank || withdraw_cycles_to_dank_principal.is_some();

        // Get the canister to transfer the cycles to.
        let withdraw_target = if no_withdrawal {
            WithdrawTarget::NoWithdrawal
        } else if to_dank {
            WithdrawTarget::Dank
        } else {
            match withdraw_cycles_to_canister {
                Some(ref target_canister_id) => {
                    let canister_id =
                        Principal::from_text(target_canister_id).with_context(|| {
                            format!("Failed to read canister id {:?}.", target_canister_id)
                        })?;
                    WithdrawTarget::Canister { canister_id }
                }
                None => match call_sender {
                    CallSender::Wallet(wallet_id) => WithdrawTarget::Canister {
                        canister_id: *wallet_id,
                    },
                    CallSender::SelectedId => {
                        let network = env.get_network_descriptor();
                        let identity_name = env
                            .get_selected_identity()
                            .expect("No selected identity.")
                            .to_string();
                        // If there is no wallet, then do not attempt to withdraw the cycles.
                        match wallet_canister_id(network, &identity_name)? {
                            Some(canister_id) => WithdrawTarget::Canister { canister_id },
                            None => {
                                let Some(my_principal) = env.get_selected_identity_principal()
                                else {
                                    bail!("Identity has no principal attached")
                                };
                                WithdrawTarget::CyclesLedger {
                                    to: Account {
                                        owner: my_principal,
                                        subaccount: to_cycles_ledger_subaccount,
                                    },
                                }
                            }
                        }
                    }
                },
            }
        };
        let principal = env
            .get_selected_identity_principal()
            .expect("Selected identity not instantiated.");
        let dank_target_principal = match withdraw_cycles_to_dank_principal {
            None => principal,
            Some(principal) => Principal::from_text(&principal)
                .with_context(|| format!("Failed to read principal {:?}.", &principal))?,
        };
        fetch_root_key_if_needed(env).await?;

        if withdraw_target != WithdrawTarget::NoWithdrawal {
            info!(
                log,
                "Beginning withdrawal of cycles; on failure try --no-wallet --no-withdrawal."
            );

            // Determine how many cycles we can withdraw.
            let status = canister::get_canister_status(env, canister_id, call_sender).await?;
            if status.status != CanisterStatus::Stopped && !skip_confirmation {
                ask_for_consent(&format!(
                    "Canister {canister} has not been stopped. Delete anyway?"
                ))?;
            }
            let agent = env.get_agent();
            let mgr = ManagementCanister::create(agent);
            let canister_id =
                Principal::from_text(canister).or_else(|_| canister_id_store.get(canister))?;

            // Set this principal to be a controller and minimize the freezing threshold to free up as many cycles as possible.
            let settings = CanisterSettings {
                controllers: Some(vec![principal]),
                compute_allocation: None,
                memory_allocation: None,
                freezing_threshold: Some(FreezingThreshold::try_from(0u8).unwrap()),
                reserved_cycles_limit: None,
                wasm_memory_limit: None,
                log_visibility: None,
            };
            info!(log, "Setting the controller to identity principal.");
            update_settings(env, canister_id, settings, call_sender).await?;

            // Install a temporary wallet wasm which will transfer the cycles out of the canister before it is deleted.
            let wasm_module = wallet_wasm(env.get_logger())?;
            info!(
                log,
                "Installing temporary wallet in canister {} to enable transfer of cycles.",
                canister
            );
            let args = blob_from_arguments(None, None, None, None, &None, false, false)?;
            let mode = InstallMode::Reinstall;
            let install_builder = mgr
                .install_code(&canister_id, &wasm_module)
                .with_raw_arg(args.to_vec())
                .with_mode(mode);
            let install_result = install_builder
                .build()
                .context("Failed to build InstallCode call.")?
                .await;
            if install_result.is_ok() {
                start_canister(env, canister_id, &CallSender::SelectedId).await?;
                let status =
                    canister::get_canister_status(env, canister_id, &CallSender::SelectedId)
                        .await?;
                let cycles = status.cycles.0.to_u128().unwrap();
                let mut attempts = 0_u128;
                loop {
                    let margin =
                        initial_margin.unwrap_or(WITHDRAWAL_COST) + attempts * WITHDRAWAL_COST / 10;
                    if margin >= cycles {
                        info!(log, "Too few cycles to withdraw: {}.", cycles);
                        break;
                    }
                    let cycles_to_withdraw = cycles - margin;
                    debug!(
                        log,
                        "Margin: {margin}. Withdrawing {cycles_to_withdraw} cycles."
                    );
                    let result = match withdraw_target {
                        WithdrawTarget::NoWithdrawal => Ok(()),
                        WithdrawTarget::Dank => {
                            info!(
                                log,
                                "Attempting to transfer {} cycles to dank principal {}.",
                                cycles_to_withdraw,
                                dank_target_principal
                            );
                            let wallet = build_wallet_canister(canister_id, agent).await?;
                            let opt_principal = Some(dank_target_principal);
                            wallet
                                .call(
                                    DANK_PRINCIPAL,
                                    "mint",
                                    Argument::from_candid((opt_principal,)),
                                    cycles_to_withdraw,
                                )
                                .await
                                .context("Failed mint call.")
                        }
                        WithdrawTarget::Canister {
                            canister_id: target_canister_id,
                        } => {
                            info!(
                                log,
                                "Attempting to transfer {} cycles to canister {}.",
                                cycles_to_withdraw,
                                target_canister_id
                            );
                            // Transfer cycles from the source canister to the target canister using the temporary wallet.
                            deposit_cycles(
                                env,
                                target_canister_id,
                                &CallSender::Wallet(canister_id),
                                cycles_to_withdraw,
                            )
                            .await
                        }
                        WithdrawTarget::CyclesLedger { to } => {
                            wallet_deposit_to_cycles_ledger(
                                agent,
                                canister_id,
                                cycles_to_withdraw,
                                to,
                            )
                            .await
                        }
                    };
                    if result.is_ok() {
                        info!(log, "Successfully withdrew {} cycles.", cycles_to_withdraw);
                        break;
                    } else {
                        let message = format!("{:?}", result);
                        if message.contains("Couldn't send message")
                            || message.contains("out of cycles")
                        {
                            info!(log, "Not enough margin. Trying again with more margin.");
                            attempts += 1;
                        } else {
                            // Unforeseen error. Report it back to user
                            result?;
                        }
                    }
                }
                stop_canister(env, canister_id, &CallSender::SelectedId).await?;
            } else {
                info!(
                    log,
                    "Failed to install temporary wallet, deleting without withdrawal."
                );
                if status.status != CanisterStatus::Stopped {
                    info!(log, "Stopping canister.")
                }
                stop_canister(env, canister_id, &CallSender::SelectedId).await?;
            }
            call_sender = &CallSender::SelectedId;
        }

        info!(
            log,
            "Deleting canister {}, with canister_id {}",
            canister,
            canister_id.to_text(),
        );

        canister::delete_canister(env, canister_id, call_sender).await?;
    }

    if let Some(canister_name) = canister_name_to_delete {
        canister_id_store.remove(&canister_name)?;
    }

    Ok(())
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterDeleteOpts,
    call_sender: &CallSender,
) -> DfxResult {
    let config = env.get_config_or_anyhow()?;

    fetch_root_key_if_needed(env).await?;

    if let Some(canister) = opts.canister.as_deref() {
        delete_canister(
            env,
            canister,
            call_sender,
            opts.no_withdrawal,
            opts.yes,
            opts.withdraw_cycles_to_canister,
            opts.withdraw_cycles_to_dank,
            opts.withdraw_cycles_to_dank_principal,
            opts.to_subaccount,
            opts.initial_margin,
        )
        .await
    } else if opts.all {
        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                delete_canister(
                    env,
                    canister,
                    call_sender,
                    opts.no_withdrawal,
                    opts.yes,
                    opts.withdraw_cycles_to_canister.clone(),
                    opts.withdraw_cycles_to_dank,
                    opts.withdraw_cycles_to_dank_principal.clone(),
                    opts.to_subaccount,
                    opts.initial_margin,
                )
                .await?;
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}

#[derive(Debug, Clone, Copy, Eq, PartialEq)]
enum WithdrawTarget {
    NoWithdrawal,
    Dank,
    CyclesLedger { to: Account },
    Canister { canister_id: Principal },
}


-----------------------

/src/dfx/src/commands/canister/deposit_cycles.rs:
-----------------------

use std::time::{SystemTime, UNIX_EPOCH};

use crate::lib::error::DfxResult;
use crate::lib::identity::wallet::get_or_create_wallet_canister;
use crate::lib::operations::canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::lib::{environment::Environment, operations::cycles_ledger};
use crate::util::clap::parsers::{cycle_amount_parser, icrc_subaccount_parser};
use anyhow::{bail, Context};
use candid::Principal;
use clap::Parser;
use dfx_core::identity::CallSender;
use icrc_ledger_types::icrc1::account::Subaccount;
use slog::{debug, info, warn};

/// Deposit cycles into the specified canister.
#[derive(Parser)]
pub struct DepositCyclesOpts {
    /// Specifies the amount of cycles to send on the call.
    /// Deducted from the wallet.
    #[arg(value_parser = cycle_amount_parser)]
    cycles: u128,

    /// Specifies the name or id of the canister to receive the cycles deposit.
    /// You must specify either a canister name/id or the --all option.
    canister: Option<String>,

    /// Deposit cycles to all of the canisters configured in the dfx.json file.
    #[arg(long, required_unless_present("canister"))]
    all: bool,

    /// Use cycles from this subaccount.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    from_subaccount: Option<Subaccount>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction deduplication, default is system time.
    /// https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,
}

async fn deposit_cycles(
    env: &dyn Environment,
    canister: &str,
    call_sender: &CallSender,
    cycles: u128,
    created_at_time: u64,
    from_subaccount: Option<Subaccount>,
) -> DfxResult {
    let log = env.get_logger();
    let canister_id_store = env.get_canister_id_store()?;
    let canister_id =
        Principal::from_text(canister).or_else(|_| canister_id_store.get(canister))?;

    info!(log, "Depositing {} cycles onto {}", cycles, canister,);

    match call_sender {
        CallSender::SelectedId => {
            cycles_ledger::withdraw(
                env.get_agent(),
                env.get_logger(),
                canister_id,
                cycles,
                created_at_time,
                from_subaccount,
            )
            .await?;
        }
        CallSender::Wallet(_) => {
            canister::deposit_cycles(env, canister_id, call_sender, cycles).await?
        }
    };

    let status = canister::get_canister_status(env, canister_id, call_sender).await;
    if let Ok(status) = status {
        info!(
            log,
            "Deposited {} cycles, updated balance: {} cycles", cycles, status.cycles
        );
    } else {
        info!(log, "Deposited {cycles} cycles.");
    }

    Ok(())
}

pub async fn exec(
    env: &dyn Environment,
    opts: DepositCyclesOpts,
    mut call_sender: &CallSender,
) -> DfxResult {
    fetch_root_key_if_needed(env).await?;

    let proxy_sender;

    if call_sender == &CallSender::SelectedId {
        match get_or_create_wallet_canister(
            env,
            env.get_network_descriptor(),
            env.get_selected_identity().expect("No selected identity"),
        )
        .await
        {
            Ok(wallet) => {
                proxy_sender = CallSender::Wallet(*wallet.canister_id_());
                call_sender = &proxy_sender;
            }
            Err(err) => {
                if matches!(err, crate::lib::identity::wallet::GetOrCreateWalletCanisterError::NoWalletConfigured { .. }) {
                    debug!(env.get_logger(), "No wallet configured");
                } else {
                    bail!(err)
                }
            }
        }
    }

    let created_at_time = opts.created_at_time.unwrap_or_else(|| {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64;
        if matches!(call_sender, CallSender::SelectedId) {
            warn!(
                env.get_logger(),
                "If you retry this operation, use --created-at-time {}", now
            );
        }
        now
    });

    // amount has been validated by cycle_amount_validator
    let cycles = opts.cycles;

    if let Some(canister) = opts.canister.as_deref() {
        deposit_cycles(
            env,
            canister,
            call_sender,
            cycles,
            created_at_time,
            opts.from_subaccount,
        )
        .await
    } else if opts.all {
        let config = env.get_config_or_anyhow()?;

        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                deposit_cycles(
                    env,
                    canister,
                    call_sender,
                    cycles,
                    created_at_time,
                    opts.from_subaccount,
                )
                .await
                .with_context(|| format!("Failed to deposit cycles into {}.", canister))?;
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}


-----------------------

/src/dfx/src/commands/canister/id.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use candid::Principal;
use clap::Parser;
use dfx_core::config::model::canister_id_store::CanisterIdStore;
use dfx_core::network::provider::{create_network_descriptor, LocalBindDetermination};

/// Prints the identifier of a canister.
#[derive(Parser)]
pub struct CanisterIdOpts {
    /// Specifies the name of the canister.
    canister: String,

    #[command(flatten)]
    network: NetworkOpt,
}

pub async fn exec(env: &dyn Environment, opts: CanisterIdOpts) -> DfxResult {
    env.get_config_or_anyhow()?;
    let network_descriptor = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        opts.network.to_network_name(),
        None,
        LocalBindDetermination::AsConfigured,
    )?;
    let canister_id_store =
        CanisterIdStore::new(env.get_logger(), &network_descriptor, env.get_config()?)?;

    let canister_name = opts.canister.as_str();
    let canister_id =
        Principal::from_text(canister_name).or_else(|_| canister_id_store.get(canister_name))?;
    println!("{}", Principal::to_text(&canister_id));
    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/info.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::lib::state_tree::canister_info::{
    read_state_tree_canister_controllers, read_state_tree_canister_module_hash,
};
use anyhow::anyhow;
use candid::Principal;
use clap::Parser;
use itertools::Itertools;

/// Get the hash of a canisterâ€™s Wasm module and its current controllers.
#[derive(Parser)]
pub struct InfoOpts {
    /// Specifies the name or id of the canister to get its canister information.
    canister: String,
}

pub async fn exec(env: &dyn Environment, opts: InfoOpts) -> DfxResult {
    let agent = env.get_agent();

    let callee_canister = opts.canister.as_str();
    let canister_id_store = env.get_canister_id_store()?;

    let canister_id = Principal::from_text(callee_canister)
        .or_else(|_| canister_id_store.get(callee_canister))?;

    fetch_root_key_if_needed(env).await?;

    let controllers_sorted: Vec<_> = read_state_tree_canister_controllers(agent, canister_id)
        .await?
        .ok_or_else(|| anyhow!("Canister {canister_id} does not exist."))?
        .iter()
        .map(Principal::to_text)
        .sorted()
        .collect();

    let module_hash_hex = match read_state_tree_canister_module_hash(agent, canister_id).await? {
        None => "None".to_string(),
        Some(blob) => format!("0x{}", hex::encode(blob)),
    };

    println!(
        "Controllers: {}\nModule hash: {}",
        controllers_sorted.join(" "),
        module_hash_hex
    );

    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/install.rs:
-----------------------

use crate::lib::canister_info::CanisterInfo;
use crate::lib::deps::get_pull_canisters_in_config;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister::install_canister::install_canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::blob_from_arguments;
use crate::util::clap::argument_from_cli::ArgumentFromCliLongOpt;
use crate::util::clap::install_mode::{InstallModeHint, InstallModeOpt};
use dfx_core::canister::{install_canister_wasm, install_mode_to_prompt};
use dfx_core::identity::CallSender;

use anyhow::bail;
use candid::Principal;
use clap::Parser;
use slog::info;
use std::path::PathBuf;

/// Installs compiled code in a canister.
#[derive(Parser, Clone)]
pub struct CanisterInstallOpts {
    /// Specifies the canister to deploy. You must specify either canister name/id or the --all option.
    canister: Option<String>,

    /// Deploys all canisters configured in the project dfx.json files.
    #[arg(
        long,
        required_unless_present("canister"),
        conflicts_with("argument"),
        conflicts_with("argument_file")
    )]
    all: bool,

    /// Specifies not to wait for the result of the call to be returned by polling the replica. Instead return a response ID.
    #[arg(long)]
    async_call: bool,

    #[command(flatten)]
    install_mode: InstallModeOpt,

    /// Upgrade the canister even if the .wasm did not change.
    #[arg(long)]
    upgrade_unchanged: bool,

    #[command(flatten)]
    argument_from_cli: ArgumentFromCliLongOpt,

    /// Specifies a particular Wasm file to install, bypassing the dfx.json project settings.
    #[arg(long, conflicts_with("all"))]
    wasm: Option<PathBuf>,

    /// Output environment variables to a file in dotenv format (without overwriting any user-defined variables, if the file already exists).
    output_env_file: Option<PathBuf>,

    /// Skips yes/no checks by answering 'yes'. Such checks usually result in data loss,
    /// so this is not recommended outside of CI.
    #[arg(long, short)]
    yes: bool,

    /// Skips upgrading the asset canister, to only install the assets themselves.
    #[arg(long)]
    no_asset_upgrade: bool,

    /// Always use Candid assist when the argument types are all optional.
    #[arg(
        long,
        conflicts_with("argument"),
        conflicts_with("argument_file"),
        conflicts_with("yes")
    )]
    always_assist: bool,
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterInstallOpts,
    call_sender: &CallSender,
) -> DfxResult {
    fetch_root_key_if_needed(env).await?;

    let mode_hint = opts.install_mode.mode_for_canister_install()?;
    let mut canister_id_store = env.get_canister_id_store()?;
    let network = env.get_network_descriptor();

    if mode_hint == InstallModeHint::Reinstall && (opts.canister.is_none() || opts.all) {
        bail!("The --mode=reinstall is only valid when specifying a single canister, because reinstallation destroys all data in the canister.");
    }

    if let Some(canister) = opts.canister.as_deref() {
        let (argument_from_cli, argument_type) = opts.argument_from_cli.get_argument_and_type()?;
        // `opts.canister` is a Principal (canister ID)
        if let Ok(canister_id) = Principal::from_text(canister) {
            if let Some(wasm_path) = &opts.wasm {
                let args = blob_from_arguments(
                    Some(env),
                    argument_from_cli.as_deref(),
                    None,
                    argument_type.as_deref(),
                    &None,
                    true,
                    opts.always_assist,
                )?;
                let wasm_module = dfx_core::fs::read(wasm_path)?;
                let mode = mode_hint.to_install_mode_with_wasm_path()?;
                info!(
                    env.get_logger(),
                    "{} code for canister {}",
                    install_mode_to_prompt(&mode),
                    canister_id,
                );
                install_canister_wasm(
                    env.get_agent(),
                    canister_id,
                    None,
                    &args,
                    mode,
                    call_sender,
                    wasm_module,
                    opts.yes,
                )
                .await?;
                Ok(())
            } else {
                bail!("When installing a canister by its ID, you must specify `--wasm` option.")
            }
        } else {
            // `opts.canister` is not a canister ID, but a canister name
            let config = env.get_config_or_anyhow()?;
            let config_interface = config.get_config();
            let env_file = config.get_output_env_file(opts.output_env_file)?;
            let pull_canisters_in_config = get_pull_canisters_in_config(env)?;
            if pull_canisters_in_config.contains_key(canister) {
                bail!(
                    "{0} is a pull dependency. Please deploy it using `dfx deps deploy {0}`",
                    canister
                );
            }
            if config_interface.is_remote_canister(canister, &network.name)? {
                bail!("Canister '{}' is a remote canister on network '{}', and cannot be installed from here.", canister, &network.name)
            }

            let canister_id = canister_id_store.get(canister)?;
            let canister_info = CanisterInfo::load(&config, canister, Some(canister_id))?;
            if let Some(wasm_path) = opts.wasm {
                // streamlined version, we can ignore most of the environment
                install_canister(
                    env,
                    &mut canister_id_store,
                    canister_id,
                    &canister_info,
                    Some(&wasm_path),
                    argument_from_cli.as_deref(),
                    argument_type.as_deref(),
                    &mode_hint,
                    call_sender,
                    opts.upgrade_unchanged,
                    None,
                    opts.yes,
                    None,
                    opts.no_asset_upgrade,
                    opts.always_assist,
                )
                .await
                .map_err(Into::into)
            } else {
                install_canister(
                    env,
                    &mut canister_id_store,
                    canister_id,
                    &canister_info,
                    None,
                    argument_from_cli.as_deref(),
                    argument_type.as_deref(),
                    &mode_hint,
                    call_sender,
                    opts.upgrade_unchanged,
                    None,
                    opts.yes,
                    env_file.as_deref(),
                    opts.no_asset_upgrade,
                    opts.always_assist,
                )
                .await
                .map_err(Into::into)
            }
        }
    } else if opts.all {
        // Install all canisters.
        let config = env.get_config_or_anyhow()?;
        let config_interface = config.get_config();
        let env_file = config.get_output_env_file(opts.output_env_file)?;
        let pull_canisters_in_config = get_pull_canisters_in_config(env)?;
        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                if pull_canisters_in_config.contains_key(canister) {
                    continue;
                }
                if config_interface.is_remote_canister(canister, &network.name)? {
                    info!(
                        env.get_logger(),
                        "Skipping canister '{}' because it is remote for network '{}'",
                        canister,
                        &network.name,
                    );
                    continue;
                }

                let canister_id = canister_id_store.get(canister)?;
                let canister_info = CanisterInfo::load(&config, canister, Some(canister_id))?;
                install_canister(
                    env,
                    &mut canister_id_store,
                    canister_id,
                    &canister_info,
                    None,
                    None,
                    None,
                    &mode_hint,
                    call_sender,
                    opts.upgrade_unchanged,
                    None,
                    opts.yes,
                    env_file.as_deref(),
                    opts.no_asset_upgrade,
                    opts.always_assist,
                )
                .await?;
            }
        }
        if !pull_canisters_in_config.is_empty() {
            info!(env.get_logger(), "There are pull dependencies defined in dfx.json. Please deploy them using `dfx deps deploy`.");
        }
        Ok(())
    } else {
        unreachable!()
    }
}


-----------------------

/src/dfx/src/commands/canister/logs.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use candid::Principal;
use clap::Parser;
use dfx_core::identity::CallSender;
use ic_utils::interfaces::management_canister::FetchCanisterLogsResponse;
use time::format_description::well_known::Rfc3339;
use time::OffsetDateTime;

/// Get the canister logs.
#[derive(Parser)]
pub struct LogsOpts {
    /// Specifies the name or id of the canister to get its canister information.
    canister: String,
}

fn format_bytes(bytes: &[u8]) -> String {
    format!("(bytes) 0x{}", hex::encode(bytes))
}

fn format_canister_logs(logs: FetchCanisterLogsResponse) -> Vec<String> {
    logs.canister_log_records
        .into_iter()
        .map(|r| {
            let time = OffsetDateTime::from_unix_timestamp_nanos(r.timestamp_nanos as i128)
                .expect("Invalid canister log record timestamp");

            let message = if let Ok(s) = String::from_utf8(r.content.clone()) {
                if format!("{s:?}").contains("\\u{") {
                    format_bytes(&r.content)
                } else {
                    s
                }
            } else {
                format_bytes(&r.content)
            };

            format!(
                "[{}. {}]: {}",
                r.idx,
                time.format(&Rfc3339).expect("Failed to format timestamp"),
                message
            )
        })
        .collect()
}

#[test]
fn test_format_canister_logs() {
    use ic_utils::interfaces::management_canister::CanisterLogRecord;

    let logs = FetchCanisterLogsResponse {
        canister_log_records: vec![
            CanisterLogRecord {
                idx: 42,
                timestamp_nanos: 1_620_328_630_000_000_001,
                content: b"Some text message".to_vec(),
            },
            CanisterLogRecord {
                idx: 43,
                timestamp_nanos: 1_620_328_630_000_000_002,
                content: vec![192, 255, 238],
            },
        ],
    };
    assert_eq!(
        format_canister_logs(logs),
        vec![
            "[42. 2021-05-06T19:17:10.000000001Z]: Some text message".to_string(),
            "[43. 2021-05-06T19:17:10.000000002Z]: (bytes) 0xc0ffee".to_string(),
        ],
    );
}

pub async fn exec(env: &dyn Environment, opts: LogsOpts, call_sender: &CallSender) -> DfxResult {
    let callee_canister = opts.canister.as_str();
    let canister_id_store = env.get_canister_id_store()?;

    let canister_id = Principal::from_text(callee_canister)
        .or_else(|_| canister_id_store.get(callee_canister))?;

    fetch_root_key_if_needed(env).await?;

    let logs = canister::get_canister_logs(env, canister_id, call_sender).await?;

    println!("{}", format_canister_logs(logs).join("\n"));

    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/metadata.rs:
-----------------------

use crate::lib::error::DfxResult;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::Environment;
use anyhow::Context;
use candid::Principal;
use clap::Parser;
use std::io::{stdout, Write};

/// Displays metadata in a canister.
#[derive(Parser)]
pub struct CanisterMetadataOpts {
    /// Specifies the name of the canister to call.
    canister_name: String,

    /// Specifies the name of the metadata to retrieve.
    metadata_name: String,
}

pub async fn exec(env: &dyn Environment, opts: CanisterMetadataOpts) -> DfxResult {
    let agent = env.get_agent();

    let callee_canister = opts.canister_name.as_str();
    let canister_id_store = env.get_canister_id_store()?;

    let canister_id = Principal::from_text(callee_canister)
        .or_else(|_| canister_id_store.get(callee_canister))?;

    fetch_root_key_if_needed(env).await?;
    let metadata = agent
        .read_state_canister_metadata(canister_id, &opts.metadata_name)
        .await
        .with_context(|| {
            format!(
                "Failed to read `{}` metadata of canister {}.",
                opts.metadata_name, canister_id
            )
        })?;

    stdout().write_all(&metadata)?;

    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/mod.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use clap::{Parser, Subcommand};
use dfx_core::identity::CallSender;
use tokio::runtime::Runtime;

mod call;
mod create;
mod delete;
mod deposit_cycles;
mod id;
mod info;
mod install;
mod logs;
mod metadata;
mod request_status;
mod send;
mod sign;
mod snapshot;
mod start;
mod status;
mod stop;
mod uninstall_code;
mod update_settings;
mod url;

/// Manages canisters deployed on a network replica.
#[derive(Parser)]
#[command(name = "canister")]
pub struct CanisterOpts {
    #[command(flatten)]
    network: NetworkOpt,

    /// Specify a wallet canister id to perform the call.
    /// If none specified, defaults to use the selected Identity's wallet canister.
    #[arg(long, global = true)]
    wallet: Option<String>,

    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Subcommand)]
pub enum SubCommand {
    Call(call::CanisterCallOpts),
    Create(create::CanisterCreateOpts),
    Delete(delete::CanisterDeleteOpts),
    DepositCycles(deposit_cycles::DepositCyclesOpts),
    Id(id::CanisterIdOpts),
    Info(info::InfoOpts),
    Install(install::CanisterInstallOpts),
    Metadata(metadata::CanisterMetadataOpts),
    RequestStatus(request_status::RequestStatusOpts),
    Send(send::CanisterSendOpts),
    Sign(sign::CanisterSignOpts),
    Snapshot(snapshot::SnapshotOpts),
    Start(start::CanisterStartOpts),
    Status(status::CanisterStatusOpts),
    Stop(stop::CanisterStopOpts),
    UninstallCode(uninstall_code::UninstallCodeOpts),
    UpdateSettings(update_settings::UpdateSettingsOpts),
    Logs(logs::LogsOpts),
    Url(url::CanisterUrlOpts),
}

pub fn exec(env: &dyn Environment, opts: CanisterOpts) -> DfxResult {
    let agent_env;
    let env = if matches!(&opts.subcmd, SubCommand::Id(_)) {
        env
    } else {
        agent_env = create_agent_environment(env, opts.network.to_network_name())?;
        &agent_env
    };
    let runtime = Runtime::new().expect("Unable to create a runtime");

    runtime.block_on(async {
        let call_sender = || CallSender::from(&opts.wallet, env.get_network_descriptor());
        match opts.subcmd {
            SubCommand::Call(v) => call::exec(env, v, &call_sender()?).await,
            SubCommand::Create(v) => create::exec(env, v, &call_sender()?).await,
            SubCommand::Delete(v) => delete::exec(env, v, &call_sender()?).await,
            SubCommand::DepositCycles(v) => deposit_cycles::exec(env, v, &call_sender()?).await,
            SubCommand::Id(v) => id::exec(env, v).await,
            SubCommand::Install(v) => install::exec(env, v, &call_sender()?).await,
            SubCommand::Info(v) => info::exec(env, v).await,
            SubCommand::Metadata(v) => metadata::exec(env, v).await,
            SubCommand::RequestStatus(v) => request_status::exec(env, v).await,
            SubCommand::Send(v) => send::exec(env, v, &call_sender()?).await,
            SubCommand::Sign(v) => sign::exec(env, v, &call_sender()?).await,
            SubCommand::Snapshot(v) => snapshot::exec(env, v, &call_sender()?).await,
            SubCommand::Start(v) => start::exec(env, v, &call_sender()?).await,
            SubCommand::Status(v) => status::exec(env, v, &call_sender()?).await,
            SubCommand::Stop(v) => stop::exec(env, v, &call_sender()?).await,
            SubCommand::UninstallCode(v) => uninstall_code::exec(env, v, &call_sender()?).await,
            SubCommand::UpdateSettings(v) => update_settings::exec(env, v, &call_sender()?).await,
            SubCommand::Logs(v) => logs::exec(env, v, &call_sender()?).await,
            SubCommand::Url(v) => url::exec(env, v).await,
        }
    })
}


-----------------------

/src/dfx/src/commands/canister/request_status.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers;
use crate::util::print_idl_blob;
use anyhow::Context;
use backoff::backoff::Backoff;
use backoff::ExponentialBackoff;
use candid::Principal;
use clap::Parser;
use ic_agent::agent::RequestStatusResponse;
use ic_agent::{AgentError, RequestId};
use std::str::FromStr;

/// Requests the status of a call from a canister.
#[derive(Parser)]
pub struct RequestStatusOpts {
    /// Specifies the request identifier.
    /// The request identifier is an hexadecimal string starting with 0x.
    #[arg(value_parser = parsers::request_id_parser)]
    request_id: String,

    /// Specifies the name or id of the canister onto which the request was made.
    /// If the request was made to the Management canister, specify the id of the
    /// canister it is updating/querying.
    /// If the call was proxied by the wallet,
    /// i.e. a `dfx canister call --async --wallet=<ID>` flag,
    /// specify the wallet canister id.
    canister: String,

    /// Specifies the format for displaying the method's return result.
    #[arg(long, value_parser = ["idl", "raw", "pp"])]
    output: Option<String>,
}

pub async fn exec(env: &dyn Environment, opts: RequestStatusOpts) -> DfxResult {
    let request_id =
        RequestId::from_str(&opts.request_id[2..]).context("Invalid argument: request_id")?;
    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let callee_canister = opts.canister.as_str();
    let canister_id_store = env.get_canister_id_store()?;

    let canister_id = Principal::from_text(callee_canister)
        .or_else(|_| canister_id_store.get(callee_canister))?;

    let mut retry_policy = ExponentialBackoff::default();
    let blob = async {
        let mut request_accepted = false;
        loop {
            match agent
                .request_status_raw(&request_id, canister_id)
                .await
                .context("Failed to fetch request status.")?
            {
                RequestStatusResponse::Replied(reply) => return Ok(reply.arg),
                RequestStatusResponse::Rejected(response) => {
                    return Err(DfxError::new(AgentError::CertifiedReject(response)))
                }
                RequestStatusResponse::Unknown => (),
                RequestStatusResponse::Received | RequestStatusResponse::Processing => {
                    // The system will return Unknown until the request is accepted
                    // and we generally cannot know how long that will take.
                    // State transitions between Received and Processing may be
                    // instantaneous. Therefore, once we know the request is accepted,
                    // we restart the waiter so the request does not time out.
                    if !request_accepted {
                        retry_policy.reset();
                        request_accepted = true;
                    }
                }
                RequestStatusResponse::Done => {
                    return Err(DfxError::new(AgentError::RequestStatusDoneNoReply(
                        String::from(request_id),
                    )))
                }
            };

            let interval = retry_policy
                .next_backoff()
                .ok_or_else(|| DfxError::new(AgentError::TimeoutWaitingForResponse()))?;
            tokio::time::sleep(interval).await;
        }
    }
    .await
    .map_err(DfxError::from)?;

    let output_type = opts.output.as_deref();
    print_idl_blob(&blob, output_type, &None)?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/send.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::sign::signed_message::SignedMessageV1;
use anyhow::{bail, Context};
use candid::{IDLArgs, Principal};
use clap::Parser;
use dfx_core::identity::CallSender;
use dfx_core::json::load_json_file;
use ic_agent::agent::{CallResponse, RequestStatusResponse};
use ic_agent::Agent;
use ic_agent::RequestId;
use std::path::PathBuf;

/// Send a previously-signed message.
#[derive(Parser)]
pub struct CanisterSendOpts {
    /// Specifies the file name of the message
    file_name: PathBuf,

    /// Send the signed request-status call in the message
    #[arg(long)]
    status: bool,
}

pub async fn exec(
    _env: &dyn Environment,
    opts: CanisterSendOpts,
    call_sender: &CallSender,
) -> DfxResult {
    if *call_sender != CallSender::SelectedId {
        bail!("`send` currently doesn't support proxying through the wallet canister, please use `dfx canister send --no-wallet ...`.");
    }
    let file_name = opts.file_name;
    let message: SignedMessageV1 = load_json_file(&file_name)?;
    message.validate()?;

    let network = message.network.clone();
    let agent = Agent::builder().with_url(&network).build()?;
    if !message.is_ic {
        agent.fetch_root_key().await?;
    }
    let content = hex::decode(&message.content).context("Failed to decode message content.")?;
    let canister_id = Principal::from_text(&message.canister_id)
        .with_context(|| format!("Failed to parse canister id {:?}.", message.canister_id))?;

    if opts.status {
        if message.call_type != "update" {
            bail!("Can only check request_status on update calls.");
        }
        let Some(signed_request_status) = message.signed_request_status else {
            bail!("No signed_request_status in [{}].", file_name.display());
        };
        let envelope = hex::decode(signed_request_status).context("Failed to decode envelope.")?;
        let Some(request_id) = message.request_id else {
            bail!("No request_id in [{}].", file_name.display());
        };
        let request_id = request_id
            .parse::<RequestId>()
            .context("Failed to decode request ID.")?;
        let response = agent
            .request_status_signed(&request_id, canister_id, envelope)
            .await
            .with_context(|| format!("Failed to read canister state of {}.", canister_id))?;
        eprint!("Response: ");
        match response {
            RequestStatusResponse::Received => eprintln!("Received, not yet processing"),
            RequestStatusResponse::Processing => eprintln!("Processing, not yet done"),
            RequestStatusResponse::Rejected(response) => {
                if let Some(error_code) = response.error_code {
                    println!(
                        "Rejected ({:?}): {}, error code {}",
                        response.reject_code, response.reject_message, error_code
                    );
                } else {
                    println!(
                        "Rejected ({:?}): {}",
                        response.reject_code, response.reject_message
                    );
                }
            }
            RequestStatusResponse::Replied(response) => {
                eprint!("Replied: ");
                if let Ok(idl) = IDLArgs::from_bytes(&response.arg) {
                    println!("{idl}");
                } else {
                    println!("{}", hex::encode(&response.arg));
                }
            }
            RequestStatusResponse::Done => println!("Done, response no longer available"),
            RequestStatusResponse::Unknown => println!("Unknown"),
        }
        return Ok(());
    }

    eprintln!("Will send message:");
    eprintln!("  Creation:    {}", message.creation);
    eprintln!("  Expiration:  {}", message.expiration);
    eprintln!("  Network:     {}", message.network);
    eprintln!("  Call type:   {}", message.call_type);
    eprintln!("  Sender:      {}", message.sender);
    eprintln!("  Canister id: {}", message.canister_id);
    eprintln!("  Method name: {}", message.method_name);
    eprintln!("  Arg:         {:?}", message.arg);

    // Not using dialoguer because it doesn't support non terminal env like bats e2e
    eprintln!("\nOkay? [y/N]");
    let mut input = String::new();
    std::io::stdin()
        .read_line(&mut input)
        .context("Failed to read stdin.")?;
    if !["y", "yes"].contains(&input.to_lowercase().trim()) {
        return Ok(());
    }

    match message.call_type.as_str() {
        "query" => {
            let response = agent
                .query_signed(canister_id, content)
                .await
                .with_context(|| format!("Query call to {} failed.", canister_id))?;
            eprint!("Response: ");
            if let Ok(idl) = IDLArgs::from_bytes(&response) {
                println!("{}", idl)
            } else {
                println!("{}", hex::encode(&response));
            }
        }
        "update" => {
            let call_response = agent
                .update_signed(canister_id, content)
                .await
                .with_context(|| format!("Update call to {} failed.", canister_id))?;
            match call_response {
                CallResponse::Poll(request_id) => {
                    eprintln!(
                        "To check the status of this update call, append `--status` to current command."
                    );
                    eprintln!("e.g. `dfx canister send message.json --status`");
                    eprintln!("Alternatively, if you have the correct identity on this machine, using `dfx canister request-status` with following arguments.");
                    eprint!("Request ID: ");
                    println!("0x{}", String::from(request_id));
                    eprint!("Canister ID: ");
                    println!("{}", canister_id);
                }
                CallResponse::Response(response) => {
                    eprint!("Response: ");
                    if let Ok(idl) = IDLArgs::from_bytes(&response) {
                        println!("{idl}");
                    } else {
                        println!("{}", hex::encode(&response));
                    }
                }
            }
        }
        // message.validate() guarantee that call_type must be query or update
        _ => unreachable!(),
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/sign.rs:
-----------------------

use crate::commands::canister::call::get_effective_canister_id;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister::get_canister_id_and_candid_path;
use crate::lib::sign::signed_message::SignedMessageV1;
use crate::util::clap::argument_from_cli::ArgumentFromCliPositionalOpt;
use crate::util::{blob_from_arguments, get_candid_type};
use anyhow::{anyhow, bail};
use candid::Principal;
use candid_parser::utils::CandidSource;
use clap::Parser;
use dfx_core::identity::CallSender;
use dfx_core::json::save_json_file;
use ic_utils::interfaces::management_canister::MgmtMethod;
use slog::info;
use std::convert::TryInto;
use std::path::{Path, PathBuf};
use std::str::FromStr;
use std::time::SystemTime;
use time::OffsetDateTime;

/// Sign a canister call and generate message file.
#[derive(Parser)]
pub struct CanisterSignOpts {
    /// Specifies the name/id of the canister to call.
    canister_name: String,

    /// Specifies the method name to call on the canister.
    method_name: String,

    #[command(flatten)]
    argument_from_cli: ArgumentFromCliPositionalOpt,

    /// Sends a query request to a canister.
    #[arg(long)]
    query: bool,

    /// Sends an update request to a canister. This is the default if the method is not a query method.
    #[arg(long, conflicts_with("query"))]
    update: bool,

    /// Specifies the config for generating random argument.
    #[arg(
        long,
        conflicts_with("argument"),
        conflicts_with("argument_file"),
        conflicts_with("always_assist")
    )]
    random: Option<String>,

    /// Specifies how long the message will be valid in seconds, default to be 300s (5 minutes)
    #[arg(long, default_value = "5m")]
    expire_after: String,

    /// Specifies the output file name.
    #[arg(long, default_value = "message.json")]
    file: PathBuf,

    /// Always use Candid assist when the argument types are all optional.
    #[arg(
        long,
        conflicts_with("argument"),
        conflicts_with("argument_file"),
        conflicts_with("random")
    )]
    always_assist: bool,
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterSignOpts,
    call_sender: &CallSender,
) -> DfxResult {
    let log = env.get_logger();
    if *call_sender != CallSender::SelectedId {
        bail!("`sign` currently doesn't support proxying through the wallet canister, please use `dfx canister sign --no-wallet ...`.");
    }

    let method_name = opts.method_name.as_str();

    let (canister_id, maybe_candid_path) =
        get_canister_id_and_candid_path(env, opts.canister_name.as_str())?;

    let method_type =
        maybe_candid_path.and_then(|path| get_candid_type(CandidSource::File(&path), method_name));
    let is_query_method = method_type.as_ref().map(|(_, f)| f.is_query());

    let (argument_from_cli, argument_type) = opts.argument_from_cli.get_argument_and_type()?;
    let is_query = match is_query_method {
        Some(true) => !opts.update,
        Some(false) => {
            if opts.query {
                bail!(
                    "Invalid method call: {} is not a query method.",
                    method_name
                );
            } else {
                false
            }
        }
        None => opts.query,
    };

    // Get the argument, get the type, convert the argument to the type and return
    // an error if any of it doesn't work.
    let arg_value = blob_from_arguments(
        Some(env),
        argument_from_cli.as_deref(),
        opts.random.as_deref(),
        argument_type.as_deref(),
        &method_type,
        false,
        opts.always_assist,
    )?;
    let agent = env.get_agent();

    let network_descriptor = env.get_network_descriptor();
    let network = network_descriptor
        .providers
        .first()
        .expect("Cannot get network provider (url).")
        .to_string();

    let sender = env
        .get_selected_identity_principal()
        .expect("Selected identity not instantiated.");

    let timeout = humantime::parse_duration(&opts.expire_after)
        .map_err(|_| anyhow!("Cannot parse expire_after as a duration (e.g. `1h`, `1h 30m`)"))?;
    //let timeout = Duration::from_secs(opts.expire_after);
    let expiration_system_time = SystemTime::now()
        .checked_add(timeout)
        .ok_or_else(|| anyhow!("Time wrapped around."))?;
    let creation = OffsetDateTime::now_utc();
    let expiration = creation
        .checked_add(timeout.try_into()?)
        .ok_or_else(|| anyhow!("Expiration datetime overflow."))?;

    let message_template = SignedMessageV1::new(
        creation,
        expiration,
        network,
        network_descriptor.is_ic,
        sender,
        canister_id,
        method_name.to_string(),
        arg_value.clone(),
    );

    let file_name = opts.file;
    if Path::new(&file_name).exists() {
        bail!(
            "[{}] already exists, please specify a different output file name.",
            file_name.display(),
        );
    }

    let effective_canister_id = if canister_id == Principal::management_canister() {
        let management_method = MgmtMethod::from_str(method_name).map_err(|_| {
            anyhow!(
                "Attempted to call an unsupported management canister method: {}",
                method_name
            )
        })?;
        get_effective_canister_id(&management_method, &arg_value)?
    } else {
        canister_id
    };

    if is_query {
        let signed_query = agent
            .query(&canister_id, method_name)
            .with_effective_canister_id(effective_canister_id)
            .with_arg(arg_value)
            .expire_at(expiration_system_time)
            .sign()?;
        let message = message_template
            .clone()
            .with_call_type("query".to_string())
            .with_content(hex::encode(signed_query.signed_query));
        save_json_file(&file_name, &message)?;
        info!(log, "Query message generated at [{}]", file_name.display());
        Ok(())
    } else {
        let signed_update = agent
            .update(&canister_id, method_name)
            .with_effective_canister_id(effective_canister_id)
            .with_arg(arg_value)
            .expire_at(expiration_system_time)
            .sign()?;
        let request_id = signed_update.request_id;
        let message = message_template
            .clone()
            .with_call_type("update".to_string())
            .with_request_id(request_id)
            .with_content(hex::encode(&signed_update.signed_update));
        let signed_request_status = agent.sign_request_status(canister_id, request_id)?;
        let message = message
            .with_signed_request_status(hex::encode(signed_request_status.signed_request_status));
        save_json_file(&file_name, &message)?;
        info!(
            log,
            "Update and request_status message generated at [{}]",
            file_name.display()
        );
        Ok(())
    }
}


-----------------------

/src/dfx/src/commands/canister/snapshot.rs:
-----------------------

use std::{
    fmt::{self, Display, Formatter},
    str::FromStr,
};

use anyhow::{bail, Context};
use clap::{Parser, Subcommand};
use dfx_core::identity::CallSender;
use ic_utils::interfaces::management_canister::CanisterStatus;
use indicatif::HumanBytes;
use itertools::Itertools;
use slog::info;
use time::{macros::format_description, OffsetDateTime};

use crate::lib::{
    environment::Environment,
    error::{DfxError, DfxResult},
    operations::canister::{
        delete_canister_snapshot, get_canister_status, list_canister_snapshots,
        load_canister_snapshot, take_canister_snapshot,
    },
    root_key::fetch_root_key_if_needed,
};

#[derive(Parser)]
pub struct SnapshotOpts {
    #[command(subcommand)]
    subcmd: SnapshotSubcommand,
}

/// Controls canister snapshots that can reset a canister to an earlier state of execution.
#[derive(Subcommand)]
enum SnapshotSubcommand {
    /// Creates a new snapshot of a canister. The canister must be stopped.
    Create {
        /// The canister to snapshot.
        canister: String,
        /// If a snapshot ID is specified, this snapshot will replace it and reuse the ID.
        #[arg(long)]
        replace: Option<SnapshotId>,
    },
    /// Loads a canister snapshot, overwriting its execution state. All data since that snapshot will be lost. The canister must be stopped.
    Load {
        /// The canister to load the snapshot in.
        canister: String,
        /// The ID of the snapshot to load.
        snapshot: SnapshotId,
    },
    /// Lists a canister's existing snapshots.
    List {
        /// The canister to list snapshots from.
        canister: String,
    },
    /// Deletes a snapshot from a canister.
    Delete {
        /// The canister to delete the snapshot from.
        canister: String,
        /// The ID of the snapshot to delete.
        snapshot: SnapshotId,
    },
}

#[derive(Clone)]
struct SnapshotId(Vec<u8>);

impl Display for SnapshotId {
    fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
        f.write_str(&hex::encode(&self.0))
    }
}

impl FromStr for SnapshotId {
    type Err = DfxError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        Ok(Self(hex::decode(s)?))
    }
}

pub async fn exec(
    env: &dyn Environment,
    opts: SnapshotOpts,
    call_sender: &CallSender,
) -> DfxResult {
    fetch_root_key_if_needed(env).await?;
    match opts.subcmd {
        SnapshotSubcommand::Create { canister, replace } => {
            create(env, canister, replace, call_sender).await?
        }
        SnapshotSubcommand::Load { canister, snapshot } => {
            load(env, canister, snapshot, call_sender).await?
        }
        SnapshotSubcommand::Delete { canister, snapshot } => {
            delete(env, canister, snapshot, call_sender).await?
        }
        SnapshotSubcommand::List { canister } => list(env, canister, call_sender).await?,
    }
    Ok(())
}

fn ensure_status(status: CanisterStatus, canister: &str, phrasing: &str) -> DfxResult {
    match status {
        CanisterStatus::Stopped => {}
        CanisterStatus::Running => bail!("Canister {canister} is running and snapshots should not be {phrasing} running canisters. Run `dfx canister stop` first"),
        CanisterStatus::Stopping => bail!("Canister {canister} is stopping but is not yet stopped. Wait a few seconds and try again"),
    }
    Ok(())
}

async fn create(
    env: &dyn Environment,
    canister: String,
    replace: Option<SnapshotId>,
    call_sender: &CallSender,
) -> DfxResult {
    let canister_id = canister
        .parse()
        .or_else(|_| env.get_canister_id_store()?.get(&canister))?;
    let status = get_canister_status(env, canister_id, call_sender)
        .await
        .with_context(|| format!("Could not retrieve status of canister {canister}"))?;
    ensure_status(status.status, &canister, "taken of")?;
    let id = take_canister_snapshot(
        env,
        canister_id,
        replace.as_ref().map(|x| &*x.0),
        call_sender,
    )
    .await
    .with_context(|| format!("Failed to take snapshot of canister {canister}"))?;
    info!(
        env.get_logger(),
        "Created a new snapshot of canister {canister}. Snapshot ID: {}",
        SnapshotId(id.id)
    );
    Ok(())
}

async fn load(
    env: &dyn Environment,
    canister: String,
    snapshot: SnapshotId,
    call_sender: &CallSender,
) -> DfxResult {
    let canister_id = canister
        .parse()
        .or_else(|_| env.get_canister_id_store()?.get(&canister))?;
    let status = get_canister_status(env, canister_id, call_sender)
        .await
        .with_context(|| format!("Could not retrieve status of canister {canister}"))?;
    ensure_status(status.status, &canister, "applied to")?;
    load_canister_snapshot(env, canister_id, &snapshot.0, call_sender)
        .await
        .with_context(|| format!("Failed to load snapshot {snapshot} in canister {canister}"))?;
    info!(
        env.get_logger(),
        "Loaded snapshot {snapshot} in canister {canister}"
    );
    Ok(())
}

async fn delete(
    env: &dyn Environment,
    canister: String,
    snapshot: SnapshotId,
    call_sender: &CallSender,
) -> DfxResult {
    let canister_id = canister
        .parse()
        .or_else(|_| env.get_canister_id_store()?.get(&canister))?;
    delete_canister_snapshot(env, canister_id, &snapshot.0, call_sender)
        .await
        .with_context(|| format!("Failed to delete snapshot {snapshot} in canister {canister}"))?;
    info!(
        env.get_logger(),
        "Deleted snapshot {snapshot} from canister {canister}"
    );
    Ok(())
}

async fn list(env: &dyn Environment, canister: String, call_sender: &CallSender) -> DfxResult {
    let canister_id = canister
        .parse()
        .or_else(|_| env.get_canister_id_store()?.get(&canister))?;
    let snapshots = list_canister_snapshots(env, canister_id, call_sender)
        .await
        .with_context(|| format!("Failed to retrieve snapshot list from canister {canister}"))?;
    if snapshots.is_empty() {
        info!(
            env.get_logger(),
            "No snapshots found in canister {canister}"
        );
    } else {
        let time_fmt = format_description!("[year]-[month]-[day] [hour]:[minute]:[second] UTC");
        let snapshots = snapshots.into_iter().format_with("\n", |s, f| {
            f(&format_args!(
                "{}: {}, taken at {}",
                SnapshotId(s.id),
                HumanBytes(s.total_size),
                OffsetDateTime::from_unix_timestamp_nanos(s.taken_at_timestamp as i128)
                    .unwrap()
                    .format(&time_fmt)
                    .unwrap()
            ))
        });
        info!(env.get_logger(), "{snapshots}");
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/canister/start.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use candid::Principal;
use clap::Parser;
use dfx_core::identity::CallSender;
use slog::info;

/// Starts a stopped canister.
#[derive(Parser)]
pub struct CanisterStartOpts {
    /// Specifies the name or id of the canister to start. You must specify either a canister name/id or the --all flag.
    canister: Option<String>,

    /// Starts all of the canisters configured in the dfx.json file.
    #[arg(long, required_unless_present("canister"))]
    all: bool,
}

async fn start_canister(
    env: &dyn Environment,
    canister: &str,
    call_sender: &CallSender,
) -> DfxResult {
    let log = env.get_logger();
    let canister_id_store = env.get_canister_id_store()?;
    let canister_id =
        Principal::from_text(canister).or_else(|_| canister_id_store.get(canister))?;

    info!(
        log,
        "Starting code for canister {}, with canister_id {}",
        canister,
        canister_id.to_text(),
    );

    canister::start_canister(env, canister_id, call_sender).await?;

    Ok(())
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterStartOpts,
    call_sender: &CallSender,
) -> DfxResult {
    fetch_root_key_if_needed(env).await?;

    if let Some(canister) = opts.canister.as_deref() {
        start_canister(env, canister, call_sender).await
    } else if opts.all {
        let config = env.get_config_or_anyhow()?;
        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                start_canister(env, canister, call_sender).await?;
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}


-----------------------

/src/dfx/src/commands/canister/status.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use candid::Principal;
use clap::Parser;
use dfx_core::identity::CallSender;
use fn_error_context::context;
use ic_utils::interfaces::management_canister::LogVisibility;

/// Returns the current status of a canister: Running, Stopping, or Stopped. Also carries information like balance, current settings, memory used and everything returned by 'info'.
#[derive(Parser)]
pub struct CanisterStatusOpts {
    /// Specifies the name of the canister to return information for.
    /// You must specify either a canister name or the --all flag.
    canister: Option<String>,

    /// Returns status information for all of the canisters configured in the dfx.json file.
    #[arg(long, required_unless_present("canister"))]
    all: bool,
}

#[context("Failed to get canister status for '{}'.", canister)]
async fn canister_status(
    env: &dyn Environment,
    canister: &str,
    call_sender: &CallSender,
) -> DfxResult {
    let canister_id_store = env.get_canister_id_store()?;
    let canister_id =
        Principal::from_text(canister).or_else(|_| canister_id_store.get(canister))?;

    let status = canister::get_canister_status(env, canister_id, call_sender).await?;

    let mut controllers: Vec<_> = status
        .settings
        .controllers
        .iter()
        .map(Principal::to_text)
        .collect();
    controllers.sort();

    let reserved_cycles_limit = if let Some(limit) = status.settings.reserved_cycles_limit {
        format!("{} Cycles", limit)
    } else {
        "Not Set".to_string()
    };

    let wasm_memory_limit = if let Some(limit) = status.settings.wasm_memory_limit {
        format!("{} Bytes", limit)
    } else {
        "Not Set".to_string()
    };
    let log_visibility = match status.settings.log_visibility {
        LogVisibility::Controllers => "controllers".to_string(),
        LogVisibility::Public => "public".to_string(),
        LogVisibility::AllowedViewers(viewers) => {
            if viewers.is_empty() {
                "allowed viewers list is empty".to_string()
            } else {
                let mut viewers: Vec<_> = viewers.iter().map(Principal::to_text).collect();
                viewers.sort();
                format!("allowed viewers: {}", viewers.join(", "))
            }
        }
    };

    println!("Canister status call result for {canister}.\nStatus: {status}\nControllers: {controllers}\nMemory allocation: {memory_allocation}\nCompute allocation: {compute_allocation}\nFreezing threshold: {freezing_threshold}\nIdle cycles burned per day: {idle_cycles_burned_per_day}\nMemory Size: {memory_size:?}\nBalance: {balance} Cycles\nReserved: {reserved} Cycles\nReserved cycles limit: {reserved_cycles_limit}\nWasm memory limit: {wasm_memory_limit}\nModule hash: {module_hash}\nNumber of queries: {queries_total}\nInstructions spent in queries: {query_instructions_total}\nTotal query request payload size (bytes): {query_req_payload_total}\nTotal query response payload size (bytes): {query_resp_payload_total}\nLog visibility: {log_visibility}",
        status = status.status,
        controllers = controllers.join(" "),
        memory_allocation = status.settings.memory_allocation,
        compute_allocation = status.settings.compute_allocation,
        freezing_threshold = status.settings.freezing_threshold,
        idle_cycles_burned_per_day = status.idle_cycles_burned_per_day,
        memory_size = status.memory_size,
        balance = status.cycles,
        reserved = status.reserved_cycles,
        module_hash = status.module_hash.map_or_else(|| "None".to_string(), |v| format!("0x{}", hex::encode(v))),
        queries_total = status.query_stats.num_calls_total,
        query_instructions_total = status.query_stats.num_instructions_total,
        query_req_payload_total = status.query_stats.request_payload_bytes_total,
        query_resp_payload_total = status.query_stats.response_payload_bytes_total,
    );
    Ok(())
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterStatusOpts,
    call_sender: &CallSender,
) -> DfxResult {
    fetch_root_key_if_needed(env).await?;

    if let Some(canister) = opts.canister.as_deref() {
        canister_status(env, canister, call_sender).await
    } else if opts.all {
        let config = env.get_config_or_anyhow()?;
        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                canister_status(env, canister, call_sender).await?;
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}


-----------------------

/src/dfx/src/commands/canister/stop.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use candid::Principal;
use clap::Parser;
use dfx_core::identity::CallSender;
use slog::info;

/// Stops a currently running canister.
#[derive(Parser)]
pub struct CanisterStopOpts {
    /// Specifies the name or id of the canister to stop.
    /// You must specify either a canister name/id or the --all option.
    canister: Option<String>,

    /// Stops all of the canisters configured in the dfx.json file.
    #[arg(long, required_unless_present("canister"))]
    all: bool,
}

async fn stop_canister(
    env: &dyn Environment,
    canister: &str,
    call_sender: &CallSender,
) -> DfxResult {
    let log = env.get_logger();
    let canister_id_store = env.get_canister_id_store()?;
    let canister_id =
        Principal::from_text(canister).or_else(|_| canister_id_store.get(canister))?;

    info!(
        log,
        "Stopping code for canister {}, with canister_id {}",
        canister,
        canister_id.to_text(),
    );

    canister::stop_canister(env, canister_id, call_sender).await?;

    Ok(())
}

pub async fn exec(
    env: &dyn Environment,
    opts: CanisterStopOpts,
    call_sender: &CallSender,
) -> DfxResult {
    fetch_root_key_if_needed(env).await?;

    if let Some(canister) = opts.canister.as_deref() {
        stop_canister(env, canister, call_sender).await
    } else if opts.all {
        let config = env.get_config_or_anyhow()?;
        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                stop_canister(env, canister, call_sender).await?;
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}


-----------------------

/src/dfx/src/commands/canister/uninstall_code.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister;
use crate::lib::root_key::fetch_root_key_if_needed;
use candid::Principal;
use clap::Parser;
use dfx_core::identity::CallSender;
use slog::info;

/// Uninstalls a canister, removing its code and state.
/// Does not delete the canister.
#[derive(Parser)]
pub struct UninstallCodeOpts {
    /// Specifies the name or id of the canister to uinstall.
    /// You must specify either a canister name/id or the --all option.
    canister: Option<String>,

    /// Uninstalls all of the canisters configured in the dfx.json file.
    #[arg(long, required_unless_present("canister"))]
    all: bool,
}

async fn uninstall_code(
    env: &dyn Environment,
    canister: &str,
    call_sender: &CallSender,
) -> DfxResult {
    let log = env.get_logger();
    let canister_id_store = env.get_canister_id_store()?;
    let canister_id =
        Principal::from_text(canister).or_else(|_| canister_id_store.get(canister))?;

    info!(
        log,
        "Uninstalling code for canister {}, with canister_id {}",
        canister,
        canister_id.to_text(),
    );

    canister::uninstall_code(env, canister_id, call_sender).await?;

    Ok(())
}

pub async fn exec(
    env: &dyn Environment,
    opts: UninstallCodeOpts,
    call_sender: &CallSender,
) -> DfxResult {
    fetch_root_key_if_needed(env).await?;

    if let Some(canister) = opts.canister.as_deref() {
        uninstall_code(env, canister, call_sender).await
    } else if opts.all {
        let config = env.get_config_or_anyhow()?;

        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                uninstall_code(env, canister, call_sender).await?;
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}


-----------------------

/src/dfx/src/commands/canister/update_settings.rs:
-----------------------

use crate::lib::canister_logs::log_visibility::LogVisibilityOpt;
use crate::lib::diagnosis::DiagnosedError;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use crate::lib::ic_attributes::{
    get_compute_allocation, get_freezing_threshold, get_log_visibility, get_memory_allocation,
    get_reserved_cycles_limit, get_wasm_memory_limit, CanisterSettings,
};
use crate::lib::operations::canister::{get_canister_status, update_settings};
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::{
    compute_allocation_parser, freezing_threshold_parser, memory_allocation_parser,
    reserved_cycles_limit_parser, wasm_memory_limit_parser,
};
use anyhow::{bail, Context};
use byte_unit::Byte;
use candid::Principal as CanisterId;
use clap::{ArgAction, Parser};
use dfx_core::cli::ask_for_consent;
use dfx_core::error::identity::InstantiateIdentityFromNameError::GetIdentityPrincipalFailed;
use dfx_core::identity::CallSender;
use fn_error_context::context;
use ic_agent::identity::Identity;
use ic_utils::interfaces::management_canister::StatusCallResult;

/// Update one or more of a canister's settings (i.e its controller, compute allocation, or memory allocation.)
#[derive(Parser, Debug)]
pub struct UpdateSettingsOpts {
    /// Specifies the canister name or id to update. You must specify either canister name/id or the --all option.
    canister: Option<String>,

    /// Updates the settings of all canisters configured in the project dfx.json files.
    #[arg(long, required_unless_present("canister"))]
    all: bool,

    /// Specifies the identity name or the principal of the new controller.
    /// Can be specified more than once, indicating the canister will have multiple controllers.
    /// If any controllers are set with this parameter, any other controllers will be removed.
    #[arg(long, action = ArgAction::Append)]
    set_controller: Option<Vec<String>>,

    /// Add a principal to the list of controllers of the canister.
    #[arg(long, action = ArgAction::Append, conflicts_with("set_controller"))]
    add_controller: Option<Vec<String>>,

    /// Removes a principal from the list of controllers of the canister.
    #[arg(long, action = ArgAction::Append, conflicts_with("set_controller"))]
    remove_controller: Option<Vec<String>>,

    /// Specifies the canister's compute allocation. This should be a percent in the range [0..100]
    #[arg(long, short, value_parser = compute_allocation_parser)]
    compute_allocation: Option<u64>,

    /// Specifies how much memory the canister is allowed to use in total.
    /// This should be a value in the range [0..12 GiB]. Can include units, e.g. "4KiB".
    /// A setting of 0 means the canister will have access to memory on a â€œbest-effortâ€ basis:
    /// It will only be charged for the memory it uses, but at any point in time may stop running
    /// if it tries to allocate more memory when there isnâ€™t space available on the subnet.
    #[arg(long, value_parser = memory_allocation_parser)]
    memory_allocation: Option<Byte>,

    /// Sets the freezing_threshold in SECONDS.
    /// A canister is considered frozen whenever the IC estimates that the canister would be depleted of cycles
    /// before freezing_threshold seconds pass, given the canister's current size and the IC's current cost for storage.
    /// A frozen canister rejects any calls made to it.
    #[arg(long, value_parser = freezing_threshold_parser)]
    freezing_threshold: Option<u64>,

    /// Sets the upper limit of the canister's reserved cycles balance.
    ///
    /// Reserved cycles are cycles that the system sets aside for future use by the canister.
    /// If a subnet's storage exceeds 450 GiB, then every time a canister allocates new storage bytes,
    /// the system sets aside some amount of cycles from the main balance of the canister.
    /// These reserved cycles will be used to cover future payments for the newly allocated bytes.
    /// The reserved cycles are not transferable and the amount of reserved cycles depends on how full the subnet is.
    ///
    /// A setting of 0 means that the canister will trap if it tries to allocate new storage while the subnet's memory usage exceeds 450 GiB.
    #[arg(long, value_parser = reserved_cycles_limit_parser)]
    reserved_cycles_limit: Option<u128>,

    /// Sets a soft limit on the Wasm memory usage of the canister.
    ///
    /// Update calls, timers, heartbeats, installs, and post-upgrades fail if the
    /// Wasm memory usage exceeds this limit. The main purpose of this setting is
    /// to protect against the case when the canister reaches the hard 4GiB
    /// limit.
    ///
    /// Must be a number between 0 B and 256 TiB, inclusive. Can include units, e.g. "4KiB".
    #[arg(long, value_parser = wasm_memory_limit_parser)]
    wasm_memory_limit: Option<Byte>,

    #[command(flatten)]
    log_visibility_opt: Option<LogVisibilityOpt>,

    /// Freezing thresholds above ~1.5 years require this flag as confirmation.
    #[arg(long)]
    confirm_very_long_freezing_threshold: bool,

    /// Skips yes/no checks by answering 'yes'. Such checks can result in loss of control,
    /// so this is not recommended outside of CI.
    #[arg(long, short)]
    yes: bool,
}

pub async fn exec(
    env: &dyn Environment,
    opts: UpdateSettingsOpts,
    call_sender: &CallSender,
) -> DfxResult {
    // sanity checks
    if let Some(threshold_in_seconds) = opts.freezing_threshold {
        if threshold_in_seconds > 50_000_000 /* ~1.5 years */ && !opts.confirm_very_long_freezing_threshold
        {
            return Err(DiagnosedError::new(
                "The freezing threshold is defined in SECONDS before the canister would run out of cycles, not in cycles.".to_string(),
                "If you truly want to set a freezing threshold that is longer than a year, please run the same command, but with the flag --confirm-very-long-freezing-threshold to confirm you want to do this.".to_string(),
            )).context("Misunderstanding is very likely.");
        }
    }

    fetch_root_key_if_needed(env).await?;

    if !opts.yes && user_is_removing_themselves_as_controller(env, call_sender, &opts)? {
        ask_for_consent("You are trying to remove yourself as a controller of this canister. This may leave this canister un-upgradeable.")?
    }

    let controllers: Option<DfxResult<Vec<_>>> = opts.set_controller.as_ref().map(|controllers| {
        let y: DfxResult<Vec<_>> = controllers
            .iter()
            .map(|controller| controller_to_principal(env, controller))
            .collect::<DfxResult<Vec<_>>>();
        y
    });
    let controllers = controllers
        .transpose()
        .context("Failed to determine all new controllers given in --set-controller.")?;

    let canister_id_store = env.get_canister_id_store()?;

    if let Some(canister_name_or_id) = opts.canister.as_deref() {
        let config = env.get_config()?;
        let config_interface = config.as_ref().map(|config| config.get_config());
        let mut controllers = controllers;
        let canister_id = CanisterId::from_text(canister_name_or_id)
            .or_else(|_| canister_id_store.get(canister_name_or_id))?;
        let textual_cid = canister_id.to_text();
        let canister_name = canister_id_store.get_name(&textual_cid).map(|x| &**x);

        let compute_allocation =
            get_compute_allocation(opts.compute_allocation, config_interface, canister_name)?;
        let memory_allocation =
            get_memory_allocation(opts.memory_allocation, config_interface, canister_name)?;
        let freezing_threshold =
            get_freezing_threshold(opts.freezing_threshold, config_interface, canister_name)?;
        let reserved_cycles_limit =
            get_reserved_cycles_limit(opts.reserved_cycles_limit, config_interface, canister_name)?;
        let wasm_memory_limit =
            get_wasm_memory_limit(opts.wasm_memory_limit, config_interface, canister_name)?;
        let mut current_status: Option<StatusCallResult> = None;
        if let Some(log_visibility) = &opts.log_visibility_opt {
            if log_visibility.require_current_settings() {
                current_status = Some(get_canister_status(env, canister_id, call_sender).await?);
            }
        }
        let log_visibility = get_log_visibility(
            env,
            opts.log_visibility_opt.as_ref(),
            current_status.as_ref(),
            config_interface,
            canister_name,
        )?;
        if let Some(added) = &opts.add_controller {
            if current_status.is_none() {
                current_status = Some(get_canister_status(env, canister_id, call_sender).await?);
            }
            let mut existing_controllers = current_status
                .as_ref()
                .unwrap()
                .settings
                .controllers
                .clone();
            for s in added {
                existing_controllers.push(controller_to_principal(env, s)?);
            }
            controllers = Some(existing_controllers);
        }
        if let Some(removed) = &opts.remove_controller {
            let controllers = if opts.add_controller.is_some() {
                controllers.as_mut().unwrap()
            } else {
                if current_status.is_none() {
                    current_status =
                        Some(get_canister_status(env, canister_id, call_sender).await?);
                }
                controllers.get_or_insert(current_status.unwrap().settings.controllers)
            };
            let removed = removed
                .iter()
                .map(|r| controller_to_principal(env, r))
                .collect::<DfxResult<Vec<_>>>()
                .context("Failed to determine all controllers to remove.")?;
            for s in removed {
                if let Some(idx) = controllers.iter().position(|x| *x == s) {
                    controllers.swap_remove(idx);
                }
            }
        }
        let settings = CanisterSettings {
            controllers,
            compute_allocation,
            memory_allocation,
            freezing_threshold,
            reserved_cycles_limit,
            wasm_memory_limit,
            log_visibility,
        };
        update_settings(env, canister_id, settings, call_sender).await?;
        display_controller_update(&opts, canister_name_or_id);
    } else if opts.all {
        // Update all canister settings.
        let config = env.get_config_or_anyhow()?;
        let config_interface = config.get_config();
        if let Some(canisters) = &config_interface.canisters {
            for canister_name in canisters.keys() {
                let mut controllers = controllers.clone();
                let canister_id = canister_id_store.get(canister_name)?;
                let compute_allocation = get_compute_allocation(
                    opts.compute_allocation,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| {
                    format!("Failed to get compute allocation for {canister_name}.")
                })?;
                let memory_allocation = get_memory_allocation(
                    opts.memory_allocation,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| format!("Failed to get memory allocation for {canister_name}."))?;
                let freezing_threshold = get_freezing_threshold(
                    opts.freezing_threshold,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| {
                    format!("Failed to get freezing threshold for {canister_name}.")
                })?;
                let reserved_cycles_limit = get_reserved_cycles_limit(
                    opts.reserved_cycles_limit,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| {
                    format!("Failed to get reserved cycles limit for {canister_name}.")
                })?;
                let wasm_memory_limit = get_wasm_memory_limit(
                    opts.wasm_memory_limit,
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| format!("Failed to get Wasm memory limit for {canister_name}."))?;
                let mut current_status: Option<StatusCallResult> = None;
                if let Some(log_visibility) = &opts.log_visibility_opt {
                    if log_visibility.require_current_settings() {
                        current_status =
                            Some(get_canister_status(env, canister_id, call_sender).await?);
                    }
                }
                let log_visibility = get_log_visibility(
                    env,
                    opts.log_visibility_opt.as_ref(),
                    current_status.as_ref(),
                    Some(config_interface),
                    Some(canister_name),
                )
                .with_context(|| format!("Failed to get log visibility for {canister_name}."))?;
                if let Some(added) = &opts.add_controller {
                    if current_status.is_none() {
                        current_status =
                            Some(get_canister_status(env, canister_id, call_sender).await?);
                    }
                    let mut existing_controllers = current_status
                        .as_ref()
                        .unwrap()
                        .settings
                        .controllers
                        .clone();
                    for s in added {
                        existing_controllers.push(controller_to_principal(env, s)?);
                    }
                    controllers = Some(existing_controllers);
                }
                if let Some(removed) = &opts.remove_controller {
                    let controllers = if opts.add_controller.is_some() {
                        controllers.as_mut().unwrap()
                    } else {
                        if current_status.is_none() {
                            current_status =
                                Some(get_canister_status(env, canister_id, call_sender).await?);
                        }
                        controllers.get_or_insert(current_status.unwrap().settings.controllers)
                    };
                    let removed = removed
                        .iter()
                        .map(|r| controller_to_principal(env, r))
                        .collect::<DfxResult<Vec<_>>>()
                        .context("Failed to determine all controllers to remove.")?;
                    for s in removed {
                        if let Some(idx) = controllers.iter().position(|x| *x == s) {
                            controllers.swap_remove(idx);
                        }
                    }
                }
                let settings = CanisterSettings {
                    controllers,
                    compute_allocation,
                    memory_allocation,
                    freezing_threshold,
                    reserved_cycles_limit,
                    wasm_memory_limit,
                    log_visibility,
                };
                update_settings(env, canister_id, settings, call_sender).await?;
                display_controller_update(&opts, canister_name);
            }
        }
    } else {
        bail!("Cannot find canister name.")
    }

    Ok(())
}

fn user_is_removing_themselves_as_controller(
    env: &dyn Environment,
    call_sender: &CallSender,
    opts: &UpdateSettingsOpts,
) -> DfxResult<bool> {
    let caller_principal = match call_sender {
        CallSender::SelectedId => env
            .get_selected_identity_principal()
            .context("Selected identity is not instantiated")?
            .to_string(),
        CallSender::Wallet(principal) => principal.to_string(),
    };
    let removes_themselves =
        matches!(&opts.remove_controller, Some(remove) if remove.contains(&caller_principal));
    let sets_without_themselves =
        matches!(&opts.set_controller, Some(set) if !set.contains(&caller_principal));
    Ok(removes_themselves || sets_without_themselves)
}

#[context("Failed to convert controller '{}' to a principal", controller)]
fn controller_to_principal(env: &dyn Environment, controller: &str) -> DfxResult<CanisterId> {
    match CanisterId::from_text(controller) {
        Ok(principal) => Ok(principal),
        Err(_) => {
            let current_id = env.get_selected_identity().unwrap();
            if current_id == controller {
                Ok(env.get_selected_identity_principal().unwrap())
            } else {
                let identity_name = controller;
                env.new_identity_manager()?
                    .instantiate_identity_from_name(identity_name, env.get_logger())
                    .and_then(|identity| identity.sender().map_err(GetIdentityPrincipalFailed))
                    .map_err(DfxError::new)
            }
        }
    }
}

fn display_controller_update(opts: &UpdateSettingsOpts, canister_name_or_id: &str) {
    if let Some(new_controllers) = opts.set_controller.as_ref() {
        let mut controllers = new_controllers.clone();
        controllers.sort();

        let plural = if controllers.len() > 1 { "s" } else { "" };

        println!(
            "Set controller{} of {:?} to: {}",
            plural,
            canister_name_or_id,
            controllers.join(" ")
        );
    };
    if let Some(added_controllers) = opts.add_controller.as_ref() {
        let mut controllers = added_controllers.clone();
        controllers.sort();

        let plural = if controllers.len() > 1 { "s" } else { "" };

        println!(
            "Added as controller{} of {:?}: {}",
            plural,
            canister_name_or_id,
            controllers.join(" "),
        );
    }
    if let Some(removed_controllers) = opts.remove_controller.as_ref() {
        let mut controllers = removed_controllers.clone();
        controllers.sort();

        let plural = if controllers.len() > 1 { "s" } else { "" };

        println!(
            "Removed from controller{} of {:?}: {}",
            plural,
            canister_name_or_id,
            controllers.join(" "),
        );
    }
}


-----------------------

/src/dfx/src/commands/canister/url.rs:
-----------------------

use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use crate::util::url::{construct_frontend_url, construct_ui_canister_url};
use candid::Principal;
use clap::Parser;
use console::Style;
use dfx_core::config::model::canister_id_store::CanisterIdStore;
use dfx_core::network::provider::{create_network_descriptor, LocalBindDetermination};

/// Prints the url of a canister.
#[derive(Parser)]
pub struct CanisterUrlOpts {
    /// Specifies the name or id of the canister.
    canister: String,

    #[command(flatten)]
    network: NetworkOpt,
}

pub async fn exec(env: &dyn Environment, opts: CanisterUrlOpts) -> DfxResult {
    let config = env.get_config_or_anyhow()?;

    let network_descriptor = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        opts.network.to_network_name(),
        None,
        LocalBindDetermination::AsConfigured,
    )?;

    let canister_id_store =
        CanisterIdStore::new(env.get_logger(), &network_descriptor, env.get_config()?)?;

    // Get canister id, try to parse first as users can input canister id or canister name.
    let canister_arg = opts.canister.as_str();
    let canister_id =
        Principal::from_text(canister_arg).or_else(|_| canister_id_store.get(canister_arg))?;

    let canister_id_text = Principal::to_text(&canister_id);
    if let Some(canister_name) = canister_id_store.get_name(&canister_id_text) {
        if let Some(canisters) = &config.get_config().canisters {
            if let Some(canister_config) = canisters.get(canister_name) {
                let canister_info = CanisterInfo::load(&config, canister_name, Some(canister_id))?;
                let green = Style::new().green();

                // Display as frontend if it's an asset canister, or custome type with a frontend.
                if canister_info.is_assets() || canister_config.frontend.is_some() {
                    let (canister_url, url2) =
                        construct_frontend_url(&network_descriptor, &canister_id)?;
                    println!("{}", green.apply_to(canister_url));
                    if let Some(canister_url2) = url2 {
                        println!("{}", green.apply_to(canister_url2));
                    }
                }

                // Display as backend.
                if !canister_info.is_assets() {
                    let url = construct_ui_canister_url(env, &canister_id)?;
                    if let Some(canister_url) = url {
                        println!("{}", green.apply_to(canister_url));
                    }
                }
            }
        }
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/completion.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::CliOpts;
use clap::CommandFactory;
use clap::Parser;
use clap_complete::generate;
use clap_complete::Shell;

/// Generate a shell completion script.
#[derive(Parser)]
pub struct CompletionOpts {
    /// The name of the command. Only needed if referring to dfx by another name, such as with an alias.
    #[clap(long, default_value("dfx"))]
    bin_name: String,

    /// The shell for which to generate completion scripts
    #[clap(default_value("bash"))]
    shell: Shell,
}

pub fn exec(env: &dyn Environment, opts: CompletionOpts) -> DfxResult {
    let em = env.get_extension_manager();

    let commands = em
        .load_installed_extension_manifests()?
        .as_clap_commands()?;

    let mut command = if commands.is_empty() {
        CliOpts::command()
    } else {
        CliOpts::command_for_update().subcommands(&commands)
    };

    generate(
        opts.shell,
        &mut command,
        opts.bin_name,
        &mut std::io::stdout(),
    );
    Ok(())
}


-----------------------

/src/dfx/src/commands/cycles/approve.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::cycles_ledger;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::cycle_amount_parser;
use crate::util::clap::parsers::icrc_subaccount_parser;
use candid::Principal;
use clap::Parser;
use icrc_ledger_types::icrc1::account::Subaccount;
use slog::warn;
use std::time::{SystemTime, UNIX_EPOCH};

/// Approves a principal to spend cycles on behalf of the approver.
#[derive(Parser)]
pub struct ApproveOpts {
    /// Allow this principal to spend cycles.
    spender: Principal,

    /// The number of cycles to approve.
    #[arg(value_parser = cycle_amount_parser)]
    amount: u128,

    /// Allow this subaccount to spend cycles.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    spender_subaccount: Option<Subaccount>,

    /// Approve cycles to be spent from this subaccount.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    from_subaccount: Option<Subaccount>,

    /// The number of previously approved cycles.
    /// See https://github.com/dfinity/ICRC-1/blob/main/standards/ICRC-2/README.md for details.
    #[arg(long, value_parser = cycle_amount_parser)]
    expected_allowance: Option<u128>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction-deduplication, default is system-time.
    /// https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,

    /// Timestamp until which the approval is valid. None means that the approval is valid indefinitely.
    #[arg(long)]
    expires_at: Option<u64>,

    /// Memo.
    #[arg(long)]
    memo: Option<u64>,
}

pub async fn exec(env: &dyn Environment, opts: ApproveOpts) -> DfxResult {
    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let created_at_time = opts.created_at_time.unwrap_or(
        SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64,
    );

    let result = cycles_ledger::approve(
        agent,
        env.get_logger(),
        opts.amount,
        opts.spender,
        opts.spender_subaccount,
        opts.from_subaccount,
        opts.expected_allowance,
        opts.expires_at,
        created_at_time,
        opts.memo,
    )
    .await;
    if result.is_err() && opts.created_at_time.is_none() {
        warn!(
            env.get_logger(),
            "If you retry this operation, use --created-at-time {}", created_at_time
        );
    }
    let block_index = result?;

    println!("Approval sent at block index {block_index}");

    Ok(())
}


-----------------------

/src/dfx/src/commands/cycles/balance.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::cycles_ledger;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::icrc_subaccount_parser;
use crate::util::{format_as_trillions, pretty_thousand_separators};
use candid::Principal;
use clap::Parser;
use icrc_ledger_types::icrc1::account::Subaccount;

/// Get the cycle balance of the selected Identity's cycles wallet.
#[derive(Parser)]
pub struct CyclesBalanceOpts {
    /// Specifies a Principal to get the balance of
    #[arg(long)]
    owner: Option<Principal>,

    /// Subaccount of the selected identity to get the balance of
    #[arg(long, value_parser = icrc_subaccount_parser)]
    subaccount: Option<Subaccount>,

    /// Get balance raw value (without upscaling to trillions of cycles).
    #[arg(long)]
    precise: bool,
}

pub async fn exec(env: &dyn Environment, opts: CyclesBalanceOpts) -> DfxResult {
    fetch_root_key_if_needed(env).await?;

    let agent = env.get_agent();

    let owner = opts.owner.unwrap_or_else(|| {
        env.get_selected_identity_principal()
            .expect("Selected identity not instantiated.")
    });

    let balance = cycles_ledger::balance(agent, owner, opts.subaccount).await?;

    if opts.precise {
        println!("{} cycles.", balance);
    } else {
        println!(
            "{} TC (trillion cycles).",
            pretty_thousand_separators(format_as_trillions(balance))
        );
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/cycles/convert.rs:
-----------------------

use crate::commands::ledger::get_icpts_from_args;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxResult, NotifyMintCyclesError};
use crate::lib::ledger_types::{Memo as ICPMemo, NotifyError, NotifyMintCyclesSuccess};
use crate::lib::nns_types::account_identifier::Subaccount as ICPSubaccount;
use crate::lib::nns_types::icpts::{ICPTs, TRANSACTION_FEE};
use crate::lib::operations::cmc::{notify_mint_cycles, transfer_cmc};
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::{e8s_parser, icrc_subaccount_parser};
use anyhow::{anyhow, bail};
use clap::Parser;
use icrc_ledger_types::icrc1::account::Subaccount as ICRCSubaccount;
use icrc_ledger_types::icrc1::transfer::Memo as ICRCMemo;

pub const MEMO_MINT_CYCLES: u64 = 0x544e494d; // == 'MINT'

/// Convert some of the user's ICP balance into cycles.
#[derive(Parser)]
pub struct ConvertOpts {
    /// Subaccount to withdraw from
    #[arg(long)]
    from_subaccount: Option<ICPSubaccount>,

    /// Subaccount to mint cycles to.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    to_subaccount: Option<ICRCSubaccount>,

    /// Transaction fee, default is 10000 e8s.
    #[arg(long)]
    fee: Option<ICPTs>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction-deduplication, default is system-time.
    // https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,

    /// ICP to mint into cycles and deposit into your cycles ledger account
    /// Can be specified as a Decimal with the fractional portion up to 8 decimal places
    /// i.e. 100.012
    #[arg(long)]
    amount: Option<ICPTs>,

    /// Specify ICP as a whole number, helpful for use in conjunction with `--e8s`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    icp: Option<u64>,

    /// Specify e8s as a whole number, helpful for use in conjunction with `--icp`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    e8s: Option<u64>,

    /// Memo used when depositing the minted cycles.
    #[arg(long)]
    deposit_memo: Option<u64>,
}

pub async fn exec(env: &dyn Environment, opts: ConvertOpts) -> DfxResult {
    let amount = get_icpts_from_args(opts.amount, opts.icp, opts.e8s)?;
    let memo = ICPMemo(MEMO_MINT_CYCLES);
    let fee = opts.fee.unwrap_or(TRANSACTION_FEE);
    let agent = env.get_agent();
    let to = agent
        .get_principal()
        .map_err(|err| anyhow!("Failed to get selected identity principal: {err}"))?;

    fetch_root_key_if_needed(env).await?;

    let height = transfer_cmc(
        agent,
        env.get_logger(),
        memo,
        amount,
        fee,
        opts.from_subaccount,
        to,
        opts.created_at_time,
    )
    .await?;
    println!("Using transfer at block height {height}");
    let result = notify_mint_cycles(
        agent,
        opts.deposit_memo.map(ICRCMemo::from),
        opts.to_subaccount,
        height,
    )
    .await;

    match result {
        Ok(NotifyMintCyclesSuccess {
            minted, balance, ..
        }) => {
            println!(
                "Account was topped up with {minted} cycles! New balance is {balance} cycles."
            );
        }
        Err(NotifyMintCyclesError::Notify(NotifyError::Refunded {
            reason,
            block_index,
        })) => match block_index {
            Some(height) => {
                println!("Refunded at block height {height} with message: {reason}")
            }
            None => println!("Refunded with message: {reason}"),
        },
        Err(other) => bail!("{other:?}"),
    };
    Ok(())
}


-----------------------

/src/dfx/src/commands/cycles/mod.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use clap::Parser;
use tokio::runtime::Runtime;

mod approve;
mod balance;
mod convert;
mod redeem_faucet_coupon;
pub mod top_up;
mod transfer;

/// Helper commands to manage the user's cycles.
#[derive(Parser)]
#[command(name = "wallet")]
pub struct CyclesOpts {
    #[command(flatten)]
    network: NetworkOpt,

    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
enum SubCommand {
    Approve(approve::ApproveOpts),
    Balance(balance::CyclesBalanceOpts),
    Convert(convert::ConvertOpts),
    TopUp(top_up::TopUpOpts),
    Transfer(transfer::TransferOpts),
    RedeemFaucetCoupon(redeem_faucet_coupon::RedeemFaucetCouponOpts),
}

pub fn exec(env: &dyn Environment, opts: CyclesOpts) -> DfxResult {
    let agent_env = create_agent_environment(env, opts.network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async {
        match opts.subcmd {
            SubCommand::Approve(v) => approve::exec(&agent_env, v).await,
            SubCommand::Balance(v) => balance::exec(&agent_env, v).await,
            SubCommand::Convert(v) => convert::exec(&agent_env, v).await,
            SubCommand::TopUp(v) => top_up::exec(&agent_env, v).await,
            SubCommand::Transfer(v) => transfer::exec(&agent_env, v).await,
            SubCommand::RedeemFaucetCoupon(v) => redeem_faucet_coupon::exec(&agent_env, v).await,
        }
    })
}


-----------------------

/src/dfx/src/commands/cycles/redeem_faucet_coupon.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::icrc_subaccount_parser;
use crate::util::{format_as_trillions, pretty_thousand_separators};
use anyhow::{anyhow, bail, Context};
use candid::{encode_args, CandidType, Decode, Deserialize, Principal};
use clap::Parser;
use icrc_ledger_types::icrc1::account::{Account, Subaccount};
use slog::{info, warn};

pub const DEFAULT_FAUCET_PRINCIPAL: Principal =
    Principal::from_slice(&[0, 0, 0, 0, 1, 112, 0, 196, 1, 1]);

/// Redeem a code at the cycles faucet.
#[derive(Parser)]
pub struct RedeemFaucetCouponOpts {
    /// The coupon code to redeem at the faucet.
    coupon_code: String,

    /// Alternative faucet address. If not set, this uses the DFINITY faucet.
    #[arg(long)]
    faucet: Option<String>,

    /// Subaccount to redeem the coupon to.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    to_subaccount: Option<Subaccount>,
}

pub async fn exec(env: &dyn Environment, opts: RedeemFaucetCouponOpts) -> DfxResult {
    let log = env.get_logger();

    let faucet_principal = if let Some(alternative_faucet) = opts.faucet {
        let canister_id_store = env.get_canister_id_store()?;
        Principal::from_text(&alternative_faucet)
            .or_else(|_| canister_id_store.get(&alternative_faucet))?
    } else {
        DEFAULT_FAUCET_PRINCIPAL
    };
    let agent = env.get_agent();
    if fetch_root_key_if_needed(env).await.is_err() {
        bail!("Failed to connect to the local replica. Did you forget to use '--network ic'?");
    } else if !env.get_network_descriptor().is_ic {
        warn!(log, "Trying to redeem a wallet coupon on a local replica. Did you forget to use '--network ic'?");
    }

    let identity = env
        .get_selected_identity_principal()
        .with_context(|| anyhow!("No identity selected."))?;
    info!(log, "Redeeming coupon. This may take up to 30 seconds...");
    let response = agent
        .update(&faucet_principal, "redeem_to_cycles_ledger")
        .with_arg(
            encode_args((
                opts.coupon_code.clone(),
                Account {
                    owner: identity,
                    subaccount: opts.to_subaccount,
                },
            ))
            .context("Failed to serialize 'redeem_to_cycles_ledger' arguments.")?,
        )
        .await
        .context("Failed 'redeem_to_cycles_ledger' call.")?;
    #[derive(CandidType, Deserialize)]
    struct RedeemResponse {
        balance: u128,
        cycles: u128,
        block_index: u128,
    }
    let result = Decode!(&response, RedeemResponse)
        .context("Failed to decode 'redeem_to_cycles_ledger' response.")?;
    info!(
        log,
        "Redeemed coupon '{}' to the cycles ledger for {} TC (trillions of cycles). New balance: {} TC.",
        opts.coupon_code,
        pretty_thousand_separators(format_as_trillions(result.cycles)),
        pretty_thousand_separators(format_as_trillions(result.balance)),
    );
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_faucet_canister_id() {
        assert_eq!(
            DEFAULT_FAUCET_PRINCIPAL,
            Principal::from_text("fg7gi-vyaaa-aaaal-qadca-cai").unwrap()
        );
    }
}


-----------------------

/src/dfx/src/commands/cycles/top_up.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::cycles_ledger;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::{cycle_amount_parser, icrc_subaccount_parser};
use candid::Principal;
use clap::Parser;
use icrc_ledger_types::icrc1::account::Subaccount;
use slog::warn;
use std::time::{SystemTime, UNIX_EPOCH};

/// Send cycles to a canister.
#[derive(Parser)]
pub struct TopUpOpts {
    /// Send cycles to this canister.
    to: String,

    /// The number of cycles to send.
    #[arg(value_parser = cycle_amount_parser)]
    amount: u128,

    /// Transfer cycles from this subaccount.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    from_subaccount: Option<Subaccount>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction deduplication, default is system time.
    /// https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,
}

pub async fn exec(env: &dyn Environment, opts: TopUpOpts) -> DfxResult {
    let agent = env.get_agent();

    let amount = opts.amount;

    fetch_root_key_if_needed(env).await?;

    let created_at_time = opts.created_at_time.unwrap_or(
        SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64,
    );

    let to = get_canister_id(env, &opts.to)?;
    let result = cycles_ledger::withdraw(
        agent,
        env.get_logger(),
        to,
        amount,
        created_at_time,
        opts.from_subaccount,
    )
    .await;
    if result.is_err() && opts.created_at_time.is_none() {
        warn!(
            env.get_logger(),
            "If you retry this operation, use --created-at-time {}", created_at_time
        );
    }
    let block_index = result?;

    println!("Transfer sent at block index {block_index}");

    Ok(())
}

fn get_canister_id(env: &dyn Environment, s: &str) -> DfxResult<Principal> {
    let principal = Principal::from_text(s).or_else(|_| {
        env.get_canister_id_store()
            .and_then(|canister_id_store| canister_id_store.get(s))
    })?;
    Ok(principal)
}


-----------------------

/src/dfx/src/commands/cycles/transfer.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::cycles_ledger;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::{cycle_amount_parser, icrc_subaccount_parser};
use candid::Principal;
use clap::Parser;
use icrc_ledger_types::icrc1::{self, account::Subaccount};
use slog::warn;
use std::time::{SystemTime, UNIX_EPOCH};

/// Transfer cycles to another principal.
#[derive(Parser)]
pub struct TransferOpts {
    /// Transfer cycles to this principal.
    to: Principal,

    /// The number of cycles to send.
    #[arg(value_parser = cycle_amount_parser)]
    amount: u128,

    /// Transfer cycles from this principal. Requires that principal's approval.
    #[arg(long)]
    from: Option<Principal>,

    /// Transfer cycles from this subaccount.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    from_subaccount: Option<Subaccount>,

    /// Deduct allowance from this subaccount.
    #[arg(long, value_parser = icrc_subaccount_parser, requires("from"))]
    spender_subaccount: Option<Subaccount>,

    /// Transfer cycles to this subaccount.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    to_subaccount: Option<Subaccount>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction-deduplication, default is system-time.
    /// https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,

    /// Memo.
    #[arg(long)]
    memo: Option<u64>,
}

pub async fn exec(env: &dyn Environment, opts: TransferOpts) -> DfxResult {
    let agent = env.get_agent();

    let amount = opts.amount;

    fetch_root_key_if_needed(env).await?;

    let created_at_time = opts.created_at_time.unwrap_or(
        SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64,
    );

    let result = if let Some(from_owner) = opts.from {
        let from = icrc1::account::Account {
            owner: from_owner,
            subaccount: opts.from_subaccount,
        };
        let to = icrc1::account::Account {
            owner: opts.to,
            subaccount: opts.to_subaccount,
        };
        cycles_ledger::transfer_from(
            agent,
            env.get_logger(),
            opts.spender_subaccount,
            from,
            to,
            amount,
            opts.memo,
            created_at_time,
        )
        .await
    } else {
        cycles_ledger::transfer(
            agent,
            env.get_logger(),
            amount,
            opts.from_subaccount,
            opts.to,
            opts.to_subaccount,
            created_at_time,
            opts.memo,
        )
        .await
    };

    if result.is_err() && opts.created_at_time.is_none() {
        warn!(
            env.get_logger(),
            "If you retry this operation, use --created-at-time {}", created_at_time
        );
    }
    let block_index = result?;

    println!("Transfer sent at block index {block_index}");

    Ok(())
}


-----------------------

/src/dfx/src/commands/deploy.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::operations::canister::deploy_canisters::deploy_canisters;
use crate::lib::operations::canister::deploy_canisters::DeployMode::{
    ComputeEvidence, ForceReinstallSingleCanister, NormalDeploy, PrepareForProposal,
};
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::argument_from_cli::ArgumentFromCliLongOpt;
use crate::util::clap::install_mode::{InstallModeHint, InstallModeOpt};
use crate::util::clap::parsers::{cycle_amount_parser, icrc_subaccount_parser};
use crate::util::clap::subnet_selection_opt::SubnetSelectionOpt;
use crate::util::url::{construct_frontend_url, construct_ui_canister_url};
use anyhow::{anyhow, bail};
use candid::Principal;
use clap::Parser;
use console::Style;
use dfx_core::config::model::network_descriptor::NetworkDescriptor;
use dfx_core::identity::CallSender;
use icrc_ledger_types::icrc1::account::Subaccount;
use slog::info;
use std::collections::BTreeMap;
use std::path::PathBuf;
use tokio::runtime::Runtime;
use url::Url;

/// Deploys all or a specific canister from the code in your project. By default, all canisters are deployed.
#[derive(Parser)]
pub struct DeployOpts {
    /// Specifies the name of the canister you want to deploy.
    /// If you donâ€™t specify a canister name, all canisters defined in the dfx.json file are deployed.
    canister_name: Option<String>,

    #[command(flatten)]
    argument_from_cli: ArgumentFromCliLongOpt,

    #[command(flatten)]
    install_mode: InstallModeOpt,

    /// Upgrade the canister even if the .wasm did not change.
    #[arg(long)]
    upgrade_unchanged: bool,

    #[command(flatten)]
    network: NetworkOpt,

    /// Specifies the initial cycle balance to deposit into the newly created canister.
    /// The specified amount needs to take the canister create fee into account.
    /// This amount is deducted from the wallet's cycle balance.
    #[arg(long, value_parser = cycle_amount_parser)]
    with_cycles: Option<u128>,

    /// Attempts to create the canister with this Canister ID.
    ///
    /// This option only works with non-mainnet replica.
    /// This option implies the --no-wallet flag.
    /// This option takes precedence over the specified_id field in dfx.json.
    #[arg(long, value_name = "PRINCIPAL", requires = "canister_name")]
    specified_id: Option<Principal>,

    /// Specify a wallet canister id to perform the call.
    /// If none specified, defaults to use the selected Identity's wallet canister.
    #[arg(long)]
    wallet: Option<String>,

    /// Performs the create call with the user Identity as the Sender of messages.
    /// Bypasses the Wallet canister.
    #[arg(long, conflicts_with("wallet"))]
    no_wallet: bool,

    /// Output environment variables to a file in dotenv format (without overwriting any user-defined variables, if the file already exists).
    #[arg(long)]
    output_env_file: Option<PathBuf>,

    /// Skips yes/no checks by answering 'yes'. Such checks usually result in data loss,
    /// so this is not recommended outside of CI.
    #[arg(long, short)]
    yes: bool,

    /// Skips upgrading the asset canister, to only install the assets themselves.
    #[arg(long)]
    no_asset_upgrade: bool,

    /// Prepare (upload) assets for later commit by proposal.
    #[arg(long, conflicts_with("compute_evidence"))]
    by_proposal: bool,

    /// Compute evidence and compare it against expected evidence
    #[arg(long, conflicts_with("by_proposal"))]
    compute_evidence: bool,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction deduplication, default is system time.
    /// https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long, requires = "canister_name")]
    created_at_time: Option<u64>,

    /// Subaccount of the selected identity to spend cycles from.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    from_subaccount: Option<Subaccount>,

    #[command(flatten)]
    subnet_selection: SubnetSelectionOpt,

    /// Always use Candid assist when the argument types are all optional.
    #[arg(
        long,
        conflicts_with("argument"),
        conflicts_with("argument_file"),
        conflicts_with("yes")
    )]
    always_assist: bool,
}

pub fn exec(env: &dyn Environment, opts: DeployOpts) -> DfxResult {
    let env = create_agent_environment(env, opts.network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");

    let canister_name = opts.canister_name.as_deref();
    let (argument_from_cli, argument_type) = opts.argument_from_cli.get_argument_and_type()?;
    if argument_from_cli.is_some() && canister_name.is_none() {
        bail!("The init argument can only be set when deploying a single canister.");
    }
    let mode_hint = opts.install_mode.mode_for_deploy()?;
    let config = env.get_config_or_anyhow()?;
    let env_file = config.get_output_env_file(opts.output_env_file)?;
    let mut subnet_selection =
        runtime.block_on(opts.subnet_selection.into_subnet_selection_type(&env))?;
    let with_cycles = opts.with_cycles;

    let deploy_mode = match (&mode_hint, canister_name) {
        (InstallModeHint::Reinstall, Some(canister_name)) => {
            let network = env.get_network_descriptor();
            if config
                .get_config()
                .is_remote_canister(canister_name, &network.name)?
            {
                bail!("The '{}' canister is remote for network '{}' and cannot be force-reinstalled from here",
                    canister_name, &network.name);
            }
            ForceReinstallSingleCanister(canister_name.to_string())
        }
        (InstallModeHint::Reinstall, None) => {
            bail!("The --mode=reinstall is only valid when deploying a single canister, because reinstallation destroys all data in the canister.");
        }
        (_, None) if opts.by_proposal => {
            bail!("The --by-proposal flag is only valid when deploying a single canister.");
        }
        (_, Some(canister_name)) if opts.by_proposal => {
            PrepareForProposal(canister_name.to_string())
        }
        (_, None) if opts.compute_evidence => {
            bail!("The --compute-evidence flag is only valid when deploying a single canister.");
        }
        (_, Some(canister_name)) if opts.compute_evidence => {
            ComputeEvidence(canister_name.to_string())
        }
        (_, _) => NormalDeploy,
    };

    let call_sender = CallSender::from(&opts.wallet, env.get_network_descriptor())
        .map_err(|e| anyhow!("Failed to determine call sender: {}", e))?;

    runtime.block_on(fetch_root_key_if_needed(&env))?;

    runtime.block_on(deploy_canisters(
        &env,
        canister_name,
        argument_from_cli.as_deref(),
        argument_type.as_deref(),
        &deploy_mode,
        &mode_hint,
        opts.upgrade_unchanged,
        with_cycles,
        opts.created_at_time,
        opts.specified_id,
        &call_sender,
        opts.from_subaccount,
        opts.no_wallet,
        opts.yes,
        env_file,
        opts.no_asset_upgrade,
        &mut subnet_selection,
        opts.always_assist,
    ))?;

    if matches!(deploy_mode, NormalDeploy | ForceReinstallSingleCanister(_)) {
        display_urls(&env)?;
    }
    Ok(())
}

fn display_urls(env: &dyn Environment) -> DfxResult {
    let config = env.get_config_or_anyhow()?;
    let network: &NetworkDescriptor = env.get_network_descriptor();
    let log = env.get_logger();
    let canister_id_store = env.get_canister_id_store()?;

    let mut frontend_urls = BTreeMap::new();
    let mut candid_urls: BTreeMap<&String, Url> = BTreeMap::new();

    if let Some(canisters) = &config.get_config().canisters {
        for (canister_name, canister_config) in canisters {
            let canister_is_remote = config
                .get_config()
                .is_remote_canister(canister_name, &network.name)?;
            if canister_is_remote {
                continue;
            }
            let canister_id = match Principal::from_text(canister_name) {
                Ok(principal) => Some(principal),
                Err(_) => canister_id_store.find(canister_name),
            };
            if let Some(canister_id) = canister_id {
                let canister_info = CanisterInfo::load(&config, canister_name, Some(canister_id))?;

                // If the canister is an assets canister or has a frontend section, we can display a frontend url.
                let is_assets = canister_info.is_assets() || canister_config.frontend.is_some();

                if is_assets {
                    let url = construct_frontend_url(network, &canister_id)?;
                    frontend_urls.insert(canister_name, url);
                }

                if !canister_info.is_assets() {
                    let url = construct_ui_canister_url(env, &canister_id)?;
                    if let Some(ui_canister_url) = url {
                        candid_urls.insert(canister_name, ui_canister_url);
                    }
                }
            }
        }
    }

    if !frontend_urls.is_empty() || !candid_urls.is_empty() {
        info!(log, "URLs:");
        let green = Style::new().green();
        if !frontend_urls.is_empty() {
            info!(log, "  Frontend canister via browser");
            for (name, (url1, url2)) in frontend_urls {
                if let Some(url2) = url2 {
                    info!(log, "    {}:", name);
                    info!(log, "      - {}", green.apply_to(url1));
                    info!(log, "      - {}", green.apply_to(url2));
                } else {
                    info!(log, "    {}: {}", name, green.apply_to(url1));
                }
            }
        }
        if !candid_urls.is_empty() {
            info!(log, "  Backend canister via Candid interface:");
            for (name, url) in candid_urls {
                info!(log, "    {}: {}", name, green.apply_to(url));
            }
        }
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/deps/deploy.rs:
-----------------------

use crate::lib::deps::deploy::try_create_canister;
use crate::lib::deps::{
    get_canister_prompt, get_pull_canister_or_principal, get_pull_canisters_in_config,
    get_pulled_wasm_path, load_init_json, load_pulled_json, validate_pulled, InitJson,
    PulledCanister,
};
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::root_key::fetch_root_key_if_needed;

use anyhow::Context;
use candid::Principal;
use clap::Parser;
use fn_error_context::context;
use ic_agent::Agent;
use ic_utils::interfaces::{management_canister::builders::InstallMode, ManagementCanister};
use slog::{info, Logger};

/// Deploy pulled dependencies locally.
#[derive(Parser)]
pub struct DepsDeployOpts {
    /// Specify the canister to deploy. You can specify its name (as defined in dfx.json) or Principal.
    /// If not specified, all pulled canisters will be deployed.
    canister: Option<String>,
}

pub async fn exec(env: &dyn Environment, opts: DepsDeployOpts) -> DfxResult {
    let logger = env.get_logger();
    let pull_canisters_in_config = get_pull_canisters_in_config(env)?;
    if pull_canisters_in_config.is_empty() {
        info!(logger, "There are no pull dependencies defined in dfx.json");
        return Ok(());
    }

    let project_root = env.get_config_or_anyhow()?.get_project_root().to_path_buf();
    let pulled_json = load_pulled_json(&project_root)?;
    validate_pulled(&pulled_json, &pull_canisters_in_config)
        .with_context(|| "Please rerun `dfx deps pull`.")?;

    let init_json = load_init_json(&project_root)?;

    fetch_root_key_if_needed(env).await?;
    let agent = env.get_agent();

    let canister_ids = match &opts.canister {
        Some(canister) => {
            let canister_id =
                get_pull_canister_or_principal(canister, &pull_canisters_in_config, &pulled_json)?;
            vec![canister_id]
        }
        None => pulled_json.canisters.keys().copied().collect(),
    };
    for canister_id in canister_ids {
        // Safe to unwrap:
        // canister_ids are guaranteed to exist in pulled.json
        let pulled_canister = pulled_json.canisters.get(&canister_id).unwrap();
        create_and_install(agent, logger, &canister_id, &init_json, pulled_canister).await?;
    }

    Ok(())
}

#[context("Failed to create and install canister {}", canister_id)]
async fn create_and_install(
    agent: &Agent,
    logger: &Logger,
    canister_id: &Principal,
    init_json: &InitJson,
    pulled_canister: &PulledCanister,
) -> DfxResult {
    let arg_raw = init_json.get_arg_raw(canister_id)?;
    try_create_canister(agent, logger, canister_id, pulled_canister).await?;
    install_pulled_canister(agent, logger, canister_id, arg_raw, pulled_canister).await?;
    Ok(())
}

#[context("Failed to install canister {}", canister_id)]
async fn install_pulled_canister(
    agent: &Agent,
    logger: &Logger,
    canister_id: &Principal,
    install_args: Vec<u8>,
    pulled_canister: &PulledCanister,
) -> DfxResult {
    let canister_prompt = get_canister_prompt(canister_id, pulled_canister);
    info!(logger, "Installing canister: {canister_prompt}");
    let pulled_canister_path = get_pulled_wasm_path(canister_id, pulled_canister.gzip)?;
    let wasm = dfx_core::fs::read(&pulled_canister_path)?;
    let mgr = ManagementCanister::create(agent);
    mgr.install_code(canister_id, &wasm)
        // always reinstall pulled canister
        .with_mode(InstallMode::Reinstall)
        .with_raw_arg(install_args)
        .await?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/deps/init.rs:
-----------------------

use crate::lib::deps::{
    create_init_json_if_not_existed, get_canister_prompt, get_pull_canister_or_principal,
    get_pull_canisters_in_config, get_pulled_service_candid_path, load_init_json, load_pulled_json,
    save_init_json, validate_pulled, InitJson, PulledJson,
};
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::util::clap::argument_from_cli::ArgumentFromCliLongOpt;
use crate::util::fuzzy_parse_argument;
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use candid_parser::{types::IDLTypes, typing::ast_to_type, utils::CandidSource};
use clap::Parser;
use slog::{info, warn, Logger};

/// Set init arguments for pulled dependencies.
#[derive(Parser)]
pub struct DepsInitOpts {
    /// Name of the pulled canister (as defined in dfx.json) or its Principal.
    /// If not specified, all pulled canisters will be set.
    canister: Option<String>,

    #[command(flatten)]
    argument_from_cli: ArgumentFromCliLongOpt,
}

pub async fn exec(env: &dyn Environment, opts: DepsInitOpts) -> DfxResult {
    let logger = env.get_logger();
    let pull_canisters_in_config = get_pull_canisters_in_config(env)?;
    if pull_canisters_in_config.is_empty() {
        info!(logger, "There are no pull dependencies defined in dfx.json");
        return Ok(());
    }

    let project_root = env.get_config_or_anyhow()?.get_project_root().to_path_buf();
    let pulled_json = load_pulled_json(&project_root)?;
    validate_pulled(&pulled_json, &pull_canisters_in_config)
        .with_context(|| "Please rerun `dfx deps pull`.")?;

    create_init_json_if_not_existed(&project_root)?;
    let mut init_json = load_init_json(&project_root)?;

    match &opts.canister {
        Some(canister) => {
            let (argument_from_cli, argument_type_from_cli) =
                opts.argument_from_cli.get_argument_and_type()?;
            let canister_id =
                get_pull_canister_or_principal(canister, &pull_canisters_in_config, &pulled_json)?;
            set_init(
                logger,
                &canister_id,
                &mut init_json,
                &pulled_json,
                argument_from_cli.as_deref(),
                argument_type_from_cli.as_deref(),
            )?;
        }
        None => {
            let mut canisters_require_init = vec![];
            for (canister_id, pulled_canister) in &pulled_json.canisters {
                if set_init(
                    logger,
                    canister_id,
                    &mut init_json,
                    &pulled_json,
                    None,
                    None,
                )
                .is_err()
                {
                    let canister_prompt = get_canister_prompt(canister_id, pulled_canister);
                    canisters_require_init.push(canister_prompt);
                }
            }
            if !canisters_require_init.is_empty() {
                let mut message = "The following canister(s) require an init argument. Please run `dfx deps init <NAME/PRINCIPAL>` to set them individually:".to_string();
                for canister_prompt in canisters_require_init {
                    message.push_str(&format!("\n{canister_prompt}"));
                }
                warn!(logger, "{message}");
            }
        }
    }

    save_init_json(&project_root, &init_json)?;
    Ok(())
}

fn set_init(
    logger: &Logger,
    canister_id: &Principal,
    init_json: &mut InitJson,
    pulled_json: &PulledJson,
    argument_from_cli: Option<&str>,
    argument_type_from_cli: Option<&str>,
) -> DfxResult {
    let pulled_canister = pulled_json
        .canisters
        .get(canister_id)
        .ok_or_else(|| anyhow!("Failed to find {canister_id} entry in pulled.json"))?;
    let canister_prompt = get_canister_prompt(canister_id, pulled_canister);
    let idl_path = get_pulled_service_candid_path(canister_id)?;
    let (env, _) = CandidSource::File(&idl_path).load()?;
    let candid_args = pulled_json.get_candid_args(canister_id)?;
    let candid_args_idl_types: IDLTypes = candid_args.parse()?;
    let mut types = vec![];
    for ty in candid_args_idl_types.args.iter() {
        types.push(ast_to_type(&env, ty)?);
    }

    match (argument_from_cli, types.is_empty()) {
        (Some(arg_str), false) => {
            if argument_type_from_cli == Some("raw") {
                let bytes = hex::decode(arg_str)
                    .map_err(|e| anyhow!("Argument is not a valid hex string: {}", e))?;
                init_json.set_init_arg(canister_id, None, &bytes);
            } else {
                let bytes = fuzzy_parse_argument(arg_str, &env, &types)?;
                init_json.set_init_arg(canister_id, Some(arg_str.to_string()), &bytes);
            }
        }
        (Some(_), true) => {
            bail!("Canister {canister_prompt} takes no init argument. Please rerun without `--argument`");
        }
        (None, false) => {
            // No argument provided from CLI but the canister requires an init argument.
            // Try to set the init argument in the following order:
            // 1. If `init.json` already contains the canister, do nothing.
            // 2. If the canister provides an `init_arg`, use it.
            // 3. Try "(null)" which works for canisters with top-level `opt`. This behavior is consistent with `dfx deploy`.
            // 4. Bail.
            let init_guide = pulled_json.get_init_guide(canister_id)?;
            let candid_args = pulled_json.get_candid_args(canister_id)?;
            let help_message = format!("init_guide => {init_guide}\ncandid:args => {candid_args}");

            if init_json.contains(canister_id) {
                info!(
                    logger,
                    "Canister {canister_prompt} already set init argument."
                );
            } else if let Some(init_arg) = pulled_json.get_init_arg(canister_id)? {
                let bytes = fuzzy_parse_argument(init_arg, &env, &types).with_context(|| {
                    format!(
                        "Pulled canister {canister_prompt} provided an invalid `init_arg`.
Please try to set an init argument with `--argument` option.
The following info might be helpful:
{help_message}"
                    )
                })?;
                init_json.set_init_arg(canister_id, Some(init_arg.to_string()), &bytes);
            } else if let Ok(bytes) = fuzzy_parse_argument("(null)", &env, &types) {
                init_json.set_init_arg(canister_id, Some("(null)".to_string()), &bytes);
                info!(
                    logger,
                    "Canister {canister_prompt} set init argument with \"(null)\"."
                );
            } else {
                bail!("Canister {canister_prompt} requires an init argument. The following info might be helpful:\n{help_message}");
            }
        }
        (None, true) => {
            init_json.set_empty_init(canister_id);
        }
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/deps/mod.rs:
-----------------------

use crate::lib::agent::create_anonymous_agent_environment;
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::{environment::Environment, error::DfxResult};
use clap::Parser;
use tokio::runtime::Runtime;

mod deploy;
mod init;
mod pull;

/// Pull dependencies and integrate locally.
#[derive(Parser)]
#[command(name = "deps")]
pub struct DepsOpts {
    #[command(flatten)]
    network: NetworkOpt,

    /// Arguments and flags for subcommands.
    #[command(subcommand)]
    subcmd: SubCommand,
}

/// Subcommands of `dfx deps`
#[derive(Parser)]
enum SubCommand {
    Pull(pull::DepsPullOpts),
    Init(init::DepsInitOpts),
    Deploy(deploy::DepsDeployOpts),
}

/// Executes `dfx deps` and its subcommands.
pub fn exec(env: &dyn Environment, opts: DepsOpts) -> DfxResult {
    // all deps subcommands should use anounymous identity
    let agent_env = create_anonymous_agent_environment(env, opts.network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async {
        match opts.subcmd {
            SubCommand::Pull(v) => pull::exec(&agent_env, v).await,
            SubCommand::Init(v) => init::exec(&agent_env, v).await,
            SubCommand::Deploy(v) => deploy::exec(&agent_env, v).await,
        }
    })
}


-----------------------

/src/dfx/src/commands/deps/pull.rs:
-----------------------

use crate::lib::agent::create_anonymous_agent_environment;
use crate::lib::deps::{
    get_candid_path_in_project, get_pull_canisters_in_config, get_pulled_canister_dir,
    get_pulled_service_candid_path, get_pulled_wasm_path, save_pulled_json,
};
use crate::lib::deps::{PulledCanister, PulledJson};
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::metadata::dfx::DfxMetadata;
use crate::lib::metadata::names::{CANDID_ARGS, CANDID_SERVICE, DFX};
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::lib::state_tree::canister_info::read_state_tree_canister_module_hash;
use crate::lib::wasm::file::{decompress_bytes, read_wasm_module};
use crate::util::download_file;
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use clap::Parser;
use dfx_core::config::model::dfinity::Pullable;
use dfx_core::fs::composite::{ensure_dir_exists, ensure_parent_dir_exists};
use fn_error_context::context;
use ic_agent::{Agent, AgentError};
use ic_wasm::metadata::get_metadata;
use sha2::{Digest, Sha256};
use slog::{error, info, trace, warn, Logger};
use std::collections::{BTreeMap, BTreeSet, VecDeque};
use std::io::Write;
use std::path::Path;

/// Pull canisters upon which the project depends.
/// This command connects to the "ic" mainnet by default.
/// You can still choose other network by setting `--network`.
#[derive(Parser)]
pub struct DepsPullOpts {
    #[command(flatten)]
    network: NetworkOpt,
}

pub async fn exec(env: &dyn Environment, opts: DepsPullOpts) -> DfxResult {
    let logger = env.get_logger();
    let pull_canisters_in_config = get_pull_canisters_in_config(env)?;
    if pull_canisters_in_config.is_empty() {
        info!(logger, "There are no pull dependencies defined in dfx.json");
        return Ok(());
    }

    let network = opts
        .network
        .to_network_name()
        .unwrap_or_else(|| "ic".to_string());
    let env = create_anonymous_agent_environment(env, Some(network))?;

    let project_root = env.get_config_or_anyhow()?.get_project_root().to_path_buf();

    fetch_root_key_if_needed(&env).await?;

    let agent = env.get_agent();

    let all_dependencies =
        resolve_all_dependencies(agent, logger, &pull_canisters_in_config).await?;

    let mut pulled_json =
        download_all_and_generate_pulled_json(agent, logger, &all_dependencies).await?;

    for (name, canister_id) in &pull_canisters_in_config {
        copy_service_candid_to_project(&project_root, name, canister_id)?;
        let pulled_canister = pulled_json
            .canisters
            .get_mut(canister_id)
            .ok_or_else(|| anyhow!("Failed to find {canister_id} entry in pulled.json"))?;
        pulled_canister.name = Some(name.clone());
    }

    save_pulled_json(&project_root, &pulled_json)?;
    Ok(())
}

async fn resolve_all_dependencies(
    agent: &Agent,
    logger: &Logger,
    pull_canisters_in_config: &BTreeMap<String, Principal>,
) -> DfxResult<Vec<Principal>> {
    let mut canisters_to_resolve: VecDeque<Principal> =
        pull_canisters_in_config.values().cloned().collect();
    let mut checked = BTreeSet::new();
    while let Some(canister_id) = canisters_to_resolve.pop_front() {
        if !checked.contains(&canister_id) {
            checked.insert(canister_id);
            let dependencies = get_dependencies(agent, logger, &canister_id).await?;
            canisters_to_resolve.extend(dependencies.iter());
        }
    }
    let all_dependencies = checked.into_iter().collect::<Vec<_>>();
    let mut message = String::new();
    message.push_str(&format!("Found {} dependencies:", all_dependencies.len()));
    for id in &all_dependencies {
        message.push('\n');
        message.push_str(&id.to_text());
    }
    info!(logger, "{}", message);
    Ok(all_dependencies)
}

#[context("Failed to get dependencies of canister {canister_id}.")]
async fn get_dependencies(
    agent: &Agent,
    logger: &Logger,
    canister_id: &Principal,
) -> DfxResult<Vec<Principal>> {
    info!(logger, "Fetching dependencies of canister {canister_id}...");
    let dfx_metadata = fetch_dfx_metadata(agent, canister_id).await?;
    let dependencies = dfx_metadata.get_pullable()?.dependencies.clone();
    Ok(dependencies)
}

async fn download_all_and_generate_pulled_json(
    agent: &Agent,
    logger: &Logger,
    all_dependencies: &[Principal],
) -> DfxResult<PulledJson> {
    let mut any_download_fail = false;
    let mut pulled_json = PulledJson::default();
    for canister_id in all_dependencies {
        match download_and_generate_pulled_canister(agent, logger, *canister_id).await {
            Ok(pulled_canister) => {
                pulled_json.canisters.insert(*canister_id, pulled_canister);
            }
            Err(e) => {
                error!(logger, "Failed to pull canister {canister_id}.\n{e}");
                any_download_fail = true;
            }
        }
    }

    if any_download_fail {
        bail!("Failed when pulling canisters.");
    }
    Ok(pulled_json)
}

// Download canister wasm, then extract metadata from it to build a PulledCanister
async fn download_and_generate_pulled_canister(
    agent: &Agent,
    logger: &Logger,
    canister_id: Principal,
) -> DfxResult<PulledCanister> {
    info!(logger, "Pulling canister {canister_id}...");

    let mut pulled_canister = PulledCanister::default();

    let dfx_metadata = fetch_dfx_metadata(agent, &canister_id).await?;
    let pullable = dfx_metadata.get_pullable()?;

    let hash_on_chain = get_hash_on_chain(agent, logger, canister_id, pullable).await?;
    pulled_canister.wasm_hash = hex::encode(&hash_on_chain);

    // skip download if cache hit
    let mut cache_hit = false;

    for gzip in [false, true] {
        let path = get_pulled_wasm_path(&canister_id, gzip)?;
        if path.exists() {
            let bytes = dfx_core::fs::read(&path)?;
            let hash_cache = Sha256::digest(bytes);
            if hash_cache.as_slice() == hash_on_chain {
                cache_hit = true;
                pulled_canister.gzip = gzip;
                pulled_canister.wasm_hash_download = hex::encode(hash_cache);
                trace!(logger, "The canister wasm was found in the cache.");
            }
            break;
        }
    }

    if !cache_hit {
        // delete files from previous pull
        let pulled_canister_dir = get_pulled_canister_dir(&canister_id)?;
        if pulled_canister_dir.exists() {
            dfx_core::fs::remove_dir_all(&pulled_canister_dir)?;
        }
        dfx_core::fs::create_dir_all(&pulled_canister_dir)?;

        // lookup `wasm_url` in dfx metadata
        let wasm_url = reqwest::Url::parse(&pullable.wasm_url)?;

        // download
        let content = download_file(&wasm_url).await?;

        // hash check
        let hash_download = Sha256::digest(&content);
        pulled_canister.wasm_hash_download = hex::encode(hash_download);

        let gzip = decompress_bytes(&content).is_ok();
        pulled_canister.gzip = gzip;
        let wasm_path = get_pulled_wasm_path(&canister_id, gzip)?;

        write_to_tempfile_then_rename(&content, &wasm_path)?;
    }

    let wasm_path = get_pulled_wasm_path(&canister_id, pulled_canister.gzip)?;

    // extract `candid:service` and save as candid file in shared cache
    let module = read_wasm_module(&wasm_path)?;
    let candid_service = get_metadata_as_string(&module, CANDID_SERVICE, &wasm_path)?;
    let service_candid_path = get_pulled_service_candid_path(&canister_id)?;
    write_to_tempfile_then_rename(candid_service.as_bytes(), &service_candid_path)?;

    // extract `candid:args`
    let candid_args = get_metadata_as_string(&module, CANDID_ARGS, &wasm_path)?;
    pulled_canister.candid_args = candid_args;

    // extract `dfx`
    let dfx_metadata_str = get_metadata_as_string(&module, DFX, &wasm_path)?;
    let dfx_metadata: DfxMetadata = serde_json::from_str(&dfx_metadata_str)?;
    let pullable = dfx_metadata.get_pullable()?;
    pulled_canister.dependencies = pullable.dependencies.clone();
    pulled_canister.init_guide = pullable.init_guide.clone();
    pulled_canister.init_arg = pullable.init_arg.clone();

    Ok(pulled_canister)
}

async fn fetch_dfx_metadata(agent: &Agent, canister_id: &Principal) -> DfxResult<DfxMetadata> {
    match fetch_metadata(agent, canister_id, DFX).await? {
        Some(dfx_metadata_raw) => {
            let dfx_metadata_str = String::from_utf8(dfx_metadata_raw)?;
            let dfx_metadata: DfxMetadata = serde_json::from_str(&dfx_metadata_str)?;
            Ok(dfx_metadata)
        }
        None => {
            bail!("`{DFX}` metadata not found in canister {canister_id}.");
        }
    }
}

#[context("Failed to fetch metadata {metadata} of canister {canister_id}.")]
async fn fetch_metadata(
    agent: &Agent,
    canister_id: &Principal,
    metadata: &str,
) -> DfxResult<Option<Vec<u8>>> {
    match agent
        .read_state_canister_metadata(*canister_id, metadata)
        .await
    {
        Ok(data) => Ok(Some(data)),
        Err(agent_error) => match agent_error {
            // replica returns such error
            AgentError::HttpError(ref e) => {
                let status = e.status;
                let content = String::from_utf8(e.content.clone())?;
                if status == 404
                    && content.starts_with(&format!("Custom section {metadata} not found"))
                {
                    Ok(None)
                } else {
                    bail!(agent_error);
                }
            }
            // ic-ref returns such error when the canister doesn't define the metadata
            AgentError::LookupPathAbsent(_) => Ok(None),
            _ => {
                bail!(agent_error)
            }
        },
    }
}

// Get expected hash of the canister wasm.
// If `wasm_hash` is specified in dfx metadata, use it.
// If `wasm_hash_url` is specified in dfx metadata, download the hash from the url.
// Otherwise, get the hash of the on chain canister.
async fn get_hash_on_chain(
    agent: &Agent,
    logger: &Logger,
    canister_id: Principal,
    pullable: &Pullable,
) -> DfxResult<Vec<u8>> {
    if pullable.wasm_hash.is_some() && pullable.wasm_hash_url.is_some() {
        warn!(logger, "Canister {canister_id} specified both `wasm_hash` and `wasm_hash_url`. `wasm_hash` will be used.");
    };
    if let Some(wasm_hash_str) = &pullable.wasm_hash {
        trace!(
            logger,
            "Canister {canister_id} specified a custom hash: {wasm_hash_str}"
        );
        Ok(hex::decode(wasm_hash_str)
            .with_context(|| format!("Failed to decode {wasm_hash_str} as sha256 hash."))?)
    } else if let Some(wasm_hash_url) = &pullable.wasm_hash_url {
        trace!(
            logger,
            "Canister {canister_id} specified a custom hash via url: {wasm_hash_url}"
        );
        let wasm_hash_url = reqwest::Url::parse(wasm_hash_url)
            .with_context(|| format!("{wasm_hash_url} is not a valid URL."))?;
        let wasm_hash_content = download_file(&wasm_hash_url)
            .await
            .with_context(|| format!("Failed to download wasm_hash from {wasm_hash_url}."))?;
        let wasm_hash_str = String::from_utf8(wasm_hash_content)
            .with_context(|| format!("Content from {wasm_hash_url} is not valid text."))?;
        // The content might contain the file name (usually from tools like shasum or sha256sum).
        // We only need the hash part.
        let wasm_hash_encoded = wasm_hash_str
            .split_whitespace()
            .next()
            .with_context(|| format!("Content from {wasm_hash_url} is empty."))?;
        Ok(hex::decode(wasm_hash_encoded)
            .with_context(|| format!("Failed to decode {wasm_hash_encoded} as sha256 hash."))?)
    } else {
        match read_state_tree_canister_module_hash(agent, canister_id).await? {
            Some(hash_on_chain) => Ok(hash_on_chain),
            None => {
                bail!(
                    "Canister {canister_id} doesn't have module hash. Perhaps it's not installed."
                );
            }
        }
    }
}

#[context("Failed to write to a tempfile then rename it to {}", path.display())]
fn write_to_tempfile_then_rename(content: &[u8], path: &Path) -> DfxResult {
    assert!(path.is_absolute());
    let dir = dfx_core::fs::parent(path)?;
    ensure_dir_exists(&dir)?;
    let mut f = tempfile::NamedTempFile::new_in(&dir)
        .with_context(|| format!("Failed to create a NamedTempFile in {dir:?}"))?;
    f.write_all(content)
        .with_context(|| format!("Failed to write the NamedTempFile at {:?}", f.path()))?;
    dfx_core::fs::rename(f.path(), path)?;
    Ok(())
}

#[context("Failed to copy candid path of pull dependency {name}")]
pub fn copy_service_candid_to_project(
    project_root: &Path,
    name: &str,
    canister_id: &Principal,
) -> DfxResult {
    let service_candid_path = get_pulled_service_candid_path(canister_id)?;
    let path_in_project = get_candid_path_in_project(project_root, canister_id);
    ensure_parent_dir_exists(&path_in_project)?;
    dfx_core::fs::copy(&service_candid_path, &path_in_project)?;
    dfx_core::fs::set_permissions_readwrite(&path_in_project)?;
    Ok(())
}

fn get_metadata_as_string(
    module: &walrus::Module,
    section: &str,
    wasm_path: &Path,
) -> DfxResult<String> {
    let metadata_bytes = get_metadata(module, section)
        .with_context(|| format!("Failed to get {} metadata from {:?}", section, wasm_path))?;
    let metadata = String::from_utf8(metadata_bytes.to_vec()).with_context(|| {
        format!(
            "Failed to read {} metadata from {:?} as UTF-8 text",
            section, wasm_path
        )
    })?;
    Ok(metadata)
}


-----------------------

/src/dfx/src/commands/diagnose.rs:
-----------------------

use crate::lib::{
    agent::create_agent_environment, environment::Environment, error::DfxResult, migrate::migrate,
    network::network_opt::NetworkOpt,
};
use clap::Parser;
use tokio::runtime::Runtime;

/// Detects known problems in the current environment caused by upgrading DFX, and suggests commands to fix them.
/// These commands can be batch-run automatically via `dfx fix`.
#[derive(Parser)]
pub struct DiagnoseOpts {
    #[command(flatten)]
    network: NetworkOpt,
}

pub fn exec(env: &dyn Environment, opts: DiagnoseOpts) -> DfxResult {
    let env = create_agent_environment(env, opts.network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async { migrate(&env, env.get_network_descriptor(), false).await })?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/extension/install.rs:
-----------------------

use crate::commands::DfxCommand;
use crate::config::cache::DiskBasedCache;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use anyhow::bail;
use clap::Parser;
use clap::Subcommand;
use dfx_core::error::extension::InstallExtensionError::OtherVersionAlreadyInstalled;
use dfx_core::extension::manager::InstallOutcome;
use semver::Version;
use slog::{error, info, warn};
use tokio::runtime::Runtime;
use url::Url;

#[derive(Parser)]
pub struct InstallOpts {
    /// Specifies the name of the extension to install.
    name: String,
    /// Installs the extension under different name. Useful when installing an extension with the same name as: already installed extension, or a built-in command.
    #[clap(long)]
    install_as: Option<String>,
    /// Installs a specific version of the extension, bypassing version checks
    #[clap(long)]
    version: Option<Version>,
    /// Specifies the URL of the catalog to use to find the extension.
    #[clap(long)]
    catalog_url: Option<Url>,
}

pub fn exec(env: &dyn Environment, opts: InstallOpts) -> DfxResult<()> {
    // creating an `extensions` directory in an otherwise empty cache directory would
    // cause the cache to be considered "installed" and later commands would fail
    DiskBasedCache::install(&env.get_cache().version_str())?;
    let spinner = env.new_spinner(format!("Installing extension: {}", opts.name).into());
    let mgr = env.get_extension_manager();
    let effective_extension_name = opts.install_as.clone().unwrap_or_else(|| opts.name.clone());
    if DfxCommand::has_subcommand(&effective_extension_name) {
        bail!("Extension '{}' cannot be installed because it conflicts with an existing command. Consider using '--install-as' flag to install this extension under different name.", opts.name)
    }

    let runtime = Runtime::new().expect("Unable to create a runtime");

    let install_outcome = runtime.block_on(async {
        mgr.install_extension(
            &opts.name,
            opts.catalog_url.as_ref(),
            opts.install_as.as_deref(),
            opts.version.as_ref(),
        )
        .await
    });
    spinner.finish_and_clear();
    let logger = env.get_logger();
    let install_as = if let Some(install_as) = &opts.install_as {
        format!(", and is available as '{}'", install_as)
    } else {
        "".to_string()
    };
    match install_outcome {
        Ok(InstallOutcome::Installed(name, version)) => {
            info!(
                logger,
                "Extension '{name}' version {version} installed successfully{install_as}"
            );
            Ok(())
        }
        Ok(InstallOutcome::AlreadyInstalled(name, version)) => {
            warn!(
                logger,
                "Extension '{name}' version {version} is already installed{install_as}"
            );
            Ok(())
        }
        Err(OtherVersionAlreadyInstalled(name, version)) => {
            error!(
                logger,
                "Extension '{name}' is already installed at version {version}"
            );
            error!(
                logger,
                r#"To upgrade, run "dfx extension uninstall {name}" and then re-run the dfx extension install command"#
            );
            bail!("Different version already installed");
        }
        Err(other) => Err(DfxError::new(other)),
    }
}


-----------------------

/src/dfx/src/commands/extension/list.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;
use std::io::Write;
use tokio::runtime::Runtime;
use url::Url;

#[derive(Parser)]
pub struct ListOpts {
    /// Specifies to list the available remote extensions.
    #[arg(long)]
    available: bool,
    /// Specifies the URL of the catalog to use to find the extension.
    #[clap(long)]
    catalog_url: Option<Url>,
}

pub fn exec(env: &dyn Environment, opts: ListOpts) -> DfxResult<()> {
    let mgr = env.get_extension_manager();

    if opts.available || opts.catalog_url.is_some() {
        let runtime = Runtime::new().expect("Unable to create a runtime");
        let extensions = runtime.block_on(async {
            mgr.list_available_extensions(opts.catalog_url.as_ref())
                .await
        })?;

        display_extension_list(
            &extensions,
            "No extensions available.",
            "Available extensions:",
        )
    } else {
        let extensions = mgr.list_installed_extensions()?;

        display_extension_list(
            &extensions,
            "No extensions installed.",
            "Installed extensions:",
        )
    }
}

fn display_extension_list(
    extensions: &Vec<String>,
    empty_msg: &str,
    header_msg: &str,
) -> DfxResult<()> {
    if extensions.is_empty() {
        eprintln!("{}", empty_msg);
        return Ok(());
    }

    eprintln!("{}", header_msg);
    for extension in extensions {
        eprint!("  ");
        std::io::stderr().flush()?;
        println!("{}", extension);
        std::io::stdout().flush()?;
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/extension/mod.rs:
-----------------------

#![allow(dead_code)]
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::{Parser, Subcommand};

mod install;
mod list;
pub mod run;
mod uninstall;

/// Manages the dfx extensions.
#[derive(Parser)]
#[command(name = "extension")]
pub struct ExtensionOpts {
    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Subcommand)]
pub enum SubCommand {
    /// Install an extension.
    Install(install::InstallOpts),
    /// Uninstall an extension.
    Uninstall(uninstall::UninstallOpts),
    /// Execute an extension.
    Run(run::RunOpts),
    /// List installed or available extensions.
    List(list::ListOpts),
}

pub fn exec(env: &dyn Environment, opts: ExtensionOpts) -> DfxResult {
    match opts.subcmd {
        SubCommand::Install(v) => install::exec(env, v),
        SubCommand::Uninstall(v) => uninstall::exec(env, v),
        SubCommand::Run(v) => run::exec(env, v),
        SubCommand::List(v) => list::exec(env, v),
    }
}


-----------------------

/src/dfx/src/commands/extension/run.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;
use std::ffi::OsString;

#[derive(Parser, Debug)]
pub struct RunOpts {
    /// Specifies the name of the extension to run.
    name: OsString,
    /// Specifies the parameters to pass to the extension.
    #[arg(allow_hyphen_values = true)]
    params: Vec<OsString>,
}

impl From<Vec<OsString>> for RunOpts {
    fn from(value: Vec<OsString>) -> Self {
        let (extension_name, params) = value.split_first().unwrap();
        RunOpts {
            name: extension_name.clone(),
            params: params.to_vec(),
        }
    }
}

pub fn exec(env: &dyn Environment, opts: RunOpts) -> DfxResult<()> {
    let mgr = env.get_extension_manager();
    let project_root = env
        .get_config()?
        .map(|c| c.get_project_root().to_path_buf());
    mgr.run_extension(opts.name, opts.params, project_root)?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/extension/uninstall.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;

#[derive(Parser)]
pub struct UninstallOpts {
    /// Specifies the name of the executable to uninstall.
    name: String,
}

pub fn exec(env: &dyn Environment, opts: UninstallOpts) -> DfxResult<()> {
    let mgr = env.get_extension_manager();
    mgr.uninstall_extension(&opts.name)?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/fix.rs:
-----------------------

use crate::lib::{
    agent::create_agent_environment, environment::Environment, error::DfxResult, migrate::migrate,
    network::network_opt::NetworkOpt,
};
use clap::Parser;
use tokio::runtime::Runtime;

/// Applies one-time fixes for known problems in the current environment caused by upgrading DFX.
/// Makes no changes that would not have been suggested by `dfx diagnose`.
#[derive(Parser)]
pub struct FixOpts {
    #[command(flatten)]
    network: NetworkOpt,
}

pub fn exec(env: &dyn Environment, opts: FixOpts) -> DfxResult {
    let env = create_agent_environment(env, opts.network.to_network_name())?;
    let runtime = Runtime::new().expect("Failed to create runtime");
    runtime.block_on(async { migrate(&env, env.get_network_descriptor(), true).await })?;
    Ok(())
}


-----------------------

/src/dfx/src/commands/generate.rs:
-----------------------

use crate::config::cache::DiskBasedCache;
use crate::lib::agent::create_anonymous_agent_environment;
use crate::lib::builders::BuildConfig;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::models::canister::CanisterPool;
use clap::Parser;
use tokio::runtime::Runtime;

/// Generate type declarations for canisters from the code in your project
#[derive(Parser)]
pub struct GenerateOpts {
    /// Specifies the name of the canister to generate type information for.
    /// If you do not specify a canister name, generates types for all canisters.
    canister_name: Option<String>,

    // Deprecated/hidden because it had/has no effect.
    // Cannot use 'hide' on a flattened  object - inlined the flattened network specifier
    #[arg(long, global = true, hide = true)]
    network: Option<String>,
}

pub fn exec(env: &dyn Environment, opts: GenerateOpts) -> DfxResult {
    let env = create_anonymous_agent_environment(env, None)?;
    let log = env.get_logger();

    // Read the config.
    let config = env.get_config_or_anyhow()?;

    // Check the cache. This will only install the cache if there isn't one installed
    // already.
    DiskBasedCache::install(&env.get_cache().version_str())?;

    // Option can be None which means generate types for all canisters
    let canisters_to_load = config
        .get_config()
        .get_canister_names_with_dependencies(opts.canister_name.as_deref())?;
    let canisters_to_generate = canisters_to_load.clone().into_iter().collect();

    let canister_pool_load = CanisterPool::load(&env, false, &canisters_to_load)?;

    // If generate for motoko canister, build first
    let mut build_before_generate = Vec::new();
    let mut build_dependees = Vec::new();
    for canister in canister_pool_load.get_canister_list() {
        let canister_name = canister.get_name();
        if let Some(info) = canister_pool_load.get_first_canister_with_name(canister_name) {
            if info.get_info().is_motoko() {
                build_before_generate.push(canister_name.to_string());
            }
            for dependent_canister in config
                .get_config()
                .get_canister_names_with_dependencies(Some(canister_name))?
            {
                if !build_dependees.contains(&dependent_canister) {
                    build_dependees.push(dependent_canister);
                }
            }
        }
    }
    let build_config =
        BuildConfig::from_config(&config, env.get_network_descriptor().is_playground())?
            .with_canisters_to_build(build_before_generate);
    let generate_config =
        BuildConfig::from_config(&config, env.get_network_descriptor().is_playground())?
            .with_canisters_to_build(canisters_to_generate);

    if build_config
        .canisters_to_build
        .as_ref()
        .map(|v| !v.is_empty())
        .unwrap_or(false)
    {
        let canister_pool_build = CanisterPool::load(&env, true, &build_dependees)?;
        slog::info!(log, "Building canisters before generate for Motoko");
        let runtime = Runtime::new().expect("Unable to create a runtime");
        runtime.block_on(canister_pool_build.build_or_fail(log, &build_config))?;
    }

    for canister in canister_pool_load.canisters_to_build(&generate_config) {
        canister.generate(&canister_pool_load, &generate_config)?;
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/deploy_wallet.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::identity::wallet::create_wallet;
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::root_key::fetch_root_key_if_needed;
use anyhow::bail;
use candid::Principal as CanisterId;
use clap::Parser;
use tokio::runtime::Runtime;

/// Installs the wallet Wasm to the provided canister id.
#[derive(Parser)]
pub struct DeployWalletOpts {
    /// The ID of the canister where the wallet Wasm will be deployed.
    canister_id: String,
}

pub fn exec(env: &dyn Environment, opts: DeployWalletOpts, network: NetworkOpt) -> DfxResult {
    let agent_env = create_agent_environment(env, network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");

    runtime.block_on(async { fetch_root_key_if_needed(&agent_env).await })?;

    let identity_name = agent_env
        .get_selected_identity()
        .expect("No selected identity.")
        .to_string();
    let network = agent_env.get_network_descriptor();

    let canister_id = opts.canister_id;
    match CanisterId::from_text(&canister_id) {
        Ok(id) => {
            runtime.block_on(async {
                create_wallet(&agent_env, network, &identity_name, Some(id)).await?;
                DfxResult::Ok(())
            })?;
        }
        Err(err) => {
            bail!(
                "Cannot convert {} to a valid canister id. Candid error: {}",
                canister_id,
                err
            );
        }
    };
    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/export.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;

/// Prints the decrypted PEM file for the identity.
#[derive(Parser)]
pub struct ExportOpts {
    /// The identity to export.
    exported_identity: String,
}

pub fn exec(env: &dyn Environment, opts: ExportOpts) -> DfxResult {
    let name = opts.exported_identity.as_str();

    let pem = env.new_identity_manager()?.export(env.get_logger(), name)?;
    print!("{}", pem);

    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/get_wallet.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::identity::wallet::get_or_create_wallet;
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::root_key::fetch_root_key_if_needed;
use clap::Parser;
use tokio::runtime::Runtime;

/// Gets the canister ID for the wallet associated with your identity on a network.
#[derive(Parser)]
pub struct GetWalletOpts {}

pub fn exec(env: &dyn Environment, _opts: GetWalletOpts, network: NetworkOpt) -> DfxResult {
    let agent_env = create_agent_environment(env, network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");

    runtime.block_on(async { fetch_root_key_if_needed(&agent_env).await })?;

    let identity_name = agent_env
        .get_selected_identity()
        .expect("No selected identity.")
        .to_string();
    let network = agent_env.get_network_descriptor();

    runtime.block_on(async {
        println!(
            "{}",
            get_or_create_wallet(&agent_env, network, &identity_name).await?
        );
        DfxResult::Ok(())
    })?;

    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/import.rs:
-----------------------

use crate::commands::identity::new::create_new_dfx_identity;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::Context;
use clap::Parser;
use dfx_core::identity::identity_manager::{IdentityCreationParameters, IdentityStorageMode};
use slog::{info, warn};
use std::fs;
use std::path::PathBuf;
use std::str::FromStr;

/// Creates a new identity from a PEM file.
#[derive(Parser)]
pub struct ImportOpts {
    /// The identity to create.
    new_identity: String,

    /// The PEM file to import.
    pem_file: Option<PathBuf>,

    /// The path to a file with your seed phrase.
    #[arg(long, conflicts_with("pem_file"), required_unless_present("pem_file"))]
    seed_file: Option<PathBuf>,

    /// DEPRECATED: Please use --storage-mode=plaintext instead
    #[arg(long)]
    disable_encryption: bool,

    /// How your private keys are stored. By default, if keyring/keychain is available, keys are stored there.
    /// Otherwise, a password-protected file is used as fallback.
    /// Mode 'plaintext' is not safe, but convenient for use in CI.
    #[arg(long, conflicts_with("disable_encryption"),
        value_parser = ["keyring", "password-protected", "plaintext"])]
    storage_mode: Option<String>,

    /// If the identity already exists, remove and re-import it.
    #[arg(long)]
    force: bool,
}

/// Executes the import subcommand.
pub fn exec(env: &dyn Environment, opts: ImportOpts) -> DfxResult {
    let log = env.get_logger();

    if opts.disable_encryption {
        warn!(log, "The flag --disable-encryption has been deprecated. Please use --storage-mode=plaintext instead.");
    }

    let mode = if opts.disable_encryption {
        IdentityStorageMode::Plaintext
    } else if let Some(mode_str) = opts.storage_mode {
        IdentityStorageMode::from_str(&mode_str)?
    } else {
        IdentityStorageMode::default()
    };
    let name = opts.new_identity.as_str();
    let params = if let Some(src_pem_file) = opts.pem_file {
        IdentityCreationParameters::PemFile { src_pem_file, mode }
    } else {
        let mnemonic =
            fs::read_to_string(opts.seed_file.unwrap()).context("Failed to read seed file")?;
        IdentityCreationParameters::SeedPhrase { mnemonic, mode }
    };

    create_new_dfx_identity(env, log, name, params, opts.force)?;

    info!(log, r#"Imported identity: "{}"."#, name);
    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/list.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;
use std::io::Write;

/// Lists existing identities.
#[derive(Parser)]
pub struct ListOpts {}

pub fn exec(env: &dyn Environment, _opts: ListOpts) -> DfxResult {
    let mgr = env.new_identity_manager()?;
    let identities = mgr.get_identity_names(env.get_logger())?;
    let current_identity = mgr.get_selected_identity_name();
    for identity in identities {
        if current_identity == &identity {
            // same identity, suffix with '*'.
            print!("{}", identity);
            std::io::stdout().flush()?;
            eprint!(" *");
            std::io::stderr().flush()?;
            println!();
        } else {
            println!("{}", identity);
        }
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/mod.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use clap::Parser;

mod deploy_wallet;
mod export;
mod get_wallet;
mod import;
mod list;
mod new;
mod principal;
mod remove;
mod rename;
mod set_wallet;
mod r#use;
mod whoami;

/// Manages identities used to communicate with the Internet Computer network.
/// Setting an identity enables you to test user-based access controls.
#[derive(Parser)]
#[command(name = "identity")]
pub struct IdentityOpts {
    #[command(flatten)]
    network: NetworkOpt,

    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
enum SubCommand {
    DeployWallet(deploy_wallet::DeployWalletOpts),
    Export(export::ExportOpts),
    GetWallet(get_wallet::GetWalletOpts),
    Import(import::ImportOpts),
    List(list::ListOpts),
    New(new::NewIdentityOpts),
    GetPrincipal(principal::GetPrincipalOpts),
    Remove(remove::RemoveOpts),
    Rename(rename::RenameOpts),
    SetWallet(set_wallet::SetWalletOpts),
    Use(r#use::UseOpts),
    Whoami(whoami::WhoAmIOpts),
}

pub fn exec(env: &dyn Environment, opts: IdentityOpts) -> DfxResult {
    match opts.subcmd {
        SubCommand::DeployWallet(v) => deploy_wallet::exec(env, v, opts.network),
        SubCommand::Export(v) => export::exec(env, v),
        SubCommand::GetWallet(v) => get_wallet::exec(env, v, opts.network),
        SubCommand::List(v) => list::exec(env, v),
        SubCommand::New(v) => new::exec(env, v),
        SubCommand::GetPrincipal(v) => principal::exec(env, v),
        SubCommand::Import(v) => import::exec(env, v),
        SubCommand::Remove(v) => remove::exec(env, v),
        SubCommand::Rename(v) => rename::exec(env, v),
        SubCommand::SetWallet(v) => set_wallet::exec(env, v, opts.network),
        SubCommand::Use(v) => r#use::exec(env, v),
        SubCommand::Whoami(v) => whoami::exec(env, v),
    }
}


-----------------------

/src/dfx/src/commands/identity/new.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::util::clap::parsers::hsm_key_id_parser;
use anyhow::Context;
use clap::Parser;
use dfx_core::error::identity::CreateNewIdentityError::SwitchBackToIdentityFailed;
use dfx_core::identity::identity_manager::{
    HardwareIdentityConfiguration, IdentityCreationParameters, IdentityStorageMode,
};
use regex::Regex;
use slog::{info, warn, Logger};
use std::str::FromStr;
use IdentityCreationParameters::{Hardware, Pem};

/// Creates a new identity.
#[derive(Parser)]
pub struct NewIdentityOpts {
    #[arg(value_parser = identity_name_validator)]
    /// The name of the identity to create. Valid characters are letters, numbers, and these symbols: .-_@
    new_identity: String,

    #[cfg_attr(
        not(windows),
        doc = r#"The file path to the opensc-pkcs11 library e.g. "/usr/local/lib/opensc-pkcs11.so""#
    )]
    #[cfg_attr(
        windows,
        doc = r#"The file path to the opensc-pkcs11 library e.g. "C:\Program Files (x86)\OpenSC Project\OpenSC\pkcs11\opensc-pkcs11.dll"#
    )]
    #[arg(long, requires("hsm_key_id"))]
    hsm_pkcs11_lib_path: Option<String>,

    /// A sequence of pairs of hex digits
    #[arg(long, requires("hsm_pkcs11_lib_path"), value_parser = hsm_key_id_parser)]
    hsm_key_id: Option<String>,

    /// DEPRECATED: Please use --storage-mode=plaintext instead
    #[arg(long)]
    disable_encryption: bool,

    /// How your private keys are stored. By default, if keyring/keychain is available, keys are stored there.
    /// Otherwise, a password-protected file is used as fallback.
    /// Mode 'plaintext' is not safe, but convenient for use in CI.
    #[arg(long, conflicts_with("disable_encryption"),
        value_parser = ["keyring", "password-protected", "plaintext"])]
    storage_mode: Option<String>,

    /// If the identity already exists, remove and re-create it.
    #[arg(long)]
    force: bool,
}

fn identity_name_validator(name: &str) -> Result<String, String> {
    let valid_name = Regex::new(r"^[A-Za-z0-9\.\-_@]+$").unwrap();
    if !valid_name.is_match(name) {
        return Err("Invalid identity name. Please only use the characters ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.-_@0123456789".to_string());
    }
    Ok(name.into())
}

pub fn exec(env: &dyn Environment, opts: NewIdentityOpts) -> DfxResult {
    let log = env.get_logger();

    if opts.disable_encryption {
        warn!(log, "The flag --disable-encryption has been deprecated. Please use --storage-mode=plaintext instead.");
    }

    let name = opts.new_identity.as_str();

    let creation_parameters = match (opts.hsm_pkcs11_lib_path, opts.hsm_key_id) {
        (Some(pkcs11_lib_path), Some(key_id)) => Hardware {
            hsm: HardwareIdentityConfiguration {
                pkcs11_lib_path,
                key_id,
            },
        },
        _ => {
            let mode = if opts.disable_encryption {
                IdentityStorageMode::Plaintext
            } else if let Some(mode_str) = opts.storage_mode {
                IdentityStorageMode::from_str(&mode_str)?
            } else {
                IdentityStorageMode::default()
            };

            Pem { mode }
        }
    };

    create_new_dfx_identity(env, log, name, creation_parameters, opts.force)?;

    info!(log, r#"Created identity: "{}"."#, name);
    Ok(())
}

pub fn create_new_dfx_identity(
    env: &dyn Environment,
    log: &Logger,
    name: &str,
    creation_parameters: IdentityCreationParameters,
    force: bool,
) -> DfxResult {
    let result =
        env.new_identity_manager()?
            .create_new_identity(log, name, creation_parameters, force);
    if let Err(SwitchBackToIdentityFailed(underlying)) = result {
        Err(underlying).with_context(||format!("Failed to switch back over to the identity you're replacing. Please run 'dfx identity use {}' to do it manually.", name))?;
    } else {
        result?;
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/principal.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::anyhow;
use clap::Parser;
use ic_agent::identity::Identity;

/// Shows the textual representation of the Principal associated with the current identity.
#[derive(Parser)]
pub struct GetPrincipalOpts {}

pub fn exec(env: &dyn Environment, _opts: GetPrincipalOpts) -> DfxResult {
    let identity = env
        .new_identity_manager()?
        .instantiate_selected_identity(env.get_logger())?;
    let principal_id = identity
        .as_ref()
        .sender()
        .map_err(|err| anyhow!("{}", err))?;
    println!("{}", principal_id.to_text());
    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/remove.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;
use slog::info;

/// Removes an existing identity.
#[derive(Parser)]
pub struct RemoveOpts {
    /// The identity to remove.
    removed_identity: String,

    /// Required if the identity has wallets configured so that users do not accidentally lose access to wallets.
    #[arg(long)]
    drop_wallets: bool,
}

pub fn exec(env: &dyn Environment, opts: RemoveOpts) -> DfxResult {
    let name = opts.removed_identity.as_str();

    let log = env.get_logger();

    env.new_identity_manager()?
        .remove(log, name, opts.drop_wallets, Some(log))?;

    info!(log, r#"Removed identity "{}"."#, name);
    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/rename.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::bail;
use clap::Parser;
use dfx_core::error::identity::RenameIdentityError::SwitchDefaultIdentitySettingsFailed;
use slog::info;

/// Renames an existing identity.
#[derive(Parser)]
pub struct RenameOpts {
    /// The current name of the identity.
    from: String,

    /// The new name of the identity.
    to: String,
}

pub fn exec(env: &dyn Environment, opts: RenameOpts) -> DfxResult {
    let from = opts.from.as_str();
    let to = opts.to.as_str();

    let log = env.get_logger();

    let mut identity_manager = env.new_identity_manager()?;
    let result = identity_manager.rename(log, env.get_project_temp_dir()?, from, to);
    if let Err(SwitchDefaultIdentitySettingsFailed(_)) = result {
        bail!("Failed to switch over default identity settings.  Please do this manually by running 'dfx identity use {}'", to);
    }
    let renamed_default = result?;

    info!(log, r#"Renamed identity "{}" to "{}"."#, from, to);
    if renamed_default {
        info!(log, r#"Now using identity: "{}"."#, to);
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/set_wallet.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use crate::lib::identity::wallet::set_wallet_id;
use crate::lib::network::network_opt::NetworkOpt;
use anyhow::{anyhow, Context};
use candid::Principal;
use clap::Parser;
use dfx_core::canister::build_wallet_canister;
use ic_utils::interfaces::wallet::BalanceResult;
use slog::{error, info};
use tokio::runtime::Runtime;

/// Sets the wallet canister ID to use for your identity on a network.
#[derive(Parser)]
pub struct SetWalletOpts {
    /// The Canister ID of the wallet to associate with this identity.
    canister_name: String,

    /// Skip verification that the ID points to a correct wallet canister. Only useful for the local network.
    #[arg(long)]
    force: bool,
}

pub fn exec(env: &dyn Environment, opts: SetWalletOpts, network: NetworkOpt) -> DfxResult {
    let agent_env = create_agent_environment(env, network.to_network_name())?;
    let env = &agent_env;
    let log = env.get_logger();

    let runtime = Runtime::new().expect("Unable to create a runtime");

    let identity_name = agent_env
        .get_selected_identity()
        .expect("No selected identity.")
        .to_string();

    let network = agent_env.get_network_descriptor();

    let canister_name = opts.canister_name.as_str();
    let canister_id = match Principal::from_text(canister_name) {
        Ok(id) => id,
        Err(_) => {
            let config = env.get_config_or_anyhow()?;
            let canister_id = env.get_canister_id_store()?.get(canister_name)?;
            let canister_info = CanisterInfo::load(&config, canister_name, Some(canister_id))?;
            canister_info.get_canister_id()?
        }
    };
    let force = opts.force;

    // Try to check the canister_id for a `wallet_balance()` if the network is not the IC and available.
    // Otherwise we just trust the user.
    if force {
        info!(
            log,
            "Skipping verification of availability of the canister on the network due to --force..."
        );
    } else {
        let agent = env.get_agent();

        runtime
            .block_on(async {
                let _ = agent.status().await.context("Failed to read network status.")?;

                info!(
                    log,
                    "Checking availability of the canister on the network..."
                );

                let canister = build_wallet_canister(canister_id, agent).await?;
                let balance = canister.wallet_balance().await;

                match balance {
                    Ok(BalanceResult { amount: 0 }) => {
                        error!(
                            log,
                            "Impossible to read the canister. Make sure this is a valid wallet{}. Use --force to skip this verification.",
                            if !network.is_ic { " and the network is running" } else { "" },
                        );
                        Err(anyhow!("Could not find the wallet or the wallet was invalid."))
                    },
                    Err(err) => {
                        Err(anyhow!("Unable to access the wallet: {}", err))
                    },
                    _ => {
                        Ok(())
                    },
                }
            })
            .map_err(DfxError::from)?;
    }

    info!(
        log,
        "Setting wallet for identity '{}' on network '{}' to id '{}'",
        identity_name,
        network.name,
        canister_id
    );
    set_wallet_id(network, &identity_name, canister_id)?;
    info!(log, "Wallet set successfully.");

    Ok(())
}


-----------------------

/src/dfx/src/commands/identity/use.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use clap::Parser;
use slog::info;

/// Specifies the identity to use.
#[derive(Parser)]
pub struct UseOpts {
    /// The identity to use.
    new_identity: String,
}

pub fn exec(env: &dyn Environment, opts: UseOpts) -> DfxResult {
    let identity = opts.new_identity.as_str();

    let log = env.get_logger();
    info!(log, r#"Using identity: "{}"."#, identity);

    env.new_identity_manager()?
        .use_identity_named(log, identity)
        .map_err(DfxError::new)
}


-----------------------

/src/dfx/src/commands/identity/whoami.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;

/// Shows the name of the current identity.
#[derive(Parser)]
pub struct WhoAmIOpts {}

pub fn exec(env: &dyn Environment, _opts: WhoAmIOpts) -> DfxResult {
    let mgr = env.new_identity_manager()?;
    let identity = mgr.get_selected_identity_name();
    println!("{}", identity);
    Ok(())
}


-----------------------

/src/dfx/src/commands/info/mod.rs:
-----------------------

mod replica_port;
mod webserver_port;
use crate::commands::info::{replica_port::get_replica_port, webserver_port::get_webserver_port};
use crate::lib::agent::create_anonymous_agent_environment;
use crate::lib::error::DfxResult;
use crate::lib::info;
use crate::lib::named_canister::get_ui_canister_url;
use crate::lib::network::network_opt::NetworkOpt;
use crate::Environment;
use anyhow::{bail, Context};
use clap::{Parser, Subcommand};
use dfx_core::config::model::dfinity::NetworksConfig;

#[derive(Subcommand, Clone, Debug)]
enum InfoType {
    /// Show the URL of the Candid UI canister
    CandidUiUrl,
    /// Show the headers that gets applied to assets in .ic-assets.json5 if "security_policy" is "standard" or "hardened".
    SecurityPolicy,
    /// Show the port of the local replica
    ReplicaPort,
    /// Show the revision of the replica shipped with this dfx binary
    ReplicaRev,
    /// Show the port of the webserver
    WebserverPort,
    /// Show the path to network configuration file
    NetworksJsonPath,
}

#[derive(Parser)]
#[command(name = "info")]
/// Get information about the replica shipped with dfx, path to networks.json, and network ports of running replica.
pub struct InfoOpts {
    #[command(subcommand)]
    info_type: InfoType,

    #[command(flatten)]
    network: NetworkOpt,
}

pub fn exec(env: &dyn Environment, opts: InfoOpts) -> DfxResult {
    let value = match opts.info_type {
        InfoType::CandidUiUrl => {
            let env = create_anonymous_agent_environment(env, opts.network.to_network_name())?;
            match get_ui_canister_url(&env)? {
                Some(url) => url.to_string(),
                None => bail!(
                    "Candid UI not installed on network {}.",
                    env.get_network_descriptor().name
                ),
            }
        }
        InfoType::SecurityPolicy => {
            ic_asset::security_policy::SecurityPolicy::Standard.to_json5_str()
        }
        InfoType::ReplicaPort => get_replica_port(env)?,
        InfoType::ReplicaRev => info::replica_rev().to_string(),
        InfoType::WebserverPort => get_webserver_port(env)?,
        InfoType::NetworksJsonPath => NetworksConfig::new()?
            .get_path()
            .to_str()
            .context("Failed to convert networks.json path to a string.")?
            .to_string(),
    };
    println!("{}", value);
    Ok(())
}


-----------------------

/src/dfx/src/commands/info/replica_port.rs:
-----------------------

use crate::lib::error::DfxResult;
use crate::Environment;
use anyhow::bail;
use dfx_core::network::provider::{create_network_descriptor, LocalBindDetermination};

pub(crate) fn get_replica_port(env: &dyn Environment) -> DfxResult<String> {
    let network_descriptor = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        None,
        None,
        LocalBindDetermination::AsConfigured,
    )?;

    let logger = None;
    if let Some(port) = network_descriptor
        .local_server_descriptor()?
        .get_running_replica_port(logger)?
    {
        Ok(format!("{}", port))
    } else {
        bail!("No replica port found");
    }
}


-----------------------

/src/dfx/src/commands/info/webserver_port.rs:
-----------------------

use crate::lib::error::DfxResult;
use crate::Environment;
use dfx_core::network::provider::{create_network_descriptor, LocalBindDetermination};

pub(crate) fn get_webserver_port(env: &dyn Environment) -> DfxResult<String> {
    let port = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        None,
        None,
        LocalBindDetermination::ApplyRunningWebserverPort,
    )?
    .local_server_descriptor()?
    .bind_address
    .port();
    Ok(format!("{}", port))
}


-----------------------

/src/dfx/src/commands/killall.rs:
-----------------------

use crate::lib::{environment::Environment, error::DfxResult};

use anyhow::Error;
use clap::Parser;
use dfx_core::config::cache::ensure_cache_versions_dir;
use slog::info;
use sysinfo::{ProcessExt, System, SystemExt};

/// Kills all dfx-related processes on the system. Useful if a process gets stuck.
#[derive(Parser)]
pub struct KillallOpts;

pub fn exec(env: &dyn Environment, _: KillallOpts) -> DfxResult {
    let mut info = System::new();
    info.refresh_processes();
    let mut n = 0;
    let self_pid = sysinfo::get_current_pid().map_err(Error::msg)?;
    // first, kill the dfx processes, so they can't restart the child processes
    for proc in info.processes_by_exact_name("dfx") {
        if proc.pid() != self_pid {
            n += 1;
            proc.kill();
        }
    }
    // then, kill anything that was installed alongside dfx
    info.refresh_processes();
    let versions_dir = ensure_cache_versions_dir()?;
    for (pid, proc) in info.processes() {
        if *pid != self_pid && proc.exe().starts_with(&versions_dir) {
            n += 1;
            proc.kill();
        }
    }
    info!(env.get_logger(), "Killed {n} processes");
    Ok(())
}


-----------------------

/src/dfx/src/commands/language_service.rs:
-----------------------

use crate::error_invalid_data;
use crate::lib::builders::BuildConfig;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::package_arguments::{self, PackageArguments};
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use clap::Parser;
use dfx_core::config::model::canister_id_store::CanisterIdStore;
use dfx_core::config::model::dfinity::{
    ConfigCanistersCanister, ConfigInterface, CONFIG_FILE_NAME,
};
use dfx_core::network::provider::{create_network_descriptor, LocalBindDetermination};
use fn_error_context::context;
use std::io::{stdout, IsTerminal};
use std::path::PathBuf;
use std::process::Stdio;

const CANISTER_ARG: &str = "canister";

/// Starts the Motoko IDE Language Server. This is meant to be run by editor plugins not the
/// end-user.
#[derive(Parser)]
#[command(hide = true)]
pub struct LanguageServiceOpts {
    /// Specifies the canister name. If you don't specify this argument, all canisters are
    /// processed.
    canister: Option<String>,

    /// Forces the language server to start even when run from a terminal.
    #[arg(long)]
    force_tty: bool,
}

// Don't read anything from stdin or output anything to stdout while this function is being
// executed or LSP will become very unhappy
pub fn exec(env: &dyn Environment, opts: LanguageServiceOpts) -> DfxResult {
    let force_tty = opts.force_tty;
    // Are we being run from a terminal? That's most likely not what we want
    if stdout().is_terminal() && !force_tty {
        Err(anyhow!("The `_language-service` command is meant to be run by editors to start a language service. You probably don't want to run it from a terminal.\nIf you _really_ want to, you can pass the --force-tty flag."))
    } else if let Some(config) = env.get_config()? {
        let main_path = get_main_path(config.get_config(), opts.canister)?;
        let packtool = &config
            .get_config()
            .get_defaults()
            .get_build()
            .get_packtool();

        let mut package_arguments = package_arguments::load(
            env.get_cache().as_ref(),
            packtool,
            config.get_project_root(),
        )?;

        // Include actor alias flags
        let canister_names = config
            .get_config()
            .get_canister_names_with_dependencies(None)?;
        let network_descriptor = create_network_descriptor(
            env.get_config()?,
            env.get_networks_config(),
            None,
            None,
            LocalBindDetermination::ApplyRunningWebserverPort,
        )?;
        let canister_id_store =
            CanisterIdStore::new(env.get_logger(), &network_descriptor, env.get_config()?)?;
        for canister_name in canister_names {
            match canister_id_store.get(&canister_name) {
                Ok(canister_id) => package_arguments.append(&mut vec![
                    "--actor-alias".to_owned(),
                    canister_name,
                    Principal::to_text(&canister_id),
                ]),
                Err(err) => eprintln!("{}", err),
            };
        }

        // Add IDL directory flag
        let build_config =
            BuildConfig::from_config(&config, env.get_network_descriptor().is_playground())?;
        package_arguments.append(&mut vec![
            "--actor-idl".to_owned(),
            (*build_config.lsp_root.to_string_lossy()).to_owned(),
        ]);

        run_ide(env, main_path, package_arguments)
    } else {
        Err(anyhow!("Cannot find dfx configuration file in the current working directory. Did you forget to create one?"))
    }
}

#[context("Failed to determine main path.")]
fn get_main_path(config: &ConfigInterface, canister_name: Option<String>) -> DfxResult<PathBuf> {
    // TODO try and point at the actual dfx.json path
    let dfx_json = CONFIG_FILE_NAME;

    let (canister_name, canister): (String, ConfigCanistersCanister) =
        match (config.canisters.as_ref(), canister_name) {
            (None, _) => Err(error_invalid_data!(
                "Missing field 'canisters' in {0}",
                dfx_json
            )),
            (Some(canisters), Some(canister_name)) => {
                let c = canisters.get(canister_name.as_str()).ok_or_else(|| {
                    error_invalid_data!(
                        "Canister {0} cannot not be found in {1}",
                        canister_name,
                        dfx_json
                    )
                })?;
                Ok((canister_name.to_string(), c.clone()))
            }
            (Some(canisters), None) => {
                if canisters.len() == 1 {
                    let (n, c) = canisters.iter().next().unwrap();
                    Ok((n.to_string(), c.clone()))
                } else {
                    Err(error_invalid_data!(
                    "There are multiple canisters in {0}, please select one using the {1} argument",
                    dfx_json,
                    CANISTER_ARG
                ))
                }
            }
        }?;
    if let Some(main) = canister.main {
        Ok(main)
    } else {
        Err(error_invalid_data!(
            "Canister {0} lacks a 'main' element in {1}",
            canister_name,
            dfx_json
        ))
    }
}

fn run_ide(
    env: &dyn Environment,
    main_path: PathBuf,
    package_arguments: PackageArguments,
) -> DfxResult {
    let output = env
        .get_cache()
        .get_binary_command("mo-ide")?
        .stdin(Stdio::inherit())
        .stdout(Stdio::inherit())
        // Point at the right canister
        .arg("--canister-main")
        .arg(main_path)
        // Tell the IDE where the stdlib and other packages are located
        .args(package_arguments)
        .output()
        .context("Failed to run 'mo-ide' binary.")?;

    if !output.status.success() {
        bail!(
            "The Motoko Language Server failed.\nStdout:\n{0}\nStderr:\n{1}",
            String::from_utf8_lossy(&output.stdout).to_string(),
            String::from_utf8_lossy(&output.stderr).to_string(),
        )
    } else {
        Ok(())
    }
}


-----------------------

/src/dfx/src/commands/ledger/account_id.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::nns_types::account_identifier::{AccountIdentifier, Subaccount};
use anyhow::Context;
use candid::Principal;
use clap::Parser;

/// Prints the ledger account identifier corresponding to a principal.
#[derive(Parser)]
pub struct AccountIdOpts {
    #[arg(long, value_name = "PRINCIPAL")]
    /// Principal controlling the account.
    pub of_principal: Option<Principal>,

    #[arg(long, value_name = "ALIAS", conflicts_with("of_principal"))]
    /// Alias or principal of the canister controlling the account.
    pub of_canister: Option<String>,

    #[arg(long, value_name = "SUBACCOUNT")]
    /// Subaccount identifier (64 character long hex string).
    pub subaccount: Option<Subaccount>,

    #[arg(
        long,
        value_name = "SUBACCOUNT_FROM_PRINCIPAL",
        conflicts_with("subaccount")
    )]
    /// Principal from which the subaccount identifier is derived.
    pub subaccount_from_principal: Option<Principal>,
}

pub async fn exec(env: &dyn Environment, opts: AccountIdOpts) -> DfxResult {
    let principal = if let Some(principal) = opts.of_principal {
        principal
    } else if let Some(alias) = opts.of_canister {
        let canister_id_store = env.get_canister_id_store()?;
        Principal::from_text(&alias).or_else(|_| canister_id_store.get(&alias))?
    } else {
        env.get_selected_identity_principal()
            .context("No identity is selected")?
    };
    let subaccount = if let Some(subaccount) = opts.subaccount {
        Some(subaccount)
    } else {
        opts.subaccount_from_principal
            .map(|principal| Subaccount::from(&principal))
    };
    println!("{}", AccountIdentifier::new(principal, subaccount));
    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/balance.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::nns_types::account_identifier::{AccountIdentifier, Subaccount};
use crate::lib::operations::ledger;
use crate::lib::root_key::fetch_root_key_if_needed;
use anyhow::anyhow;
use candid::Principal;
use clap::Parser;
use std::str::FromStr;

/// Prints the account balance of the user
#[derive(Parser)]
pub struct BalanceOpts {
    /// Specifies an AccountIdentifier to get the balance of
    of: Option<String>,

    /// Subaccount of the selected identity to get the balance of
    #[arg(long, conflicts_with("of"))]
    subaccount: Option<Subaccount>,

    /// Canister ID of the ledger canister.
    #[arg(long)]
    ledger_canister_id: Option<Principal>,
}

pub async fn exec(env: &dyn Environment, opts: BalanceOpts) -> DfxResult {
    fetch_root_key_if_needed(env).await?;
    let sender = env
        .get_selected_identity_principal()
        .expect("Selected identity not instantiated.");
    let subacct = opts.subaccount;
    let acc_id = opts
        .of
        .map_or_else(
            || Ok(AccountIdentifier::new(sender, subacct)),
            |v| AccountIdentifier::from_str(&v),
        )
        .map_err(|err| anyhow!(err))?;
    let agent = env.get_agent();

    let balance = ledger::balance(agent, &acc_id, opts.ledger_canister_id).await?;

    println!("{balance}");

    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/create_canister.rs:
-----------------------

use crate::commands::ledger::get_icpts_from_args;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::error::NotifyCreateCanisterError::Notify;
use crate::lib::ledger_types::Memo;
use crate::lib::ledger_types::NotifyError::Refunded;
use crate::lib::nns_types::account_identifier::Subaccount;
use crate::lib::nns_types::icpts::{ICPTs, TRANSACTION_FEE};
use crate::lib::operations::cmc::{notify_create, transfer_cmc};
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::e8s_parser;
use crate::util::clap::subnet_selection_opt::SubnetSelectionOpt;
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use clap::Parser;

pub const MEMO_CREATE_CANISTER: u64 = 1095062083_u64;

/// Create a canister from ICP
#[derive(Parser)]
pub struct CreateCanisterOpts {
    /// Specify the controller of the new canister
    controller: String,

    /// Subaccount to withdraw from
    #[arg(long)]
    from_subaccount: Option<Subaccount>,

    /// ICP to mint into cycles and deposit into destination canister
    /// Can be specified as a Decimal with the fractional portion up to 8 decimal places
    /// i.e. 100.012
    #[arg(long)]
    amount: Option<ICPTs>,

    /// Specify ICP as a whole number, helpful for use in conjunction with `--e8s`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    icp: Option<u64>,

    /// Specify e8s as a whole number, helpful for use in conjunction with `--icp`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    e8s: Option<u64>,

    /// Transaction fee, default is 10000 e8s.
    #[arg(long)]
    fee: Option<ICPTs>,

    /// Max fee, default is 10000 e8s.
    #[arg(long)]
    max_fee: Option<ICPTs>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction-deduplication, default is system-time. // https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,

    #[command(flatten)]
    subnet_selection: SubnetSelectionOpt,
}

pub async fn exec(env: &dyn Environment, opts: CreateCanisterOpts) -> DfxResult {
    let amount = get_icpts_from_args(opts.amount, opts.icp, opts.e8s)?;

    let fee = opts.fee.unwrap_or(TRANSACTION_FEE);
    let memo = Memo(MEMO_CREATE_CANISTER);

    let controller = Principal::from_text(&opts.controller).with_context(|| {
        format!(
            "Failed to parse {:?} as controller principal.",
            &opts.controller
        )
    })?;

    let agent = env.get_agent();
    let calling_identity = agent.get_principal().map_err(|err| anyhow!(err))?;

    fetch_root_key_if_needed(env).await?;

    let subnet_selection = opts
        .subnet_selection
        .into_subnet_selection_type(env)
        .await?;

    let height = transfer_cmc(
        agent,
        env.get_logger(),
        memo,
        amount,
        fee,
        opts.from_subaccount,
        calling_identity,
        opts.created_at_time,
    )
    .await?;
    println!("Using transfer at block height {height}");

    let result = notify_create(agent, controller, height, subnet_selection).await;

    match result {
        Ok(principal) => {
            println!("Canister created with id: {:?}", principal.to_text());
        }
        Err(Notify(Refunded {
            reason,
            block_index,
        })) => {
            match block_index {
                Some(height) => {
                    println!("Refunded at block height {height} with message: {reason}")
                }
                None => println!("Refunded with message: {reason}"),
            };
        }
        Err(other) => bail!("{other:?}"),
    };
    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/fabricate_cycles.rs:
-----------------------

use super::get_icpts_from_args;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::nns_types::icpts::ICPTs;
use crate::lib::operations::canister;
use crate::lib::root_key::fetch_root_key_or_anyhow;
use crate::util::clap::parsers::{cycle_amount_parser, e8s_parser, trillion_cycle_amount_parser};
use crate::util::currency_conversion::as_cycles_with_current_exchange_rate;
use candid::Principal;
use clap::Parser;
use dfx_core::identity::CallSender;
use fn_error_context::context;
use slog::info;

const DEFAULT_CYCLES_TO_FABRICATE: u128 = 10_000_000_000_000_u128;

/// Local development only: Fabricate cycles out of thin air and deposit them into the specified canister(s).
/// Can specify a number of ICP/e8s (which will be converted to cycles using the current exchange rate) or a number of cycles.
/// If no amount is specified, 10T cycles are added.
#[derive(Parser)]
pub struct FabricateCyclesOpts {
    /// Specifies the amount of cycles to fabricate.
    #[arg(
        long,
        value_parser = cycle_amount_parser,
        conflicts_with("t"),
        conflicts_with("amount"),
        conflicts_with("icp"),
        conflicts_with("e8s")
    )]
    cycles: Option<u128>,

    /// ICP to mint into cycles and deposit into destination canister
    /// Can be specified as a Decimal with the fractional portion up to 8 decimal places
    /// i.e. 100.012
    #[arg(
        long,
        conflicts_with("cycles"),
        conflicts_with("icp"),
        conflicts_with("e8s"),
        conflicts_with("t")
    )]
    amount: Option<ICPTs>,

    /// Specify ICP as a whole number, helpful for use in conjunction with `--e8s`
    #[arg(
        long,
        value_parser = e8s_parser,
        conflicts_with("amount"),
        conflicts_with("cycles"),
        conflicts_with("t")
    )]
    icp: Option<u64>,

    /// Specify e8s as a whole number, helpful for use in conjunction with `--icp`
    #[arg(
        long,
        value_parser = e8s_parser,
        conflicts_with("amount"),
        conflicts_with("cycles"),
        conflicts_with("t")
    )]
    e8s: Option<u64>,

    /// Specifies the amount of trillion cycles to fabricate.
    #[arg(
        long,
        value_parser = trillion_cycle_amount_parser,
        conflicts_with("amount")
    )]
    t: Option<u128>,

    /// Specifies the name or id of the canister to receive the cycles deposit.
    /// You must specify either a canister name/id or the --all option.
    #[arg(long, required_unless_present("all"))]
    canister: Option<String>,

    /// Deposit cycles to all of the canisters configured in the dfx.json file.
    #[arg(long, required_unless_present("canister"))]
    all: bool,
}

#[context("Failed to deposit {} cycles into canister '{}'.", cycles, canister)]
async fn deposit_minted_cycles(
    env: &dyn Environment,
    canister: &str,
    call_sender: &CallSender,
    cycles: u128,
) -> DfxResult {
    let log = env.get_logger();
    let canister_id_store = env.get_canister_id_store()?;
    let canister_id =
        Principal::from_text(canister).or_else(|_| canister_id_store.get(canister))?;

    info!(log, "Fabricating {} cycles onto {}", cycles, canister,);

    canister::provisional_deposit_cycles(env, canister_id, call_sender, cycles).await?;

    let status = canister::get_canister_status(env, canister_id, call_sender).await;
    if status.is_ok() {
        info!(
            log,
            "Fabricated {} cycles, updated balance: {} cycles",
            cycles,
            status.unwrap().cycles
        );
    } else {
        info!(log, "Fabricated {} cycles.", cycles);
    }

    Ok(())
}

pub async fn exec(env: &dyn Environment, opts: FabricateCyclesOpts) -> DfxResult {
    let cycles = cycles_to_fabricate(env, &opts).await?;

    fetch_root_key_or_anyhow(env).await?;

    if let Some(canister) = opts.canister.as_deref() {
        deposit_minted_cycles(env, canister, &CallSender::SelectedId, cycles).await
    } else if opts.all {
        let config = env.get_config_or_anyhow()?;
        if let Some(canisters) = &config.get_config().canisters {
            for canister in canisters.keys() {
                deposit_minted_cycles(env, canister, &CallSender::SelectedId, cycles).await?;
            }
        }
        Ok(())
    } else {
        unreachable!()
    }
}

#[context("Failed to determine amount of cycles to fabricate.")]
async fn cycles_to_fabricate(env: &dyn Environment, opts: &FabricateCyclesOpts) -> DfxResult<u128> {
    if let Some(cycles) = opts.cycles {
        Ok(cycles)
    } else if let Some(t_cycles) = opts.t {
        Ok(t_cycles)
    } else if opts.amount.is_some() || opts.icp.is_some() || opts.e8s.is_some() {
        let icpts = get_icpts_from_args(opts.amount, opts.icp, opts.e8s)?;
        let cycles = as_cycles_with_current_exchange_rate(&icpts).await?;
        let log = env.get_logger();
        info!(
            log,
            "At the current exchange rate, {} e8s produces approximately {} cycles.",
            icpts.get_e8s().to_string(),
            cycles.to_string()
        );
        Ok(cycles)
    } else {
        Ok(DEFAULT_CYCLES_TO_FABRICATE)
    }
}


-----------------------

/src/dfx/src/commands/ledger/mod.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::nns_types::icpts::ICPTs;
use anyhow::anyhow;
use clap::Parser;
use fn_error_context::context;
use tokio::runtime::Runtime;

mod account_id;
mod balance;
pub mod create_canister;
mod fabricate_cycles;
mod notify;
pub mod show_subnet_types;
mod top_up;
mod transfer;

/// Ledger commands.
#[derive(Parser)]
#[command(name = "ledger")]
pub struct LedgerOpts {
    #[command(flatten)]
    network: NetworkOpt,

    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
enum SubCommand {
    AccountId(account_id::AccountIdOpts),
    Balance(balance::BalanceOpts),
    CreateCanister(create_canister::CreateCanisterOpts),
    FabricateCycles(fabricate_cycles::FabricateCyclesOpts),
    Notify(notify::NotifyOpts),
    ShowSubnetTypes(show_subnet_types::ShowSubnetTypesOpts),
    TopUp(top_up::TopUpOpts),
    Transfer(transfer::TransferOpts),
}

pub fn exec(env: &dyn Environment, opts: LedgerOpts) -> DfxResult {
    let agent_env = create_agent_environment(env, opts.network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async {
        match opts.subcmd {
            SubCommand::AccountId(v) => account_id::exec(&agent_env, v).await,
            SubCommand::Balance(v) => balance::exec(&agent_env, v).await,
            SubCommand::CreateCanister(v) => create_canister::exec(&agent_env, v).await,
            SubCommand::FabricateCycles(v) => fabricate_cycles::exec(&agent_env, v).await,
            SubCommand::Notify(v) => notify::exec(&agent_env, v).await,
            SubCommand::ShowSubnetTypes(v) => show_subnet_types::exec(&agent_env, v).await,
            SubCommand::TopUp(v) => top_up::exec(&agent_env, v).await,
            SubCommand::Transfer(v) => transfer::exec(&agent_env, v).await,
        }
    })
}

#[context("Failed to determine icp amount from supplied arguments.")]
pub(crate) fn get_icpts_from_args(
    amount: Option<ICPTs>,
    icp: Option<u64>,
    e8s: Option<u64>,
) -> DfxResult<ICPTs> {
    match amount {
        None => {
            let icp = match icp {
                Some(icps) => ICPTs::from_icpts(icps).map_err(|err| anyhow!(err))?,
                None => ICPTs::from_e8s(0),
            };
            let icp_from_e8s = match e8s {
                Some(e8s) => ICPTs::from_e8s(e8s),
                None => ICPTs::from_e8s(0),
            };
            let amount = icp + icp_from_e8s;
            Ok(amount.map_err(|err| anyhow!(err))?)
        }
        Some(amount) => Ok(amount),
    }
}


-----------------------

/src/dfx/src/commands/ledger/notify.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::{Parser, Subcommand};

mod convert;
mod create_canister;
mod top_up;

/// Notify the ledger about a send transaction to the cycles minting canister.
/// This command should only be used if `dfx ledger create-canister` or `dfx ledger top-up`
/// successfully sent a message to the ledger, and a transaction was recorded at some block height, but
/// for some reason the subsequent notify failed.
#[derive(Parser)]
pub struct NotifyOpts {
    #[command(subcommand)]
    subcmd: Subcmd,
}

#[derive(Subcommand)]
pub enum Subcmd {
    CreateCanister(create_canister::NotifyCreateOpts),
    Convert(convert::ConvertOpts),
    TopUp(top_up::NotifyTopUpOpts),
}

pub async fn exec(env: &dyn Environment, opts: NotifyOpts) -> DfxResult {
    match opts.subcmd {
        Subcmd::CreateCanister(opts) => create_canister::exec(env, opts).await,
        Subcmd::Convert(opts) => convert::exec(env, opts).await,
        Subcmd::TopUp(opts) => top_up::exec(env, opts).await,
    }
}


-----------------------

/src/dfx/src/commands/ledger/notify/convert.rs:
-----------------------

use crate::lib::error::NotifyMintCyclesError;
use crate::lib::ledger_types::NotifyError::{self};
use crate::lib::ledger_types::NotifyMintCyclesSuccess;
use crate::lib::operations::cmc::notify_mint_cycles;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::lib::{environment::Environment, error::DfxResult};
use crate::util::clap::parsers::icrc_subaccount_parser;
use anyhow::bail;
use clap::Parser;
use icrc_ledger_types::icrc1::account::Subaccount as ICRCSubaccount;
use icrc_ledger_types::icrc1::transfer::Memo as ICRCMemo;

#[derive(Parser)]
pub struct ConvertOpts {
    /// BlockHeight at which the send transaction was recorded.
    block_height: u64,

    /// Subaccount where the minted cycles are deposited.
    #[arg(long, value_parser = icrc_subaccount_parser)]
    to_subaccount: Option<ICRCSubaccount>,

    /// Memo used when depositing the minted cycles.
    #[arg(long)]
    deposit_memo: Option<u64>,
}

pub async fn exec(env: &dyn Environment, opts: ConvertOpts) -> DfxResult {
    let block_height = opts.block_height;

    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let result = notify_mint_cycles(
        agent,
        opts.deposit_memo.map(ICRCMemo::from),
        opts.to_subaccount,
        block_height,
    )
    .await;

    match result {
        Ok(NotifyMintCyclesSuccess {
            minted, balance, ..
        }) => {
            println!(
                "Canister was topped up with {minted} cycles! New balance is {balance} cycles."
            );
        }
        Err(NotifyMintCyclesError::Notify(NotifyError::Refunded {
            reason,
            block_index,
        })) => match block_index {
            Some(height) => {
                println!("Refunded at block height {height} with message: {reason}")
            }
            None => println!("Refunded with message: {reason}"),
        },
        Err(other) => bail!("{other:?}"),
    };
    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/notify/create_canister.rs:
-----------------------

use crate::lib::error::NotifyCreateCanisterError::Notify;
use crate::lib::ledger_types::NotifyError::Refunded;
use crate::lib::operations::cmc::notify_create;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::lib::{environment::Environment, error::DfxResult};
use crate::util::clap::subnet_selection_opt::SubnetSelectionOpt;
use anyhow::bail;
use candid::Principal;
use clap::Parser;

#[derive(Parser)]
pub struct NotifyCreateOpts {
    /// BlockHeight at which the send transaction was recorded.
    block_height: u64,

    /// The controller of the created canister.
    controller: Principal,

    #[command(flatten)]
    subnet_selection: SubnetSelectionOpt,
}

pub async fn exec(env: &dyn Environment, opts: NotifyCreateOpts) -> DfxResult {
    let block_height = opts.block_height;
    let controller = opts.controller;

    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let subnet_selection = opts
        .subnet_selection
        .into_subnet_selection_type(env)
        .await?;
    let result = notify_create(agent, controller, block_height, subnet_selection).await;

    match result {
        Ok(principal) => {
            println!("Canister created with id: {:?}", principal.to_text());
        }
        Err(Notify(Refunded {
            reason,
            block_index,
        })) => {
            match block_index {
                Some(height) => {
                    println!("Refunded at block height {height} with message: {reason}")
                }
                None => println!("Refunded with message: {reason}"),
            };
        }
        Err(other) => bail!("{other:?}"),
    };
    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/notify/top_up.rs:
-----------------------

use crate::lib::error::NotifyTopUpError::Notify;
use crate::lib::ledger_types::NotifyError::Refunded;
use crate::lib::operations::cmc::notify_top_up;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::lib::{environment::Environment, error::DfxResult};
use anyhow::bail;
use candid::Principal;
use clap::Parser;

#[derive(Parser)]
pub struct NotifyTopUpOpts {
    /// BlockHeight at which the send transation was recorded.
    block_height: u64,

    /// The principal of the canister to top up.
    canister: Principal,
}

pub async fn exec(env: &dyn Environment, opts: NotifyTopUpOpts) -> DfxResult {
    // validated by e8s_validator
    let block_height = opts.block_height;
    let canister = opts.canister;

    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let result = notify_top_up(agent, canister, block_height).await;

    match result {
        Ok(cycles) => {
            println!("Canister {canister} topped up with {cycles} cycles");
        }
        Err(Notify(Refunded {
            reason,
            block_index,
        })) => match block_index {
            Some(height) => {
                println!("Refunded at block height {height} with message: {reason}");
            }
            None => println!("Refunded with message: {reason}"),
        },
        Err(other) => bail!("{other:?}"),
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/show_subnet_types.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::ledger_types::{GetSubnetTypesToSubnetsResult, MAINNET_CYCLE_MINTER_CANISTER_ID};
use crate::lib::root_key::fetch_root_key_if_needed;
use anyhow::Context;
use candid::{Decode, Encode, Principal};
use clap::Parser;

const GET_SUBNET_TYPES_TO_SUBNETS_METHOD: &str = "get_subnet_types_to_subnets";

/// Show available subnet types in the cycles minting canister.
#[derive(Parser)]
pub struct ShowSubnetTypesOpts {
    #[arg(long)]
    /// Canister ID of the cycles minting canister.
    cycles_minting_canister_id: Option<Principal>,
}

pub async fn exec(env: &dyn Environment, opts: ShowSubnetTypesOpts) -> DfxResult {
    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let cycles_minting_canister_id = opts
        .cycles_minting_canister_id
        .unwrap_or(MAINNET_CYCLE_MINTER_CANISTER_ID);

    let result = agent
        .update(
            &cycles_minting_canister_id,
            GET_SUBNET_TYPES_TO_SUBNETS_METHOD,
        )
        .with_arg(Encode!(&()).context("Failed to encode get_subnet_types_to_subnets arguments.")?)
        .await
        .context("get_subnet_types_to_subnets call failed.")?;
    let result = Decode!(&result, GetSubnetTypesToSubnetsResult)
        .context("Failed to decode get_subnet_types_to_subnets response")?;

    let available_subnet_types: Vec<String> = result.data.into_iter().map(|(x, _)| x).collect();

    println!("{:?}", available_subnet_types);

    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/top_up.rs:
-----------------------

use crate::commands::ledger::get_icpts_from_args;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxResult, NotifyTopUpError::Notify};
use crate::lib::ledger_types::Memo;
use crate::lib::ledger_types::NotifyError::Refunded;
use crate::lib::nns_types::account_identifier::Subaccount;
use crate::lib::nns_types::icpts::{ICPTs, TRANSACTION_FEE};
use crate::lib::operations::cmc::{notify_top_up, transfer_cmc};
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::e8s_parser;
use anyhow::{bail, Context};
use candid::Principal;
use clap::Parser;

const MEMO_TOP_UP_CANISTER: u64 = 1347768404_u64;

/// Top up a canister with cycles minted from ICP
#[derive(Parser)]
pub struct TopUpOpts {
    /// Specify the canister id or name to top up
    canister: String,

    /// Subaccount to withdraw from
    #[arg(long)]
    from_subaccount: Option<Subaccount>,

    /// ICP to mint into cycles and deposit into destination canister
    /// Can be specified as a Decimal with the fractional portion up to 8 decimal places
    /// i.e. 100.012
    #[arg(long)]
    amount: Option<ICPTs>,

    /// Specify ICP as a whole number, helpful for use in conjunction with `--e8s`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    icp: Option<u64>,

    /// Specify e8s as a whole number, helpful for use in conjunction with `--icp`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    e8s: Option<u64>,

    /// Transaction fee, default is 10000 e8s.
    #[arg(long)]
    fee: Option<ICPTs>,

    /// Max fee, default is 10000 e8s.
    #[arg(long)]
    max_fee: Option<ICPTs>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction-deduplication, default is system-time. // https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,
}

pub async fn exec(env: &dyn Environment, opts: TopUpOpts) -> DfxResult {
    let amount = get_icpts_from_args(opts.amount, opts.icp, opts.e8s)?;

    let fee = opts.fee.unwrap_or(TRANSACTION_FEE);

    let memo = Memo(MEMO_TOP_UP_CANISTER);

    let to = Principal::from_text(&opts.canister)
        .or_else(|_| env.get_canister_id_store()?.get(&opts.canister))
        .with_context(|| {
            format!(
                "Failed to parse {:?} as target canister principal or name.",
                &opts.canister
            )
        })?;

    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let height = transfer_cmc(
        agent,
        env.get_logger(),
        memo,
        amount,
        fee,
        opts.from_subaccount,
        to,
        opts.created_at_time,
    )
    .await?;
    println!("Using transfer at block height {height}");
    let result = notify_top_up(agent, to, height).await;

    match result {
        Ok(cycles) => {
            println!("Canister was topped up with {cycles} cycles!");
        }
        Err(Notify(Refunded {
            reason,
            block_index,
        })) => match block_index {
            Some(height) => {
                println!("Refunded at block height {height} with message: {reason}")
            }
            None => println!("Refunded with message: {reason}"),
        },
        Err(other) => bail!("{other:?}"),
    };
    Ok(())
}


-----------------------

/src/dfx/src/commands/ledger/transfer.rs:
-----------------------

use crate::commands::ledger::get_icpts_from_args;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::ledger_types::{Memo, MAINNET_LEDGER_CANISTER_ID};
use crate::lib::nns_types::account_identifier::{AccountIdentifier, Subaccount};
use crate::lib::nns_types::icpts::{ICPTs, TRANSACTION_FEE};
use crate::lib::operations::ledger::transfer;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::clap::parsers::{e8s_parser, memo_parser};
use anyhow::{anyhow, Context};
use candid::Principal;
use clap::Parser;
use std::str::FromStr;

/// Transfer ICP from the user to the destination account identifier.
#[derive(Parser)]
pub struct TransferOpts {
    /// AccountIdentifier of transfer destination.
    to: String,

    /// Subaccount to transfer from.
    #[arg(long)]
    from_subaccount: Option<Subaccount>,

    /// ICPs to transfer to the destination AccountIdentifier
    /// Can be specified as a Decimal with the fractional portion up to 8 decimal places
    /// i.e. 100.012
    #[arg(long)]
    amount: Option<ICPTs>,

    /// Specify ICP as a whole number, helpful for use in conjunction with `--e8s`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    icp: Option<u64>,

    /// Specify e8s as a whole number, helpful for use in conjunction with `--icp`
    #[arg(long, value_parser = e8s_parser, conflicts_with("amount"))]
    e8s: Option<u64>,

    /// Specify a numeric memo for this transaction.
    #[arg(long, value_parser = memo_parser)]
    memo: u64,

    /// Transaction fee, default is 10000 e8s.
    #[arg(long)]
    fee: Option<ICPTs>,

    #[arg(long)]
    /// Canister ID of the ledger canister.
    ledger_canister_id: Option<Principal>,

    /// Transaction timestamp, in nanoseconds, for use in controlling transaction-deduplication, default is system-time. // https://internetcomputer.org/docs/current/developer-docs/integrations/icrc-1/#transaction-deduplication-
    #[arg(long)]
    created_at_time: Option<u64>,
}

pub async fn exec(env: &dyn Environment, opts: TransferOpts) -> DfxResult {
    let amount = get_icpts_from_args(opts.amount, opts.icp, opts.e8s)?;

    let fee = opts.fee.unwrap_or(TRANSACTION_FEE);

    let memo = Memo(opts.memo);

    let to = AccountIdentifier::from_str(&opts.to)
        .map_err(|e| anyhow!(e))
        .with_context(|| {
            format!(
                "Failed to parse transfer destination from string '{}'.",
                &opts.to
            )
        })?
        .to_address();

    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;

    let canister_id = opts
        .ledger_canister_id
        .unwrap_or(MAINNET_LEDGER_CANISTER_ID);

    let _block_height = transfer(
        agent,
        env.get_logger(),
        &canister_id,
        memo,
        amount,
        fee,
        opts.from_subaccount,
        to,
        opts.created_at_time,
    )
    .await?;

    Ok(())
}


-----------------------

/src/dfx/src/commands/mod.rs:
-----------------------

use crate::commands::completion::CompletionOpts;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::bail;
use clap::Subcommand;

mod beta;
mod build;
mod cache;
mod canister;
mod completion;
mod cycles;
mod deploy;
mod deps;
mod diagnose;
mod extension;
mod fix;
mod generate;
mod identity;
mod info;
mod killall;
mod language_service;
mod ledger;
mod new;
mod ping;
mod quickstart;
mod remote;
mod schema;
mod start;
mod stop;
mod toolchain;
mod upgrade;
mod wallet;

#[derive(Subcommand)]
pub enum DfxCommand {
    #[command(hide = true)]
    Beta(beta::BetaOpts),
    Build(build::CanisterBuildOpts),
    Cache(cache::CacheOpts),
    Canister(canister::CanisterOpts),
    Completion(CompletionOpts),
    Cycles(cycles::CyclesOpts),
    Deploy(deploy::DeployOpts),
    Deps(deps::DepsOpts),
    Diagnose(diagnose::DiagnoseOpts),
    Fix(fix::FixOpts),
    Extension(extension::ExtensionOpts),
    Generate(generate::GenerateOpts),
    Identity(identity::IdentityOpts),
    Info(info::InfoOpts),
    Killall(killall::KillallOpts),
    #[command(name = "_language-service")]
    LanguageServices(language_service::LanguageServiceOpts),
    Ledger(ledger::LedgerOpts),
    New(new::NewOpts),
    Ping(ping::PingOpts),
    Quickstart(quickstart::QuickstartOpts),
    Remote(remote::RemoteOpts),
    Schema(schema::SchemaOpts),
    Start(start::StartOpts),
    Stop(stop::StopOpts),
    #[command(hide = true)]
    Toolchain(toolchain::ToolchainOpts),
    #[command(hide = true)]
    Upgrade(upgrade::UpgradeOpts),
    Wallet(wallet::WalletOpts),
}

pub fn exec(env: &dyn Environment, cmd: DfxCommand) -> DfxResult {
    match cmd {
        DfxCommand::Beta(v) => beta::exec(env, v),
        DfxCommand::Build(v) => build::exec(env, v),
        DfxCommand::Cache(v) => cache::exec(env, v),
        DfxCommand::Canister(v) => canister::exec(env, v),
        DfxCommand::Completion(v) => completion::exec(env, v),
        DfxCommand::Cycles(v) => cycles::exec(env, v),
        DfxCommand::Deploy(v) => deploy::exec(env, v),
        DfxCommand::Deps(v) => deps::exec(env, v),
        DfxCommand::Diagnose(v) => diagnose::exec(env, v),
        DfxCommand::Fix(v) => fix::exec(env, v),
        DfxCommand::Extension(v) => extension::exec(env, v),
        DfxCommand::Generate(v) => generate::exec(env, v),
        DfxCommand::Identity(v) => identity::exec(env, v),
        DfxCommand::Info(v) => info::exec(env, v),
        DfxCommand::Killall(v) => killall::exec(env, v),
        DfxCommand::LanguageServices(v) => language_service::exec(env, v),
        DfxCommand::Ledger(v) => ledger::exec(env, v),
        DfxCommand::New(v) => new::exec(env, v),
        DfxCommand::Ping(v) => ping::exec(env, v),
        DfxCommand::Quickstart(v) => quickstart::exec(env, v),
        DfxCommand::Remote(v) => remote::exec(env, v),
        DfxCommand::Schema(v) => schema::exec(v),
        DfxCommand::Start(v) => start::exec(env, v),
        DfxCommand::Stop(v) => stop::exec(env, v),
        DfxCommand::Toolchain(v) => toolchain::exec(env, v),
        DfxCommand::Upgrade(v) => upgrade::exec(env, v),
        DfxCommand::Wallet(v) => wallet::exec(env, v),
    }
}

pub fn exec_without_env(cmd: DfxCommand) -> DfxResult {
    match cmd {
        DfxCommand::Schema(v) => schema::exec(v),
        _ => bail!("Cannot execute this command without environment."),
    }
}


-----------------------

/src/dfx/src/commands/new.rs:
-----------------------

use crate::config::cache::DiskBasedCache;
use crate::lib::environment::Environment;
use crate::lib::error::{DfxError, DfxResult};
use crate::lib::info::replica_rev;
use crate::lib::manifest::{get_latest_version, is_upgrade_necessary};
use crate::lib::program;
use crate::util::assets;
use crate::util::clap::parsers::project_name_parser;
use crate::util::command::direct_or_shell_command;
use anyhow::{anyhow, bail, ensure, Context, Error};
use clap::builder::PossibleValuesParser;
use clap::Parser;
use console::{style, Style};
use dfx_core::config::project_templates::{
    find_project_template, get_project_template, get_sorted_templates, project_template_cli_names,
    Category, ProjectTemplate, ProjectTemplateName, ResourceLocation,
};
use dfx_core::json::{load_json_file, save_json_file};
use dialoguer::theme::ColorfulTheme;
use dialoguer::{FuzzySelect, MultiSelect};
use fn_error_context::context;
use indicatif::HumanBytes;
use semver::Version;
use slog::{info, warn, Logger};
use std::collections::{BTreeMap, HashMap};
use std::io::{self, IsTerminal, Read};
use std::path::{Path, PathBuf};
use std::process::{Command, ExitStatus, Stdio};
use std::time::Duration;
use tar::Archive;

// const DRY_RUN: &str = "dry_run";
// const PROJECT_NAME: &str = "project_name";
const RELEASE_ROOT: &str = "https://sdk.dfinity.org";

// The dist-tag to use when getting the version from NPM.
const AGENT_JS_DEFAULT_INSTALL_DIST_TAG: &str = "latest";

// Tested on a phone tethering connection. This should be fine with
// little impact to the user, given that "new" is supposedly a
// heavy-weight operation. Thus, worst case we are utilizing the user
// expectation for the duration to have a more expensive version
// check.
const CHECK_VERSION_TIMEOUT: Duration = Duration::from_secs(2);

const BACKEND_MOTOKO: &str = "motoko";

/// Creates a new project.
#[derive(Parser)]
pub struct NewOpts {
    /// Specifies the name of the project to create.
    #[arg(value_parser = project_name_parser)]
    project_name: String,

    /// Choose the type of canister in the starter project.
    #[arg(long, value_parser=backend_project_template_name_parser())]
    r#type: Option<String>,

    /// Provides a preview the directories and files to be created without adding them to the file system.
    #[arg(long)]
    dry_run: bool,

    /// Choose the type of frontend in the starter project. Defaults to vanilla.
    #[arg(long, value_parser=frontend_project_template_name_parser(), default_missing_value = "vanilla")]
    frontend: Option<String>,

    /// Skip installing the frontend code example.
    #[arg(long, conflicts_with = "frontend")]
    no_frontend: bool,

    /// Overrides which version of the JavaScript Agent to install. By default, will contact
    /// NPM to decide.
    #[arg(long, requires("frontend"))]
    agent_version: Option<String>,

    #[arg(long, value_parser=extras_project_template_name_parser())]
    extras: Vec<String>,
}

fn backend_project_template_name_parser() -> PossibleValuesParser {
    PossibleValuesParser::new(project_template_cli_names(Category::Backend))
}

fn frontend_project_template_name_parser() -> PossibleValuesParser {
    let mut options = project_template_cli_names(Category::Frontend);
    options.push("none".to_string());
    PossibleValuesParser::new(options)
}

fn extras_project_template_name_parser() -> PossibleValuesParser {
    let mut options = project_template_cli_names(Category::Extra);
    options.push("frontend-tests".to_string());
    PossibleValuesParser::new(options)
}

enum Status<'a> {
    Create(&'a Path, usize),
    CreateDir(&'a Path),
}

impl std::fmt::Display for Status<'_> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::result::Result<(), std::fmt::Error> {
        match self {
            Status::Create(path, size) => write!(
                f,
                "{:<12} {} ({})...",
                style("CREATE").green().bold(),
                path.to_str().unwrap_or("<unknown>"),
                HumanBytes(*size as u64),
            )?,
            Status::CreateDir(path) => write!(
                f,
                "{:<12} {}...",
                style("CREATE_DIR").blue().bold(),
                path.to_str().unwrap_or("<unknown>"),
            )?,
        };

        Ok(())
    }
}

pub fn create_file(log: &Logger, path: &Path, content: &[u8], dry_run: bool) -> DfxResult {
    if !dry_run {
        if let Some(p) = path.parent() {
            std::fs::create_dir_all(p)
                .with_context(|| format!("Failed to create directory {}.", p.to_string_lossy()))?;
        }
        std::fs::write(path, content)
            .with_context(|| format!("Failed to write to {}.", path.to_string_lossy()))?;
    }

    info!(log, "{}", Status::Create(path, content.len()));
    Ok(())
}

fn json_patch_file(
    _log: &Logger,
    patch_path: &Path,
    patch_content: &[u8],
    dry_run: bool,
) -> DfxResult {
    if !dry_run {
        let patch: json_patch::Patch = serde_json::from_slice(patch_content)
            .with_context(|| format!("Failed to parse {}", patch_path.display()))?;
        let to_patch = patch_path.with_extension("json");
        ensure!(
            to_patch.exists(),
            "Failed to patch {}: not found",
            to_patch.display()
        );
        let mut value = load_json_file(&to_patch)?;
        json_patch::patch(&mut value, &patch)
            .with_context(|| format!("Failed to patch {}", to_patch.display()))?;
        save_json_file(&to_patch, &value)?;
    }
    Ok(())
}

fn patch_file(_log: &Logger, patch_path: &Path, patch_content: &[u8], dry_run: bool) -> DfxResult {
    if !dry_run {
        let patch_content = std::str::from_utf8(patch_content)
            .with_context(|| format!("Failed to parse {}", patch_path.display()))?;
        let patch = patch::Patch::from_single(patch_content)
            .map_err(|e| anyhow!("Failed to parse {}: {e}", patch_path.display()))?;
        let to_patch = patch_path.with_extension("");
        let existing_content = dfx_core::fs::read_to_string(&to_patch)?;
        let patched_content = apply_patch::apply_to(&patch, &existing_content)
            .with_context(|| format!("Failed to patch {}", to_patch.display()))?;
        dfx_core::fs::write(&to_patch, patched_content)?;
    }
    Ok(())
}

#[allow(dead_code)]
pub fn create_dir<P: AsRef<Path>>(log: &Logger, path: P, dry_run: bool) -> DfxResult {
    let path = path.as_ref();
    if path.is_dir() {
        return Ok(());
    }

    if !dry_run {
        std::fs::create_dir_all(path)
            .with_context(|| format!("Failed to create directory {}.", path.to_string_lossy()))?;
    }

    info!(log, "{}", Status::CreateDir(path));
    Ok(())
}

#[context("Failed to init git at {}.", project_name.to_string_lossy())]
pub fn init_git(log: &Logger, project_name: &Path) -> DfxResult {
    let init_status = std::process::Command::new("git")
        .arg("init")
        .current_dir(project_name)
        .stderr(Stdio::null())
        .stdout(Stdio::null())
        .status();

    if init_status.is_ok() && init_status.unwrap().success() {
        info!(log, "Creating git repository...");
        std::process::Command::new("git")
            .arg("add")
            .current_dir(project_name)
            .arg(".")
            .output()
            .context("Failed to run 'git add'.")?;
        std::process::Command::new("git")
            .arg("commit")
            .current_dir(project_name)
            .arg("-a")
            .arg("--message=Initial commit.")
            .output()
            .context("Failed to run 'git commit'.")?;
    }

    Ok(())
}

fn replace_variables(mut s: String, variables: &BTreeMap<String, String>) -> String {
    variables.iter().for_each(|(name, value)| {
        s = s.replace(&format!("__{name}__"), value);
    });
    s
}

#[context("Failed to unpack archive to {}.", root.to_string_lossy())]
fn write_files_from_entries<R: Sized + Read>(
    log: &Logger,
    archive: &mut Archive<R>,
    root: &Path,
    dry_run: bool,
    variables: &BTreeMap<String, String>,
) -> DfxResult {
    for entry in archive.entries()? {
        let mut file = entry?;

        if file.header().entry_type().is_dir() {
            continue;
        }

        let mut v = Vec::new();
        file.read_to_end(&mut v).map_err(DfxError::from)?;

        let v = match String::from_utf8(v) {
            Err(err) => err.into_bytes(),
            Ok(s) => replace_variables(s, variables).into_bytes(),
        };

        // Perform path replacements.
        let p = root
            .join(file.header().path()?)
            .to_str()
            .expect("Non unicode project name path.")
            .to_string();

        let p = replace_variables(p, variables);

        let p = PathBuf::from(p);
        if p.extension() == Some("json-patch".as_ref()) {
            json_patch_file(log, &p, &v, dry_run)?;
        } else if p.extension() == Some("patch".as_ref()) {
            patch_file(log, &p, &v, dry_run)?;
        } else {
            create_file(log, p.as_path(), &v, dry_run)?;
        }
    }

    Ok(())
}

#[context("Failed to scaffold frontend code.")]
fn scaffold_frontend_code(
    env: &dyn Environment,
    dry_run: bool,
    project_name: &Path,
    frontend: &ProjectTemplate,
    frontend_tests: Option<ProjectTemplate>,
    agent_version: &Option<String>,
    variables: &BTreeMap<String, String>,
) -> DfxResult {
    let log = env.get_logger();
    let node_installed = program_installed(program::NODE);
    let npm_installed = program_installed(program::NPM);

    let project_name_str = project_name
        .to_str()
        .ok_or_else(|| anyhow!("Invalid argument: project_name"))?;

    // Check if node and npm are available, and if so create the files for the frontend build.
    if node_installed && npm_installed {
        let js_agent_version = if let Some(v) = agent_version {
            v.clone()
        } else {
            get_agent_js_version_from_npm(AGENT_JS_DEFAULT_INSTALL_DIST_TAG)
                .map_err(|err| anyhow!("Cannot execute npm: {}", err))?
        };

        let mut variables = variables.clone();
        variables.insert("js_agent_version".to_string(), js_agent_version);
        variables.insert(
            "project_name_uppercase".to_string(),
            project_name_str.to_uppercase(),
        );

        write_project_template_resources(log, frontend, project_name, dry_run, &variables)?;

        if let Some(frontend_tests) = frontend_tests {
            write_project_template_resources(
                log,
                &frontend_tests,
                project_name,
                dry_run,
                &variables,
            )?;
        }

        // Only install node dependencies if we're not running in dry run.
        if !dry_run {
            run_post_create_command(env, project_name, frontend, &variables)?;
        }
    } else {
        if !node_installed {
            warn!(
                log,
                "Node could not be found. Skipping installing the frontend example code."
            );
        }
        if !npm_installed {
            warn!(
                log,
                "npm could not be found. Skipping installing the frontend example code."
            );
        }

        warn!(
            log,
            "You can bypass this check by using the --frontend flag."
        );
        write_files_from_entries(
            log,
            &mut assets::new_project_assets_files()?,
            project_name,
            dry_run,
            variables,
        )?;
    }
    Ok(())
}

fn program_installed(program: &str) -> bool {
    let result = Command::new(program).arg("--version").output();
    matches!(result, Ok(output) if output.status.success())
}

fn get_agent_js_version_from_npm(dist_tag: &str) -> DfxResult<String> {
    let output = Command::new(program::NPM)
        .arg("show")
        .arg("@dfinity/agent")
        .arg(&format!("dist-tags.{}", dist_tag))
        .stdout(Stdio::piped())
        .stderr(Stdio::inherit())
        .output()
        .context("Failed to execute 'npm show @dfinity/agent'")?;
    if !output.status.success() {
        return Err(anyhow::anyhow!(
            "npm command failed with exit code {}",
            output.status.code().unwrap_or_default()
        ));
    }
    let result = String::from_utf8_lossy(&output.stdout).trim().to_string();
    Ok(result)
}

pub fn exec(env: &dyn Environment, mut opts: NewOpts) -> DfxResult {
    let log = env.get_logger();
    let dry_run = opts.dry_run;

    let backend_template_name = if let Some(r#type) = opts.r#type {
        ProjectTemplateName(r#type)
    } else if opts.frontend.is_none() && opts.extras.is_empty() && io::stdout().is_terminal() {
        opts = get_opts_interactively(opts)?;
        ProjectTemplateName(opts.r#type.unwrap())
    } else {
        ProjectTemplateName(BACKEND_MOTOKO.to_string())
    };
    let project_name = Path::new(opts.project_name.as_str());
    if project_name.exists() {
        bail!("Cannot create a new project because the directory already exists.");
    }

    let current_version = env.get_version();
    let version_str = format!("{}", current_version);

    // It is fine for the following command to timeout or fail. We
    // drop the error.
    let latest_version = get_latest_version(RELEASE_ROOT, Some(CHECK_VERSION_TIMEOUT)).ok();

    if is_upgrade_necessary(latest_version.as_ref(), current_version) {
        warn_upgrade(log, latest_version.as_ref(), current_version);
    }

    DiskBasedCache::install(&env.get_cache().version_str())?;

    info!(
        log,
        r#"Creating new project "{}"..."#,
        project_name.display()
    );
    if dry_run {
        warn!(
            log,
            r#"Running in dry mode. Nothing will be committed to disk."#
        );
    }

    let project_name_str = project_name
        .to_str()
        .ok_or_else(|| anyhow!("Invalid argument: project_name"))?;

    let (backend_name, frontend_name) = if project_name_str.contains('-') {
        (
            format!("{project_name_str}-backend"),
            format!("{project_name_str}-frontend"),
        )
    } else {
        (
            format!("{project_name_str}_backend"),
            format!("{project_name_str}_frontend"),
        )
    };

    let variables: BTreeMap<String, String> = BTreeMap::from([
        ("project_name".to_string(), project_name_str.to_string()),
        (
            "project_name_ident".to_string(),
            project_name_str.replace('-', "_"),
        ),
        ("backend_name".to_string(), backend_name.clone()),
        (
            "backend_name_ident".to_string(),
            backend_name.replace('-', "_"),
        ),
        ("frontend_name".to_string(), frontend_name.clone()),
        (
            "frontend_name_ident".to_string(),
            frontend_name.replace('-', "_"),
        ),
        ("dfx_version".to_string(), version_str.clone()),
        ("dot".to_string(), ".".to_string()),
        ("ic_commit".to_string(), replica_rev().to_string()),
    ]);

    write_files_from_entries(
        log,
        &mut assets::new_project_base_files().context("Failed to get base project archive.")?,
        project_name,
        dry_run,
        &variables,
    )?;

    let frontend: Option<ProjectTemplate> =
        if opts.no_frontend || matches!(opts.frontend.as_ref(), Some(s) if s == "none") {
            None
        } else {
            let name = ProjectTemplateName(opts.frontend.unwrap_or("vanilla".to_string()));
            Some(get_project_template(&name))
        };

    let backend = get_project_template(&backend_template_name);

    let extras: Vec<ProjectTemplate> = opts
        .extras
        .iter()
        .filter(|s| *s != "frontend-tests")
        .map(|s| get_project_template(&ProjectTemplateName(s.clone())))
        .collect();

    let frontend_tests = if opts.extras.iter().any(|s| s == "frontend-tests") {
        let Some(ref frontend) = frontend else {
            bail!("Cannot add frontend tests to --no-frontend")
        };

        let template_name = ProjectTemplateName(format!("{}-tests", frontend.name.0));
        let Some(template) = find_project_template(&template_name) else {
            bail!(format!(
                "Cannot add frontend tests to --frontend-type {}",
                frontend.name.0
            ))
        };
        Some(template)
    } else {
        None
    };

    let requirements = get_requirements(&backend, frontend.as_ref(), &extras)?;
    for requirement in &requirements {
        write_project_template_resources(log, requirement, project_name, dry_run, &variables)?;
    }

    write_project_template_resources(log, &backend, project_name, dry_run, &variables)?;

    for extra in &extras {
        write_project_template_resources(log, extra, project_name, dry_run, &variables)?;
    }

    if let Some(frontend) = frontend {
        scaffold_frontend_code(
            env,
            dry_run,
            project_name,
            &frontend,
            frontend_tests,
            &opts.agent_version,
            &variables,
        )?;
    };

    if !dry_run {
        // If on mac, we should validate that XCode toolchain was installed.
        #[cfg(target_os = "macos")]
        {
            let mut should_git = true;
            if let Ok(code) = Command::new("xcode-select")
                .arg("-p")
                .stderr(Stdio::null())
                .stdout(Stdio::null())
                .status()
            {
                if !code.success() {
                    // git is not installed.
                    should_git = false;
                }
            } else {
                // Could not find XCode Toolchain on Mac, that's weird.
                should_git = false;
            }

            if should_git {
                init_git(log, project_name)?;
            }
        }

        #[cfg(not(target_os = "macos"))]
        {
            init_git(log, project_name)?;
        }

        run_post_create_command(env, project_name, &backend, &variables)?;
        for extra in extras {
            run_post_create_command(env, project_name, &extra, &variables)?
        }
        for requirement in &requirements {
            run_post_create_command(env, project_name, requirement, &variables)?;
        }
    }

    // Print welcome message.
    info!(
        log,
        // This needs to be included here because we cannot use the result of a function for
        // the format!() rule (and so it cannot be moved in the util::assets module).
        include_str!("../../assets/welcome.txt"),
        version_str,
        assets::dfinity_logo(),
        project_name_str
    );

    Ok(())
}

fn get_requirements(
    backend: &ProjectTemplate,
    frontend: Option<&ProjectTemplate>,
    extras: &[ProjectTemplate],
) -> DfxResult<Vec<ProjectTemplate>> {
    let mut requirements = vec![];

    let mut have = HashMap::new();
    have.insert(backend.name.clone(), backend.clone());
    if let Some(frontend) = frontend {
        have.insert(frontend.name.clone(), frontend.clone());
    }
    for extra in extras {
        have.insert(extra.name.clone(), extra.clone());
    }

    loop {
        let new_requirements = have
            .iter()
            .flat_map(|(_, template)| template.requirements.clone())
            .filter(|requirement| !have.contains_key(requirement))
            .collect::<Vec<_>>();

        for new_requirement in &new_requirements {
            let Some(requirement) = find_project_template(new_requirement) else {
                bail!("Did not find required project template {}", new_requirement)
            };
            have.insert(requirement.name.clone(), requirement.clone());
            requirements.push(requirement);
        }

        if new_requirements.is_empty() {
            break;
        }
    }

    Ok(requirements)
}

fn run_post_create_command(
    env: &dyn Environment,
    root: &Path,
    project_template: &ProjectTemplate,
    variables: &BTreeMap<String, String>,
) -> DfxResult {
    let log = env.get_logger();

    for command in &project_template.post_create {
        let command = replace_variables(command.clone(), variables);
        let mut cmd = direct_or_shell_command(&command, root)?;

        let spinner = project_template
            .post_create_spinner_message
            .as_ref()
            .map(|msg| env.new_spinner(msg.clone().into()));

        let status = cmd
            .stderr(Stdio::inherit())
            .stdout(Stdio::inherit())
            .status()
            .with_context(|| {
                format!(
                    "Failed to run post-create command '{}' for project template '{}.",
                    &command, &project_template.name
                )
            });

        if let Some(spinner) = spinner {
            let message = match status {
                Ok(status) if status.success() => "Done.",
                _ => "Failed.",
            };
            spinner.finish_with_message(message.into());
        }
        if let Some(warning) = &project_template.post_create_failure_warning {
            warn_on_post_create_error(log, status, &command, warning);
        } else {
            fail_on_post_create_error(command, status)?;
        }
    }
    Ok(())
}

fn warn_on_post_create_error(
    log: &Logger,
    status: Result<ExitStatus, Error>,
    command: &str,
    warning: &str,
) {
    match status {
        Ok(status) if status.success() => {}
        Ok(status) => match status.code() {
            Some(code) => {
                warn!(
                    log,
                    "Post-create command '{command}' failed with exit code {code}. {warning}",
                );
            }
            None => {
                warn!(log, "Post-create command '{command}' failed. {warning}");
            }
        },
        Err(e) => {
            warn!(
                log,
                "Failed to execute post-create command '{command}': {e}. {warning}"
            );
        }
    }
}

fn fail_on_post_create_error(
    command: String,
    status: Result<ExitStatus, Error>,
) -> Result<(), Error> {
    let status = status?;
    if !status.success() {
        match status.code() {
            Some(code) => {
                bail!("Post-create command '{command}' failed with exit code {code}.")
            }
            None => bail!("Post-create command '{command}' failed."),
        }
    }
    Ok(())
}

fn write_project_template_resources(
    logger: &Logger,
    template: &ProjectTemplate,
    project_name: &Path,
    dry_run: bool,
    variables: &BTreeMap<String, String>,
) -> DfxResult {
    let mut resources = match template.resource_location {
        ResourceLocation::Bundled { get_archive_fn } => get_archive_fn()?,
    };
    write_files_from_entries(logger, &mut resources, project_name, dry_run, variables)
}

fn get_opts_interactively(opts: NewOpts) -> DfxResult<NewOpts> {
    let theme = ColorfulTheme::default();
    let backend_templates = get_sorted_templates(Category::Backend);
    let backends_list = backend_templates
        .iter()
        .map(|t| t.display.clone())
        .collect::<Vec<_>>();

    let backend = FuzzySelect::with_theme(&theme)
        .items(&backends_list)
        .default(0)
        .with_prompt("Select a backend language:")
        .interact()?;
    let backend = &backend_templates[backend];
    let frontend_templates = get_sorted_templates(Category::Frontend);
    let mut frontends_list = frontend_templates
        .iter()
        .map(|t| t.display.clone())
        .collect::<Vec<_>>();
    frontends_list.push("None".to_string());
    let frontend = FuzzySelect::with_theme(&theme)
        .items(&frontends_list)
        .default(0)
        .with_prompt("Select a frontend framework:")
        .interact()?;
    let frontend = frontend_templates.get(frontend);

    let extra_templates: Vec<_> = get_sorted_templates(Category::Extra);
    let mut extras_display_names = extra_templates
        .iter()
        .map(|t| t.display.clone())
        .collect::<Vec<_>>();

    let mut extras_template_names = extra_templates
        .iter()
        .map(|t| t.name.0.clone())
        .collect::<Vec<_>>();
    if let Some(frontend_template) = frontend {
        let fe_tests = ProjectTemplateName(format!("{}-tests", frontend_template.name.0));
        if find_project_template(&fe_tests).is_some() {
            extras_display_names.push("Frontend tests".to_string());
            extras_template_names.push("frontend-tests".to_string());
        }
    }
    let extras = MultiSelect::with_theme(&theme)
        .items(&extras_display_names)
        .with_prompt("Add extra features (space to select, enter to confirm)")
        .interact()?;

    let extras = extras
        .into_iter()
        .map(|x| extras_template_names[x].clone())
        .collect();

    let opts = NewOpts {
        extras,
        frontend: frontend.map(|f| f.name.0.clone()),
        r#type: Some(backend.name.0.clone()),
        ..opts
    };
    Ok(opts)
}

fn warn_upgrade(log: &Logger, latest_version: Option<&Version>, current_version: &Version) {
    warn!(log, "You seem to be running an outdated version of dfx.");

    let red = Style::new().red();
    let green = Style::new().green();
    let yellow = Style::new().yellow();

    let mut version_comparison =
        format!("Current version: {}", red.apply_to(current_version.clone()));
    if let Some(v) = latest_version {
        version_comparison += format!(
            "{} latest version: {}",
            yellow.apply_to(" â†’ "),
            green.apply_to(v)
        )
        .as_str();
    }

    warn!(
        log,
        "\nYou are strongly encouraged to upgrade by running 'dfx upgrade'!"
    );
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn project_name_is_valid() {
        assert!(project_name_parser("a").is_ok());
        assert!(project_name_parser("a_").is_ok());
        assert!(project_name_parser("a_1").is_ok());
        assert!(project_name_parser("A").is_ok());
        assert!(project_name_parser("A1").is_ok());
        assert!(project_name_parser("a_good_name_").is_ok());
        assert!(project_name_parser("a_good_name").is_ok());
    }

    #[test]
    fn project_name_is_invalid() {
        assert!(project_name_parser("_a_good_name_").is_err());
        assert!(project_name_parser("__also_good").is_err());
        assert!(project_name_parser("_1").is_err());
        assert!(project_name_parser("_a").is_err());
        assert!(project_name_parser("1").is_err());
        assert!(project_name_parser("1_").is_err());
        assert!(project_name_parser("-").is_err());
        assert!(project_name_parser("_").is_err());
        assert!(project_name_parser("ðŸ•¹").is_err());
        assert!(project_name_parser("ä¸å¥½").is_err());
        assert!(project_name_parser("a:b").is_err());
    }
}


-----------------------

/src/dfx/src/commands/ping.rs:
-----------------------

use crate::lib::environment::{create_agent, Environment};
use crate::lib::error::{DfxError, DfxResult};
use anyhow::{bail, Context};
use clap::Parser;
use dfx_core::identity::Identity;
use dfx_core::network::provider::{
    command_line_provider_to_url, create_network_descriptor, get_network_context,
    LocalBindDetermination,
};
use dfx_core::util::expiry_duration;
use slog::warn;
use std::time::Duration;
use tokio::runtime::Runtime;

/// Pings an Internet Computer network and returns its status.
#[derive(Parser)]
pub struct PingOpts {
    /// The provider to use.
    /// A valid URL (starting with `http:` or `https:`) can be used here, and a special
    /// ephemeral network will be created specifically for this request. E.g.
    /// "http://localhost:12345/" is a valid network name.
    network: Option<String>,

    /// Repeatedly ping until the replica is healthy
    #[arg(long)]
    wait_healthy: bool,
}

pub fn exec(env: &dyn Environment, opts: PingOpts) -> DfxResult {
    // For ping, "provider" could either be a URL or a network name.
    // If not passed, we default to the "local" network.
    let agent_url = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        opts.network,
        None,
        LocalBindDetermination::ApplyRunningWebserverPort,
    )
    .and_then(|network_descriptor| {
        let url = network_descriptor.first_provider()?.to_string();
        Ok(url)
    })
    .or_else::<DfxError, _>(|err| {
        let logger = env.get_logger();
        warn!(logger, "{:#}", err);
        let network_name = get_network_context()?;
        let url = command_line_provider_to_url(&network_name)?;
        Ok(url)
    })?;

    let timeout = expiry_duration();
    let identity = Box::new(Identity::anonymous());
    let agent = create_agent(env.get_logger().clone(), &agent_url, identity, timeout)?;

    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async {
        if opts.wait_healthy {
            let mut retries = 0;

            loop {
                let status = agent.status().await;
                if let Ok(status) = status {
                    let healthy = match &status.replica_health_status {
                        Some(s) if s == "healthy" => true,
                        None => true,
                        _ => false,
                    };
                    if healthy {
                        println!("{}", status);
                        break;
                    } else {
                        eprintln!("{}", status);
                    }
                }
                if retries >= 60 {
                    bail!("Timed out waiting for replica to become healthy");
                }
                tokio::time::sleep(Duration::from_secs(1)).await;
                retries += 1;
            }
        } else {
            let status = agent
                .status()
                .await
                .context("Failed while waiting for agent status.")?;
            println!("{}", status);
        }

        Ok(())
    })
}


-----------------------

/src/dfx/src/commands/quickstart.rs:
-----------------------

use crate::lib::error::NotifyCreateCanisterError::Notify;
use crate::lib::ledger_types::NotifyError::Refunded;
use crate::util::clap::subnet_selection_opt::SubnetSelectionType;
use crate::{
    commands::ledger::create_canister::MEMO_CREATE_CANISTER,
    lib::{
        agent::create_agent_environment,
        environment::Environment,
        error::DfxResult,
        identity::wallet::set_wallet_id,
        ledger_types::Memo,
        nns_types::{
            account_identifier::AccountIdentifier,
            icpts::{ICPTs, TRANSACTION_FEE},
        },
        operations::{
            canister::install_canister::install_wallet,
            cmc::{notify_create, transfer_cmc},
            ledger::{balance, xdr_permyriad_per_icp},
        },
    },
    util::assets::wallet_wasm,
};
use anyhow::{bail, Context};
use candid::Principal;
use clap::Parser;
use dfx_core::identity::wallet::wallet_canister_id;
use dialoguer::{Confirm, Input};
use ic_agent::Agent;
use ic_utils::interfaces::{
    management_canister::builders::InstallMode, ManagementCanister, WalletCanister,
};
use indicatif::ProgressBar;
use num_traits::Inv;
use rust_decimal::Decimal;
use slog::Logger;
use tokio::runtime::Runtime;

/// Use the `dfx quickstart` command to perform initial one time setup for your identity and/or wallet. This command
/// can be run anytime to repeat the setup process or to be used as an informational command, printing
/// information about your ICP balance, current ICP to XDR conversion rate, and more.
///
/// If setup tasks remain, this command is equivalent to running `dfx identity set-wallet` (for importing) or
/// `dfx ledger create-canister` followed by `dfx identity deploy-wallet` (for creating), though more steps may
/// be added in future versions.
#[derive(Parser)]
pub struct QuickstartOpts;

pub fn exec(env: &dyn Environment, _: QuickstartOpts) -> DfxResult {
    let env = create_agent_environment(env, Some("ic".to_string()))?;
    let agent = env.get_agent();
    let ident = env.get_selected_identity().unwrap();
    let principal = env.get_selected_identity_principal().unwrap();
    eprintln!("Your DFX user principal: {principal}");
    let acct = AccountIdentifier::new(principal, None);
    eprintln!("Your ledger account address: {acct}");
    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async {
        let balance = balance(agent, &acct, None).await?;
        eprintln!("Your ICP balance: {balance}");
        let xdr_conversion_rate = xdr_permyriad_per_icp(agent).await?;
        let xdr_per_icp = Decimal::from_i128_with_scale(xdr_conversion_rate as i128, 4);
        let icp_per_tc = xdr_per_icp.inv();
        eprintln!("Conversion rate: 1 ICP <> {xdr_per_icp} XDR");
        let wallet = wallet_canister_id(env.get_network_descriptor(), ident)?;
        if let Some(wallet) = wallet {
            step_print_wallet(agent, wallet).await?;
        } else if Confirm::new()
            .with_prompt("Import an existing wallet?")
            .interact()?
        {
            step_import_wallet(&env, agent, ident).await?;
        } else {
            step_deploy_wallet(
                &env,
                agent,
                acct,
                ident,
                principal,
                balance.to_decimal(),
                xdr_per_icp,
                icp_per_tc,
            )
            .await?;
        }
        Ok(())
    })
}

async fn step_print_wallet(agent: &Agent, wallet: Principal) -> DfxResult {
    eprintln!("Mainnet wallet canister: {wallet}");
    if let Ok(wallet_canister) = WalletCanister::create(agent, wallet).await {
        if let Ok(balance) = wallet_canister.wallet_balance().await {
            eprintln!(
                "Mainnet wallet balance: {:.2} TC",
                Decimal::from(balance.amount) / Decimal::from(1_000_000_000_000_u64)
            );
        }
    }
    Ok(())
}

async fn step_import_wallet(env: &dyn Environment, agent: &Agent, ident: &str) -> DfxResult {
    let id = Input::<Principal>::new()
        .with_prompt("Paste the principal ID of the existing wallet")
        .interact_text()?;
    let wallet = if let Ok(wallet) = WalletCanister::create(agent, id).await {
        wallet
    } else {
        let mgmt = ManagementCanister::create(agent);
        let wasm = wallet_wasm(env.get_logger())?;
        mgmt.install_code(&id, &wasm)
            .with_mode(InstallMode::Install)
            .await?;
        WalletCanister::create(agent, id).await?
    };
    set_wallet_id(env.get_network_descriptor(), ident, id)?;
    eprintln!("Successfully imported wallet {id}.");
    if let Ok(balance) = wallet.wallet_balance().await {
        eprintln!(
            "Mainnet wallet balance: {:.2} TC",
            Decimal::from(balance.amount) / Decimal::from(1_000_000_000_000_u64)
        );
    }
    Ok(())
}

async fn step_deploy_wallet(
    env: &dyn Environment,
    agent: &Agent,
    acct: AccountIdentifier,
    ident: &str,
    ident_principal: Principal,
    balance: Decimal,
    xdr_per_icp: Decimal,
    icp_per_tc: Decimal,
) -> DfxResult {
    let possible_tc = xdr_per_icp * balance;
    let needed_tc = Decimal::new(10, 0) - possible_tc;
    if needed_tc.is_sign_positive() {
        let needed_icp = needed_tc * icp_per_tc;
        step_explain_deploy(acct, needed_icp.round_dp(8));
        return Ok(());
    }
    let to_spend = Decimal::new(10, 0) * icp_per_tc;
    let rounded = to_spend.round_dp(8);
    if !Confirm::new()
        .with_prompt(format!(
            "Spend {rounded:.8} ICP to create a new wallet with 10 TC?"
        ))
        .interact()?
    {
        eprintln!("Run this command again at any time to continue from here.");
        return Ok(());
    }
    let wallet = step_interact_ledger(agent, env.get_logger(), ident_principal, rounded).await?;
    step_finish_wallet(env, agent, wallet, ident).await?;
    Ok(())
}

async fn step_interact_ledger(
    agent: &Agent,
    logger: &Logger,
    ident_principal: Principal,
    to_spend: Decimal,
) -> DfxResult<Principal> {
    let send_spinner = ProgressBar::new_spinner();
    send_spinner.set_message(format!(
        "Sending {to_spend:.8} ICP to the cycles minting canister..."
    ));
    send_spinner.enable_steady_tick(100);
    let icpts = ICPTs::from_decimal(to_spend)?;
    let height = transfer_cmc(
        agent,
        logger,
        Memo(MEMO_CREATE_CANISTER /* ðŸ‘½ */),
        icpts,
        TRANSACTION_FEE,
        None,
        ident_principal,
        None,
    )
    .await
    .context("Failed to transfer to the cycles minting canister")?;
    send_spinner.finish_with_message(format!(
        "Sent {icpts} to the cycles minting canister at height {height}"
    ));
    let notify_spinner = ProgressBar::new_spinner();
    notify_spinner.set_message("Notifying the cycles minting canister...");
    notify_spinner.enable_steady_tick(100);
    let res = notify_create(
        agent,
        ident_principal,
        height,
        SubnetSelectionType::default(),
    )
    .await;
    let wallet = match res {
        Ok(principal) => Ok(principal),
        Err(Notify(Refunded {
            reason,
            block_index,
        })) => {
            match block_index {
                Some(height) => {
                    bail!("Refunded at block height {height} with message: {reason}")
                }
                None => bail!("Refunded with message: {reason}"),
            };
        }
        Err(err) => Err(err),
    }.with_context(|| format!("Failed to notify the CMC of the transfer. Write down that height ({height}), and once the error is fixed, use `dfx ledger notify create-canister`."))?;

    notify_spinner.finish_with_message(format!(
        "Created wallet canister with principal ID {wallet}"
    ));
    Ok(wallet)
}

async fn step_finish_wallet(
    env: &dyn Environment,
    agent: &Agent,
    wallet: Principal,
    ident: &str,
) -> DfxResult {
    let install_spinner = ProgressBar::new_spinner();
    install_spinner.set_message("Installing the wallet code to the canister...");
    install_spinner.enable_steady_tick(100);
    install_wallet(env, agent, wallet, InstallMode::Install)
        .await
        .context("Failed to install the wallet code to the canister")?;
    set_wallet_id(env.get_network_descriptor(), ident, wallet)
        .context("Failed to record the wallet's principal as your associated wallet")?;
    install_spinner.finish_with_message("Installed the wallet code to the canister");
    eprintln!("Success! Run this command again at any time to print all this information again.");
    Ok(())
}

fn step_explain_deploy(acct: AccountIdentifier, needed_icp: Decimal) {
    eprintln!("\nYou need {needed_icp:.8} more ICP to deploy a 10 TC wallet canister on mainnet.");
    eprintln!("Deposit at least {needed_icp:.8} ICP into the address {acct}, and then run this command again, to deploy a mainnet wallet.");
    eprintln!("\nAlternatively:");
    eprintln!("- If you have ICP in an NNS account, you can create a new canister through the NNS interface");
    eprintln!("- If you have a Discord account, you can request free cycles at https://faucet.dfinity.org");
    eprintln!("Either of these options will ask for your DFX user principal, listed above.");
    eprintln!("And either of these options will hand you back a wallet canister principal; when you run the command again, select the 'import an existing wallet' option.");
}


-----------------------

/src/dfx/src/commands/remote/generate_binding.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::models::canister::CanisterPool;
use anyhow::Context;
use candid_parser::utils::CandidSource;
use clap::Parser;
use slog::info;

/// Generate bindings for remote canisters from their .did declarations
#[derive(Parser)]
pub struct GenerateBindingOpts {
    /// Specifies the name of the canister to generate bindings for.
    /// You must specify either canister name/id or the --all option.
    /// Generates bindings into <canister-name>.main from <canister-name>.remote.candid
    canister: Option<String>,

    /// Builds bindings for all canisters.
    #[arg(long, required_unless_present("canister"))]
    // destructive operations (see --overwrite) can happen
    // therefore it is safer to require the explicit --all flag
    #[allow(dead_code)]
    all: bool,

    /// Overwrite main file if it already exists.
    #[arg(long)]
    overwrite: bool,
}

pub fn exec(env: &dyn Environment, opts: GenerateBindingOpts) -> DfxResult {
    let env = create_agent_environment(env, None)?;
    let config = env.get_config_or_anyhow()?;
    let log = env.get_logger();

    //collects specified canister, or all if canister is None (= --all is set)
    let canister_names = config
        .get_config()
        .get_canister_names_with_dependencies(opts.canister.as_deref())?;
    let canister_pool = CanisterPool::load(&env, false, &canister_names)?;

    for canister in canister_pool.get_canister_list() {
        let info = canister.get_info();
        if let Some(candid) = info.get_remote_candid() {
            let main_optional = info.get_main_file();
            if let Some(main) = main_optional {
                if !candid.exists() {
                    info!(
                        log,
                        "Candid file {} for canister {} does not exist. Skipping.",
                        candid.to_string_lossy(),
                        canister.get_name()
                    );
                    continue;
                }
                if main.exists() {
                    if opts.overwrite {
                        info!(
                            log,
                            "Overwriting main file {} of canister {}.",
                            main.display(),
                            canister.get_name()
                        );
                    } else {
                        info!(
                            log,
                            "Main file {} of canister {} already exists. Skipping. Use --overwrite if you want to re-create it.",
                            main.display(),
                            canister.get_name()
                        );
                        continue;
                    }
                }
                let (type_env, did_types) = CandidSource::File(&candid).load()?;
                let extension = main.extension().unwrap_or_default();
                let bindings = if extension == "mo" {
                    Some(candid_parser::bindings::motoko::compile(
                        &type_env, &did_types,
                    ))
                } else if extension == "rs" {
                    let config = candid_parser::bindings::rust::Config::new();
                    Some(candid_parser::bindings::rust::compile(
                        &config, &type_env, &did_types,
                    ))
                } else if extension == "js" {
                    Some(candid_parser::bindings::javascript::compile(
                        &type_env, &did_types,
                    ))
                } else if extension == "ts" {
                    Some(candid_parser::bindings::typescript::compile(
                        &type_env, &did_types,
                    ))
                } else {
                    info!(
                        log,
                        "Unsupported filetype found in {}.main: {}. Use one of the following: .mo, .rs, .js, .ts",
                        canister.get_name(),
                        main.display()
                    );
                    None
                };

                if let Some(bindings_string) = bindings {
                    std::fs::write(main, &bindings_string).with_context(|| {
                        format!("Failed to write bindings to {}.", main.display())
                    })?;
                    info!(
                        log,
                        "Generated {} using {} for canister {}.",
                        main.display(),
                        candid.display(),
                        canister.get_name()
                    )
                }
            } else {
                info!(
                    log,
                    "Canister {} is missing attribute 'main'. Without this attribute I do not know where to generate the bindings.",
                    canister.get_name()
                );
            }
        } else {
            info!(
                log,
                "Canister {} is not remote. Skipping.",
                canister.get_name()
            );
        }
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/remote/mod.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::network::network_opt::NetworkOpt;
use clap::Parser;

mod generate_binding;

/// Commands used to work with remote canisters
#[derive(Parser)]
pub struct RemoteOpts {
    #[command(flatten)]
    network: NetworkOpt,

    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
enum SubCommand {
    GenerateBinding(generate_binding::GenerateBindingOpts),
}

pub fn exec(env: &dyn Environment, opts: RemoteOpts) -> DfxResult {
    match opts.subcmd {
        SubCommand::GenerateBinding(v) => generate_binding::exec(env, v),
    }
}


-----------------------

/src/dfx/src/commands/schema.rs:
-----------------------

use crate::lib::{error::DfxResult, metadata::dfx::DfxMetadata};
use anyhow::Context;
use clap::{Parser, ValueEnum};
use dfx_core::config::model::dfinity::{ConfigInterface, TopLevelConfigNetworks};
use dfx_core::extension::catalog::ExtensionCatalog;
use dfx_core::extension::manifest::{ExtensionDependencies, ExtensionManifest};
use schemars::schema_for;
use std::path::PathBuf;

#[derive(ValueEnum, Clone)]
enum ForFile {
    Dfx,
    Networks,
    DfxMetadata,
    ExtensionDependencies,
    ExtensionManifest,
    ExtensionCatalog,
}

/// Prints the schema for dfx.json.
#[derive(Parser)]
pub struct SchemaOpts {
    #[arg(long, value_enum)]
    r#for: Option<ForFile>,

    /// Outputs the schema to the specified file.
    #[arg(long)]
    outfile: Option<PathBuf>,
}

pub fn exec(opts: SchemaOpts) -> DfxResult {
    let schema = match opts.r#for {
        Some(ForFile::Networks) => schema_for!(TopLevelConfigNetworks),
        Some(ForFile::DfxMetadata) => schema_for!(DfxMetadata),
        Some(ForFile::ExtensionDependencies) => schema_for!(ExtensionDependencies),
        Some(ForFile::ExtensionManifest) => schema_for!(ExtensionManifest),
        Some(ForFile::ExtensionCatalog) => schema_for!(ExtensionCatalog),
        _ => schema_for!(ConfigInterface),
    };
    let nice_schema =
        serde_json::to_string_pretty(&schema).context("Failed to produce pretty schema.")?;
    if let Some(outfile) = opts.outfile {
        std::fs::write(&outfile, nice_schema)
            .with_context(|| format!("Failed to write schema to {}.", outfile.to_string_lossy()))?;
    } else {
        println!("{}", nice_schema);
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/start.rs:
-----------------------

use crate::actors::pocketic_proxy::{signals::PortReadySubscribe, PocketIcProxyConfig};
use crate::actors::{
    start_btc_adapter_actor, start_canister_http_adapter_actor, start_pocketic_actor,
    start_pocketic_proxy_actor, start_replica_actor, start_shutdown_controller,
};
use crate::config::dfx_version_str;
use crate::error_invalid_argument;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::info::replica_rev;
use crate::lib::integrations::status::wait_for_integrations_initialized;
use crate::lib::network::id::write_network_id;
use crate::lib::replica::status::ping_and_wait;
use crate::util::get_reusable_socket_addr;
use actix::Recipient;
use anyhow::{anyhow, bail, Context, Error};
use clap::{ArgAction, Parser};
use dfx_core::{
    config::model::{
        bitcoin_adapter, canister_http_adapter,
        local_server_descriptor::{LocalNetworkScopeDescriptor, LocalServerDescriptor},
        network_descriptor::NetworkDescriptor,
        replica_config::{CachedConfig, ReplicaConfig},
        settings_digest::get_settings_digest,
    },
    fs,
    json::{load_json_file, save_json_file},
    network::provider::{create_network_descriptor, LocalBindDetermination},
};
use fn_error_context::context;
use os_str_bytes::{OsStrBytes, OsStringBytes};
use slog::{info, warn, Logger};
use std::io::Read;
use std::net::SocketAddr;
use std::path::{Path, PathBuf};
use std::process::Command;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use sysinfo::{Pid, System, SystemExt};
use tokio::runtime::Runtime;

/// Starts the local replica and a web server for the current project.
#[derive(Parser)]
pub struct StartOpts {
    /// Specifies the host name and port number to bind the frontend to.
    #[arg(long)]
    host: Option<String>,

    /// Exits the dfx leaving the replica running. Will wait until the replica replies before exiting.
    #[arg(long)]
    background: bool,

    /// Cleans the state of the current project.
    #[arg(long)]
    clean: bool,

    /// Address of bitcoind node.  Implies --enable-bitcoin.
    #[arg(long, action = ArgAction::Append, conflicts_with = "pocketic")]
    bitcoin_node: Vec<SocketAddr>,

    /// enable bitcoin integration
    #[arg(long, conflicts_with = "pocketic")]
    enable_bitcoin: bool,

    /// enable canister http requests (on by default for --pocketic)
    #[arg(long)]
    enable_canister_http: bool,

    /// The delay (in milliseconds) an update call should take. Lower values may be expedient in CI.
    #[arg(long, default_value_t = 600)]
    artificial_delay: u32,

    /// Start even if the network config was modified.
    #[arg(long, conflicts_with = "pocketic")]
    force: bool,

    /// A list of domains that can be served. These are used for canister resolution [default: localhost]
    #[arg(long)]
    domain: Vec<String>,

    /// Runs PocketIC instead of the replica
    #[clap(long, alias = "emulator")]
    pocketic: bool,
}

// The frontend webserver is brought up by the bg process; thus, the fg process
// needs to wait and verify it's up before exiting.
// Because the user may have specified to start on port 0, here we wait for
// webserver_port_path to get written to and modify the frontend_url so we
// ping the correct address.
async fn fg_ping_and_wait(
    webserver_port_path: &Path,
    frontend_url: &str,
    logger: &Logger,
    local_server_descriptor: &LocalServerDescriptor,
) -> DfxResult {
    let port = wait_for_port(webserver_port_path).await?;

    let mut frontend_url_mod = frontend_url.to_string();
    let port_offset = frontend_url_mod
        .as_str()
        .rfind(':')
        .ok_or_else(|| anyhow!("Malformed frontend url: {}", frontend_url))?;
    frontend_url_mod.replace_range((port_offset + 1).., port.as_str());
    ping_and_wait(&frontend_url_mod).await?;

    wait_for_integrations_initialized(&frontend_url_mod, logger, local_server_descriptor).await
}

async fn wait_for_port(webserver_port_path: &Path) -> DfxResult<String> {
    let mut retries = 0;
    loop {
        let tokio_file = tokio::fs::File::open(&webserver_port_path)
            .await
            .with_context(|| {
                format!("Failed to open {}.", webserver_port_path.to_string_lossy())
            })?;
        let mut std_file = tokio_file.into_std().await;
        let mut contents = String::new();
        std_file.read_to_string(&mut contents).with_context(|| {
            format!("Failed to read {}.", webserver_port_path.to_string_lossy())
        })?;
        if !contents.is_empty() {
            break Ok(contents);
        }
        if retries >= 30 {
            bail!("Timed out waiting for replica to become healthy");
        }
        tokio::time::sleep(Duration::from_secs(1)).await;
        retries += 1;
    }
}

/// Start the Internet Computer locally. Spawns a proxy to forward and
/// manage browser requests. Responsible for running the network (one
/// replica at the moment) and the proxy.
pub fn exec(
    env: &dyn Environment,
    StartOpts {
        host,
        background,
        clean,
        force,
        bitcoin_node,
        enable_bitcoin,
        enable_canister_http,
        artificial_delay,
        domain,
        pocketic,
    }: StartOpts,
) -> DfxResult {
    if !background {
        info!(
            env.get_logger(),
            "Running dfx start for version {}",
            dfx_version_str()
        );
    }
    let project_config = env.get_config()?;

    let network_descriptor_logger = if background {
        None // so we don't print it out twice
    } else {
        Some(env.get_logger().clone())
    };

    let network_descriptor = create_network_descriptor(
        project_config,
        env.get_networks_config(),
        None,
        network_descriptor_logger,
        LocalBindDetermination::AsConfigured,
    )?;

    let network_descriptor = apply_command_line_parameters(
        env.get_logger(),
        network_descriptor,
        host,
        None,
        enable_bitcoin,
        bitcoin_node,
        enable_canister_http,
        domain,
        artificial_delay,
        pocketic,
    )?;

    let local_server_descriptor = network_descriptor.local_server_descriptor()?;

    let pid_file_path = local_server_descriptor.dfx_pid_path();

    check_previous_process_running(local_server_descriptor)?;

    // As we know no start process is running in this project, we can
    // clean up the state if it is necessary.
    if clean {
        clean_state(local_server_descriptor, env.get_project_temp_dir()?)?;
    }

    let (frontend_url, address_and_port) = frontend_address(local_server_descriptor, background)?;

    fs::create_dir_all(&local_server_descriptor.data_dir_by_settings_digest())?;

    if !local_server_descriptor.network_id_path().exists() {
        write_network_id(local_server_descriptor)?;
    }
    if let LocalNetworkScopeDescriptor::Shared { network_id_path } = &local_server_descriptor.scope
    {
        fs::copy(&local_server_descriptor.network_id_path(), network_id_path)?;
        let effective_config_path_by_settings_digest =
            local_server_descriptor.effective_config_path_by_settings_digest();
        if effective_config_path_by_settings_digest.exists() {
            fs::copy(
                &effective_config_path_by_settings_digest,
                &local_server_descriptor.effective_config_path(),
            )?;
        }
    }

    clean_older_state_dirs(local_server_descriptor)?;
    let state_root = local_server_descriptor.state_dir();

    let btc_adapter_socket_holder_path = local_server_descriptor.btc_adapter_socket_holder_path();
    let canister_http_adapter_socket_holder_path =
        local_server_descriptor.canister_http_adapter_socket_holder_path();

    let pid_file_path = empty_writable_path(pid_file_path)?;
    let btc_adapter_pid_file_path =
        empty_writable_path(local_server_descriptor.btc_adapter_pid_path())?;
    let btc_adapter_config_path =
        empty_writable_path(local_server_descriptor.btc_adapter_config_path())?;
    let canister_http_adapter_pid_file_path =
        empty_writable_path(local_server_descriptor.canister_http_adapter_pid_path())?;
    let canister_http_adapter_config_path =
        empty_writable_path(local_server_descriptor.canister_http_adapter_config_path())?;
    let pocketic_proxy_pid_file_path =
        empty_writable_path(local_server_descriptor.pocketic_proxy_pid_path())?;
    let pocketic_proxy_port_file_path =
        empty_writable_path(local_server_descriptor.pocketic_proxy_port_path())?;
    let webserver_port_path = empty_writable_path(local_server_descriptor.webserver_port_path())?;

    let previous_config_path = local_server_descriptor.effective_config_path();

    // dfx info replica-port will read these port files to find out which port to use,
    // so we need to make sure only one has a valid port in it.
    let replica_config_dir = local_server_descriptor.replica_configuration_dir();
    fs::create_dir_all(&replica_config_dir)?;

    let replica_port_path = empty_writable_path(local_server_descriptor.replica_port_path())?;
    let pocketic_port_path = empty_writable_path(local_server_descriptor.pocketic_port_path())?;

    if background {
        send_background()?;
        return Runtime::new()
            .expect("Unable to create a runtime")
            .block_on(async {
                fg_ping_and_wait(
                    &webserver_port_path,
                    &frontend_url,
                    env.get_logger(),
                    local_server_descriptor,
                )
                .await
            });
    }
    local_server_descriptor.describe(env.get_logger());

    write_pid(&pid_file_path);
    fs::write(&webserver_port_path, address_and_port.port().to_string())?;

    let btc_adapter_config = configure_btc_adapter_if_enabled(
        local_server_descriptor,
        &btc_adapter_config_path,
        &btc_adapter_socket_holder_path,
    )?;
    let btc_adapter_socket_path = btc_adapter_config
        .as_ref()
        .and_then(|cfg| cfg.get_socket_path());

    let canister_http_adapter_config = configure_canister_http_adapter_if_enabled(
        local_server_descriptor,
        &canister_http_adapter_config_path,
        &canister_http_adapter_socket_holder_path,
    )?;
    let canister_http_socket_path = canister_http_adapter_config
        .as_ref()
        .and_then(|cfg| cfg.get_socket_path());
    let subnet_type = local_server_descriptor
        .replica
        .subnet_type
        .unwrap_or_default();
    let log_level = local_server_descriptor
        .replica
        .log_level
        .unwrap_or_default();

    let proxy_domains = local_server_descriptor
        .proxy
        .domain
        .clone()
        .map(|v| v.into_vec());

    let replica_config = {
        let replica_config =
            ReplicaConfig::new(&state_root, subnet_type, log_level, artificial_delay);
        let mut replica_config = if let Some(port) = local_server_descriptor.replica.port {
            replica_config.with_port(port)
        } else {
            replica_config.with_random_port(&replica_port_path)
        };
        if let Some(btc_adapter_config) = btc_adapter_config.as_ref() {
            replica_config = replica_config.with_btc_adapter_enabled();
            if let Some(btc_adapter_socket) = btc_adapter_config.get_socket_path() {
                replica_config = replica_config.with_btc_adapter_socket(btc_adapter_socket);
            }
        }
        if let Some(canister_http_adapter_config) = canister_http_adapter_config.as_ref() {
            replica_config = replica_config.with_canister_http_adapter_enabled();
            if let Some(socket_path) = canister_http_adapter_config.get_socket_path() {
                replica_config = replica_config.with_canister_http_adapter_socket(socket_path);
            }
        }
        replica_config
    };

    let effective_config = if pocketic {
        CachedConfig::pocketic(&replica_config, replica_rev().into(), None)
    } else {
        CachedConfig::replica(&replica_config, replica_rev().into())
    };

    let is_shared_network = matches!(
        &local_server_descriptor.scope,
        LocalNetworkScopeDescriptor::Shared { .. }
    );
    if is_shared_network && !pocketic {
        save_json_file(
            &local_server_descriptor.effective_config_path_by_settings_digest(),
            &effective_config,
        )?;
    } else if !clean && !force && previous_config_path.exists() {
        let previous_config = load_json_file(&previous_config_path)
            .context("Failed to read replica configuration. Rerun with `--clean`.")?;
        if !effective_config.can_share_state(&previous_config) {
            bail!(
                "The network state can't be reused with this configuration. Rerun with `--clean`."
            )
        }
    }
    save_json_file(&previous_config_path, &effective_config)?;

    let network_descriptor = network_descriptor.clone();

    let system = actix::System::new();
    let _proxy = system.block_on(async move {
        let shutdown_controller = start_shutdown_controller(env)?;

        let port_ready_subscribe: Recipient<PortReadySubscribe> = if pocketic {
            let server = start_pocketic_actor(
                env,
                replica_config,
                local_server_descriptor,
                shutdown_controller.clone(),
                pocketic_port_path,
            )?;
            server.recipient()
        } else {
            let btc_adapter_ready_subscribe = btc_adapter_config
                .map(|btc_adapter_config| {
                    start_btc_adapter_actor(
                        env,
                        btc_adapter_config_path,
                        btc_adapter_config.get_socket_path(),
                        shutdown_controller.clone(),
                        btc_adapter_pid_file_path,
                    )
                })
                .transpose()?;
            let canister_http_adapter_ready_subscribe = canister_http_adapter_config
                .map(|canister_http_adapter_config| {
                    start_canister_http_adapter_actor(
                        env,
                        canister_http_adapter_config_path,
                        canister_http_adapter_config.get_socket_path(),
                        shutdown_controller.clone(),
                        canister_http_adapter_pid_file_path,
                    )
                })
                .transpose()?;

            let replica = start_replica_actor(
                env,
                replica_config,
                local_server_descriptor,
                shutdown_controller.clone(),
                btc_adapter_ready_subscribe,
                canister_http_adapter_ready_subscribe,
            )?;
            replica.recipient()
        };

        let pocketic_proxy_config = PocketIcProxyConfig {
            bind: address_and_port,
            replica_url: None,
            fetch_root_key: !network_descriptor.is_ic,
            domains: proxy_domains,
            verbose: env.get_verbose_level() > 0,
        };
        let proxy = start_pocketic_proxy_actor(
            env,
            pocketic_proxy_config,
            Some(port_ready_subscribe),
            shutdown_controller,
            pocketic_proxy_pid_file_path,
            pocketic_proxy_port_file_path,
        )?;
        Ok::<_, Error>(proxy)
    })?;
    system.run()?;

    if let Some(btc_adapter_socket_path) = btc_adapter_socket_path {
        let _ = std::fs::remove_file(btc_adapter_socket_path);
    }
    if let Some(canister_http_socket_path) = canister_http_socket_path {
        let _ = std::fs::remove_file(canister_http_socket_path);
    }

    Ok(())
}

fn clean_older_state_dirs(local_server_descriptor: &LocalServerDescriptor) -> DfxResult {
    let directories_to_keep = 10;
    let settings_digest = local_server_descriptor.settings_digest.as_ref().unwrap();

    let data_dir = &local_server_descriptor.data_directory;
    if !data_dir.is_dir() {
        return Ok(());
    }
    let mut state_dirs = fs::read_dir(data_dir)?
        .filter_map(|e| match e {
            Ok(entry) if is_candidate_state_dir(&entry.path(), settings_digest) => {
                Some(Ok(entry.path()))
            }
            Ok(_) => None,
            Err(e) => Some(Err(e)),
        })
        .collect::<Result<Vec<_>, _>>()?;

    // keep the X most recent directories
    state_dirs.sort_by_cached_key(|p| {
        p.metadata()
            .map(|m| m.modified().unwrap_or(SystemTime::UNIX_EPOCH))
            .unwrap_or(SystemTime::UNIX_EPOCH)
    });
    state_dirs = state_dirs
        .iter()
        .rev()
        .skip(directories_to_keep)
        .cloned()
        .collect();

    for dir in state_dirs {
        fs::remove_dir_all(&dir)?;
    }
    Ok(())
}

fn is_candidate_state_dir(path: &Path, settings_digest: &str) -> bool {
    path.is_dir()
        && path
            .file_name()
            .map(|f| {
                let filename: String = f.to_string_lossy().into();
                filename != *settings_digest
            })
            .unwrap_or(true)
}

pub fn apply_command_line_parameters(
    logger: &Logger,
    network_descriptor: NetworkDescriptor,
    host: Option<String>,
    replica_port: Option<String>,
    enable_bitcoin: bool,
    bitcoin_nodes: Vec<SocketAddr>,
    enable_canister_http: bool,
    domain: Vec<String>,
    artificial_delay: u32,
    pocketic: bool,
) -> DfxResult<NetworkDescriptor> {
    if enable_canister_http {
        warn!(
            logger,
            "The --enable-canister-http parameter is deprecated."
        );
        warn!(logger, "Canister HTTP suppport is enabled by default.  It can be disabled through dfx.json or networks.json.");
    }

    let _ = network_descriptor.local_server_descriptor()?;
    let mut local_server_descriptor = network_descriptor.local_server_descriptor.unwrap();

    if let Some(host) = host {
        let host: SocketAddr = host
            .parse()
            .map_err(|e| anyhow!("Invalid argument: Invalid host: {}", e))?;
        local_server_descriptor = local_server_descriptor.with_bind_address(host);
    }
    if let Some(replica_port) = replica_port {
        let replica_port: u16 = replica_port
            .parse()
            .map_err(|err| error_invalid_argument!("Invalid port number: {}", err))?;
        local_server_descriptor = local_server_descriptor.with_replica_port(replica_port);
    }
    if enable_bitcoin || !bitcoin_nodes.is_empty() {
        local_server_descriptor = local_server_descriptor.with_bitcoin_enabled();
    }

    if !bitcoin_nodes.is_empty() {
        local_server_descriptor = local_server_descriptor.with_bitcoin_nodes(bitcoin_nodes)
    }

    if !domain.is_empty() {
        local_server_descriptor = local_server_descriptor.with_proxy_domains(domain)
    }

    let settings_digest = get_settings_digest(
        replica_rev(),
        &local_server_descriptor,
        artificial_delay,
        pocketic,
    );

    local_server_descriptor = local_server_descriptor.with_settings_digest(settings_digest);

    Ok(NetworkDescriptor {
        local_server_descriptor: Some(local_server_descriptor),
        ..network_descriptor
    })
}

#[context("Failed to clean existing replica state.")]
fn clean_state(
    local_server_descriptor: &LocalServerDescriptor,
    temp_dir: Option<PathBuf>,
) -> DfxResult {
    if local_server_descriptor.data_directory.is_dir() {
        fs::remove_dir_all(&local_server_descriptor.data_directory).with_context(|| {
            format!(
                "Cannot remove directory at '{}'",
                local_server_descriptor.data_directory.display()
            )
        })?;
    }

    if let Some(temp_dir) = temp_dir {
        let local_dir = temp_dir.join("local");
        if local_dir.is_dir() {
            fs::remove_dir_all(&local_dir).with_context(|| {
                format!("Cannot remove directory at '{}'.", local_dir.display())
            })?;
        }
    }
    Ok(())
}

#[context("Failed to spawn background dfx.")]
fn send_background() -> DfxResult<()> {
    // Background strategy is different; we spawn `dfx` with the same arguments
    // (minus --background), ping and exit.
    let exe = std::env::current_exe().context("Failed to get current executable.")?;
    let mut cmd = Command::new(exe);
    // Skip 1 because arg0 is this executable's path.
    cmd.args(
        std::env::args()
            .skip(1)
            .filter(|a| !a.eq("--background"))
            .filter(|a| !a.eq("--clean")),
    );

    cmd.spawn().context("Failed to spawn child process.")?;
    Ok(())
}

#[context("Failed to get frontend address.")]
fn frontend_address(
    local_server_descriptor: &LocalServerDescriptor,
    background: bool,
) -> DfxResult<(String, SocketAddr)> {
    let mut address_and_port = local_server_descriptor.bind_address;

    if !background {
        // Since the user may have provided port "0", we need to grab a dynamically
        // allocated port and construct a resuable SocketAddr which the actix
        // HttpServer will bind to
        address_and_port =
            get_reusable_socket_addr(address_and_port.ip(), address_and_port.port())?;
    }
    let ip = if address_and_port.is_ipv6() {
        format!("[{}]", address_and_port.ip())
    } else {
        address_and_port.ip().to_string()
    };
    let frontend_url = format!("http://{}:{}", ip, address_and_port.port());
    Ok((frontend_url, address_and_port))
}

fn check_previous_process_running(
    local_server_descriptor: &LocalServerDescriptor,
) -> DfxResult<()> {
    for pid_path in local_server_descriptor.dfx_pid_paths() {
        if pid_path.exists() {
            // Read and verify it's not running. If it is just return.
            if let Ok(s) = std::fs::read_to_string(&pid_path) {
                if let Ok(pid) = s.parse::<Pid>() {
                    // If we find the pid in the file, we tell the user and don't start!
                    let mut system = System::new();
                    system.refresh_processes();
                    if let Some(_process) = system.process(pid) {
                        bail!("dfx is already running.");
                    }
                }
            }
        }
    }
    Ok(())
}

fn write_pid(pid_file_path: &Path) {
    if let Ok(pid) = sysinfo::get_current_pid() {
        let _ = std::fs::write(pid_file_path, pid.to_string());
    }
}

#[context("Failed to configure btc adapter.")]
pub fn configure_btc_adapter_if_enabled(
    local_server_descriptor: &LocalServerDescriptor,
    config_path: &Path,
    uds_holder_path: &Path,
) -> DfxResult<Option<bitcoin_adapter::Config>> {
    if !local_server_descriptor.bitcoin.enabled {
        return Ok(None);
    };

    let log_level = local_server_descriptor.bitcoin.log_level;

    let nodes = if let Some(ref nodes) = local_server_descriptor.bitcoin.nodes {
        nodes.clone()
    } else {
        bitcoin_adapter::default_nodes()
    };

    let config = write_btc_adapter_config(uds_holder_path, config_path, nodes, log_level)?;
    Ok(Some(config))
}

#[context("Failed to create persistent socket path for {} at {}.", prefix, uds_holder_path.to_string_lossy())]
fn create_new_persistent_socket_path(uds_holder_path: &Path, prefix: &str) -> DfxResult<PathBuf> {
    let pid = sysinfo::get_current_pid()
        .map_err(|s| anyhow!("Unable to obtain pid of current process: {}", s))?;
    let timestamp_seconds = SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .unwrap()
        .as_secs();

    // Unix domain socket names can only be so long.
    // An attempt to use a path under .dfx/ resulted in this error:
    //    path must be shorter than libc::sockaddr_un.sun_path
    let uds_path = std::env::temp_dir().join(format!("{}.{}.{}", prefix, pid, timestamp_seconds));
    std::fs::write(uds_holder_path, uds_path.to_raw_bytes()).with_context(|| {
        format!(
            "unable to write unix domain socket path to {}",
            uds_holder_path.to_string_lossy()
        )
    })?;
    Ok(uds_path)
}

#[context("Failed to get persistent socket path for {} at {}.", prefix, uds_holder_path.to_string_lossy())]
fn get_persistent_socket_path(uds_holder_path: &Path, prefix: &str) -> DfxResult<PathBuf> {
    if let Ok(uds_path) = std::fs::read(uds_holder_path) {
        Ok(PathBuf::assert_from_raw_vec(uds_path))
    } else {
        create_new_persistent_socket_path(uds_holder_path, prefix)
    }
}

#[context("Failed to write btc adapter config to {}.", config_path.to_string_lossy())]
fn write_btc_adapter_config(
    uds_holder_path: &Path,
    config_path: &Path,
    nodes: Vec<SocketAddr>,
    log_level: bitcoin_adapter::BitcoinAdapterLogLevel,
) -> DfxResult<bitcoin_adapter::Config> {
    let socket_path = get_persistent_socket_path(uds_holder_path, "ic-btc-adapter-socket")?;

    let adapter_config = bitcoin_adapter::Config::new(nodes, socket_path, log_level);

    let contents = serde_json::to_string_pretty(&adapter_config)
        .context("Unable to serialize btc adapter configuration to json")?;
    std::fs::write(config_path, contents).with_context(|| {
        format!(
            "Unable to write btc adapter configuration to {}",
            config_path.to_string_lossy()
        )
    })?;

    Ok(adapter_config)
}

pub fn empty_writable_path(path: PathBuf) -> DfxResult<PathBuf> {
    std::fs::write(&path, "")
        .with_context(|| format!("Unable to write to {}", path.to_string_lossy()))?;
    Ok(path)
}

#[context("Failed to configure canister http adapter.")]
pub fn configure_canister_http_adapter_if_enabled(
    local_server_descriptor: &LocalServerDescriptor,
    config_path: &Path,
    uds_holder_path: &Path,
) -> DfxResult<Option<canister_http_adapter::Config>> {
    if !local_server_descriptor.canister_http.enabled {
        return Ok(None);
    };

    let socket_path =
        get_persistent_socket_path(uds_holder_path, "ic-https-outcalls-adapter-socket")?;

    let log_level = local_server_descriptor.canister_http.log_level;
    let adapter_config = canister_http_adapter::Config::new(socket_path, log_level);

    let contents = serde_json::to_string_pretty(&adapter_config)
        .context("Unable to serialize canister http adapter configuration to json")?;
    std::fs::write(config_path, contents)
        .with_context(|| format!("Unable to write {}", config_path.to_string_lossy()))?;

    Ok(Some(adapter_config))
}


-----------------------

/src/dfx/src/commands/stop.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::bail;
use clap::Parser;
use dfx_core::network::provider::{create_network_descriptor, LocalBindDetermination};
use std::time::Duration;
use sysinfo::{Pid, Process, ProcessExt, Signal, System, SystemExt};

/// Stops the local network replica.
#[derive(Parser)]
pub struct StopOpts {}

fn list_all_descendants<'a>(system: &'a System, proc: &'a Process) -> Vec<&'a Process> {
    let mut result = Vec::new();

    for process in system.processes().values() {
        if let Some(ppid) = process.parent() {
            if ppid == proc.pid() {
                result.extend(list_all_descendants(system, process));
            }
        }
    }
    result.push(proc);

    result
}

/// Recursively list all descendants of a process.
fn descendant_pids(system: &System, proc: &Process) -> Vec<Pid> {
    let processes = list_all_descendants(system, proc);
    processes.iter().map(|proc| proc.pid()).collect()
}

fn wait_until_all_exited(mut system: System, mut pids: Vec<Pid>) -> DfxResult {
    let mut retries = 0;

    loop {
        system.refresh_processes();

        pids.retain(|&pid| system.process(pid).is_some());

        if pids.is_empty() {
            return Ok(());
        }
        if retries >= 30 {
            let remaining = pids
                .iter()
                .map(|pid| format!("{pid}"))
                .collect::<Vec<_>>()
                .join(" ");
            bail!("Failed to kill all processes.  Remaining: {remaining}");
        }
        std::thread::sleep(Duration::from_secs(1));
        retries += 1;
    }
}

pub fn exec(env: &dyn Environment, _opts: StopOpts) -> DfxResult {
    let network_descriptor = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        None,
        Some(env.get_logger().clone()),
        LocalBindDetermination::AsConfigured,
    )?;

    let mut found = false;
    for pid_file_path in network_descriptor
        .local_server_descriptor()?
        .dfx_pid_paths()
    {
        if pid_file_path.exists() {
            // Read and verify it's not running. If it is just return.
            if let Ok(s) = std::fs::read_to_string(&pid_file_path) {
                if let Ok(pid) = s.parse::<Pid>() {
                    found = true;
                    let mut system = System::new();
                    system.refresh_processes();
                    let descendant_pids = if let Some(proc) = system.process(pid) {
                        let descendants = descendant_pids(&system, proc);
                        proc.kill_with(Signal::Term);
                        descendants
                    } else {
                        vec![]
                    };

                    wait_until_all_exited(system, descendant_pids)?;
                }
            }
            // We ignore errors here because there is no effect for the user. We're just being nice.
            let _ = std::fs::remove_file(&pid_file_path);
        }
    }
    if !found {
        eprintln!("No local network replica found. Nothing to do.");
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/toolchain/mod.rs:
-----------------------

use crate::lib::dfxvm::display_dfxvm_installation_instructions;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::bail;
use clap::Parser;
use std::ffi::OsString;

/// Manage the dfx toolchains (obsolete)
#[derive(Parser)]
#[command(name = "toolchain", disable_help_flag = true)]
pub struct ToolchainOpts {
    #[arg(allow_hyphen_values = true)]
    _params: Vec<OsString>,
}

pub fn exec(_env: &dyn Environment, _opts: ToolchainOpts) -> DfxResult {
    println!("The toolchain command has been removed.");
    println!("Please use the dfx version manager (dfxvm) to manage dfx versions.");
    println!();
    display_dfxvm_installation_instructions();
    println!();
    bail!("toolchain command removed");
}


-----------------------

/src/dfx/src/commands/upgrade.rs:
-----------------------

use crate::lib::dfxvm::display_dfxvm_installation_instructions;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::bail;
use clap::Parser;

/// Upgrade DFX (removed: use https://github.com/dfinity/dfxvm instead)
#[derive(Parser)]
pub struct UpgradeOpts {
    /// Current Version.
    #[arg(long)]
    current_version: Option<String>,

    #[arg(long, default_value = "https://sdk.dfinity.org", hide = true)]
    release_root: String,
}

pub fn exec(_env: &dyn Environment, _opts: UpgradeOpts) -> DfxResult {
    println!(
        "dfx upgrade has been removed. Please use the dfx version manager (dfxvm) to upgrade."
    );
    println!();
    display_dfxvm_installation_instructions();
    println!();
    bail!("dfx upgrade has been removed");
}


-----------------------

/src/dfx/src/commands/wallet/add_controller.rs:
-----------------------

use crate::commands::wallet::wallet_update;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::Context;
use candid::Principal;
use clap::Parser;

/// Add a wallet controller.
#[derive(Parser)]
pub struct AddControllerOpts {
    /// Principal of the controller to add.
    controller: String,
}

pub async fn exec(env: &dyn Environment, opts: AddControllerOpts) -> DfxResult {
    let controller = Principal::from_text(&opts.controller).with_context(|| {
        format!(
            "Failed to parse {:?} as controller principal.",
            opts.controller
        )
    })?;
    wallet_update(env, "add_controller", controller).await?;
    println!("Added {} as a controller.", controller);
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/authorize.rs:
-----------------------

use crate::commands::wallet::wallet_update;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::Context;
use candid::Principal;
use clap::Parser;

/// Authorize a wallet custodian.
#[derive(Parser)]
pub struct AuthorizeOpts {
    /// Principal of the custodian to authorize.
    custodian: String,
}

pub async fn exec(env: &dyn Environment, opts: AuthorizeOpts) -> DfxResult {
    let custodian = Principal::from_text(&opts.custodian).with_context(|| {
        format!(
            "Failed to parse {:?} as custodian principal.",
            opts.custodian
        )
    })?;
    wallet_update(env, "authorize", custodian).await?;
    println!("Authorized {} as a custodian.", custodian);
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/balance.rs:
-----------------------

use crate::commands::wallet::get_wallet;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::util::{format_as_trillions, pretty_thousand_separators};
use anyhow::Context;
use clap::Parser;

/// Get the cycle balance of the selected Identity's cycles wallet.
#[derive(Parser)]
pub struct WalletBalanceOpts {
    /// Get balance raw value (without upscaling to trillions of cycles).
    #[arg(long)]
    precise: bool,
}

pub async fn exec(env: &dyn Environment, opts: WalletBalanceOpts) -> DfxResult {
    let balance = get_wallet(env)
        .await?
        .wallet_balance()
        .await
        .context("Failed to fetch wallet balance.")?;

    if opts.precise {
        println!("{} cycles.", balance.amount);
    } else {
        println!(
            "{} TC (trillion cycles).",
            pretty_thousand_separators(format_as_trillions(balance.amount))
        );
    }

    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/controllers.rs:
-----------------------

use crate::commands::wallet::wallet_query;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use candid::Principal;
use clap::Parser;

/// List the wallet's controllers.
#[derive(Parser)]
pub struct ControllersOpts {}

pub async fn exec(env: &dyn Environment, _opts: ControllersOpts) -> DfxResult {
    let (controllers,): (Vec<Principal>,) = wallet_query(env, "get_controllers", ()).await?;
    for controller in controllers.iter() {
        println!("{}", controller);
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/custodians.rs:
-----------------------

use crate::commands::wallet::wallet_query;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use candid::Principal;
use clap::Parser;

/// List the wallet's custodians.
#[derive(Parser)]
pub struct CustodiansOpts {}

pub async fn exec(env: &dyn Environment, _opts: CustodiansOpts) -> DfxResult {
    let (custodians,): (Vec<Principal>,) = wallet_query(env, "get_custodians", ()).await?;
    for custodian in custodians.iter() {
        println!("{}", custodian);
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/deauthorize.rs:
-----------------------

use crate::commands::wallet::wallet_update;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::Context;
use candid::Principal;
use clap::Parser;

/// Deauthorize a wallet custodian.
#[derive(Parser)]
pub struct DeauthorizeOpts {
    /// Principal of the custodian to deauthorize.
    custodian: String,
}

pub async fn exec(env: &dyn Environment, opts: DeauthorizeOpts) -> DfxResult {
    let custodian = Principal::from_text(&opts.custodian).with_context(|| {
        format!(
            "Failed to parse {:?} as custodian principal.",
            opts.custodian
        )
    })?;
    wallet_update(env, "deauthorize", custodian).await?;
    println!("Deauthorized {} as a custodian.", opts.custodian);
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/list_addresses.rs:
-----------------------

use crate::commands::wallet::wallet_query;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;
use ic_utils::interfaces::wallet::AddressEntry;

/// Print wallet's address book.
#[derive(Parser)]
pub struct AddressesOpts {}

pub async fn exec(env: &dyn Environment, _opts: AddressesOpts) -> DfxResult {
    let (entries,): (Vec<AddressEntry>,) = wallet_query(env, "list_addresses", ()).await?;
    for entry in entries {
        let name = entry.name.unwrap_or_else(|| "No name set.".to_string());
        println!(
            "Id: {}, Kind: {:?}, Role: {:?}, Name: {}",
            entry.id, entry.kind, entry.role, name
        );
    }
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/mod.rs:
-----------------------

use crate::lib::agent::create_agent_environment;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::identity::wallet::get_or_create_wallet_canister;
use crate::lib::network::network_opt::NetworkOpt;
use crate::lib::root_key::fetch_root_key_if_needed;
use anyhow::Context;
use candid::utils::ArgumentDecoder;
use candid::CandidType;
use clap::Parser;
use fn_error_context::context;
use ic_utils::call::SyncCall;
use ic_utils::interfaces::WalletCanister;
use tokio::runtime::Runtime;

mod add_controller;
mod authorize;
mod balance;
mod controllers;
mod custodians;
mod deauthorize;
mod list_addresses;
mod name;
mod redeem_faucet_coupon;
mod remove_controller;
mod send;
mod set_name;
mod upgrade;

/// Helper commands to manage the user's cycles wallet.
#[derive(Parser)]
#[command(name = "wallet")]
pub struct WalletOpts {
    #[command(flatten)]
    network: NetworkOpt,

    #[command(subcommand)]
    subcmd: SubCommand,
}

#[derive(Parser)]
enum SubCommand {
    Addresses(list_addresses::AddressesOpts),
    AddController(add_controller::AddControllerOpts),
    Authorize(authorize::AuthorizeOpts),
    Balance(balance::WalletBalanceOpts),
    Controllers(controllers::ControllersOpts),
    Custodians(custodians::CustodiansOpts),
    Deauthorize(deauthorize::DeauthorizeOpts),
    Name(name::NameOpts),
    RedeemFaucetCoupon(redeem_faucet_coupon::RedeemFaucetCouponOpts),
    RemoveController(remove_controller::RemoveControllerOpts),
    Send(send::SendOpts),
    SetName(set_name::SetNameOpts),
    Upgrade(upgrade::UpgradeOpts),
}

pub fn exec(env: &dyn Environment, opts: WalletOpts) -> DfxResult {
    let agent_env = create_agent_environment(env, opts.network.to_network_name())?;
    let runtime = Runtime::new().expect("Unable to create a runtime");
    runtime.block_on(async {
        match opts.subcmd {
            SubCommand::Addresses(v) => list_addresses::exec(&agent_env, v).await,
            SubCommand::AddController(v) => add_controller::exec(&agent_env, v).await,
            SubCommand::Authorize(v) => authorize::exec(&agent_env, v).await,
            SubCommand::Balance(v) => balance::exec(&agent_env, v).await,
            SubCommand::Controllers(v) => controllers::exec(&agent_env, v).await,
            SubCommand::Custodians(v) => custodians::exec(&agent_env, v).await,
            SubCommand::Deauthorize(v) => deauthorize::exec(&agent_env, v).await,
            SubCommand::Name(v) => name::exec(&agent_env, v).await,
            SubCommand::RedeemFaucetCoupon(v) => redeem_faucet_coupon::exec(&agent_env, v).await,
            SubCommand::RemoveController(v) => remove_controller::exec(&agent_env, v).await,
            SubCommand::Send(v) => send::exec(&agent_env, v).await,
            SubCommand::SetName(v) => set_name::exec(&agent_env, v).await,
            SubCommand::Upgrade(v) => upgrade::exec(&agent_env, v).await,
        }
    })
}

#[context("Failed to call query function '{}' on wallet.", method)]
async fn wallet_query<A, O>(env: &dyn Environment, method: &str, arg: A) -> DfxResult<O>
where
    A: CandidType + Sync + Send,
    O: for<'de> ArgumentDecoder<'de> + Sync + Send,
{
    let identity_name = env
        .get_selected_identity()
        .expect("No selected identity.")
        .to_string();
    // Network descriptor will always be set.
    let network = env.get_network_descriptor();
    let wallet = get_or_create_wallet_canister(env, network, &identity_name).await?;

    let out: O = wallet
        .query(method)
        .with_arg(arg)
        .build()
        .call()
        .await
        .context("Query to wallet failed.")?;
    Ok(out)
}

#[context("Failed to call update function '{}' on wallet.", method)]
async fn wallet_update<A, O>(env: &dyn Environment, method: &str, arg: A) -> DfxResult<O>
where
    A: CandidType + Sync + Send,
    O: for<'de> ArgumentDecoder<'de> + Sync + Send,
{
    let wallet = get_wallet(env).await?;
    let out: O = wallet.update(method).with_arg(arg).build().await?;
    Ok(out)
}

#[context("Failed to setup wallet caller.")]
async fn get_wallet(env: &dyn Environment) -> DfxResult<WalletCanister<'_>> {
    let identity_name = env
        .get_selected_identity()
        .expect("No selected identity.")
        .to_string();
    // Network descriptor will always be set.
    let network = env.get_network_descriptor();
    fetch_root_key_if_needed(env).await?;
    let wallet = get_or_create_wallet_canister(env, network, &identity_name).await?;
    Ok(wallet)
}


-----------------------

/src/dfx/src/commands/wallet/name.rs:
-----------------------

use crate::commands::wallet::wallet_query;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;

/// Get wallet name.
#[derive(Parser)]
pub struct NameOpts {}

pub async fn exec(env: &dyn Environment, _opts: NameOpts) -> DfxResult {
    let (maybe_name,): (Option<String>,) = wallet_query(env, "name", ()).await?;
    match maybe_name {
        Some(name) => println!("{}", name),
        None => println!(
            "Name hasn't been set. Call `dfx wallet set-name` to give this cycles wallet a name."
        ),
    };
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/redeem_faucet_coupon.rs:
-----------------------

use crate::commands::wallet::get_wallet;
use crate::lib::diagnosis::DiagnosedError;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::identity::wallet::set_wallet_id;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::{format_as_trillions, pretty_thousand_separators};
use anyhow::{anyhow, bail, Context};
use candid::{encode_args, Decode, Principal};
use clap::Parser;
use dfx_core::cli::ask_for_consent;
use slog::{info, warn};

pub const DEFAULT_FAUCET_PRINCIPAL: Principal =
    Principal::from_slice(&[0, 0, 0, 0, 1, 112, 0, 196, 1, 1]);

/// Redeem a code at the cycles faucet.
#[derive(Parser)]
pub struct RedeemFaucetCouponOpts {
    /// The coupon code to redeem at the faucet.
    coupon_code: String,

    /// Alternative faucet address. If not set, this uses the DFINITY faucet.
    #[arg(long)]
    faucet: Option<String>,

    /// Skips yes/no checks by answering 'yes'. Not recommended outside of CI.
    #[arg(long, short)]
    yes: bool,
}

pub async fn exec(env: &dyn Environment, opts: RedeemFaucetCouponOpts) -> DfxResult {
    let log = env.get_logger();

    let faucet_principal = if let Some(alternative_faucet) = opts.faucet {
        let canister_id_store = env.get_canister_id_store()?;
        Principal::from_text(&alternative_faucet)
            .or_else(|_| canister_id_store.get(&alternative_faucet))?
    } else {
        DEFAULT_FAUCET_PRINCIPAL
    };
    let agent = env.get_agent();
    if fetch_root_key_if_needed(env).await.is_err() {
        bail!("Failed to connect to the local replica. Did you forget to use '--network ic'?");
    } else if !env.get_network_descriptor().is_ic {
        warn!(log, "Trying to redeem a wallet coupon on a local replica. Did you forget to use '--network ic'?");
    }

    info!(log, "Redeeming coupon. This may take up to 30 seconds...");
    let wallet = get_wallet(env).await;
    match wallet {
        // identity has a wallet already - faucet should top up the wallet
        Ok(wallet_canister) => {
            let wallet_principal = wallet_canister.canister_id_();
            let response = agent
                .update(&faucet_principal, "redeem_to_wallet")
                .with_arg(
                    encode_args((opts.coupon_code.clone(), wallet_principal))
                        .context("Failed to serialize redeem_to_wallet arguments.")?,
                )
                .await
                .context("Failed redeem_to_wallet call.")?;
            let redeemed_cycles =
                Decode!(&response, u128).context("Failed to decode redeem_to_wallet response.")?;
            info!(
                log,
                "Redeemed coupon code {} for {} TC (trillion cycles).",
                opts.coupon_code,
                pretty_thousand_separators(format_as_trillions(redeemed_cycles))
            );

            Ok(())
        }
        // identity has no wallet yet - faucet will provide one
        _ => {
            if !opts.yes {
                ask_for_consent("`dfx cycles` is now recommended instead of `dfx wallet`. Are you sure you want to create a new cycles wallet anyway?")?;
            }
            let identity = env
                .get_selected_identity()
                .with_context(|| anyhow!("No identity selected."))?;
            let response = agent
                .update(&faucet_principal, "redeem")
                .with_arg(
                    encode_args((opts.coupon_code.clone(),))
                        .context("Failed to serialize 'redeem' arguments.")?,
                )
                .await
                .context("Failed 'redeem' call.")?;
            let new_wallet_address =
                Decode!(&response, Principal).context("Failed to decode 'redeem' response.")?;
            info!(
                log,
                "Redeemed coupon {} for a new wallet: {}", opts.coupon_code, &new_wallet_address
            );
            set_wallet_id(env.get_network_descriptor(), identity, new_wallet_address)
                .with_context(|| {
                    DiagnosedError::new(
                        format!(
                            "dfx failed while trying to set your new wallet, '{}'",
                            &new_wallet_address
                        ),
                        format!("Please save your new wallet's ID '{}' and set the wallet manually afterwards using 'dfx identity set-wallet'.", &new_wallet_address),
                    )
                })?;
            info!(log, "New wallet set.");
            Ok(())
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_faucet_canister_id() {
        assert_eq!(
            DEFAULT_FAUCET_PRINCIPAL,
            Principal::from_text("fg7gi-vyaaa-aaaal-qadca-cai").unwrap()
        );
    }
}


-----------------------

/src/dfx/src/commands/wallet/remove_controller.rs:
-----------------------

use crate::commands::wallet::wallet_update;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::Context;
use candid::Principal;
use clap::Parser;

/// Remove a wallet controller.
#[derive(Parser)]
pub struct RemoveControllerOpts {
    /// Principal of the controller to remove.
    controller: String,
}

pub async fn exec(env: &dyn Environment, opts: RemoveControllerOpts) -> DfxResult {
    let controller = Principal::from_text(&opts.controller).with_context(|| {
        format!(
            "Failed to parse {:?} as controller principal.",
            opts.controller
        )
    })?;
    wallet_update(env, "remove_controller", controller).await?;
    println!("Removed {} as a controller.", controller);
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/send.rs:
-----------------------

use crate::commands::wallet::get_wallet;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::util::clap::parsers::cycle_amount_parser;
use anyhow::{anyhow, Context};
use candid::CandidType;
use candid::Principal;
use clap::Parser;

/// Send cycles to a canister.
#[derive(Parser)]
pub struct SendOpts {
    /// Canister ID of the destination canister.
    destination: String,

    /// Specifies the amount of cycles to send.
    /// Deducted from the wallet.
    #[arg(value_parser = cycle_amount_parser)]
    amount: u128,
}

pub async fn exec(env: &dyn Environment, opts: SendOpts) -> DfxResult {
    #[derive(CandidType)]
    struct In {
        canister: Principal,
        amount: u128,
    }
    let canister = Principal::from_text(&opts.destination).with_context(|| {
        format!(
            "Failed to parse {:?} as destination principal.",
            &opts.destination
        )
    })?;
    let amount = opts.amount;
    let res = get_wallet(env).await?.wallet_send(canister, amount).await;
    res.map_err(|err| {
        anyhow!(
            "Sending cycles to {} failed with: {}",
            opts.destination,
            err
        )
    })
}


-----------------------

/src/dfx/src/commands/wallet/set_name.rs:
-----------------------

use crate::commands::wallet::wallet_update;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use clap::Parser;

/// Set wallet name.
#[derive(Parser)]
pub struct SetNameOpts {
    /// Name of the wallet.
    name: String,
}

pub async fn exec(env: &dyn Environment, opts: SetNameOpts) -> DfxResult {
    wallet_update(env, "set_name", opts.name.clone()).await?;
    println!("Set name to {}.", opts.name);
    Ok(())
}


-----------------------

/src/dfx/src/commands/wallet/upgrade.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::operations::canister::install_canister::install_wallet;
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::lib::state_tree::canister_info::read_state_tree_canister_module_hash;
use anyhow::bail;
use clap::Parser;
use dfx_core::identity::wallet::wallet_canister_id;
use ic_utils::interfaces::management_canister::builders::InstallMode;

/// Upgrade the wallet's Wasm module to the current Wasm bundled with DFX.
#[derive(Parser)]
pub struct UpgradeOpts {}

pub async fn exec(env: &dyn Environment, _opts: UpgradeOpts) -> DfxResult {
    let identity_name = env
        .get_selected_identity()
        .expect("No selected identity.")
        .to_string();

    // Network descriptor will always be set.
    let network = env.get_network_descriptor();

    let canister_id = if let Some(principal) = wallet_canister_id(network, &identity_name)? {
        principal
    } else {
        bail!(
            "There is no wallet defined for identity '{}' on network '{}'.  Nothing to do.",
            identity_name,
            &network.name
        );
    };

    let agent = env.get_agent();

    fetch_root_key_if_needed(env).await?;
    if read_state_tree_canister_module_hash(agent, canister_id)
        .await?
        .is_none()
    {
        bail!("The cycles wallet canister is empty. Try running `dfx identity deploy-wallet` to install code for the cycles wallet in this canister.")
    }

    let agent = env.get_agent();

    install_wallet(env, agent, canister_id, InstallMode::Upgrade(None)).await?;

    println!("Upgraded the wallet wasm module.");
    Ok(())
}


-----------------------

/src/dfx/src/config/cache.rs:
-----------------------

use crate::config::dfx_version;
use crate::util;
use dfx_core;
use dfx_core::config::cache::{
    binary_command_from_version, delete_version, get_bin_cache, get_binary_path_from_version,
    is_version_installed, Cache,
};
use dfx_core::error::cache::{
    DeleteCacheError, GetBinaryCommandPathError, InstallCacheError, IsCacheInstalledError,
};
use indicatif::{ProgressBar, ProgressDrawTarget};
use rand::distributions::Alphanumeric;
use rand::{thread_rng, Rng};
use semver::Version;
use std::io::{stderr, IsTerminal};
#[cfg(unix)]
use std::os::unix::fs::PermissionsExt;
use std::path::PathBuf;

// POSIX permissions for files in the cache.
#[cfg(unix)]
const EXEC_READ_USER_ONLY_PERMISSION: u32 = 0o500;
#[cfg(unix)]
const READ_USER_ONLY_PERMISSION: u32 = 0o400;

pub struct DiskBasedCache {
    version: Version,
}

impl DiskBasedCache {
    pub fn with_version(version: &Version) -> DiskBasedCache {
        DiskBasedCache {
            version: version.clone(),
        }
    }
    pub fn install(version: &str) -> Result<(), InstallCacheError> {
        install_version(version, false).map(|_| {})
    }
    pub fn force_install(version: &str) -> Result<(), InstallCacheError> {
        install_version(version, true).map(|_| {})
    }
}

#[allow(dead_code)]
impl Cache for DiskBasedCache {
    fn version_str(&self) -> String {
        format!("{}", self.version)
    }

    fn is_installed(&self) -> Result<bool, IsCacheInstalledError> {
        is_version_installed(&self.version_str())
    }

    fn delete(&self) -> Result<(), DeleteCacheError> {
        delete_version(&self.version_str()).map(|_| {})
    }

    fn get_binary_command_path(
        &self,
        binary_name: &str,
    ) -> Result<PathBuf, GetBinaryCommandPathError> {
        Self::install(&self.version_str())?;
        let path = get_binary_path_from_version(&self.version_str(), binary_name)?;
        Ok(path)
    }

    fn get_binary_command(
        &self,
        binary_name: &str,
    ) -> Result<std::process::Command, GetBinaryCommandPathError> {
        Self::install(&self.version_str())?;
        let path = binary_command_from_version(&self.version_str(), binary_name)?;
        Ok(path)
    }
}

pub fn install_version(v: &str, force: bool) -> Result<PathBuf, InstallCacheError> {
    let p = get_bin_cache(v)?;
    if !force && is_version_installed(v).unwrap_or(false) {
        return Ok(p);
    }

    if Version::parse(v).map_err(|e| InstallCacheError::MalformedSemverString(v.to_string(), e))?
        == *dfx_version()
    {
        // Dismiss as fast as possible. We use the current_exe variable after an
        // expensive step, and if this fails we can't continue anyway.
        let current_exe = dfx_core::foundation::get_current_exe()?;

        let b: Option<ProgressBar> = if stderr().is_terminal() {
            let b = ProgressBar::new_spinner();
            b.set_draw_target(ProgressDrawTarget::stderr());
            b.set_message(format!("Installing version {} of dfx...", v));
            b.enable_steady_tick(80);
            Some(b)
        } else {
            None
        };

        let rand_string: String = thread_rng()
            .sample_iter(&Alphanumeric)
            .take(12)
            .map(|byte| byte as char)
            .collect();
        let temp_p = get_bin_cache(&format!("_{}_{}", v, rand_string))?;
        dfx_core::fs::create_dir_all(&temp_p)?;

        let mut binary_cache_assets =
            util::assets::binary_cache().map_err(InstallCacheError::ReadBinaryCacheStoreFailed)?;
        // Write binaries and set them to be executable.
        for file in binary_cache_assets
            .entries()
            .map_err(InstallCacheError::ReadBinaryCacheEntriesFailed)?
        {
            let mut file = file.map_err(InstallCacheError::ReadBinaryCacheEntryFailed)?;

            if file.header().entry_type().is_dir() {
                continue;
            }
            dfx_core::fs::tar_unpack_in(temp_p.as_path(), &mut file)?;
            // On *nix we need to set the execute permission as the tgz doesn't include it
            #[cfg(unix)]
            {
                let archive_path = dfx_core::fs::get_archive_path(&file)?;
                let mode = if archive_path.starts_with("base/") {
                    READ_USER_ONLY_PERMISSION
                } else {
                    EXEC_READ_USER_ONLY_PERMISSION
                };
                let full_path = temp_p.join(archive_path);
                let mut perms = dfx_core::fs::read_permissions(full_path.as_path())?;
                perms.set_mode(mode);
                dfx_core::fs::set_permissions(full_path.as_path(), perms)?;
            }
        }

        // Copy our own binary in the cache.
        let dfx = temp_p.join("dfx");
        #[allow(clippy::needless_borrows_for_generic_args)]
        dfx_core::fs::write(&dfx, dfx_core::fs::read(&current_exe)?)?;
        // On *nix we need to set the execute permission as the tgz doesn't include it
        #[cfg(unix)]
        {
            let mut perms = dfx_core::fs::read_permissions(&dfx)?;
            perms.set_mode(EXEC_READ_USER_ONLY_PERMISSION);
            dfx_core::fs::set_permissions(&dfx, perms)?;
        }

        // atomically install cache version into place
        if force && p.exists() {
            dfx_core::fs::remove_dir_all(&p)?;
        }

        if dfx_core::fs::rename(temp_p.as_path(), &p).is_ok() {
            if let Some(b) = b {
                b.finish_with_message(format!("Installed dfx {} to cache.", v));
            }
        } else {
            dfx_core::fs::remove_dir_all(temp_p.as_path())?;
            if let Some(b) = b {
                b.finish_with_message(format!("dfx {} was already installed in cache.", v));
            }
        }
        Ok(p)
    } else {
        Err(InstallCacheError::InvalidCacheForDfxVersion(v.to_owned()))
    }
}


-----------------------

/src/dfx/src/config/mod.rs:
-----------------------

use lazy_static::lazy_static;
use semver::Version;

pub mod cache;

lazy_static! {
    // This expect cannot happen, we make sure that CARGO_PKG_VERSION is correct.
    static ref VERSION: Version =
        Version::parse(env!("CARGO_PKG_VERSION")).expect("Cannot parse version.");

    static ref VERSION_STR: String = env!("CARGO_PKG_VERSION").to_string();
}

/// Returns the version of DFX that was built.
/// In debug, add a timestamp of the upstream compilation at the end of version to ensure all
/// debug runs are unique (and cached uniquely).
/// That timestamp is taken from the DFX_TIMESTAMP_DEBUG_MODE_ONLY env var that is set in
/// Nix.
pub fn dfx_version() -> &'static Version {
    &VERSION
}

pub fn dfx_version_str() -> &'static str {
    &VERSION_STR
}


-----------------------

/src/dfx/src/lib/agent.rs:
-----------------------

use crate::lib::environment::AgentEnvironment;
use crate::lib::error::DfxResult;
use crate::Environment;
use dfx_core::identity::ANONYMOUS_IDENTITY_NAME;
use dfx_core::network::provider::{create_network_descriptor, LocalBindDetermination};

use dfx_core::util::expiry_duration;
use fn_error_context::context;

#[context("Failed to create AgentEnvironment.")]
pub fn create_agent_environment<'a>(
    env: &'a (dyn Environment + 'a),
    network: Option<String>,
) -> DfxResult<AgentEnvironment<'a>> {
    let network_descriptor = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        network,
        None,
        LocalBindDetermination::ApplyRunningWebserverPort,
    )?;
    let timeout = expiry_duration();
    AgentEnvironment::new(env, network_descriptor, timeout, None)
}

pub fn create_anonymous_agent_environment<'a>(
    env: &'a (dyn Environment + 'a),
    network: Option<String>,
) -> DfxResult<AgentEnvironment<'a>> {
    let network_descriptor = create_network_descriptor(
        env.get_config()?,
        env.get_networks_config(),
        network,
        None,
        LocalBindDetermination::ApplyRunningWebserverPort,
    )?;
    let timeout = expiry_duration();
    AgentEnvironment::new(
        env,
        network_descriptor,
        timeout,
        Some(ANONYMOUS_IDENTITY_NAME),
    )
}


-----------------------

/src/dfx/src/lib/builders/assets.rs:
-----------------------

use crate::lib::builders::{
    BuildConfig, BuildOutput, CanisterBuilder, IdlBuildOutput, WasmBuildOutput,
};
use crate::lib::canister_info::assets::AssetsCanisterInfo;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::{BuildError, DfxError, DfxResult};
use crate::lib::models::canister::CanisterPool;
use crate::lib::program;
use crate::util;
use anyhow::{anyhow, Context};
use candid::Principal as CanisterId;
use console::style;
use dfx_core::config::cache::Cache;
use dfx_core::config::model::network_descriptor::NetworkDescriptor;
use fn_error_context::context;
use slog::{o, Logger};
use std::fs;
use std::path::Path;
use std::sync::Arc;

/// Set of extras that can be specified in the dfx.json.
struct AssetsBuilderExtra {
    /// A list of canister names to use as dependencies.
    dependencies: Vec<CanisterId>,
    /// A command to run to build this canister's assets. This is optional if
    /// the canister does not have a frontend or can be built using the default
    /// `npm run build` command.
    build: Vec<String>,
    /// The NPM workspace that the project is located in.
    workspace: Option<String>,
}

impl AssetsBuilderExtra {
    #[context("Failed to create AssetBuilderExtra for canister '{}'.", info.get_name())]
    fn try_from(info: &CanisterInfo, pool: &CanisterPool) -> DfxResult<Self> {
        let dependencies = info.get_dependencies()
            .iter()
            .map(|name| {
                pool.get_first_canister_with_name(name)
                    .map(|c| c.canister_id())
                    .map_or_else(
                        || Err(anyhow!("A canister with the name '{}' was not found in the current project.", name.clone())),
                        DfxResult::Ok,
                    )
            })
            .collect::<DfxResult<Vec<CanisterId>>>().with_context( || format!("Failed to collect dependencies (canister ids) of canister {}.", info.get_name()))?;
        let info = info.as_info::<AssetsCanisterInfo>()?;
        let build = info.get_build_tasks().to_owned();
        let workspace = info.get_npm_workspace().map(str::to_owned);

        Ok(AssetsBuilderExtra {
            dependencies,
            build,
            workspace,
        })
    }
}
pub struct AssetsBuilder {
    _cache: Arc<dyn Cache>,
    logger: Logger,
}
unsafe impl Send for AssetsBuilder {}
unsafe impl Sync for AssetsBuilder {}

impl AssetsBuilder {
    #[context("Failed to create AssetBuilder.")]
    pub fn new(env: &dyn Environment) -> DfxResult<Self> {
        Ok(AssetsBuilder {
            _cache: env.get_cache(),
            logger: env.get_logger().new(o!("module" => "assets")),
        })
    }
}

impl CanisterBuilder for AssetsBuilder {
    #[context("Failed to get dependencies for canister '{}'.", info.get_name())]
    fn get_dependencies(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
    ) -> DfxResult<Vec<CanisterId>> {
        Ok(AssetsBuilderExtra::try_from(info, pool)?.dependencies)
    }

    #[context("Failed to build asset canister '{}'.", info.get_name())]
    fn build(
        &self,
        _pool: &CanisterPool,
        info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult<BuildOutput> {
        let wasm_path = info
            .get_output_root()
            .join(Path::new("assetstorage.wasm.gz"));
        unpack_did(info.get_output_root())?;
        let canister_assets = util::assets::assets_wasm(&self.logger)?;
        fs::write(&wasm_path, canister_assets).context("Failed to write asset canister wasm")?;
        let idl_path = info.get_output_root().join(Path::new("assetstorage.did"));
        Ok(BuildOutput {
            canister_id: info.get_canister_id().expect("Could not find canister ID."),
            wasm: WasmBuildOutput::File(wasm_path),
            idl: IdlBuildOutput::File(idl_path),
        })
    }

    fn postbuild(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
        config: &BuildConfig,
    ) -> DfxResult {
        let AssetsBuilderExtra {
            build,
            dependencies,
            workspace,
        } = AssetsBuilderExtra::try_from(info, pool)?;

        let vars = super::get_and_write_environment_variables(
            info,
            &config.network_name,
            pool,
            &dependencies,
            config.env_file.as_deref(),
        )?;

        build_frontend(
            pool.get_logger(),
            info.get_workspace_root(),
            &config.network_name,
            vars,
            &build,
            workspace.as_deref(),
        )?;

        let assets_canister_info = info.as_info::<AssetsCanisterInfo>()?;
        assets_canister_info.assert_source_paths()?;

        Ok(())
    }

    fn get_candid_path(
        &self,
        _pool: &CanisterPool,
        info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult<std::path::PathBuf> {
        let idl_path = info.get_output_root().join(Path::new("assetstorage.did"));
        Ok(idl_path)
    }
}

fn unpack_did(generate_output_dir: &Path) -> DfxResult<()> {
    let mut canister_assets =
        util::assets::assetstorage_canister().context("Failed to load asset canister archive.")?;
    for file in canister_assets
        .entries()
        .context("Failed to read asset canister archive entries.")?
    {
        let mut file = file.context("Failed to read asset canister archive entry.")?;

        if !file.header().entry_type().is_dir() && file.path()?.ends_with("assetstorage.did") {
            // See https://github.com/alexcrichton/tar-rs/issues/261
            fs::create_dir_all(generate_output_dir)
                .with_context(|| format!("Failed to create {}.", generate_output_dir.display()))?;
            file.unpack_in(generate_output_dir).with_context(|| {
                format!(
                    "Failed to unpack archive content to {}.",
                    generate_output_dir.display()
                )
            })?;
            break;
        }
    }
    Ok(())
}

#[context("Failed to build frontend for network '{}'.", network_name)]
fn build_frontend(
    logger: &slog::Logger,
    project_root: &Path,
    network_name: &str,
    vars: Vec<super::Env<'_>>,
    build: &[String],
    workspace: Option<&str>,
) -> DfxResult {
    let custom_build_frontend = !build.is_empty();
    let build_frontend = project_root.join("package.json").exists();
    // If there is no package.json or custom build command, we don't have a frontend and can quit early.

    if custom_build_frontend {
        for command in build {
            slog::info!(
                logger,
                r#"{} '{}'"#,
                style("Executing").green().bold(),
                command
            );

            super::run_command(command, &vars, project_root)
                .with_context(|| format!("Failed to run {}.", command))?;
        }
    } else if build_frontend {
        // Frontend build.
        slog::info!(logger, "Building frontend...");
        let mut cmd = std::process::Command::new(program::NPM);

        // Provide DFX_NETWORK at build time
        cmd.env("DFX_NETWORK", network_name);

        cmd.arg("run").arg("build");

        if let Some(workspace) = workspace {
            cmd.arg("--workspace").arg(workspace);
        }

        if NetworkDescriptor::is_ic(network_name, &vec![]) {
            cmd.env("NODE_ENV", "production");
        }

        for (var, value) in vars {
            cmd.env(var.as_ref(), value);
        }

        cmd.current_dir(project_root)
            .stdout(std::process::Stdio::piped())
            .stderr(std::process::Stdio::piped());
        slog::debug!(logger, "Running {:?}...", cmd);

        let output = cmd
            .output()
            .with_context(|| format!("Error executing {:#?}", cmd))?;
        if !output.status.success() {
            return Err(DfxError::new(BuildError::CommandError(
                format!("{:?}", cmd),
                output.status,
                String::from_utf8_lossy(&output.stdout).to_string(),
                String::from_utf8_lossy(&output.stderr).to_string(),
            )));
        } else if !output.stderr.is_empty() {
            // Cannot use eprintln, because it would interfere with the progress bar.
            slog::warn!(logger, "{}", String::from_utf8_lossy(&output.stderr));
        }
    }
    Ok(())
}


-----------------------

/src/dfx/src/lib/builders/custom.rs:
-----------------------

use crate::lib::builders::{
    BuildConfig, BuildOutput, CanisterBuilder, IdlBuildOutput, WasmBuildOutput,
};
use crate::lib::canister_info::custom::CustomCanisterInfo;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::models::canister::CanisterPool;
use crate::util::download_file_to_path;
use anyhow::{anyhow, Context};
use candid::Principal as CanisterId;
use console::style;
use fn_error_context::context;
use slog::info;
use slog::Logger;
use std::path::PathBuf;
use url::Url;

/// Set of extras that can be specified in the dfx.json.
struct CustomBuilderExtra {
    /// A list of canister names to use as dependencies.
    dependencies: Vec<CanisterId>,
    /// Where to download the wasm from
    input_wasm_url: Option<Url>,
    /// Where the wasm output will be located.
    wasm: PathBuf,
    /// Where to download the candid from
    input_candid_url: Option<Url>,
    /// A command to run to build this canister. This is optional if the canister
    /// only needs to exist.
    build: Vec<String>,
}

impl CustomBuilderExtra {
    #[context("Failed to create CustomBuilderExtra for canister '{}'.", info.get_name())]
    fn try_from(info: &CanisterInfo, pool: &CanisterPool) -> DfxResult<Self> {
        let dependencies = info.get_dependencies()
            .iter()
            .map(|name| {
                pool.get_first_canister_with_name(name)
                    .map(|c| c.canister_id())
                    .map_or_else(
                        || Err(anyhow!("A canister with the name '{}' was not found in the current project.", name.clone())),
                        DfxResult::Ok,
                    )
            })
            .collect::<DfxResult<Vec<CanisterId>>>().with_context( || format!("Failed to collect dependencies (canister ids) of canister {}.", info.get_name()))?;
        let info = info.as_info::<CustomCanisterInfo>()?;
        let input_wasm_url = info.get_input_wasm_url().to_owned();
        let wasm = info.get_output_wasm_path().to_owned();
        let input_candid_url = info.get_input_candid_url().to_owned();
        let build = info.get_build_tasks().to_owned();

        Ok(CustomBuilderExtra {
            dependencies,
            input_wasm_url,
            wasm,
            input_candid_url,
            build,
        })
    }
}

/// A Builder for a Wasm type canister, which has an optional build step.
/// This will set environment variables for the external tool;
///   `CANISTER_ID`     => Its own canister ID (in textual format).
///   `CANDID_PATH`     => Its own candid path.
///   `CANISTER_ID_{}`  => The canister ID of all dependencies. `{}` is replaced by the name.
///   `CANDID_{}`       => The candid path of all dependencies. `{}` is replaced by the name.
pub struct CustomBuilder {
    logger: Logger,
}

impl CustomBuilder {
    #[context("Failed to create CustomBuilder.")]
    pub fn new(env: &dyn Environment) -> DfxResult<Self> {
        Ok(CustomBuilder {
            logger: env.get_logger().clone(),
        })
    }
}

impl CanisterBuilder for CustomBuilder {
    #[context("Failed to get dependencies for canister '{}'.", info.get_name())]
    fn get_dependencies(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
    ) -> DfxResult<Vec<CanisterId>> {
        Ok(CustomBuilderExtra::try_from(info, pool)?.dependencies)
    }

    #[context("Failed to build custom canister {}.", info.get_name())]
    fn build(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
        config: &BuildConfig,
    ) -> DfxResult<BuildOutput> {
        let CustomBuilderExtra {
            input_candid_url: _,
            input_wasm_url: _,
            wasm,
            build,
            dependencies,
        } = CustomBuilderExtra::try_from(info, pool)?;

        let canister_id = info.get_canister_id().unwrap();
        let vars = super::get_and_write_environment_variables(
            info,
            &config.network_name,
            pool,
            &dependencies,
            config.env_file.as_deref(),
        )?;

        for command in build {
            info!(
                self.logger,
                r#"{} '{}'"#,
                style("Executing").green().bold(),
                command
            );

            super::run_command(&command, &vars, info.get_workspace_root())
                .with_context(|| format!("Failed to run {}.", command))?;
        }

        Ok(BuildOutput {
            canister_id,
            wasm: WasmBuildOutput::File(wasm),
            idl: IdlBuildOutput::File(info.get_output_idl_path().to_path_buf()),
        })
    }

    fn get_candid_path(
        &self,
        _pool: &CanisterPool,
        info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult<PathBuf> {
        // get the path to candid file
        Ok(info.get_output_idl_path().to_path_buf())
    }
}

pub async fn custom_download(info: &CanisterInfo, pool: &CanisterPool) -> DfxResult {
    let candid = info.get_output_idl_path();
    let CustomBuilderExtra {
        input_candid_url,
        input_wasm_url,
        wasm,
        build: _,
        dependencies: _,
    } = CustomBuilderExtra::try_from(info, pool)?;

    if let Some(url) = input_wasm_url {
        download_file_to_path(&url, &wasm).await?;
    }
    if let Some(url) = input_candid_url {
        download_file_to_path(&url, candid).await?;
    }

    Ok(())
}


-----------------------

/src/dfx/src/lib/builders/mod.rs:
-----------------------

use crate::config::dfx_version_str;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::{BuildError, DfxError, DfxResult};
use crate::lib::models::canister::CanisterPool;
use crate::util::command::direct_or_shell_command;
use anyhow::{bail, Context};
use candid::Principal as CanisterId;
use candid_parser::utils::CandidSource;
use dfx_core::config::model::dfinity::{Config, Profile};
use dfx_core::network::provider::get_network_context;
use dfx_core::util;
use fn_error_context::context;
use handlebars::Handlebars;
use std::borrow::Cow;
use std::collections::BTreeMap;
use std::ffi::OsStr;
use std::fmt::Write;
use std::fs;
use std::io::Read;
use std::path::{Path, PathBuf};
use std::process::Stdio;
use std::sync::Arc;

mod assets;
mod custom;
mod motoko;
mod pull;
mod rust;

pub use custom::custom_download;

#[derive(Debug)]
pub enum WasmBuildOutput {
    // Wasm(Vec<u8>),
    File(PathBuf),
    // pull dependencies has no wasm output to be installed by `dfx canister install` or `dfx deploy`
    None,
}

#[derive(Debug)]
pub enum IdlBuildOutput {
    // IDLProg(IDLProg),
    File(PathBuf),
}

/// The output of a build.
#[derive(Debug)]
pub struct BuildOutput {
    pub canister_id: CanisterId,
    pub wasm: WasmBuildOutput,
    pub idl: IdlBuildOutput,
}

/// A stateless canister builder. This is meant to not keep any state and be passed everything.
pub trait CanisterBuilder {
    /// Returns the dependencies of this canister, if any. This should not be a transitive
    /// list.
    fn get_dependencies(
        &self,
        _pool: &CanisterPool,
        _info: &CanisterInfo,
    ) -> DfxResult<Vec<CanisterId>> {
        Ok(Vec::new())
    }

    fn prebuild(
        &self,
        _pool: &CanisterPool,
        _info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult {
        Ok(())
    }

    /// Build a canister. The canister contains all information related to a single canister,
    /// while the config contains information related to this particular build.
    fn build(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
        config: &BuildConfig,
    ) -> DfxResult<BuildOutput>;

    fn postbuild(
        &self,
        _pool: &CanisterPool,
        _info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult {
        Ok(())
    }

    /// Generate type declarations for the canister
    fn generate(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
        config: &BuildConfig,
    ) -> DfxResult {
        let generate_output_dir = info
            .get_declarations_config()
            .output
            .as_ref()
            .context("`output` must not be None")?;

        if generate_output_dir.exists() {
            let generate_output_dir = dfx_core::fs::canonicalize(generate_output_dir)
                .with_context(|| {
                    format!(
                        "Failed to canonicalize output dir {}.",
                        generate_output_dir.to_string_lossy()
                    )
                })?;
            if !generate_output_dir.starts_with(info.get_workspace_root()) {
                bail!(
                    "Directory at '{}' is outside the workspace root.",
                    generate_output_dir.as_path().display()
                );
            }
            std::fs::remove_dir_all(&generate_output_dir).with_context(|| {
                format!(
                    "Failed to remove dir: {}",
                    generate_output_dir.to_string_lossy()
                )
            })?;
        }

        let bindings = info
            .get_declarations_config()
            .bindings
            .as_ref()
            .context("`bindings` must not be None")?;

        if bindings.is_empty() {
            eprintln!("`{}.declarations.bindings` in dfx.json was set to be an empty list, so no type declarations will be generated.", &info.get_name());
            return Ok(());
        }

        eprintln!(
            "Generating type declarations for canister {}:",
            &info.get_name()
        );

        std::fs::create_dir_all(generate_output_dir).with_context(|| {
            format!(
                "Failed to create dir: {}",
                generate_output_dir.to_string_lossy()
            )
        })?;

        let did_from_build = self.get_candid_path(pool, info, config)?;
        if !did_from_build.exists() {
            bail!(
                "Candid file: {} doesn't exist.",
                did_from_build.to_string_lossy()
            );
        }

        let (env, ty) = CandidSource::File(did_from_build.as_path()).load()?;

        // Typescript
        if bindings.contains(&"ts".to_string()) {
            let output_did_ts_path = generate_output_dir
                .join(info.get_name())
                .with_extension("did.d.ts");
            let content =
                ensure_trailing_newline(candid_parser::bindings::typescript::compile(&env, &ty));
            std::fs::write(&output_did_ts_path, content).with_context(|| {
                format!(
                    "Failed to write to {}.",
                    output_did_ts_path.to_string_lossy()
                )
            })?;
            eprintln!("  {}", &output_did_ts_path.display());

            compile_handlebars_files("ts", info, generate_output_dir)?;
        }

        // Javascript
        if bindings.contains(&"js".to_string()) {
            // <canister.did.js>
            let output_did_js_path = generate_output_dir
                .join(info.get_name())
                .with_extension("did.js");
            let content =
                ensure_trailing_newline(candid_parser::bindings::javascript::compile(&env, &ty));
            std::fs::write(&output_did_js_path, content).with_context(|| {
                format!(
                    "Failed to write to {}.",
                    output_did_js_path.to_string_lossy()
                )
            })?;
            eprintln!("  {}", &output_did_js_path.display());

            compile_handlebars_files("js", info, generate_output_dir)?;
        }

        // Motoko
        if bindings.contains(&"mo".to_string()) {
            let output_mo_path = generate_output_dir
                .join(info.get_name())
                .with_extension("mo");
            let content =
                ensure_trailing_newline(candid_parser::bindings::motoko::compile(&env, &ty));
            std::fs::write(&output_mo_path, content).with_context(|| {
                format!("Failed to write to {}.", output_mo_path.to_string_lossy())
            })?;
            eprintln!("  {}", &output_mo_path.display());
        }

        // Candid
        if bindings.contains(&"did".to_string()) {
            let output_did_path = generate_output_dir
                .join(info.get_name())
                .with_extension("did");
            dfx_core::fs::copy(&did_from_build, &output_did_path)?;
            dfx_core::fs::set_permissions_readwrite(&output_did_path)?;
            eprintln!("  {}", &output_did_path.display());
        }

        Ok(())
    }

    /// Get the path to the provided candid file for the canister.
    /// No need to guarantee the file exists, as the caller will handle that.
    fn get_candid_path(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
        config: &BuildConfig,
    ) -> DfxResult<PathBuf>;
}

fn compile_handlebars_files(
    lang: &str,
    info: &CanisterInfo,
    generate_output_dir: &Path,
) -> DfxResult {
    // index.js
    let mut language_bindings = crate::util::assets::language_bindings()
        .context("Failed to get language bindings archive.")?;
    for f in language_bindings
        .entries()
        .context("Failed to read language bindings archive entries.")?
    {
        let mut file = f.context("Failed to read language bindings archive entry.")?;

        let pathname: PathBuf = file
            .path()
            .context("Failed to read language bindings entry path name.")?
            .to_path_buf();
        let file_extension = format!("{}.hbs", lang);
        let is_template = pathname
            .to_str()
            .map_or(false, |name| name.ends_with(&file_extension));

        if is_template {
            let mut file_contents = String::new();
            file.read_to_string(&mut file_contents)
                .context("Failed to read language bindings archive file content.")?;

            // create the handlebars registry
            let handlebars = Handlebars::new();

            let mut data: BTreeMap<String, &String> = BTreeMap::new();

            let canister_name = &info.get_name().to_string();
            let canister_name_ident = &canister_name.replace('-', "_");

            let node_compatibility = info.get_declarations_config().node_compatibility;

            // Insert only if node outputs are specified
            let actor_export = if node_compatibility {
                // leave empty for nodejs
                "".to_string()
            } else {
                format!(
                    r#"

export const {canister_name_ident} = canisterId ? createActor(canisterId) : undefined;"#,
                )
                .to_string()
            };

            data.insert("canister_name".to_string(), canister_name);
            data.insert("canister_name_ident".to_string(), canister_name_ident);
            data.insert("actor_export".to_string(), &actor_export);

            // Switches to prefixing the canister id with the env variable for frontend declarations as new default
            let process_string_prefix: String = match &info.get_declarations_config().env_override {
                Some(s) => format!(r#""{}""#, s.clone()),
                None => {
                    format!(
                        "process.env.{}{}",
                        "CANISTER_ID_",
                        &canister_name_ident.to_ascii_uppercase(),
                    )
                }
            };

            data.insert(
                "canister_name_process_env".to_string(),
                &process_string_prefix,
            );

            let new_file_contents = handlebars.render_template(&file_contents, &data).unwrap();
            let new_path = generate_output_dir.join(pathname.with_extension(""));
            std::fs::write(&new_path, new_file_contents)
                .with_context(|| format!("Failed to write to {}.", new_path.display()))?;
        }
    }

    Ok(())
}

// TODO: this function was copied from src/lib/models/canister.rs
fn ensure_trailing_newline(s: String) -> String {
    if s.ends_with('\n') {
        s
    } else {
        let mut s = s;
        s.push('\n');
        s
    }
}

/// Execute a command and return its output bytes.
/// If the catch_output is false, the return bytes will always be empty.
pub fn execute_command(
    command: &str,
    vars: &[Env<'_>],
    cwd: &Path,
    catch_output: bool,
) -> DfxResult<Vec<u8>> {
    // No commands, noop.
    if command.is_empty() {
        return Ok(vec![]);
    }
    let mut cmd = direct_or_shell_command(command, cwd)?;

    if !catch_output {
        cmd.stdin(Stdio::inherit())
            .stdout(Stdio::inherit())
            .stderr(Stdio::inherit());
    }
    for (key, value) in vars {
        cmd.env(key.as_ref(), value);
    }
    let output = cmd
        .output()
        .with_context(|| format!("Error executing custom build step {cmd:#?}"))?;
    if output.status.success() {
        Ok(output.stdout)
    } else {
        Err(DfxError::new(BuildError::CustomToolError(
            output.status.code(),
        )))
    }
}

pub fn run_command(command: &str, vars: &[Env<'_>], cwd: &Path) -> DfxResult<()> {
    execute_command(command, vars, cwd, false)?;
    Ok(())
}

pub fn command_output(command: &str, vars: &[Env<'_>], cwd: &Path) -> DfxResult<Vec<u8>> {
    execute_command(command, vars, cwd, true)
}

type Env<'a> = (Cow<'static, str>, Cow<'a, OsStr>);

pub fn get_and_write_environment_variables<'a>(
    info: &CanisterInfo,
    network_name: &'a str,
    pool: &'a CanisterPool,
    dependencies: &[CanisterId],
    write_path: Option<&Path>,
) -> DfxResult<Vec<Env<'a>>> {
    // should not return Err unless write_environment_variables does
    use Cow::*;
    let mut vars = vec![
        (
            Borrowed("DFX_VERSION"),
            Borrowed(dfx_version_str().as_ref()),
        ),
        (Borrowed("DFX_NETWORK"), Borrowed(network_name.as_ref())),
    ];
    for dep in dependencies {
        let canister = pool.get_canister(dep).unwrap();
        if let Some(candid_path) = canister.get_info().get_remote_candid_if_remote() {
            vars.push((
                Owned(format!(
                    "CANISTER_CANDID_PATH_{}",
                    canister.get_name().replace('-', "_").to_ascii_uppercase()
                )),
                Owned(candid_path.as_os_str().to_owned()),
            ));
        } else if let Some(output) = canister.get_build_output() {
            let candid_path = match &output.idl {
                IdlBuildOutput::File(p) => p.as_os_str(),
            };

            vars.push((
                Owned(format!(
                    "CANISTER_CANDID_PATH_{}",
                    canister.get_name().replace('-', "_").to_ascii_uppercase()
                )),
                Borrowed(candid_path),
            ));
        }
    }
    for canister in pool.get_canister_list() {
        vars.push((
            Owned(format!(
                "CANISTER_ID_{}",
                canister.get_name().replace('-', "_").to_ascii_uppercase(),
            )),
            Owned(canister.canister_id().to_text().into()),
        ));
    }
    if let Ok(id) = info.get_canister_id() {
        vars.push((Borrowed("CANISTER_ID"), Owned(format!("{}", id).into())));
    }
    vars.push((
        Borrowed("CANISTER_CANDID_PATH"),
        Owned(info.get_output_idl_path().into()),
    ));

    if let Some(write_path) = write_path {
        write_environment_variables(&vars, write_path)?;
    }
    Ok(vars)
}

fn write_environment_variables(vars: &[Env<'_>], write_path: &Path) -> DfxResult {
    const START_TAG: &str = "\n# DFX CANISTER ENVIRONMENT VARIABLES";
    const END_TAG: &str = "\n# END DFX CANISTER ENVIRONMENT VARIABLES";
    let mut write_string = String::from(START_TAG);
    for (var, val) in vars {
        if let Some(val) = val.to_str() {
            write!(write_string, "\n{var}='{val}'").unwrap();
        }
    }
    write_string.push_str(END_TAG);
    if write_path.try_exists()? {
        // modify the existing file
        let mut existing_file = fs::read_to_string(write_path)?;
        let start_pos = existing_file.rfind(START_TAG);
        if let Some(start_pos) = start_pos {
            // the file exists and already contains our variables, modify only that section
            let end_pos = existing_file[start_pos + START_TAG.len()..].find(END_TAG);
            if let Some(end_pos) = end_pos {
                // the section is correctly formed
                let end_pos = end_pos + END_TAG.len() + start_pos + START_TAG.len();
                existing_file.replace_range(start_pos..end_pos, &write_string);
                dfx_core::fs::write(write_path, existing_file)?;
                return Ok(());
            } else {
                // the file has been edited, so we don't know how much to delete, so we append instead
            }
        }
        // append to the existing file
        existing_file.push_str(&write_string);
        dfx_core::fs::write(write_path, existing_file)?;
    } else {
        // no existing file, okay to clobber
        dfx_core::fs::write(write_path, write_string)?;
    }
    Ok(())
}

#[derive(Clone, Debug)]
pub struct BuildConfig {
    profile: Profile,
    pub build_mode_check: bool,
    pub network_name: String,
    pub network_is_playground: bool,

    /// The root of all IDL files.
    pub idl_root: PathBuf,
    /// The root for all language server files.
    pub lsp_root: PathBuf,
    /// The root for all build files.
    pub build_root: PathBuf,
    /// If only a subset of canisters should be built, then canisters_to_build contains these canisters' names.
    /// If all canisters should be built, then this is None.
    pub canisters_to_build: Option<Vec<String>>,
    /// If environment variables should be output to a `.env` file, `env_file` is set to its path.
    pub env_file: Option<PathBuf>,
}

impl BuildConfig {
    #[context("Failed to create build config.")]
    pub fn from_config(config: &Config, network_is_playground: bool) -> DfxResult<Self> {
        let config_intf = config.get_config();
        let network_name = util::network_to_pathcompat(&get_network_context()?);
        let network_root = config.get_temp_path()?.join(&network_name);
        let canister_root = network_root.join("canisters");

        Ok(BuildConfig {
            network_name,
            network_is_playground,
            profile: config_intf.profile.unwrap_or(Profile::Debug),
            build_mode_check: false,
            build_root: canister_root.clone(),
            idl_root: canister_root.join("idl/"), // TODO: possibly move to `network_root.join("idl/")`
            lsp_root: network_root.join("lsp/"),
            canisters_to_build: None,
            env_file: config.get_output_env_file(None)?,
        })
    }

    pub fn with_build_mode_check(self, build_mode_check: bool) -> Self {
        Self {
            build_mode_check,
            ..self
        }
    }

    pub fn with_canisters_to_build(self, canisters: Vec<String>) -> Self {
        Self {
            canisters_to_build: Some(canisters),
            ..self
        }
    }

    pub fn with_env_file(self, env_file: Option<PathBuf>) -> Self {
        Self { env_file, ..self }
    }
}

pub struct BuilderPool {
    builders: BTreeMap<&'static str, Arc<dyn CanisterBuilder>>,
}

impl BuilderPool {
    #[context("Failed to create new builder pool.")]
    pub fn new(env: &dyn Environment) -> DfxResult<Self> {
        let builders = BTreeMap::from([
            (
                "assets",
                Arc::new(assets::AssetsBuilder::new(env)?) as Arc<dyn CanisterBuilder>,
            ),
            ("custom", Arc::new(custom::CustomBuilder::new(env)?)),
            ("motoko", Arc::new(motoko::MotokoBuilder::new(env)?)),
            ("rust", Arc::new(rust::RustBuilder::new(env)?)),
            ("pull", Arc::new(pull::PullBuilder::new(env)?)),
        ]);

        Ok(Self { builders })
    }

    pub fn get(&self, info: &CanisterInfo) -> Arc<dyn CanisterBuilder> {
        self.builders[info.get_type_specific_properties().name()].clone()
    }
}


-----------------------

/src/dfx/src/lib/builders/motoko.rs:
-----------------------

use crate::lib::builders::{
    BuildConfig, BuildOutput, CanisterBuilder, IdlBuildOutput, WasmBuildOutput,
};
use crate::lib::canister_info::motoko::MotokoCanisterInfo;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::{BuildError, DfxError, DfxResult};
use crate::lib::metadata::names::{CANDID_ARGS, CANDID_SERVICE};
use crate::lib::models::canister::CanisterPool;
use crate::lib::package_arguments::{self, PackageArguments};
use crate::util::assets::management_idl;
use anyhow::Context;
use candid::Principal as CanisterId;
use dfx_core::config::cache::Cache;
use dfx_core::config::model::dfinity::{MetadataVisibility, Profile};
use fn_error_context::context;
use slog::{info, o, trace, warn, Logger};
use std::collections::{BTreeMap, BTreeSet};
use std::convert::TryFrom;
use std::fmt::Debug;
use std::path::{Path, PathBuf};
use std::process::Output;
use std::sync::Arc;

pub struct MotokoBuilder {
    logger: slog::Logger,
    cache: Arc<dyn Cache>,
}
unsafe impl Send for MotokoBuilder {}
unsafe impl Sync for MotokoBuilder {}

impl MotokoBuilder {
    #[context("Failed to create MotokoBuilder.")]
    pub fn new(env: &dyn Environment) -> DfxResult<Self> {
        Ok(MotokoBuilder {
            logger: env.get_logger().new(o! {
                "module" => "motoko"
            }),
            cache: env.get_cache(),
        })
    }
}

#[context("Failed to find imports for canister at '{}'.", info.get_main_path().display())]
fn get_imports(cache: &dyn Cache, info: &MotokoCanisterInfo) -> DfxResult<BTreeSet<MotokoImport>> {
    #[context("Failed recursive dependency detection at {}.", file.display())]
    fn get_imports_recursive(
        cache: &dyn Cache,
        workspace_root: &Path,
        file: &Path,
        result: &mut BTreeSet<MotokoImport>,
    ) -> DfxResult {
        if result.contains(&MotokoImport::Relative(file.to_path_buf())) {
            return Ok(());
        }

        result.insert(MotokoImport::Relative(file.to_path_buf()));

        let mut command = cache.get_binary_command("moc")?;
        command.current_dir(workspace_root);
        let command = command.arg("--print-deps").arg(file);
        let output = command
            .output()
            .with_context(|| format!("Error executing {:#?}", command))?;
        let output = String::from_utf8_lossy(&output.stdout);

        for line in output.lines() {
            let import = MotokoImport::try_from(line).context("Failed to create MotokoImport.")?;
            match import {
                MotokoImport::Relative(path) => {
                    get_imports_recursive(cache, workspace_root, path.as_path(), result)?;
                }
                _ => {
                    result.insert(import);
                }
            }
        }

        Ok(())
    }

    let mut result = BTreeSet::new();
    get_imports_recursive(
        cache,
        info.get_workspace_root(),
        info.get_main_path(),
        &mut result,
    )?;

    Ok(result)
}

impl CanisterBuilder for MotokoBuilder {
    #[context("Failed to get dependencies for canister '{}'.", info.get_name())]
    fn get_dependencies(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
    ) -> DfxResult<Vec<CanisterId>> {
        let motoko_info = info.as_info::<MotokoCanisterInfo>()?;
        let imports = get_imports(self.cache.as_ref(), &motoko_info)?;

        Ok(imports
            .iter()
            .filter_map(|import| {
                if let MotokoImport::Canister(name) = import {
                    pool.get_first_canister_with_name(name)
                } else {
                    None
                }
            })
            .map(|canister| canister.canister_id())
            .collect())
    }

    #[context("Failed to build Motoko canister '{}'.", canister_info.get_name())]
    fn build(
        &self,
        pool: &CanisterPool,
        canister_info: &CanisterInfo,
        config: &BuildConfig,
    ) -> DfxResult<BuildOutput> {
        let motoko_info = canister_info.as_info::<MotokoCanisterInfo>()?;
        let profile = config.profile;
        let input_path = motoko_info.get_main_path();
        let output_wasm_path = motoko_info.get_output_wasm_path();

        let id_map = pool
            .get_canister_list()
            .iter()
            .map(|c| (c.get_name().to_string(), c.canister_id().to_text()))
            .collect();

        std::fs::create_dir_all(motoko_info.get_output_root()).with_context(|| {
            format!(
                "Failed to create {}.",
                motoko_info.get_output_root().to_string_lossy()
            )
        })?;
        let cache = &self.cache;
        let idl_dir_path = &config.idl_root;
        std::fs::create_dir_all(idl_dir_path)
            .with_context(|| format!("Failed to create {}.", idl_dir_path.to_string_lossy()))?;

        // If the management canister is being imported, emit the candid file.
        if get_imports(cache.as_ref(), &motoko_info)?
            .contains(&MotokoImport::Ic("aaaaa-aa".to_string()))
        {
            let management_idl_path = idl_dir_path.join("aaaaa-aa.did");
            dfx_core::fs::write(management_idl_path, management_idl()?)?;
        }

        let dependencies = self
            .get_dependencies(pool, canister_info)
            .unwrap_or_default();
        super::get_and_write_environment_variables(
            canister_info,
            &config.network_name,
            pool,
            &dependencies,
            config.env_file.as_deref(),
        )?;

        let package_arguments = package_arguments::load(
            cache.as_ref(),
            motoko_info.get_packtool(),
            canister_info.get_workspace_root(),
        )?;

        let moc_arguments = match motoko_info.get_args() {
            Some(args) => [
                package_arguments,
                args.split_whitespace().map(str::to_string).collect(),
            ]
            .concat(),
            None => package_arguments,
        };

        let candid_service_metadata_visibility = canister_info
            .get_metadata(CANDID_SERVICE)
            .map(|m| m.visibility)
            .unwrap_or(MetadataVisibility::Public);

        let candid_args_metadata_visibility = canister_info
            .get_metadata(CANDID_ARGS)
            .map(|m| m.visibility)
            .unwrap_or(MetadataVisibility::Public);

        // Generate wasm
        let params = MotokoParams {
            build_target: match profile {
                Profile::Release => BuildTarget::Release,
                _ => BuildTarget::Debug,
            },
            suppress_warning: false,
            input: input_path,
            package_arguments: &moc_arguments,
            candid_service_metadata_visibility,
            candid_args_metadata_visibility,
            output: output_wasm_path,
            idl_path: idl_dir_path,
            idl_map: &id_map,
            workspace_root: canister_info.get_workspace_root(),
        };
        motoko_compile(&self.logger, cache.as_ref(), &params)?;

        Ok(BuildOutput {
            canister_id: canister_info
                .get_canister_id()
                .expect("Could not find canister ID."),
            wasm: WasmBuildOutput::File(motoko_info.get_output_wasm_path().to_path_buf()),
            idl: IdlBuildOutput::File(canister_info.get_output_idl_path().to_path_buf()),
        })
    }

    fn get_candid_path(
        &self,
        _pool: &CanisterPool,
        info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult<PathBuf> {
        // get the path to candid file from dfx build
        Ok(info.get_output_idl_path().to_path_buf())
    }
}

type CanisterIdMap = BTreeMap<String, String>;
enum BuildTarget {
    Release,
    Debug,
}

struct MotokoParams<'a> {
    build_target: BuildTarget,
    workspace_root: &'a Path,
    idl_path: &'a Path,
    idl_map: &'a CanisterIdMap,
    package_arguments: &'a PackageArguments,
    candid_service_metadata_visibility: MetadataVisibility,
    candid_args_metadata_visibility: MetadataVisibility,
    output: &'a Path,
    input: &'a Path,
    // The following fields are control flags for dfx and will not be used by self.to_args()
    suppress_warning: bool,
}

impl MotokoParams<'_> {
    fn to_args(&self, cmd: &mut std::process::Command) {
        cmd.arg(self.input);
        cmd.arg("-o").arg(self.output);
        match self.build_target {
            BuildTarget::Release => cmd.args(["-c", "--release"]),
            BuildTarget::Debug => cmd.args(["-c", "--debug"]),
        };
        cmd.arg("--idl").arg("--stable-types");
        if self.candid_service_metadata_visibility == MetadataVisibility::Public {
            // moc defaults to private metadata, if this argument is not present.
            cmd.arg("--public-metadata").arg(CANDID_SERVICE);
        }
        if self.candid_args_metadata_visibility == MetadataVisibility::Public {
            // moc defaults to private metadata, if this argument is not present.
            cmd.arg("--public-metadata").arg(CANDID_ARGS);
        }
        if !self.idl_map.is_empty() {
            cmd.arg("--actor-idl").arg(self.idl_path);
            for (name, canister_id) in self.idl_map.iter() {
                cmd.args(["--actor-alias", name, canister_id]);
            }
        };
        cmd.args(self.package_arguments);
    }
}

/// Compile a motoko file.
#[context("Failed to compile Motoko.")]
fn motoko_compile(logger: &Logger, cache: &dyn Cache, params: &MotokoParams<'_>) -> DfxResult {
    let mut cmd = cache.get_binary_command("moc")?;
    cmd.current_dir(params.workspace_root);
    params.to_args(&mut cmd);
    run_command(logger, &mut cmd, params.suppress_warning).context("Failed to run 'moc'.")?;
    Ok(())
}

#[derive(Debug, PartialOrd, Ord, PartialEq, Eq)]
enum MotokoImport {
    Canister(String),
    Ic(String),
    Lib(String),
    Relative(PathBuf),
}

impl TryFrom<&str> for MotokoImport {
    type Error = DfxError;

    fn try_from(line: &str) -> Result<Self, DfxError> {
        let (url, fullpath) = match line.find(' ') {
            Some(index) => {
                if index >= line.len() - 1 {
                    return Err(DfxError::new(BuildError::DependencyError(format!(
                        "Unknown import {}",
                        line
                    ))));
                }
                let (url, fullpath) = line.split_at(index + 1);
                (url.trim_end(), Some(fullpath))
            }
            None => (line, None),
        };
        let import = match url.find(':') {
            Some(index) => {
                if index >= line.len() - 1 {
                    return Err(DfxError::new(BuildError::DependencyError(format!(
                        "Unknown import {}",
                        url
                    ))));
                }
                let (prefix, name) = url.split_at(index + 1);
                match prefix {
                    "canister:" => MotokoImport::Canister(name.to_owned()),
                    "ic:" => MotokoImport::Ic(name.to_owned()),
                    "mo:" => MotokoImport::Lib(name.to_owned()),
                    _ => {
                        return Err(DfxError::new(BuildError::DependencyError(format!(
                            "Unknown import {}",
                            url
                        ))))
                    }
                }
            }
            None => match fullpath {
                Some(fullpath) => {
                    let path = PathBuf::from(fullpath);
                    if !path.is_file() {
                        return Err(DfxError::new(BuildError::DependencyError(format!(
                            "Cannot find import file {}",
                            path.display()
                        ))));
                    };
                    MotokoImport::Relative(path)
                }
                None => {
                    return Err(DfxError::new(BuildError::DependencyError(format!(
                        "Cannot resolve relative import {}",
                        url
                    ))))
                }
            },
        };

        Ok(import)
    }
}

fn run_command(
    logger: &slog::Logger,
    cmd: &mut std::process::Command,
    suppress_warning: bool,
) -> DfxResult<Output> {
    trace!(logger, r#"Running {}..."#, format!("{:?}", cmd));

    let output = cmd.output().context("Error while executing command.")?;
    if !output.status.success() {
        Err(DfxError::new(BuildError::CommandError(
            format!("{:?}", cmd),
            output.status,
            String::from_utf8_lossy(&output.stdout).to_string(),
            String::from_utf8_lossy(&output.stderr).to_string(),
        )))
    } else {
        if !output.stdout.is_empty() {
            info!(logger, "{}", String::from_utf8_lossy(&output.stdout));
        }
        if !suppress_warning && !output.stderr.is_empty() {
            warn!(logger, "{}", String::from_utf8_lossy(&output.stderr));
        }
        Ok(output)
    }
}


-----------------------

/src/dfx/src/lib/builders/pull.rs:
-----------------------

use crate::lib::builders::{
    BuildConfig, BuildOutput, CanisterBuilder, IdlBuildOutput, WasmBuildOutput,
};
use crate::lib::canister_info::pull::PullCanisterInfo;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::models::canister::CanisterPool;
use candid::Principal as CanisterId;
use fn_error_context::context;
use slog::o;
use std::path::PathBuf;

pub struct PullBuilder {
    _logger: slog::Logger,
}

impl PullBuilder {
    #[context("Failed to create PullBuilder.")]
    pub fn new(env: &dyn Environment) -> DfxResult<Self> {
        Ok(Self {
            _logger: env.get_logger().new(o! {
                "module" => "pull"
            }),
        })
    }
}

impl CanisterBuilder for PullBuilder {
    #[context("Failed to get dependencies for canister '{}'.", info.get_name())]
    fn get_dependencies(
        &self,
        _pool: &CanisterPool,
        info: &CanisterInfo,
    ) -> DfxResult<Vec<CanisterId>> {
        Ok(vec![])
    }

    #[context("Failed to build Pull canister '{}'.", canister_info.get_name())]
    fn build(
        &self,
        _pool: &CanisterPool,
        canister_info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult<BuildOutput> {
        let pull_info = canister_info.as_info::<PullCanisterInfo>()?;
        Ok(BuildOutput {
            canister_id: *pull_info.get_canister_id(),
            // It's impossible to know if the downloaded wasm is gzip or not with only the info in `dfx.json`.
            wasm: WasmBuildOutput::None,
            idl: IdlBuildOutput::File(canister_info.get_output_idl_path().to_path_buf()),
        })
    }

    fn get_candid_path(
        &self,
        _pool: &CanisterPool,
        info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult<PathBuf> {
        Ok(info.get_output_idl_path().to_path_buf())
    }
}


-----------------------

/src/dfx/src/lib/builders/rust.rs:
-----------------------

use crate::lib::builders::{
    BuildConfig, BuildOutput, CanisterBuilder, IdlBuildOutput, WasmBuildOutput,
};
use crate::lib::canister_info::rust::RustCanisterInfo;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::models::canister::CanisterPool;
use anyhow::{anyhow, bail, Context};
use candid::Principal as CanisterId;
use fn_error_context::context;
use slog::{info, o};
use std::path::PathBuf;
use std::process::Command;
use std::process::Stdio;

pub struct RustBuilder {
    logger: slog::Logger,
}

impl RustBuilder {
    #[context("Failed to create RustBuilder.")]
    pub fn new(env: &dyn Environment) -> DfxResult<Self> {
        Ok(RustBuilder {
            logger: env.get_logger().new(o! {
                "module" => "rust"
            }),
        })
    }
}

impl CanisterBuilder for RustBuilder {
    #[context("Failed to get dependencies for canister '{}'.", info.get_name())]
    fn get_dependencies(
        &self,
        pool: &CanisterPool,
        info: &CanisterInfo,
    ) -> DfxResult<Vec<CanisterId>> {
        let dependencies = info.get_dependencies()
            .iter()
            .map(|name| {
                pool.get_first_canister_with_name(name)
                    .map(|c| c.canister_id())
                    .map_or_else(
                        || Err(anyhow!("A canister with the name '{}' was not found in the current project.", name.clone())),
                        DfxResult::Ok,
                    )
            })
            .collect::<DfxResult<Vec<CanisterId>>>().with_context(|| format!("Failed to collect dependencies (canister ids) for canister {}.", info.get_name()))?;
        Ok(dependencies)
    }

    #[context("Failed to build Rust canister '{}'.", canister_info.get_name())]
    fn build(
        &self,
        pool: &CanisterPool,
        canister_info: &CanisterInfo,
        config: &BuildConfig,
    ) -> DfxResult<BuildOutput> {
        let rust_info = canister_info.as_info::<RustCanisterInfo>()?;
        let package = rust_info.get_package();

        let canister_id = canister_info.get_canister_id().unwrap();

        let mut cargo = Command::new("cargo");
        cargo
            .stdout(Stdio::inherit())
            .stderr(Stdio::inherit())
            .current_dir(canister_info.get_workspace_root())
            .arg("build")
            .arg("--target")
            .arg("wasm32-unknown-unknown")
            .arg("--release")
            .arg("-p")
            .arg(package)
            .arg("--locked");

        let dependencies = self
            .get_dependencies(pool, canister_info)
            .unwrap_or_default();
        let vars = super::get_and_write_environment_variables(
            canister_info,
            &config.network_name,
            pool,
            &dependencies,
            config.env_file.as_deref(),
        )?;
        for (key, val) in vars {
            cargo.env(key.as_ref(), val);
        }

        info!(
            self.logger,
            "Executing: cargo build --target wasm32-unknown-unknown --release -p {} --locked",
            package
        );
        let output = cargo.output().context("Failed to run 'cargo build'. You might need to run `cargo update` (or a similar command like `cargo vendor`) if you have updated `Cargo.toml`, because `dfx build` uses the --locked flag with Cargo.")?;

        if !output.status.success() {
            bail!("Failed to compile the rust package: {}", package);
        }

        Ok(BuildOutput {
            canister_id,
            wasm: WasmBuildOutput::File(rust_info.get_output_wasm_path().to_path_buf()),
            idl: IdlBuildOutput::File(canister_info.get_output_idl_path().to_path_buf()),
        })
    }

    fn get_candid_path(
        &self,
        _pool: &CanisterPool,
        info: &CanisterInfo,
        _config: &BuildConfig,
    ) -> DfxResult<PathBuf> {
        Ok(info.get_output_idl_path().to_path_buf())
    }
}


-----------------------

/src/dfx/src/lib/canister_info.rs:
-----------------------

#![allow(dead_code)]
use crate::lib::error::DfxResult;
use crate::lib::metadata::config::CanisterMetadataConfig;

use anyhow::{anyhow, bail, Context};
use candid::Principal as CanisterId;
use candid::Principal;
use core::panic;
use dfx_core::config::model::dfinity::{
    CanisterDeclarationsConfig, CanisterMetadataSection, CanisterTypeProperties, Config, Pullable,
    TechStack, WasmOptLevel,
};
use dfx_core::fs::canonicalize;
use dfx_core::network::provider::get_network_context;
use dfx_core::util;
use fn_error_context::context;
use std::path::{Path, PathBuf};
use url::Url;

pub mod assets;
pub mod custom;
pub mod motoko;
pub mod pull;
pub mod rust;
use crate::lib::deps::get_candid_path_in_project;

pub trait CanisterInfoFactory {
    fn create(info: &CanisterInfo) -> DfxResult<Self>
    where
        Self: std::marker::Sized;
}

/// Information about a canister project (source code, destination, etc).
#[derive(Debug)]
pub struct CanisterInfo {
    name: String,

    declarations_config: CanisterDeclarationsConfig,
    remote_id: Option<Principal>, // id on the currently selected network
    remote_candid: Option<PathBuf>, // always exists if the field is configured

    workspace_root: PathBuf,
    output_root: PathBuf, // <project dir>/.dfx/<network>/canisters/<canister>

    canister_id: Option<CanisterId>,

    packtool: Option<String>,
    args: Option<String>,
    type_specific: CanisterTypeProperties,

    dependencies: Vec<String>,
    post_install: Vec<String>,
    main: Option<PathBuf>,
    shrink: Option<bool>,
    optimize: Option<WasmOptLevel>,
    metadata: CanisterMetadataConfig,
    pullable: Option<Pullable>,
    pull_dependencies: Vec<(String, CanisterId)>,
    tech_stack: Option<TechStack>,
    gzip: bool,
    init_arg: Option<String>,
    init_arg_file: Option<String>,
    output_idl_path: PathBuf,
}

impl CanisterInfo {
    #[context("Failed to load canister info for '{}'.", name)]
    pub fn load(
        config: &Config,
        name: &str,
        canister_id: Option<CanisterId>,
    ) -> DfxResult<CanisterInfo> {
        let workspace_root = config.get_path().parent().unwrap();
        let build_defaults = config.get_config().get_defaults().get_build();
        let network_name = get_network_context()?;
        let build_root = config
            .get_temp_path()?
            .join(util::network_to_pathcompat(&network_name))
            .join("canisters");
        std::fs::create_dir_all(&build_root)
            .with_context(|| format!("Failed to create {}.", build_root.to_string_lossy()))?;

        let canister_map = config
            .get_config()
            .canisters
            .as_ref()
            .ok_or_else(|| anyhow!("No canisters in the configuration file."))?;

        let canister_config = canister_map
            .get(name)
            .ok_or_else(|| anyhow!("Cannot find canister '{}',", name.to_string()))?;

        let dependencies = canister_config.dependencies.clone();

        let mut pull_dependencies = vec![];

        for dep in &dependencies {
            let dep_config = canister_map.get(dep).ok_or_else(|| {
                anyhow!(
                    "Cannot find canister '{}' which is a dependency of '{}'",
                    dep,
                    name.to_string()
                )
            })?;

            if let CanisterTypeProperties::Pull { id } = dep_config.type_specific {
                pull_dependencies.push((dep.to_string(), id))
            }
        }

        let declarations_config_pre = canister_config.declarations.clone();

        let remote_id = canister_config
            .remote
            .as_ref()
            .and_then(|remote| remote.id.get(&network_name))
            .copied();
        let remote_candid = canister_config.remote.as_ref().and_then(|r| {
            r.candid
                .as_ref()
                .and_then(|candid| canonicalize(candid).ok())
        });

        // Fill the default config values if None provided
        let declarations_config = CanisterDeclarationsConfig {
            output: declarations_config_pre
                .output
                .or_else(|| Some(workspace_root.join("src/declarations").join(name))),
            bindings: declarations_config_pre
                .bindings
                .or_else(|| Some(vec!["js".to_string(), "ts".to_string(), "did".to_string()])),
            env_override: declarations_config_pre.env_override,
            node_compatibility: declarations_config_pre.node_compatibility,
        };

        let output_root = build_root.join(name);

        let output_idl_path: PathBuf =
            if let (Some(_id), Some(candid)) = (&remote_id, &remote_candid) {
                workspace_root.join(candid)
            } else {
                match &canister_config.type_specific {
                    CanisterTypeProperties::Rust {
                        package: _,
                        crate_name: _,
                        candid,
                    } => workspace_root.join(candid),
                    CanisterTypeProperties::Assets { .. } => output_root.join("assetstorage.did"),
                    CanisterTypeProperties::Custom {
                        wasm: _,
                        candid,
                        build: _,
                    } => {
                        if Url::parse(candid).is_ok() {
                            output_root.join(name).with_extension("did")
                        } else {
                            workspace_root.join(candid)
                        }
                    }
                    CanisterTypeProperties::Motoko => output_root.join(name).with_extension("did"),
                    CanisterTypeProperties::Pull { id } => {
                        get_candid_path_in_project(workspace_root, id)
                    }
                }
            };

        let type_specific = canister_config.type_specific.clone();

        let args = match &canister_config.args {
            Some(args) if !args.is_empty() => canister_config.args.clone(),
            _ => build_defaults.get_args(),
        };

        let post_install = canister_config.post_install.clone().into_vec();
        let metadata = CanisterMetadataConfig::new(&canister_config.metadata, &network_name);

        let gzip = canister_config.gzip.unwrap_or(false);
        let init_arg = canister_config.init_arg.clone();
        let init_arg_file = canister_config.init_arg_file.clone();

        let canister_info = CanisterInfo {
            name: name.to_string(),
            declarations_config,
            remote_id,
            remote_candid,
            workspace_root: workspace_root.to_path_buf(),
            output_root,
            canister_id,
            packtool: build_defaults.get_packtool(),
            args,
            type_specific,
            dependencies,
            post_install,
            main: canister_config.main.clone(),
            shrink: canister_config.shrink,
            optimize: canister_config.optimize,
            metadata,
            pullable: canister_config.pullable.clone(),
            tech_stack: canister_config.tech_stack.clone(),
            pull_dependencies,
            gzip,
            init_arg,
            init_arg_file,
            output_idl_path,
        };

        Ok(canister_info)
    }

    pub fn get_name(&self) -> &str {
        self.name.as_str()
    }
    pub fn get_declarations_config(&self) -> &CanisterDeclarationsConfig {
        &self.declarations_config
    }
    pub fn is_remote(&self) -> bool {
        self.remote_id.is_some()
    }
    pub fn get_remote_id(&self) -> Option<Principal> {
        self.remote_id
    }
    pub fn get_remote_candid(&self) -> Option<PathBuf> {
        self.remote_candid.as_ref().cloned()
    }
    pub fn get_remote_candid_if_remote(&self) -> Option<PathBuf> {
        if self.remote_id.is_some() {
            self.get_remote_candid()
        } else {
            None
        }
    }
    pub fn get_workspace_root(&self) -> &Path {
        &self.workspace_root
    }
    pub fn get_output_root(&self) -> &Path {
        &self.output_root
    }

    #[context("Failed to get canister id for '{}'.", self.name)]
    pub fn get_canister_id(&self) -> DfxResult<CanisterId> {
        match &self.canister_id {
            Some(canister_id) => Ok(*canister_id),
            None => {
                // If we get here, it means there is a logic error in the code.
                // It's not because the user did anything in the wrong order.
                // We need the network type (ephemeral/persistent) in order to load
                // the canister id, so we can't load it here.
                panic!("It is only valid to call get_canister_id after setting the canister id.");
            }
        }
    }

    pub fn get_dependencies(&self) -> &[String] {
        &self.dependencies
    }

    pub fn get_main_file(&self) -> Option<&Path> {
        self.main.as_deref()
    }

    pub fn get_packtool(&self) -> &Option<String> {
        &self.packtool
    }

    pub fn get_post_install(&self) -> &[String] {
        &self.post_install
    }

    pub fn get_args(&self) -> &Option<String> {
        &self.args
    }

    pub fn get_shrink(&self) -> Option<bool> {
        self.shrink
    }

    pub fn get_optimize(&self) -> Option<WasmOptLevel> {
        // Cycles defaults to O3, Size defaults to Oz
        self.optimize.map(|level| match level {
            WasmOptLevel::Cycles => WasmOptLevel::O3,
            WasmOptLevel::Size => WasmOptLevel::Oz,
            other => other,
        })
    }

    /// Path to the wasm module in .dfx that will be install.
    pub fn get_build_wasm_path(&self) -> PathBuf {
        let mut gzip_original = false;
        if let CanisterTypeProperties::Custom { wasm, .. } = &self.type_specific {
            if wasm.ends_with(".gz") {
                gzip_original = true;
            }
        } else if self.is_assets() {
            gzip_original = true;
        }
        let ext = if self.gzip || gzip_original {
            "wasm.gz"
        } else {
            "wasm"
        };
        self.output_root.join(&self.name).with_extension(ext)
    }

    /// Path to the candid file which contains no init types.
    ///
    /// To be imported by dependents.
    pub fn get_service_idl_path(&self) -> PathBuf {
        self.output_root.join("service.did")
    }

    /// Path to the candid file which contains init types.
    ///
    /// To be used when installing the canister.
    pub fn get_constructor_idl_path(&self) -> PathBuf {
        self.output_root.join("constructor.did")
    }

    /// Path to the init_args.txt file which only contains init types.
    ///
    pub fn get_init_args_txt_path(&self) -> PathBuf {
        self.output_root.join("init_args.txt")
    }

    pub fn get_index_js_path(&self) -> PathBuf {
        self.output_root.join("index").with_extension("js")
    }

    /// Path to the candid file from canister builder which should contain init types.
    ///
    /// To be separated into service.did and init_args.
    pub fn get_output_idl_path(&self) -> &Path {
        self.output_idl_path.as_path()
    }

    #[context("Failed to create <Type>CanisterInfo for canister '{}'.", self.name, )]
    pub fn as_info<T: CanisterInfoFactory>(&self) -> DfxResult<T> {
        T::create(self)
    }

    pub fn get_type_specific_properties(&self) -> &CanisterTypeProperties {
        &self.type_specific
    }

    pub fn is_motoko(&self) -> bool {
        matches!(self.type_specific, CanisterTypeProperties::Motoko { .. })
    }

    pub fn is_custom(&self) -> bool {
        matches!(self.type_specific, CanisterTypeProperties::Custom { .. })
    }

    pub fn is_rust(&self) -> bool {
        matches!(self.type_specific, CanisterTypeProperties::Rust { .. })
    }

    pub fn is_assets(&self) -> bool {
        matches!(self.type_specific, CanisterTypeProperties::Assets { .. })
    }

    pub fn is_pull(&self) -> bool {
        matches!(self.type_specific, CanisterTypeProperties::Pull { .. })
    }

    pub fn get_metadata(&self, name: &str) -> Option<&CanisterMetadataSection> {
        self.metadata.get(name)
    }

    pub fn metadata(&self) -> &CanisterMetadataConfig {
        &self.metadata
    }

    pub fn get_pullable(&self) -> Option<Pullable> {
        self.pullable.clone()
    }

    pub fn get_pull_dependencies(&self) -> &[(String, CanisterId)] {
        &self.pull_dependencies
    }

    pub fn get_tech_stack(&self) -> Option<&TechStack> {
        self.tech_stack.as_ref()
    }

    pub fn get_gzip(&self) -> bool {
        self.gzip
    }

    /// Get the init arg from the dfx.json configuration.
    ///
    /// If the `init_arg` field is defined, it will be returned.
    /// If the `init_arg_file` field is defined, the content of the file will be returned.
    /// If both fields are defined, an error will be returned.
    /// If neither field is defined, `None` will be returned.
    pub fn get_init_arg(&self) -> DfxResult<Option<String>> {
        let init_arg_value = match (&self.init_arg, &self.init_arg_file) {
            (Some(_), Some(_)) => {
                bail!("At most one of the fields 'init_arg' and 'init_arg_file' should be defined in `dfx.json`.
Please remove one of them or leave both undefined.");
            }
            (Some(arg), None) => Some(arg.clone()),
            (None, Some(arg_file)) => {
                // The file path is relative to the workspace root.
                let absolute_path = self.get_workspace_root().join(arg_file);
                let content = dfx_core::fs::read_to_string(&absolute_path)?;
                Some(content)
            }
            (None, None) => None,
        };

        Ok(init_arg_value)
    }
}


-----------------------

/src/dfx/src/lib/canister_info/assets.rs:
-----------------------

use crate::lib::canister_info::{CanisterInfo, CanisterInfoFactory};
use crate::lib::error::DfxResult;
use anyhow::{bail, Context};
use dfx_core::config::model::dfinity::CanisterTypeProperties;
use fn_error_context::context;
use std::path::{Path, PathBuf};

pub struct AssetsCanisterInfo {
    input_root: PathBuf,
    source_paths: Vec<PathBuf>,

    output_wasm_path: PathBuf,
    build: Vec<String>,
    workspace: Option<String>,
}

impl AssetsCanisterInfo {
    pub fn get_source_paths(&self) -> Vec<PathBuf> {
        self.source_paths
            .iter()
            .map(|sp| self.input_root.join(sp))
            .collect::<_>()
    }
    pub fn get_build_tasks(&self) -> &[String] {
        &self.build
    }
    pub fn get_npm_workspace(&self) -> Option<&str> {
        self.workspace.as_deref()
    }

    #[context("Failed to assert source paths.")]
    pub fn assert_source_paths(&self) -> DfxResult<()> {
        let source_paths = self.get_source_paths();
        let input_root = &self.input_root;
        let source_paths: Vec<PathBuf> = source_paths.iter().map(|x| input_root.join(x)).collect();
        for source_path in &source_paths {
            let canonical = dfx_core::fs::canonicalize(source_path).with_context(|| {
                format!(
                    "Unable to determine canonical location of asset source path {}",
                    source_path.to_string_lossy()
                )
            })?;
            if !canonical.starts_with(input_root) {
                bail!(
                    "Directory at '{}' is outside the workspace root.",
                    source_path.to_path_buf().display()
                );
            }
        }
        Ok(())
    }
}

impl CanisterInfoFactory for AssetsCanisterInfo {
    fn create(info: &CanisterInfo) -> DfxResult<Self> {
        let input_root = info.get_workspace_root().to_path_buf();
        // If there are no "source" field, we just ignore this.
        let (source_paths, build, workspace) = if let CanisterTypeProperties::Assets {
            source,
            build,
            workspace,
        } = info.type_specific.clone()
        {
            (source, build.into_vec(), workspace)
        } else {
            bail!(
                "Attempted to construct an assets canister from a type:{} canister config",
                info.type_specific.name()
            )
        };

        let output_root = info.get_output_root();

        let output_wasm_path = output_root.join(Path::new("assetstorage.wasm.gz"));

        Ok(AssetsCanisterInfo {
            input_root,
            source_paths,
            output_wasm_path,
            build,
            workspace,
        })
    }
}


-----------------------

/src/dfx/src/lib/canister_info/custom.rs:
-----------------------

use crate::lib::canister_info::{CanisterInfo, CanisterInfoFactory};
use crate::lib::error::DfxResult;
use anyhow::{anyhow, bail};
use dfx_core::config::model::dfinity::CanisterTypeProperties;
use std::path::{Path, PathBuf};
use url::Url;

enum CustomFileLocation {
    OutputPath(PathBuf),
    DownloadUrl(Url),
}

pub struct CustomCanisterInfo {
    input_wasm_url: Option<Url>,
    output_wasm_path: PathBuf,
    input_candid_url: Option<Url>,
    build: Vec<String>,
}

impl CustomCanisterInfo {
    pub fn get_input_wasm_url(&self) -> &Option<Url> {
        &self.input_wasm_url
    }
    pub fn get_output_wasm_path(&self) -> &Path {
        self.output_wasm_path.as_path()
    }
    pub fn get_input_candid_url(&self) -> &Option<Url> {
        &self.input_candid_url
    }
    pub fn get_build_tasks(&self) -> &[String] {
        &self.build
    }
}

impl CanisterInfoFactory for CustomCanisterInfo {
    fn create(info: &CanisterInfo) -> DfxResult<Self> {
        let workspace_root = info.get_workspace_root();
        let (wasm, build, candid) = if let CanisterTypeProperties::Custom {
            wasm,
            build,
            candid,
        } = info.type_specific.clone()
        {
            (wasm, build.into_vec(), candid)
        } else {
            bail!(
                "Attempted to construct a custom canister from a type:{} canister config",
                info.type_specific.name()
            )
        };
        let (input_wasm_url, output_wasm_path) = if let Ok(input_wasm_url) = Url::parse(&wasm) {
            if !build.is_empty() {
                bail!(
                    "Canister '{}' defines its wasm field as a URL, and has a build step.",
                    info.name
                );
            }
            let filename = input_wasm_url.path_segments().ok_or_else(|| {
                anyhow!(
                    "unable to determine path segments for url {}",
                    &input_wasm_url
                )
            })?;
            let filename = filename.last().ok_or_else(|| {
                anyhow!("Unable to determine filename for url {}", &input_wasm_url)
            })?;
            let output_wasm_path = info
                .get_output_root()
                .join(format!("download-{}", filename));
            (Some(input_wasm_url), output_wasm_path)
        } else {
            let output_wasm_path = workspace_root.join(wasm);
            (None, output_wasm_path)
        };
        let input_candid_url = if info.get_remote_candid_if_remote().is_some() {
            None
        } else if let Ok(input_candid_url) = Url::parse(&candid) {
            Some(input_candid_url)
        } else {
            None
        };

        Ok(Self {
            input_wasm_url,
            output_wasm_path,
            input_candid_url,
            build,
        })
    }
}


-----------------------

/src/dfx/src/lib/canister_info/motoko.rs:
-----------------------

use crate::lib::canister_info::{CanisterInfo, CanisterInfoFactory};
use crate::lib::error::DfxResult;
use anyhow::{ensure, Context};
use dfx_core::config::model::dfinity::CanisterTypeProperties;
use std::path::{Path, PathBuf};

pub struct MotokoCanisterInfo {
    input_path: PathBuf,
    output_root: PathBuf,

    output_wasm_path: PathBuf,
    output_stable_path: PathBuf,
    output_did_js_path: PathBuf,
    output_canister_js_path: PathBuf,
    output_assets_root: PathBuf,

    packtool: Option<String>,
    moc_args: Option<String>,

    workspace_root: PathBuf,
}

impl MotokoCanisterInfo {
    pub fn get_main_path(&self) -> &Path {
        self.input_path.as_path()
    }
    pub fn get_output_wasm_path(&self) -> &Path {
        self.output_wasm_path.as_path()
    }
    pub fn get_output_stable_path(&self) -> &Path {
        self.output_stable_path.as_path()
    }
    pub fn get_output_did_js_path(&self) -> &Path {
        self.output_did_js_path.as_path()
    }
    pub fn get_output_canister_js_path(&self) -> &Path {
        self.output_canister_js_path.as_path()
    }
    pub fn get_output_assets_root(&self) -> &Path {
        self.output_assets_root.as_path()
    }
    pub fn get_output_root(&self) -> &Path {
        self.output_root.as_path()
    }
    pub fn get_packtool(&self) -> &Option<String> {
        &self.packtool
    }
    pub fn get_args(&self) -> &Option<String> {
        &self.moc_args
    }
    pub fn get_workspace_root(&self) -> &Path {
        self.workspace_root.as_path()
    }
}

impl CanisterInfoFactory for MotokoCanisterInfo {
    fn create(info: &CanisterInfo) -> DfxResult<MotokoCanisterInfo> {
        let workspace_root = info.get_workspace_root();
        let name = info.get_name();
        ensure!(
            matches!(info.type_specific, CanisterTypeProperties::Motoko { .. }),
            "Attempted to construct a custom canister from a type:{} canister config",
            info.type_specific.name()
        );
        let main_path = info
            .get_main_file()
            .context("`main` attribute is required on Motoko canisters in dfx.json (and Motoko is the default canister type if not otherwise specified)")?;
        let input_path = workspace_root.join(main_path);
        let output_root = info.get_output_root().to_path_buf();
        let output_wasm_path = output_root.join(name).with_extension("wasm");
        let output_stable_path = output_wasm_path.with_extension("most");
        let output_did_js_path = output_wasm_path.with_extension("did.js");
        let output_canister_js_path = output_wasm_path.with_extension("js");
        let output_assets_root = output_root.join("assets");
        let workspace_root = workspace_root.to_path_buf();

        Ok(MotokoCanisterInfo {
            input_path,
            output_root,
            output_wasm_path,
            output_stable_path,
            output_did_js_path,
            output_canister_js_path,
            output_assets_root,
            packtool: info.get_packtool().clone(),
            moc_args: info.get_args().clone(),
            workspace_root,
        })
    }
}


-----------------------

/src/dfx/src/lib/canister_info/pull.rs:
-----------------------

use crate::lib::canister_info::{CanisterInfo, CanisterInfoFactory};
use crate::lib::error::DfxResult;
use anyhow::bail;
use candid::Principal;
use dfx_core::config::model::dfinity::CanisterTypeProperties;

pub struct PullCanisterInfo {
    name: String,
    canister_id: Principal,
}

impl PullCanisterInfo {
    pub fn get_name(&self) -> &str {
        &self.name
    }

    pub fn get_canister_id(&self) -> &Principal {
        &self.canister_id
    }
}

impl CanisterInfoFactory for PullCanisterInfo {
    fn create(info: &CanisterInfo) -> DfxResult<Self> {
        let name = info.get_name().to_string();
        let canister_id = {
            if let CanisterTypeProperties::Pull { id } = info.type_specific.clone() {
                id
            } else {
                bail!(
                    "Attempted to construct a pull canister from a type:{} canister config",
                    info.type_specific.name()
                );
            }
        };

        Ok(Self { name, canister_id })
    }
}


-----------------------

/src/dfx/src/lib/canister_info/rust.rs:
-----------------------

use crate::lib::canister_info::{CanisterInfo, CanisterInfoFactory};
use crate::lib::error::DfxResult;
use anyhow::{bail, ensure, Context};
use cargo_metadata::Metadata;
use dfx_core::config::model::dfinity::CanisterTypeProperties;
use itertools::Itertools;
use std::path::{Path, PathBuf};
use std::process::{Command, Stdio};

pub struct RustCanisterInfo {
    package: String,
    output_wasm_path: PathBuf,
}

impl RustCanisterInfo {
    pub fn get_package(&self) -> &str {
        &self.package
    }

    pub fn get_output_wasm_path(&self) -> &Path {
        self.output_wasm_path.as_path()
    }
}

impl CanisterInfoFactory for RustCanisterInfo {
    fn create(info: &CanisterInfo) -> DfxResult<Self> {
        let metadata = Command::new("cargo")
            .args(["metadata", "--no-deps", "--format-version=1", "--locked"])
            .stderr(Stdio::inherit())
            .stdout(Stdio::piped())
            .output()
            .context("Failed to run `cargo metadata`")?;
        if !metadata.status.success() {
            bail!("`cargo metadata` was unsuccessful");
        }

        let (package, crate_name) = if let CanisterTypeProperties::Rust {
            package,
            crate_name,
            candid: _,
        } = info.type_specific.clone()
        {
            (package, crate_name)
        } else {
            bail!(
                "Attempted to construct a custom canister from a type:{} canister config",
                info.type_specific.name()
            );
        };
        let metadata: Metadata = serde_json::from_slice(&metadata.stdout)
            .context("Failed to read metadata from `cargo metadata`")?;
        let package_info = metadata
            .packages
            .iter()
            .find(|x| x.name == package)
            .with_context(|| format!("No package `{package}` found"))?;
        let (phrasing, crate_name) = if let Some(crate_name) = crate_name {
            (
                format!("crate `{crate_name}` in package `{package}`"),
                crate_name,
            )
        } else {
            (format!("crate `{package}`"), package.clone())
        };
        let mut candidate_targets = package_info.targets.iter().filter(|x| {
            x.crate_types.iter().any(|c| {
                (c == "cdylib" && x.name == crate_name.replace('-', "_"))
                    || (c == "bin" && x.name == crate_name)
            })
        });
        let Some(target) = candidate_targets.next() else {
            if let Some(wrong_type_crate) =
                package_info.targets.iter().find(|x| x.name == crate_name)
            {
                bail!(
                    "The {phrasing} was of type {}, must be either bin or cdylib",
                    wrong_type_crate.crate_types.iter().format("/")
                )
            } else {
                bail!("No {phrasing} found")
            }
        };
        ensure!(
            candidate_targets.next().is_none(),
            "More than one bin/cdylib {phrasing} found"
        );

        let wasm_name = target.name.replace('-', "_");
        let output_wasm_path = metadata
            .target_directory
            .join(format!("wasm32-unknown-unknown/release/{wasm_name}.wasm"))
            .into();

        Ok(Self {
            package,
            output_wasm_path,
        })
    }
}


-----------------------

/src/dfx/src/lib/canister_logs/log_visibility.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::util::clap::parsers::{log_visibility_parser, principal_parser};
use anyhow::anyhow;
use candid::Principal;
use clap::{ArgAction, Args};
use dfx_core::cli::ask_for_consent;
use ic_utils::interfaces::management_canister::{LogVisibility, StatusCallResult};

#[derive(Args, Clone, Debug, Default)]
pub struct LogVisibilityOpt {
    /// Specifies who is allowed to read the canister's logs.
    /// Can be either "controllers" or "public".
    #[arg(
        long,
        value_parser = log_visibility_parser,
        conflicts_with("add_log_viewer"),
        conflicts_with("remove_log_viewer"),
        conflicts_with("set_log_viewer"),
    )]
    log_visibility: Option<LogVisibility>,

    /// Add a principal to the list of log viewers of the canister.
    #[arg(long, action = ArgAction::Append, value_parser = principal_parser, conflicts_with("set_log_viewer"))]
    add_log_viewer: Option<Vec<Principal>>,

    /// Removes a principal from the list of log viewers of the canister.
    #[arg(long, action = ArgAction::Append, value_parser = principal_parser, conflicts_with("set_log_viewer"))]
    remove_log_viewer: Option<Vec<Principal>>,

    /// Specifies the the principal of the log viewer of the canister.
    /// Can be specified more than once.
    #[arg(
        long,
        action = ArgAction::Append,
        value_parser = principal_parser,
        conflicts_with("add_log_viewer"),
        conflicts_with("remove_log_viewer"),
    )]
    set_log_viewer: Option<Vec<Principal>>,
}

impl LogVisibilityOpt {
    pub fn require_current_settings(&self) -> bool {
        self.add_log_viewer.is_some() || self.remove_log_viewer.is_some()
    }

    pub fn from(
        log_visibility: &Option<LogVisibility>,
        log_viewer: &Option<Vec<Principal>>,
    ) -> Option<LogVisibilityOpt> {
        if let Some(log_visibility) = log_visibility {
            return Some(LogVisibilityOpt {
                log_visibility: Some(log_visibility.clone()),
                add_log_viewer: None,
                remove_log_viewer: None,
                set_log_viewer: None,
            });
        }

        if let Some(log_viewer) = log_viewer {
            return Some(LogVisibilityOpt {
                log_visibility: None,
                add_log_viewer: None,
                remove_log_viewer: None,
                set_log_viewer: Some(log_viewer.clone()),
            });
        }

        None
    }

    pub fn to_log_visibility(
        &self,
        env: &dyn Environment,
        current_status: Option<&StatusCallResult>,
    ) -> DfxResult<LogVisibility> {
        let logger = env.get_logger();

        // For public and controllers.
        if let Some(log_visibility) = self.log_visibility.as_ref() {
            return Ok(log_visibility.clone());
        }

        // For setting viewers.
        if let Some(principals) = self.set_log_viewer.as_ref() {
            return Ok(LogVisibility::AllowedViewers(principals.clone()));
        }

        // Get the current viewer list for adding and removing, only for update-settings.
        let mut current_visibility: Option<LogVisibility> = None;
        let mut viewers = match current_status {
            Some(status) => {
                current_visibility = Some(status.settings.log_visibility.clone());
                match &status.settings.log_visibility {
                    LogVisibility::AllowedViewers(viewers) => viewers.clone(),
                    _ => vec![],
                }
            }
            None => vec![],
        };

        // Adding.
        if let Some(added) = self.add_log_viewer.as_ref() {
            if let Some(LogVisibility::Public) = current_visibility {
                let msg = "Current log is public to everyone. Adding log reviewers will make the log only visible to the reviewers.";
                ask_for_consent(msg)?;
            }

            for principal in added {
                if !viewers.iter().any(|x| x == principal) {
                    viewers.push(*principal);
                }
            }
        }

        // Removing.
        if let Some(removed) = self.remove_log_viewer.as_ref() {
            if let Some(visibility) = &current_visibility {
                match visibility {
                    LogVisibility::Public | LogVisibility::Controllers => {
                        return Err(anyhow!("Removing reviewers is not allowed with 'public' or 'controllers' log visibility."));
                    }
                    _ => (),
                }
            }
            for principal in removed {
                if let Some(idx) = viewers.iter().position(|x| x == principal) {
                    viewers.swap_remove(idx);
                } else {
                    slog::warn!(
                        logger,
                        "Principal '{}' is not in the allowed list.",
                        principal.to_text()
                    );
                }
            }
        }

        Ok(LogVisibility::AllowedViewers(viewers))
    }
}


-----------------------

/src/dfx/src/lib/canister_logs/mod.rs:
-----------------------

pub mod log_visibility;


-----------------------

/src/dfx/src/lib/cycles_ledger_types/create_canister.rs:
-----------------------

use candid::{CandidType, Nat, Principal};
use ic_utils::interfaces::management_canister::builders::CanisterSettings;
use serde::Deserialize;
use thiserror::Error;

#[derive(CandidType, Clone, Debug)]
pub struct CreateCanisterArgs {
    pub from_subaccount: Option<icrc_ledger_types::icrc1::account::Subaccount>,
    pub created_at_time: Option<u64>,
    pub amount: u128,
    pub creation_args: Option<CmcCreateCanisterArgs>,
}
#[derive(CandidType, Clone, Debug)]
pub struct CmcCreateCanisterArgs {
    pub subnet_selection: Option<SubnetSelection>,
    pub settings: Option<CanisterSettings>,
}

#[derive(Deserialize, CandidType, Clone, Debug, PartialEq, Eq, Error)]
pub enum CmcCreateCanisterError {
    #[error("Failed to create canister: {create_error}\n{refund_amount} cycles refunded.")]
    Refunded {
        refund_amount: u128,
        create_error: String,
    },
    #[error("Failed to create canister: {create_error}\nRefund failed: {refund_error}")]
    RefundFailed {
        create_error: String,
        refund_error: String,
    },
}

#[derive(CandidType, Clone, Debug)]
pub enum SubnetSelection {
    /// Choose a random subnet that satisfies the specified properties
    Filter(SubnetFilter),
    /// Choose a specific subnet
    Subnet { subnet: Principal },
}
#[derive(CandidType, Clone, Debug)]
pub struct SubnetFilter {
    pub subnet_type: Option<String>,
}
#[derive(CandidType, Clone, Debug, Deserialize, Error)]
pub enum CreateCanisterError {
    #[error("Insufficient funds. Current balance: {balance}")]
    InsufficientFunds { balance: u128 },
    #[error("Local clock too far behind.")]
    TooOld,
    #[error("Local clock too far ahead.")]
    CreatedInFuture { ledger_time: u64 },
    #[error("Cycles ledger temporarily unavailable.")]
    TemporarilyUnavailable,
    #[error("Duplicate of block {duplicate_of}.")]
    Duplicate {
        duplicate_of: Nat,
        canister_id: Option<Principal>,
    },
    #[error("Cycles ledger failed to create canister: {error}")]
    FailedToCreate {
        fee_block: Option<Nat>,
        refund_block: Option<Nat>,
        error: String,
    },
    #[error("Ledger error {error_code}: {message}")]
    GenericError { error_code: Nat, message: String },
}
#[derive(Deserialize, CandidType, Clone, Debug, PartialEq, Eq)]
pub struct CreateCanisterSuccess {
    pub block_id: Nat,
    pub canister_id: Principal,
}


-----------------------

/src/dfx/src/lib/cycles_ledger_types/deposit.rs:
-----------------------

use candid::CandidType;
use icrc_ledger_types::icrc1::account::Account;

#[derive(CandidType, Debug, Clone)]
pub struct DepositArg {
    pub to: Account,
    pub memo: Option<Vec<u8>>,
}


-----------------------

/src/dfx/src/lib/cycles_ledger_types/mod.rs:
-----------------------

pub mod create_canister;
pub mod deposit;
pub mod withdraw;


-----------------------

/src/dfx/src/lib/cycles_ledger_types/withdraw.rs:
-----------------------

// Copied from https://github.com/dfinity/cycles-ledger/blob/main/cycles-ledger/src/endpoints.rs
use candid::{CandidType, Nat, Principal};
use ic_cdk::api::call::RejectionCode;
use icrc_ledger_types::icrc1::account::Subaccount;
use icrc_ledger_types::icrc1::transfer::BlockIndex;
use serde::Deserialize;

pub type NumCycles = Nat;

#[derive(CandidType, Deserialize, Clone, Debug, PartialEq, Eq)]
pub struct WithdrawArgs {
    #[serde(default)]
    pub from_subaccount: Option<Subaccount>,
    pub to: Principal,
    #[serde(default)]
    pub created_at_time: Option<u64>,
    pub amount: NumCycles,
}

#[derive(CandidType, Deserialize, Clone, Debug, PartialEq, Eq)]
pub enum WithdrawError {
    BadFee {
        expected_fee: NumCycles,
    },
    InsufficientFunds {
        balance: NumCycles,
    },
    TooOld,
    CreatedInFuture {
        ledger_time: u64,
    },
    TemporarilyUnavailable,
    Duplicate {
        duplicate_of: BlockIndex,
    },
    FailedToWithdraw {
        fee_block: Option<Nat>,
        rejection_code: RejectionCode,
        rejection_reason: String,
    },
    GenericError {
        error_code: Nat,
        message: String,
    },
    InvalidReceiver {
        receiver: Principal,
    },
}


-----------------------

/src/dfx/src/lib/deps/deploy.rs:
-----------------------

use super::{get_canister_prompt, PulledCanister};
use crate::lib::error::DfxResult;
use crate::lib::state_tree::canister_info::read_state_tree_canister_controllers;
use anyhow::bail;
use candid::Principal;
use fn_error_context::context;
use ic_agent::Agent;
use ic_utils::interfaces::ManagementCanister;
use slog::{info, Logger};

// not use operations::canister::create_canister because we don't want to modify canister_id_store
#[context("Failed to create canister {}", canister_id)]
pub async fn try_create_canister(
    agent: &Agent,
    logger: &Logger,
    canister_id: &Principal,
    pulled_canister: &PulledCanister,
) -> DfxResult {
    let canister_prompt = get_canister_prompt(canister_id, pulled_canister);
    match read_state_tree_canister_controllers(agent, *canister_id).await? {
        Some(cs) if cs.len() == 1 && cs[0] == Principal::anonymous() => Ok(()),
        Some(_) => {
            bail!("Canister {canister_id} has been created before and its controller is not the anonymous identity. Please stop and delete it and then deploy again.");
        }
        None => {
            let mgr = ManagementCanister::create(agent);
            info!(logger, "Creating canister: {canister_prompt}");
            mgr.create_canister()
                .as_provisional_create_with_specified_id(*canister_id)
                .await?;
            Ok(())
        }
    }
}


-----------------------

/src/dfx/src/lib/deps/mod.rs:
-----------------------

use crate::lib::{environment::Environment, error::DfxResult};
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use dfx_core::{
    config::cache::get_cache_root,
    fs::composite::ensure_parent_dir_exists,
    json::{load_json_file, save_json_file},
};
use fn_error_context::context;
use serde::{Deserialize, Serialize};
use sha2::{Digest, Sha256};
use std::{
    collections::BTreeMap,
    path::{Path, PathBuf},
};

pub mod deploy;

#[derive(Serialize, Deserialize, Default)]
pub struct PulledJson {
    pub canisters: BTreeMap<Principal, PulledCanister>,
}

#[derive(Serialize, Deserialize, Default)]
pub struct PulledCanister {
    /// Name of `type: pull` in dfx.json. Omitted if indirect dependency.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// From the dfx metadata of the downloaded wasm module
    #[serde(skip_serializing_if = "Vec::is_empty")]
    #[serde(default)]
    pub dependencies: Vec<Principal>,
    /// The expected module hash of the canister wasm
    /// Will be one of the following:
    ///   - wasm_hash if defined in the dfx metadata
    ///   - wasm_hash_url content if defined in the dfx metadata
    ///   - otherwise read from canister_status
    /// This field is kept here so that users can compare the hash of the downloaded wasm with it
    /// If matched, we get extra confidence that the downloaded wasm is correct
    /// If not matched, it is still acceptable
    pub wasm_hash: String,
    /// The downloaded wasm hash when `dfx deps pull`
    /// It is allowed to be different from `wasm_hash`
    pub wasm_hash_download: String,
    /// From the dfx metadata of the downloaded wasm module
    pub init_guide: String,
    /// From the dfx metadata of the downloaded wasm module
    pub init_arg: Option<String>,
    /// From the candid:args metadata of the downloaded wasm module
    pub candid_args: String,
    /// The downloaded wasm is gzip or not
    pub gzip: bool,
}

impl PulledJson {
    pub fn get_init_guide(&self, canister_id: &Principal) -> DfxResult<&str> {
        match self.canisters.get(canister_id) {
            Some(o) => Ok(&o.init_guide),
            None => bail!("Failed to find {canister_id} in pulled.json"),
        }
    }

    pub fn get_init_arg(&self, canister_id: &Principal) -> DfxResult<Option<&str>> {
        match self.canisters.get(canister_id) {
            Some(o) => Ok(o.init_arg.as_deref()),
            None => bail!("Failed to find {canister_id} in pulled.json"),
        }
    }

    pub fn get_candid_args(&self, canister_id: &Principal) -> DfxResult<&str> {
        let pulled_canister = self
            .canisters
            .get(canister_id)
            .ok_or_else(|| anyhow!("Failed to find {canister_id} in pulled.json"))?;
        Ok(&pulled_canister.candid_args)
    }
}

#[derive(Serialize, Deserialize, Default)]
pub struct InitJson {
    canisters: BTreeMap<Principal, InitItem>,
}

#[derive(Serialize, Deserialize, Default)]
pub struct InitItem {
    /// Init argument in IDL string
    arg_str: Option<String>,
    /// Hex encoded bytes of init argument
    arg_raw: Option<String>,
}

impl InitJson {
    /// Set `init_arg` for a pull dependency.
    ///
    /// The input `arg_str` is optional since users may specify the raw argument directly.
    pub fn set_init_arg(
        &mut self,
        canister_id: &Principal,
        arg_str: Option<String>,
        arg_raw: &[u8],
    ) {
        self.canisters.insert(
            *canister_id,
            InitItem {
                arg_str,
                arg_raw: Some(hex::encode(arg_raw)),
            },
        );
    }

    /// Set empty `init_arg` for a pull dependency.
    pub fn set_empty_init(&mut self, canister_id: &Principal) {
        self.canisters.insert(*canister_id, InitItem::default());
    }

    /// Whether already set `init
    pub fn contains(&self, canister_id: &Principal) -> bool {
        self.canisters.contains_key(canister_id)
    }

    pub fn get_arg_raw(&self, canister_id: &Principal) -> DfxResult<Vec<u8>> {
        let init_item = self.canisters.get(canister_id).ok_or_else(|| {
            anyhow!(
                "Failed to find {0} entry in init.json. Please run `dfx deps init {0}`.",
                canister_id
            )
        })?;
        match &init_item.arg_raw {
            Some(s) => Ok(hex::decode(s)?),
            None => Ok(vec![]),
        }
    }
}

/// Map from canister name to its Principal as defined in `dfx.json.
#[context("Failed to get pull canisters defined in dfx.json.")]
pub fn get_pull_canisters_in_config(
    env: &dyn Environment,
) -> DfxResult<BTreeMap<String, Principal>> {
    Ok(env
        .get_config_or_anyhow()?
        .get_config()
        .get_pull_canisters()?)
}

/// Validate following properties:
///   - whether `pulled.json` is consistent with `dfx.json`
///     - pull canisters in `dfx.json` are in `pulled.json` with the same name
///   - whether the wasm modules in pulled cache are consistent with `pulled.json`
///     - This can happen when the user manually modifies the wasm file in the cache
///     - Or the same canister was pulled in different projects and the downloaded wasm is different
pub fn validate_pulled(
    pulled_json: &PulledJson,
    pull_canisters_in_config: &BTreeMap<String, Principal>,
) -> DfxResult {
    for (name, canister_id) in pull_canisters_in_config {
        let pulled_canister = pulled_json
            .canisters
            .get(canister_id)
            .ok_or_else(|| anyhow!("Failed to find {name}:{canister_id} in pulled.json."))?;
        match &pulled_canister.name {
            Some(s) if s == name => continue,
            Some(other_name) => bail!(
                "{canister_id} is \"{name}\" in dfx.json, but it has name \"{}\" in pulled.json.",
                other_name
            ),
            None => bail!(
                "{canister_id} is \"{name}\" in dfx.json, but it doesn't have name in pulled.json."
            ),
        }
    }

    for (canister_id, pulled_canister) in &pulled_json.canisters {
        let pulled_canister_path = get_pulled_wasm_path(canister_id, pulled_canister.gzip)?;
        let bytes = dfx_core::fs::read(&pulled_canister_path)?;
        let hash_cache = Sha256::digest(bytes);
        let hash_in_json = hex::decode(&pulled_canister.wasm_hash_download)
            .with_context(|| format!{"In pulled.json, the `wasm_hash_download` field of {canister_id} is invalid."})?;
        if hash_cache.as_slice() != hash_in_json {
            let hash_cache = hex::encode(hash_cache.as_slice());
            let hash_in_json = &pulled_canister.wasm_hash_download;
            bail!(
                "The wasm of {canister_id} in pulled cache has different hash than in pulled.json:
    The pulled cache is at {pulled_canister_path:?}. Its hash is:
        {hash_cache}
    The hash (wasm_hash_download) in pulled.json is:
        {hash_in_json}
The pulled cache may be modified manually or the same canister was pulled in different projects."
            );
        }
    }

    Ok(())
}

fn get_deps_dir(project_root: &Path) -> PathBuf {
    project_root.join("deps")
}

/// The path of the candid file of a direct dependency.
///
/// `deps/candid/<PRINCIPAL>.did`.
pub fn get_candid_path_in_project(project_root: &Path, canister_id: &Principal) -> PathBuf {
    get_deps_dir(project_root)
        .join("candid")
        .join(canister_id.to_text())
        .with_extension("did")
}

fn get_init_json_path(project_root: &Path) -> PathBuf {
    get_deps_dir(project_root).join("init.json")
}

fn get_pulled_json_path(project_root: &Path) -> PathBuf {
    get_deps_dir(project_root).join("pulled.json")
}

/// Load `pulled.json` in `deps/`.
#[context("Failed to read pulled.json. Please (re)run `dfx deps pull`.")]
pub fn load_pulled_json(project_root: &Path) -> DfxResult<PulledJson> {
    let pulled_json_path = get_pulled_json_path(project_root);
    let pulled_json = load_json_file(&pulled_json_path)?;
    Ok(pulled_json)
}

/// Save `pulled.json` in `deps/`.
#[context("Failed to save pulled.json")]
pub fn save_pulled_json(project_root: &Path, pulled_json: &PulledJson) -> DfxResult {
    let pulled_json_path = get_pulled_json_path(project_root);
    ensure_parent_dir_exists(&pulled_json_path)?;
    save_json_file(&pulled_json_path, pulled_json)?;
    Ok(())
}

/// Create `init.json` in `deps/`.
#[context("Failed to create init.json")]
pub fn create_init_json_if_not_existed(project_root: &Path) -> DfxResult {
    let init_json_path = get_init_json_path(project_root);
    if !init_json_path.exists() {
        let init_json = InitJson::default();
        ensure_parent_dir_exists(&init_json_path)?;
        save_json_file(&init_json_path, &init_json)?;
    }
    Ok(())
}

/// Load the `init.json` in `deps/`.
#[context("Failed to read init.json. Please run `dfx deps init`.")]
pub fn load_init_json(project_root: &Path) -> DfxResult<InitJson> {
    let init_json_path = get_init_json_path(project_root);
    let init_json = load_json_file(&init_json_path)?;
    Ok(init_json)
}

/// Save `init.json` in `deps/`.
#[context("Failed to save init.json")]
pub fn save_init_json(project_root: &Path, init_json: &InitJson) -> DfxResult {
    let init_json_path = get_init_json_path(project_root);
    ensure_parent_dir_exists(&init_json_path)?;
    save_json_file(&init_json_path, init_json)?;
    Ok(())
}

/// The path of the downloaded .wasm or .wasm.gz file.
#[context("Failed to get the wasm path of pulled canister \"{canister_id}\"")]
pub fn get_pulled_wasm_path(canister_id: &Principal, gzip: bool) -> DfxResult<PathBuf> {
    let p = get_pulled_canister_dir(canister_id)?.join("canister");
    match gzip {
        true => Ok(p.with_extension("wasm.gz")),
        false => Ok(p.with_extension("wasm")),
    }
}

/// The path of service.did file extracted from the downloaded wasm.
#[context("Failed to get the service candid path of pulled canister \"{canister_id}\"")]
pub fn get_pulled_service_candid_path(canister_id: &Principal) -> DfxResult<PathBuf> {
    Ok(get_pulled_canister_dir(canister_id)?.join("service.did"))
}

/// The path of the dir contains wasm and service.did.
pub fn get_pulled_canister_dir(canister_id: &Principal) -> DfxResult<PathBuf> {
    let p = get_cache_root()?;
    Ok(p.join("pulled").join(canister_id.to_text()))
}

/// Get the principal of a pull dependency which must exist in `pulled.json`.
///
/// The input can be one of:
///   - <PRINCIPAL> of any pull dependency
///   - <NAME> of direct dependencies defined in `dfx.json`
pub fn get_pull_canister_or_principal(
    canister: &str,
    pull_canisters_in_config: &BTreeMap<String, Principal>,
    pulled_json: &PulledJson,
) -> DfxResult<Principal> {
    match pull_canisters_in_config.get(canister) {
        Some(canister_id) => Ok(*canister_id),
        None => {
            let p = Principal::from_text(canister).with_context(||
                format!("{canister} is not a valid Principal nor a `type: pull` canister specified in dfx.json")
            )?;
            if pulled_json.canisters.get(&p).is_none() {
                bail!("Could not find {} in pulled.json", &p);
            }
            Ok(p)
        }
    }
}

/// The prompt of a pull dependency.
///
/// Will be one of the following:
///   - "<CANISTER_ID>" if it is not a direct dependency
///   - "<CANISTER_ID> (<NAME>)" if it is a direct dependency with NAME defined in `dfx.json`
pub fn get_canister_prompt(canister_id: &Principal, pulled_canister: &PulledCanister) -> String {
    match &pulled_canister.name {
        Some(name) => format!("{canister_id} ({name})"),
        None => canister_id.to_text(),
    }
}


-----------------------

/src/dfx/src/lib/dfxvm.rs:
-----------------------

use console::Style;

pub fn display_dfxvm_installation_instructions() {
    println!("You can install dfxvm by running the following command:");
    println!();
    let command = Style::new()
        .cyan()
        .apply_to(r#"sh -ci "$(curl -fsSL https://internetcomputer.org/install.sh)""#);
    println!("    {command}");
}


-----------------------

/src/dfx/src/lib/diagnosis.rs:
-----------------------

use crate::lib::error_code;
use anyhow::Error as AnyhowError;
use ic_agent::agent::{RejectCode, RejectResponse};
use ic_agent::AgentError;
use ic_asset::error::{GatherAssetDescriptorsError, SyncError, UploadContentError};
use regex::Regex;
use std::path::Path;
use thiserror::Error as ThisError;

/// Contains two Option<Strings> that can be displayed to the user:
///   - Error explanation: Goes into a bit of detail on what the error is and/or where the user can find out more about it.
///   - Action suggestion: Tells the user how to move forward to resolve the error.
pub type Diagnosis = (Option<String>, Option<String>);
pub const NULL_DIAGNOSIS: Diagnosis = (None, None);

#[derive(ThisError, Debug)]
// This message will appear in the context trace of the stack. The diagnosis should not be displayed there yet.
#[error("Diagnosis was added here.")]
/// If you do not need the generic error diagnosis to run, you can add a DiagnosedError with .context(err: DiagnosedError).
/// In that case, no extra diagnosis is attempted and the last-added explanation and suggestion are printed out.
pub struct DiagnosedError {
    /// A user-friendly explanation of what went wrong.
    pub error_explanation: Option<String>,

    /// Suggestions for the user on how to move forward to recover from the error.
    pub action_suggestion: Option<String>,
}

impl DiagnosedError {
    pub fn new(error_explanation: String, action_suggestion: String) -> Self {
        Self {
            error_explanation: Some(error_explanation),
            action_suggestion: Some(action_suggestion),
        }
    }
}

/// Attempts to give helpful suggestions on how to resolve errors.
pub fn diagnose(err: &AnyhowError) -> Diagnosis {
    if let Some(diagnosed_error) = err.downcast_ref::<DiagnosedError>() {
        return (
            diagnosed_error.error_explanation.clone(),
            diagnosed_error.action_suggestion.clone(),
        );
    }

    if let Some(agent_err) = err.downcast_ref::<AgentError>() {
        if wallet_method_not_found(agent_err) {
            return diagnose_bad_wallet();
        }
        if not_a_controller(agent_err) {
            return diagnose_http_403();
        } else if *agent_err == AgentError::CertificateNotAuthorized() {
            return subnet_not_authorized();
        }
    }

    if let Some(sync_error) = err.downcast_ref::<SyncError>() {
        if duplicate_asset_key_dist_and_src(sync_error) {
            return diagnose_duplicate_asset_key_dist_and_src();
        }
    }

    NULL_DIAGNOSIS
}

fn not_a_controller(err: &AgentError) -> bool {
    // Newer replicas include the error code in the reject response.
    if matches!(
        err,
        AgentError::UncertifiedReject(RejectResponse {
            reject_code: RejectCode::CanisterError,
            error_code: Some(error_code),
            ..
        }) if error_code == error_code::CANISTER_INVALID_CONTROLLER
    ) {
        return true;
    }

    // Older replicas do not include the error code in the reject response.
    // differing behavior between replica and ic-ref:
    // replica gives HTTP403, ic-ref gives HTTP400 with message "Wrong sender"
    matches!(err, AgentError::HttpError(payload) if payload.status == 403)
        || matches!(err, AgentError::HttpError(payload) if payload.status == 400 &&
            matches!(std::str::from_utf8(payload.content.as_slice()), Ok("Wrong sender")))
}

fn wallet_method_not_found(err: &AgentError) -> bool {
    match err {
        AgentError::CertifiedReject(RejectResponse {
            reject_code: RejectCode::CanisterError,
            reject_message,
            ..
        }) if reject_message.contains("Canister has no update method 'wallet_") => true,
        AgentError::UncertifiedReject(RejectResponse {
            reject_code: RejectCode::CanisterError,
            reject_message,
            ..
        }) if reject_message.contains("Canister has no query method 'wallet_") => true,
        _ => false,
    }
}

fn diagnose_http_403() -> Diagnosis {
    let error_explanation = "Each canister has a set of controllers. Only those controllers have access to the canister's management functions (like install_code or stop_canister).\n\
        The principal you are using to call a management function is not part of the controllers.";
    let action_suggestion = "To make the management function call succeed, you have to make sure the principal that calls the function is a controller.
To see the current controllers of a canister, use the 'dfx canister info (--network ic)' command.
To figure out which principal is calling the management function, look at the command you entered:
    If you used '--wallet <wallet id>', then the wallet's principal (the '<wallet id>') is calling the function.
    If you used '--no-wallet' or none of the flags, then your own principal is calling the function. You can see your own principal by running 'dfx identity get-principal'.
To add a principal to the list of controllers, one of the existing controllers has to add the new principal. The base command to do this is 'dfx canister update-settings --add-controller <controller principal to add> <canister id/name or --all> (--network ic)'.
If your wallet is a controller, but not your own principal, then you have to make your wallet perform the call by adding '--wallet <your wallet id>' to the command.

The most common way this error is solved is by running 'dfx canister update-settings --network ic --wallet \"$(dfx identity get-wallet)\" --all --add-controller \"$(dfx identity get-principal)\"'.";
    (
        Some(error_explanation.to_string()),
        Some(action_suggestion.to_string()),
    )
}

fn subnet_not_authorized() -> Diagnosis {
    let action_suggestion = "If you are connecting to a node directly instead of a boundary node, try using --provisional-create-canister-effective-canister-id with a canister id in the subnet's canister range. First non-root subnet: 5v3p4-iyaaa-aaaaa-qaaaa-cai, second non-root subnet: jrlun-jiaaa-aaaab-aaaaa-cai";
    (None, Some(action_suggestion.to_string()))
}

fn duplicate_asset_key_dist_and_src(sync_error: &SyncError) -> bool {
    fn is_src_to_dist(path0: &Path, path1: &Path) -> bool {
        // .../dist/<canister name>/... and .../src/<canister name>/assets/...
        let path0 = path0.to_string_lossy();
        let path1 = path1.to_string_lossy();
        let re = Regex::new(r"(?P<project_dir>.*)/dist/(?P<canister>[^/]*)/(?P<rest>.*)").unwrap();

        if let Some(caps) = re.captures(&path0) {
            let project_dir = caps["project_dir"].to_string();
            let canister = caps["canister"].to_string();
            let rest = caps["rest"].to_string();
            let transformed = format!("{}/src/{}/assets/{}", project_dir, canister, rest);
            return transformed == path1;
        }
        false
    }
    matches!(sync_error,
        SyncError::UploadContentFailed(
            UploadContentError::GatherAssetDescriptorsFailed(
                GatherAssetDescriptorsError::DuplicateAssetKey(_key, path0, path1)))
        if is_src_to_dist(path0, path1)
    )
}

fn diagnose_duplicate_asset_key_dist_and_src() -> Diagnosis {
    let explanation = "An asset key was found in both the dist and src directories.
One or both of the following are a likely explanation:
    - webpack.config.js is configured to copy assets from the src directory to the dist/ directory.
    - there are leftover files in the dist/ directory from a previous build.";
    let suggestion = r#"Perform the following steps:
    1. Remove the CopyPlugin step from webpack.config.js.  It looks like this:
        new CopyPlugin({
              patterns: [
                {
                  from: path.join(__dirname, "src", frontendDirectory, "assets"),
                  to: path.join(__dirname, "dist", frontendDirectory),
                },
              ],
            }),
    2. Delete all files from the dist/ directory."

See also release notes: https://forum.dfinity.org/t/dfx-0-11-0-is-promoted-with-breaking-changes/14327"#;

    (Some(explanation.to_string()), Some(suggestion.to_string()))
}

fn diagnose_bad_wallet() -> Diagnosis {
    let explanation = "\
A wallet has been previously configured (e.g. via `dfx identity set-wallet`).
However, it did not contain a function that dfx was looking for.
This may be because:
    - a wallet was correctly installed, but is outdated
    - `dfx identity set-wallet` was used on a non-wallet canister";
    let suggestion = "\
If you have had the wallet for a while, then you may need to update it with
`dfx wallet upgrade`. The release notes indicate when there is a new wallet.
If you recently ran `dfx identity set-wallet`, and the canister may have been
wrong, you can set a new wallet with
`dfx identity set-wallet <PRINCIPAL> --identity <IDENTITY>`.
If you're using a local replica and configuring a wallet was a mistake, you can
recreate the replica with `dfx stop && dfx start --clean` to start over.";
    (Some(explanation.to_string()), Some(suggestion.to_string()))
}


-----------------------

/src/dfx/src/lib/environment.rs:
-----------------------

use crate::config::cache::DiskBasedCache;
use crate::config::dfx_version;
use crate::lib::error::DfxResult;
use crate::lib::progress_bar::ProgressBar;
use crate::lib::warning::{is_warning_disabled, DfxWarning::MainnetPlainTextIdentity};
use anyhow::anyhow;
use candid::Principal;
use dfx_core::config::cache::Cache;
use dfx_core::config::model::canister_id_store::CanisterIdStore;
use dfx_core::config::model::dfinity::{Config, NetworksConfig};
use dfx_core::config::model::network_descriptor::NetworkDescriptor;
use dfx_core::error::canister_id_store::CanisterIdStoreError;
use dfx_core::error::identity::NewIdentityManagerError;
use dfx_core::error::load_dfx_config::LoadDfxConfigError;
use dfx_core::extension::manager::ExtensionManager;
use dfx_core::identity::identity_manager::{IdentityManager, InitializeIdentity};
use fn_error_context::context;
use ic_agent::{Agent, Identity};
use semver::Version;
use slog::{warn, Logger, Record};
use std::borrow::Cow;
use std::cell::RefCell;
use std::path::PathBuf;
use std::sync::Arc;
use std::time::Duration;

pub trait Environment {
    fn get_cache(&self) -> Arc<dyn Cache>;
    fn get_config(&self) -> Result<Option<Arc<Config>>, LoadDfxConfigError>;
    fn get_networks_config(&self) -> Arc<NetworksConfig>;
    fn get_config_or_anyhow(&self) -> anyhow::Result<Arc<Config>>;

    /// Return a temporary directory for the current project.
    /// If there is no project (no dfx.json), there is no project temp dir.
    fn get_project_temp_dir(&self) -> DfxResult<Option<PathBuf>>;

    fn get_version(&self) -> &Version;

    /// This is value of the name passed to dfx `--identity <name>`
    /// Notably, it is _not_ the name of the default identity or selected identity
    fn get_identity_override(&self) -> Option<&str>;

    // Explicit lifetimes are actually needed for mockall to work properly.
    #[allow(clippy::needless_lifetimes)]
    fn get_agent<'a>(&'a self) -> &'a Agent;

    #[allow(clippy::needless_lifetimes)]
    fn get_network_descriptor<'a>(&'a self) -> &'a NetworkDescriptor;

    fn get_logger(&self) -> &slog::Logger;
    fn get_verbose_level(&self) -> i64;
    fn new_spinner(&self, message: Cow<'static, str>) -> ProgressBar;
    fn new_progress(&self, message: &str) -> ProgressBar;

    fn new_identity_manager(&self) -> Result<IdentityManager, NewIdentityManagerError> {
        IdentityManager::new(
            self.get_logger(),
            self.get_identity_override(),
            InitializeIdentity::Allow,
        )
    }

    // Explicit lifetimes are actually needed for mockall to work properly.
    #[allow(clippy::needless_lifetimes)]
    fn log<'a>(&self, record: &Record<'a>) {
        self.get_logger().log(record);
    }

    fn get_selected_identity(&self) -> Option<&String>;

    fn get_selected_identity_principal(&self) -> Option<Principal>;

    fn get_effective_canister_id(&self) -> Principal;

    fn get_override_effective_canister_id(&self) -> Option<Principal>;

    fn get_extension_manager(&self) -> &ExtensionManager;

    fn get_canister_id_store(&self) -> Result<CanisterIdStore, CanisterIdStoreError> {
        CanisterIdStore::new(
            self.get_logger(),
            self.get_network_descriptor(),
            self.get_config()?,
        )
    }
}

pub enum ProjectConfig {
    NotLoaded,
    NoProject,
    Loaded(Arc<Config>),
}

pub struct EnvironmentImpl {
    project_config: RefCell<ProjectConfig>,
    shared_networks_config: Arc<NetworksConfig>,

    cache: Arc<dyn Cache>,

    version: Version,

    logger: Option<slog::Logger>,
    verbose_level: i64,

    identity_override: Option<String>,

    effective_canister_id: Option<Principal>,

    extension_manager: ExtensionManager,
}

impl EnvironmentImpl {
    pub fn new(extension_manager: ExtensionManager) -> DfxResult<Self> {
        let shared_networks_config = NetworksConfig::new()?;
        let version = dfx_version().clone();

        Ok(EnvironmentImpl {
            cache: Arc::new(DiskBasedCache::with_version(&version)),
            project_config: RefCell::new(ProjectConfig::NotLoaded),
            shared_networks_config: Arc::new(shared_networks_config),
            version: version.clone(),
            logger: None,
            verbose_level: 0,
            identity_override: None,
            effective_canister_id: None,
            extension_manager,
        })
    }

    pub fn with_logger(mut self, logger: slog::Logger) -> Self {
        self.logger = Some(logger);
        self
    }

    pub fn with_identity_override(mut self, identity: Option<String>) -> Self {
        self.identity_override = identity;
        self
    }

    pub fn with_verbose_level(mut self, verbose_level: i64) -> Self {
        self.verbose_level = verbose_level;
        self
    }

    pub fn with_effective_canister_id(mut self, effective_canister_id: Option<String>) -> Self {
        match effective_canister_id {
            None => {
                self.effective_canister_id = None;
                self
            }
            Some(canister_id) => match Principal::from_text(canister_id) {
                Ok(principal) => {
                    self.effective_canister_id = Some(principal);
                    self
                }
                Err(_) => self,
            },
        }
    }

    fn load_config(&self) -> Result<(), LoadDfxConfigError> {
        let config = Config::from_current_dir(Some(&self.extension_manager))?;

        let project_config = config.map_or(ProjectConfig::NoProject, |config| {
            ProjectConfig::Loaded(Arc::new(config))
        });
        self.project_config.replace(project_config);
        Ok(())
    }
}

impl Environment for EnvironmentImpl {
    fn get_cache(&self) -> Arc<dyn Cache> {
        Arc::clone(&self.cache)
    }

    fn get_config(&self) -> Result<Option<Arc<Config>>, LoadDfxConfigError> {
        if matches!(*self.project_config.borrow(), ProjectConfig::NotLoaded) {
            self.load_config()?;
        }

        let config = if let ProjectConfig::Loaded(ref config) = *self.project_config.borrow() {
            Some(Arc::clone(config))
        } else {
            None
        };
        Ok(config)
    }

    fn get_networks_config(&self) -> Arc<NetworksConfig> {
        self.shared_networks_config.clone()
    }

    fn get_config_or_anyhow(&self) -> anyhow::Result<Arc<Config>> {
        self.get_config()?.ok_or_else(|| anyhow!(
            "Cannot find dfx configuration file in the current working directory. Did you forget to create one?"
        ))
    }

    fn get_project_temp_dir(&self) -> DfxResult<Option<PathBuf>> {
        Ok(self.get_config()?.map(|c| c.get_temp_path()).transpose()?)
    }

    fn get_version(&self) -> &Version {
        &self.version
    }

    fn get_identity_override(&self) -> Option<&str> {
        self.identity_override.as_deref()
    }

    fn get_agent(&self) -> &Agent {
        unreachable!("Agent only available from an AgentEnvironment");
    }

    fn get_network_descriptor(&self) -> &NetworkDescriptor {
        // It's not valid to call get_network_descriptor on an EnvironmentImpl.
        // All of the places that call this have an AgentEnvironment anyway.
        unreachable!("NetworkDescriptor only available from an AgentEnvironment");
    }

    fn get_logger(&self) -> &slog::Logger {
        self.logger
            .as_ref()
            .expect("Log was not setup, but is being used.")
    }

    fn get_verbose_level(&self) -> i64 {
        self.verbose_level
    }

    fn new_spinner(&self, message: Cow<'static, str>) -> ProgressBar {
        // Only show the progress bar if the level is INFO or more.
        if self.verbose_level >= 0 {
            ProgressBar::new_spinner(message)
        } else {
            ProgressBar::discard()
        }
    }

    fn new_progress(&self, _message: &str) -> ProgressBar {
        ProgressBar::discard()
    }

    fn get_selected_identity(&self) -> Option<&String> {
        None
    }

    fn get_selected_identity_principal(&self) -> Option<Principal> {
        None
    }

    fn get_effective_canister_id(&self) -> Principal {
        self.effective_canister_id
            .unwrap_or(Principal::from_slice(&[0, 0, 0, 0, 0, 0, 0, 0, 1, 1]))
    }

    fn get_override_effective_canister_id(&self) -> Option<Principal> {
        self.effective_canister_id
    }

    fn get_extension_manager(&self) -> &ExtensionManager {
        &self.extension_manager
    }
}

pub struct AgentEnvironment<'a> {
    backend: &'a dyn Environment,
    agent: Agent,
    network_descriptor: NetworkDescriptor,
    identity_manager: IdentityManager,
    effective_canister_id: Option<Principal>,
}

impl<'a> AgentEnvironment<'a> {
    #[context("Failed to create AgentEnvironment for network '{}'.", network_descriptor.name)]
    pub fn new(
        backend: &'a dyn Environment,
        network_descriptor: NetworkDescriptor,
        timeout: Duration,
        use_identity: Option<&str>,
    ) -> DfxResult<Self> {
        let logger = backend.get_logger().clone();
        let mut identity_manager = backend.new_identity_manager()?;
        let identity = if let Some(identity_name) = use_identity {
            identity_manager.instantiate_identity_from_name(identity_name, &logger)?
        } else {
            identity_manager.instantiate_selected_identity(&logger)?
        };
        if network_descriptor.is_ic
            && identity.insecure
            && !is_warning_disabled(MainnetPlainTextIdentity)
        {
            warn!(logger, "The {} identity is not stored securely. Do not use it to control a lot of cycles/ICP. Create a new identity with `dfx identity new` \
                and use it in mainnet-facing commands with the `--identity` flag", identity.name());
        }
        let url = network_descriptor.first_provider()?;
        let effective_canister_id = if let Some(d) = &network_descriptor.local_server_descriptor {
            d.effective_config()?
                .and_then(|c| c.get_effective_canister_id())
        } else {
            None
        };

        Ok(AgentEnvironment {
            backend,
            agent: create_agent(logger, url, identity, timeout)?,
            network_descriptor: network_descriptor.clone(),
            identity_manager,
            effective_canister_id,
        })
    }
}

impl<'a> Environment for AgentEnvironment<'a> {
    fn get_cache(&self) -> Arc<dyn Cache> {
        self.backend.get_cache()
    }

    fn get_config(&self) -> Result<Option<Arc<Config>>, LoadDfxConfigError> {
        self.backend.get_config()
    }

    fn get_networks_config(&self) -> Arc<NetworksConfig> {
        self.backend.get_networks_config()
    }

    fn get_config_or_anyhow(&self) -> anyhow::Result<Arc<Config>> {
        self.get_config()?.ok_or_else(|| anyhow!(
            "Cannot find dfx configuration file in the current working directory. Did you forget to create one?"
        ))
    }

    fn get_project_temp_dir(&self) -> DfxResult<Option<PathBuf>> {
        self.backend.get_project_temp_dir()
    }

    fn get_version(&self) -> &Version {
        self.backend.get_version()
    }

    fn get_identity_override(&self) -> Option<&str> {
        self.backend.get_identity_override()
    }

    fn get_agent(&self) -> &Agent {
        &self.agent
    }

    fn get_network_descriptor(&self) -> &NetworkDescriptor {
        &self.network_descriptor
    }

    fn get_logger(&self) -> &slog::Logger {
        self.backend.get_logger()
    }

    fn get_verbose_level(&self) -> i64 {
        self.backend.get_verbose_level()
    }

    fn new_spinner(&self, message: Cow<'static, str>) -> ProgressBar {
        self.backend.new_spinner(message)
    }

    fn new_progress(&self, message: &str) -> ProgressBar {
        self.backend.new_progress(message)
    }

    fn get_selected_identity(&self) -> Option<&String> {
        Some(self.identity_manager.get_selected_identity_name())
    }

    fn get_selected_identity_principal(&self) -> Option<Principal> {
        self.identity_manager.get_selected_identity_principal()
    }

    fn get_effective_canister_id(&self) -> Principal {
        self.backend
            .get_override_effective_canister_id()
            .unwrap_or_else(|| {
                self.effective_canister_id
                    .unwrap_or_else(|| self.backend.get_effective_canister_id())
            })
    }

    fn get_override_effective_canister_id(&self) -> Option<Principal> {
        self.backend.get_override_effective_canister_id()
    }

    fn get_extension_manager(&self) -> &ExtensionManager {
        self.backend.get_extension_manager()
    }
}

#[context("Failed to create agent with url {}.", url)]
pub fn create_agent(
    _logger: Logger,
    url: &str,
    identity: Box<dyn Identity + Send + Sync>,
    timeout: Duration,
) -> DfxResult<Agent> {
    let disable_query_verification =
        std::env::var("DFX_DISABLE_QUERY_VERIFICATION").is_ok_and(|x| !x.trim().is_empty());
    let agent = Agent::builder()
        .with_url(url)
        .with_boxed_identity(identity)
        .with_verify_query_signatures(!disable_query_verification)
        .with_ingress_expiry(Some(timeout))
        .build()?;
    Ok(agent)
}


-----------------------

/src/dfx/src/lib/error/build.rs:
-----------------------

use crate::lib::error::DfxError;
use candid::Principal;
use std::process::ExitStatus;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum BuildError {
    #[error("The pre-build all step failed")]
    PreBuildAllStepFailed(#[source] Box<DfxError>),

    #[error("The post-build all step failed")]
    PostBuildAllStepFailed(#[source] Box<DfxError>),

    #[error("The pre-build step failed for canister '{0}' ({1})")]
    PreBuildStepFailed(Principal, String, #[source] Box<DfxError>),

    #[error("The build step failed for canister '{0}' ({1})")]
    BuildStepFailed(Principal, String, #[source] Box<DfxError>),

    #[error("The post-build step failed for canister '{0}' ({1})")]
    PostBuildStepFailed(Principal, String, #[source] Box<DfxError>),

    #[error("The command '{0}' failed with exit status '{1}'.\nStdout:\n{2}\nStderr:\n{3}")]
    CommandError(String, ExitStatus, String, String),

    #[error("The dependency analyzer failed: {0}")]
    DependencyError(String),

    #[error("The JavaScript bindings generator failed: {0}")]
    JsBindGenError(String),

    #[error("The custom tool failed.")]
    CustomToolError(Option<i32>),
}


-----------------------

/src/dfx/src/lib/error/mod.rs:
-----------------------

pub mod build;
pub mod notify_create_canister;
pub mod notify_mint_cycles;
pub mod notify_top_up;
pub mod project;

pub use build::BuildError;
pub use notify_create_canister::NotifyCreateCanisterError;
pub use notify_mint_cycles::NotifyMintCyclesError;
pub use notify_top_up::NotifyTopUpError;
pub use project::ProjectError;

/// The type to represent DFX results.
pub type DfxResult<T = ()> = anyhow::Result<T>;

/// The type to represent DFX errors.
pub type DfxError = anyhow::Error;

#[macro_export]
macro_rules! error_invalid_argument {
    ($($args:tt)*) => {
        anyhow::anyhow!("Invalid argument: {}", format_args!($($args)*))
    }
}

#[macro_export]
macro_rules! error_invalid_config {
    ($($args:tt)*) => {
        anyhow::anyhow!("Invalid configuration: {}", format_args!($($args)*))
    }
}

#[macro_export]
macro_rules! error_invalid_data {
    ($($args:tt)*) => {
        anyhow::anyhow!("Invalid data: {}", format_args!($($args)*))
    }
}

#[macro_export]
macro_rules! error_unknown {
    ($($args:tt)*) => {
        anyhow::anyhow!("Unknown error: {}", format_args!($($args)*))
    }
}


-----------------------

/src/dfx/src/lib/error/notify_create_canister.rs:
-----------------------

use crate::lib::ledger_types::NotifyError;
use ic_agent::AgentError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum NotifyCreateCanisterError {
    #[error("Failed when calling notify_create_canister")]
    Call(#[source] AgentError),

    #[error("Failed to decode notify_create_canister response")]
    DecodeResponse(#[source] candid::Error),

    #[error("Failed to encode notify_create_canister arguments")]
    EncodeArguments(#[source] candid::Error),

    #[error("Failure reported by notify_create_canister: {0:?}")]
    Notify(NotifyError),

    #[error("Failed to determine caller principal: {0}")]
    GetCallerPrincipal(String),
}


-----------------------

/src/dfx/src/lib/error/notify_mint_cycles.rs:
-----------------------

use crate::lib::ledger_types::NotifyError;
use ic_agent::AgentError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum NotifyMintCyclesError {
    #[error("Failed when calling notify_mint_cycles")]
    Call(#[source] AgentError),

    #[error("Failed to decode notify_mint_cycles response")]
    DecodeResponse(#[source] candid::Error),

    #[error("Failed to encode notify_mint_cycles arguments")]
    EncodeArguments(#[source] candid::Error),

    #[error("Failure reported by notify_mint_cycles: {0:?}")]
    Notify(NotifyError),
}


-----------------------

/src/dfx/src/lib/error/notify_top_up.rs:
-----------------------

use crate::lib::ledger_types::NotifyError;
use ic_agent::AgentError;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum NotifyTopUpError {
    #[error("Failed when calling notify_top_up")]
    Call(#[source] AgentError),

    #[error("Failed to decode notify_top_up response")]
    DecodeResponse(#[source] candid::Error),

    #[error("Failed to encode notify_top_up arguments")]
    EncodeArguments(#[source] candid::Error),

    #[error("Failure reported by notify_top_up: {0:?}")]
    Notify(NotifyError),
}


-----------------------

/src/dfx/src/lib/error/project.rs:
-----------------------

use dfx_core::error::fs::{
    CanonicalizePathError, CreateDirAllError, ReadFileError, WriteFileError,
};
use thiserror::Error;

#[derive(Error, Debug)]
pub enum ProjectError {
    #[error(transparent)]
    CanonicalizePath(#[from] CanonicalizePathError),

    #[error(transparent)]
    CreateDirAll(#[from] CreateDirAllError),

    #[error(transparent)]
    StructuredFileError(#[from] dfx_core::error::structured_file::StructuredFileError),

    #[error(transparent)]
    ReadFile(#[from] ReadFileError),

    #[error(transparent)]
    WriteFile(#[from] WriteFileError),

    #[error("Can't convert string '{0}' to path")]
    ConvertingStringToPathFailed(String, #[source] std::convert::Infallible),

    #[error("Tried joining '{0}' and '{1}', but they form an invalid URL")]
    InvalidUrl(url::Url, String, #[source] url::ParseError),

    #[error("The key 'canisters' is missing in dfx.json.")]
    DfxJsonMissingCanisters,

    #[error("The '{0}' value in dfx.json is not an object.")]
    ValueInDfxJsonIsNotJsonObject(String),

    #[error("Unable to parse as url or file")]
    UnableToParseAsUrlOrFile(#[source] url::ParseError),

    #[error("Could not create HTTP client")]
    CouldNotCreateHttpClient(#[source] reqwest::Error),

    #[error("Failed to load project definition from '{0}'")]
    FailedToLoadProjectDefinition(url::Url, #[source] serde_json::Error),

    #[error("Failed to load canister ids from '{0}'")]
    FailedToLoadCanisterIds(url::Url, #[source] serde_json::Error),

    #[error("Failed to get contents of URL '{0}'.")]
    NotFound404(url::Url),

    #[error("Failed to GET resource located at '{0}'")]
    FailedToGetResource(url::Url, #[source] reqwest::Error),

    #[error("Failed to GET resource located at '{0}', server returned an error")]
    GettingResourceReturnedHTTPError(url::Url, #[source] reqwest::Error),

    #[error("Failed to get body from '{0}'")]
    FailedToGetBodyFromResponse(url::Url, #[source] reqwest::Error),

    #[error("Malformed network mapping '{0}': {1} network name is empty")]
    MalformedNetworkMapping(String, String),
}


-----------------------

/src/dfx/src/lib/error_code.rs:
-----------------------

// See https://github.com/dfinity/ic/blob/master/rs/types/error_types/src/lib.rs

pub const CANISTER_INVALID_CONTROLLER: &str = "IC0512";


-----------------------

/src/dfx/src/lib/ic_attributes/mod.rs:
-----------------------

use crate::lib::canister_logs::log_visibility::LogVisibilityOpt;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use anyhow::{anyhow, Context, Error};
use byte_unit::Byte;
use candid::Principal;
use dfx_core::config::model::dfinity::ConfigInterface;
use fn_error_context::context;
use ic_utils::interfaces::management_canister::{
    attributes::{ComputeAllocation, FreezingThreshold, MemoryAllocation, ReservedCyclesLimit},
    builders::WasmMemoryLimit,
    LogVisibility, StatusCallResult,
};
use num_traits::ToPrimitive;
use std::convert::TryFrom;

#[derive(Default, Debug, Clone)]
pub struct CanisterSettings {
    pub controllers: Option<Vec<Principal>>,
    pub compute_allocation: Option<ComputeAllocation>,
    pub memory_allocation: Option<MemoryAllocation>,
    pub freezing_threshold: Option<FreezingThreshold>,
    pub reserved_cycles_limit: Option<ReservedCyclesLimit>,
    pub wasm_memory_limit: Option<WasmMemoryLimit>,
    pub log_visibility: Option<LogVisibility>,
}

impl From<CanisterSettings>
    for ic_utils::interfaces::management_canister::builders::CanisterSettings
{
    fn from(value: CanisterSettings) -> Self {
        Self {
            controllers: value.controllers,
            compute_allocation: value
                .compute_allocation
                .map(u8::from)
                .map(candid::Nat::from),
            memory_allocation: value
                .memory_allocation
                .map(u64::from)
                .map(candid::Nat::from),
            freezing_threshold: value
                .freezing_threshold
                .map(u64::from)
                .map(candid::Nat::from),
            reserved_cycles_limit: value
                .reserved_cycles_limit
                .map(u128::from)
                .map(candid::Nat::from),
            wasm_memory_limit: value
                .wasm_memory_limit
                .map(u64::from)
                .map(candid::Nat::from),
            log_visibility: value.log_visibility,
        }
    }
}

impl TryFrom<ic_utils::interfaces::management_canister::builders::CanisterSettings>
    for CanisterSettings
{
    type Error = Error;
    fn try_from(
        value: ic_utils::interfaces::management_canister::builders::CanisterSettings,
    ) -> Result<Self, anyhow::Error> {
        Ok(Self {
            controllers: value.controllers,
            compute_allocation: value
                .compute_allocation
                .and_then(|alloc| alloc.0.to_u8())
                .map(|alloc| {
                    ComputeAllocation::try_from(alloc)
                        .context("Compute allocation must be a percentage.")
                })
                .transpose()?,
            memory_allocation: value
                .memory_allocation
                .and_then(|alloc| alloc.0.to_u64())
                .map(|alloc| {
                    MemoryAllocation::try_from(alloc).context(
                        "Memory allocation must be between 0 and 2^48 (i.e 256TB), inclusively.",
                    )
                })
                .transpose()?,
            freezing_threshold: value
                .freezing_threshold
                .and_then(|threshold| threshold.0.to_u64())
                .map(|threshold| {
                    FreezingThreshold::try_from(threshold)
                        .context("Freezing threshold must be between 0 and 2^64-1, inclusively.")
                })
                .transpose()?,
            reserved_cycles_limit: value
                .reserved_cycles_limit
                .and_then(|limit| limit.0.to_u128())
                .map(|limit| {
                    ReservedCyclesLimit::try_from(limit).context(
                        "Reserved cycles limit must be between 0 and 2^128-1, inclusively.",
                    )
                })
                .transpose()?,
            wasm_memory_limit: value
                .wasm_memory_limit
                .and_then(|limit| limit.0.to_u64())
                .map(|limit| {
                    WasmMemoryLimit::try_from(limit)
                        .context("Wasm memory limit must be between 0 and 2^48-1, inclusively.")
                })
                .transpose()?,
            log_visibility: value.log_visibility,
        })
    }
}

#[context("Failed to get compute allocation.")]
pub fn get_compute_allocation(
    compute_allocation: Option<u64>,
    config_interface: Option<&ConfigInterface>,
    canister_name: Option<&str>,
) -> DfxResult<Option<ComputeAllocation>> {
    let compute_allocation = match (compute_allocation, config_interface, canister_name) {
        (Some(compute_allocation), _, _) => Some(compute_allocation),
        (None, Some(config_interface), Some(canister_name)) => {
            config_interface.get_compute_allocation(canister_name)? as _
        }
        _ => None,
    };
    compute_allocation
        .map(|arg| {
            ComputeAllocation::try_from(arg).context("Compute Allocation must be a percentage.")
        })
        .transpose()
}

#[context("Failed to get memory allocation.")]
pub fn get_memory_allocation(
    memory_allocation: Option<Byte>,
    config_interface: Option<&ConfigInterface>,
    canister_name: Option<&str>,
) -> DfxResult<Option<MemoryAllocation>> {
    let memory_allocation = match (memory_allocation, config_interface, canister_name) {
        (Some(memory_allocation), _, _) => Some(memory_allocation),
        (None, Some(config_interface), Some(canister_name)) => {
            config_interface.get_memory_allocation(canister_name)?
        }
        _ => None,
    };
    memory_allocation
        .map(|arg| {
            u64::try_from(arg.get_bytes())
                .map_err(|e| anyhow!(e))
                .and_then(|n| Ok(MemoryAllocation::try_from(n)?))
                .context("Memory allocation must be between 0 and 2^48 (i.e 256TB), inclusively.")
        })
        .transpose()
}

#[context("Failed to get freezing threshold.")]
pub fn get_freezing_threshold(
    freezing_threshold: Option<u64>,
    config_interface: Option<&ConfigInterface>,
    canister_name: Option<&str>,
) -> DfxResult<Option<FreezingThreshold>> {
    let freezing_threshold = match (freezing_threshold, config_interface, canister_name) {
        (Some(freezing_threshold), _, _) => Some(freezing_threshold),
        (None, Some(config_interface), Some(canister_name)) => config_interface
            .get_freezing_threshold(canister_name)?
            .map(|dur| dur.as_secs()),
        _ => None,
    };
    freezing_threshold
        .map(|arg| {
            FreezingThreshold::try_from(arg)
                .context("Must be a duration between 0 and 2^64-1 inclusive.")
        })
        .transpose()
}

#[context("Failed to get reserved cycles limit")]
pub fn get_reserved_cycles_limit(
    reserved_cycles_limit: Option<u128>,
    config_interface: Option<&ConfigInterface>,
    canister_name: Option<&str>,
) -> DfxResult<Option<ReservedCyclesLimit>> {
    let reserved_cycles_limit = match (reserved_cycles_limit, config_interface, canister_name) {
        (Some(reserved_cycles_limit), _, _) => Some(reserved_cycles_limit),
        (None, Some(config_interface), Some(canister_name)) => {
            config_interface.get_reserved_cycles_limit(canister_name)?
        }
        _ => None,
    };
    reserved_cycles_limit
        .map(|arg| {
            ReservedCyclesLimit::try_from(arg)
                .context("Must be a limit between 0 and 2^128-1 inclusive.")
        })
        .transpose()
}

pub fn get_wasm_memory_limit(
    wasm_memory_limit: Option<Byte>,
    config_interface: Option<&ConfigInterface>,
    canister_name: Option<&str>,
) -> DfxResult<Option<WasmMemoryLimit>> {
    let wasm_memory_limit = match (wasm_memory_limit, config_interface, canister_name) {
        (Some(memory_limit), _, _) => Some(memory_limit),
        (None, Some(config_interface), Some(canister_name)) => {
            config_interface.get_wasm_memory_limit(canister_name)?
        }
        _ => None,
    };
    wasm_memory_limit
        .map(|arg| {
            u64::try_from(arg.get_bytes())
                .map_err(|e| anyhow!(e))
                .and_then(|n| Ok(WasmMemoryLimit::try_from(n)?))
                .context("Wasm memory limit must be between 0 and 2^48 (i.e 256TB), inclusively.")
        })
        .transpose()
}

pub fn get_log_visibility(
    env: &dyn Environment,
    log_visibility: Option<&LogVisibilityOpt>,
    current_settings: Option<&StatusCallResult>,
    config_interface: Option<&ConfigInterface>,
    canister_name: Option<&str>,
) -> DfxResult<Option<LogVisibility>> {
    let log_visibility = match (log_visibility, config_interface, canister_name) {
        (Some(log_visibility), _, _) => {
            Some(log_visibility.to_log_visibility(env, current_settings)?)
        }
        (None, Some(config_interface), Some(canister_name)) => {
            config_interface.get_log_visibility(canister_name)?
        }
        _ => None,
    };
    Ok(log_visibility)
}


-----------------------

/src/dfx/src/lib/identity/mod.rs:
-----------------------

pub mod wallet;


-----------------------

/src/dfx/src/lib/identity/wallet.rs:
-----------------------

use crate::lib::error::{DfxError, DfxResult};
use crate::lib::root_key::fetch_root_key_if_needed;
use crate::util::assets::wallet_wasm;
use crate::Environment;
use anyhow::{bail, Context};
use candid::Principal;
use dfx_core::canister::build_wallet_canister;
use dfx_core::config::model::network_descriptor::NetworkDescriptor;
use dfx_core::error::canister::CanisterBuilderError;
use dfx_core::error::wallet_config::WalletConfigError;
use dfx_core::identity::wallet::{get_wallet_config_path, wallet_canister_id};
use dfx_core::identity::{Identity, WalletGlobalConfig, WalletNetworkMap};
use ic_agent::agent::{RejectCode, RejectResponse};
use ic_agent::AgentError;
use ic_utils::interfaces::management_canister::builders::InstallMode;
use ic_utils::interfaces::{ManagementCanister, WalletCanister};
use slog::info;
use std::collections::BTreeMap;
use std::path::PathBuf;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum GetOrCreateWalletCanisterError {
    #[error(
        "No wallet configured for combination of identity '{identity}' and network '{network}'"
    )]
    NoWalletConfigured { identity: String, network: String },

    #[error("Failed to create wallet")]
    CreationFailed(#[source] Box<DfxError>),

    #[error(transparent)]
    WalletConfigError(#[from] WalletConfigError),

    #[error(transparent)]
    CanisterBuilderError(#[from] CanisterBuilderError),
}

/// Gets the currently configured wallet canister. If none exists yet and `create` is true, then this creates a new wallet. WARNING: Creating a new wallet costs ICP!
///
/// While developing locally, this always creates a new wallet, even if `create` is false.
/// This can be inhibited by setting the DFX_DISABLE_AUTO_WALLET env var.
pub async fn get_or_create_wallet(
    env: &dyn Environment,
    network: &NetworkDescriptor,
    name: &str,
) -> Result<Principal, GetOrCreateWalletCanisterError> {
    match wallet_canister_id(network, name)? {
        None => {
            // If the network is not the IC, we ignore the error and create a new wallet for the identity.
            if !network.is_ic && std::env::var("DFX_DISABLE_AUTO_WALLET").is_err() {
                create_wallet(env, network, name, None)
                    .await
                    .map_err(|err| GetOrCreateWalletCanisterError::CreationFailed(Box::new(err)))
            } else {
                Err(GetOrCreateWalletCanisterError::NoWalletConfigured {
                    identity: name.into(),
                    network: network.name.to_string(),
                })
            }
        }
        Some(principal) => Ok(principal),
    }
}

pub async fn create_wallet(
    env: &dyn Environment,
    network: &NetworkDescriptor,
    name: &str,
    some_canister_id: Option<Principal>,
) -> DfxResult<Principal> {
    fetch_root_key_if_needed(env).await?;
    let agent = env.get_agent();
    let mgr = ManagementCanister::create(agent);
    info!(
        env.get_logger(),
        "Creating a wallet canister on the {} network.", network.name
    );

    let wasm = wallet_wasm(env.get_logger())?;

    let canister_id = match some_canister_id {
        Some(id) => id,
        None => {
            mgr.create_canister()
                .as_provisional_create_with_amount(None)
                .with_effective_canister_id(env.get_effective_canister_id())
                .await
                .context("Failed create canister call.")?
                .0
        }
    };

    match mgr
        .install_code(&canister_id, wasm.as_slice())
        .with_mode(InstallMode::Install)
        .await
    {
        Err(AgentError::CertifiedReject(RejectResponse {
            reject_code: RejectCode::CanisterError,
            reject_message,
            ..
        })) if reject_message.contains("not empty") => {
            bail!(
                r#"The wallet canister "{canister_id}" already exists for user "{name}" on "{}" network."#,
                network.name
            )
        }
        res => res.context("Failed while installing wasm.")?,
    }

    let wallet = build_wallet_canister(canister_id, agent).await?;

    wallet
        .wallet_store_wallet_wasm(wasm)
        .await
        .context("Failed to store wallet wasm.")?;

    set_wallet_id(network, name, canister_id)?;

    info!(
        env.get_logger(),
        r#"The wallet canister on the "{}" network for user "{}" is "{}""#,
        network.name,
        name,
        canister_id,
    );

    Ok(canister_id)
}

/// Gets the currently configured wallet canister. If none exists yet and `create` is true, then this creates a new wallet. WARNING: Creating a new wallet costs ICP!
///
/// While developing locally, this always creates a new wallet, even if `create` is false.
/// This can be inhibited by setting the DFX_DISABLE_AUTO_WALLET env var.
#[allow(clippy::needless_lifetimes)]
pub async fn get_or_create_wallet_canister<'env>(
    env: &'env dyn Environment,
    network: &NetworkDescriptor,
    name: &str,
) -> Result<WalletCanister<'env>, GetOrCreateWalletCanisterError> {
    // without this async block, #[context] gives a spurious error
    async {
        let wallet_canister_id = get_or_create_wallet(env, network, name).await?;
        let agent = env.get_agent();
        build_wallet_canister(wallet_canister_id, agent)
            .await
            .map_err(GetOrCreateWalletCanisterError::CanisterBuilderError)
    }
    .await
}

pub fn set_wallet_id(
    network: &NetworkDescriptor,
    name: &str,
    id: Principal,
) -> Result<(), WalletConfigError> {
    let (wallet_path, mut config) = wallet_config(network, name)?;
    // Update the wallet map in it.
    let identities = &mut config.identities;
    let network_map = identities
        .entry(name.to_string())
        .or_insert(WalletNetworkMap {
            networks: BTreeMap::new(),
        });

    network_map.networks.insert(network.name.clone(), id);

    Identity::save_wallet_config(&wallet_path, &config)?;
    Ok(())
}

fn wallet_config(
    network: &NetworkDescriptor,
    name: &str,
) -> Result<(PathBuf, WalletGlobalConfig), WalletConfigError> {
    let wallet_path = get_wallet_config_path(network, name)?;

    // Read the config file.
    Ok((
        wallet_path.clone(),
        if wallet_path.exists() {
            Identity::load_wallet_config(&wallet_path)?
        } else {
            WalletGlobalConfig {
                identities: BTreeMap::new(),
            }
        },
    ))
}


-----------------------

/src/dfx/src/lib/info/mod.rs:
-----------------------

const REPLICA_REV_STR: &str = env!("DFX_ASSET_REPLICA_REV");

pub fn replica_rev() -> &'static str {
    REPLICA_REV_STR
}


-----------------------

/src/dfx/src/lib/info/replica_rev.rs:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/dfx/src/lib/info/replica_rev.rs

-----------------------

/src/dfx/src/lib/installers/assets/mod.rs:
-----------------------

use crate::lib::canister_info::assets::AssetsCanisterInfo;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::error::DfxResult;
use anyhow::Context;
use fn_error_context::context;
use ic_agent::Agent;
use slog::Logger;
use std::path::Path;

#[context("Failed to store assets in canister '{}'.", info.get_name())]
pub async fn post_install_store_assets(
    info: &CanisterInfo,
    agent: &Agent,
    logger: &Logger,
) -> DfxResult {
    let assets_canister_info = info.as_info::<AssetsCanisterInfo>()?;
    let source_paths = assets_canister_info.get_source_paths();
    let source_paths: Vec<&Path> = source_paths.iter().map(|p| p.as_path()).collect::<_>();

    let canister_id = info
        .get_canister_id()
        .context("Could not find canister ID.")?;

    let canister = ic_utils::Canister::builder()
        .with_agent(agent)
        .with_canister_id(canister_id)
        .build()
        .context("Failed to build asset canister caller.")?;

    ic_asset::sync(&canister, &source_paths, false, logger)
        .await
        .with_context(|| {
            format!(
                "Failed asset sync with canister {}.",
                canister.canister_id_()
            )
        })?;

    Ok(())
}

#[context("Failed to store assets in canister '{}'.", info.get_name())]
pub async fn prepare_assets_for_proposal(
    info: &CanisterInfo,
    agent: &Agent,
    logger: &Logger,
) -> DfxResult {
    let assets_canister_info = info.as_info::<AssetsCanisterInfo>()?;
    let source_paths = assets_canister_info.get_source_paths();
    let source_paths: Vec<&Path> = source_paths.iter().map(|p| p.as_path()).collect::<_>();

    let canister_id = info
        .get_canister_id()
        .context("Could not find canister ID.")?;

    let canister = ic_utils::Canister::builder()
        .with_agent(agent)
        .with_canister_id(canister_id)
        .build()
        .context("Failed to build asset canister caller.")?;

    ic_asset::prepare_sync_for_proposal(&canister, &source_paths, logger)
        .await
        .with_context(|| {
            format!(
                "Failed asset sync with canister {}.",
                canister.canister_id_()
            )
        })?;

    Ok(())
}


-----------------------

/src/dfx/src/lib/installers/mod.rs:
-----------------------

pub mod assets;


-----------------------

/src/dfx/src/lib/integrations/bitcoin.rs:
-----------------------

use crate::actors::replica::BitcoinIntegrationConfig;
use crate::lib::error::DfxResult;
use crate::lib::integrations::initialize_integration_canister;
use crate::util::assets::bitcoin_wasm;
use candid::Principal;
use fn_error_context::context;
use ic_agent::Agent;
use slog::{debug, Logger};

pub const MAINNET_BITCOIN_CANISTER_ID: Principal =
    Principal::from_slice(&[0x00, 0x00, 0x00, 0x00, 0x01, 0xA0, 0x00, 0x01, 0x01, 0x01]);

#[context("Failed to initialize bitcoin canister")]
pub async fn initialize_bitcoin_canister(
    agent: &Agent,
    logger: &Logger,
    bitcoin_integration_config: BitcoinIntegrationConfig,
) -> DfxResult {
    debug!(logger, "Initializing bitcoin canister");

    let name = "bitcoin integration";
    let canister_id = MAINNET_BITCOIN_CANISTER_ID;
    let wasm = bitcoin_wasm(logger)?;
    let init_arg = &bitcoin_integration_config.canister_init_arg;

    initialize_integration_canister(agent, logger, name, canister_id, &wasm, init_arg).await
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_bitcoin_canister_id() {
        assert_eq!(
            MAINNET_BITCOIN_CANISTER_ID,
            Principal::from_text("g4xu7-jiaaa-aaaan-aaaaq-cai").unwrap()
        );
    }
}


-----------------------

/src/dfx/src/lib/integrations/mod.rs:
-----------------------

use crate::lib::deps::deploy::try_create_canister;
use crate::lib::deps::PulledCanister;
use crate::lib::environment::create_agent;
use crate::lib::error::DfxResult;
use crate::lib::state_tree::canister_info::read_state_tree_canister_module_hash;
use crate::util::blob_from_arguments;
use anyhow::bail;
use candid::Principal;
use dfx_core::identity::Identity;
use dfx_core::{error::root_key::FetchRootKeyError, util::expiry_duration};
use fn_error_context::context;
use ic_agent::Agent;
use ic_utils::interfaces::management_canister::builders::InstallMode;
use ic_utils::interfaces::ManagementCanister;
use sha2::Digest;
use slog::{debug, info, Logger};
use std::time::Duration;

pub mod bitcoin;
pub mod status;

pub async fn create_integrations_agent(url: &str, logger: &Logger) -> DfxResult<Agent> {
    let timeout = expiry_duration();
    let identity = Box::new(Identity::anonymous());
    let agent = create_agent(logger.clone(), url, identity, timeout).unwrap();
    agent
        .fetch_root_key()
        .await
        .map_err(FetchRootKeyError::AgentError)?;
    Ok(agent)
}
#[context("Failed to install {name} integration canister {canister_id}")]
pub async fn initialize_integration_canister(
    agent: &Agent,
    logger: &Logger,
    name: &str,
    canister_id: Principal,
    wasm: &[u8],
    init_arg: &str,
) -> DfxResult {
    if already_installed(agent, &canister_id, wasm).await? {
        debug!(logger, "Canister {canister_id} already installed");
        return Ok(());
    }

    let pulled_canister = PulledCanister {
        name: Some(name.to_string()),
        ..Default::default()
    };
    try_create_canister(agent, logger, &canister_id, &pulled_canister).await?;

    let install_arg = blob_from_arguments(None, Some(init_arg), None, None, &None, true, false)?;
    install_canister(agent, logger, &canister_id, wasm, install_arg, name).await
}

#[context("Failed to install canister {canister_id}")]
async fn install_canister(
    agent: &Agent,
    logger: &Logger,
    canister_id: &Principal,
    wasm: &[u8],
    install_args: Vec<u8>,
    name: &str,
) -> DfxResult {
    info!(logger, "Installing canister: {name}");
    ManagementCanister::create(agent)
        .install_code(canister_id, wasm)
        // always reinstall pulled canister
        .with_mode(InstallMode::Reinstall)
        .with_raw_arg(install_args)
        .await?;
    Ok(())
}

#[context("Failed to determine if canister {canister_id} is already installed")]
async fn already_installed(agent: &Agent, canister_id: &Principal, wasm: &[u8]) -> DfxResult<bool> {
    let installed_module_hash = read_state_tree_canister_module_hash(agent, *canister_id).await?;
    let expected_module_hash: [u8; 32] = sha2::Sha256::digest(wasm).into();
    let result = matches!(installed_module_hash, Some(hash) if hash == expected_module_hash);

    Ok(result)
}

#[context("Failed to wait until canister {canister_id} is installed")]
pub async fn wait_for_canister_installed(agent: &Agent, canister_id: &Principal) -> DfxResult {
    let mut retries = 0;
    loop {
        if read_state_tree_canister_module_hash(agent, *canister_id)
            .await?
            .is_some()
        {
            break;
        }
        if retries >= 60 {
            bail!("Canister {canister_id} was never installed");
        }
        tokio::time::sleep(Duration::from_secs(1)).await;
        retries += 1;
    }
    Ok(())
}


-----------------------

/src/dfx/src/lib/integrations/status.rs:
-----------------------

use crate::lib::error::DfxResult;
use crate::lib::integrations::bitcoin::MAINNET_BITCOIN_CANISTER_ID;
use crate::lib::integrations::{create_integrations_agent, wait_for_canister_installed};
use dfx_core::config::model::local_server_descriptor::LocalServerDescriptor;
use slog::Logger;

pub async fn wait_for_integrations_initialized(
    agent_url: &str,
    logger: &Logger,
    local_server_descriptor: &LocalServerDescriptor,
) -> DfxResult {
    if !local_server_descriptor.bitcoin.enabled {
        return Ok(());
    }

    let agent = create_integrations_agent(agent_url, logger).await?;

    if local_server_descriptor.bitcoin.enabled {
        wait_for_canister_installed(&agent, &MAINNET_BITCOIN_CANISTER_ID).await?;
    }

    Ok(())
}


-----------------------

/src/dfx/src/lib/ledger_types/mod.rs:
-----------------------

// DISCLAIMER:
// Do not modify this file arbitrarily.
// The contents are borrowed from:
// https://gitlab.com/dfinity-lab/public/ic/-/blob/master/rs/rosetta-api/ledger_canister/src/lib.rs
use crate::lib::nns_types::account_identifier::Subaccount;
use crate::lib::nns_types::icpts::ICPTs;
use candid::CandidType;
use candid::Nat;
use candid::Principal;
use ic_utils::interfaces::management_canister::builders::CanisterSettings;
use icrc_ledger_types::icrc1::account::Subaccount as ICRCSubaccount;
use icrc_ledger_types::icrc1::transfer::BlockIndex as ICRCBlockIndex;
use serde::{Deserialize, Serialize};
use std::fmt;

use super::cycles_ledger_types::create_canister::SubnetSelection;

/// Id of the ledger canister on the IC.
pub const MAINNET_LEDGER_CANISTER_ID: Principal =
    Principal::from_slice(&[0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x01, 0x01]);

pub const MAINNET_CYCLE_MINTER_CANISTER_ID: Principal =
    Principal::from_slice(&[0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x01, 0x01]);

pub type AccountIdBlob = [u8; 32];

/// Arguments for the `transfer` call.
#[derive(CandidType)]
pub struct TransferArgs {
    pub memo: Memo,
    pub amount: ICPTs,
    pub fee: ICPTs,
    pub from_subaccount: Option<Subaccount>,
    pub to: AccountIdBlob,
    pub created_at_time: Option<TimeStamp>,
}

/// Result of the `transfer` call.
pub type TransferResult = Result<BlockHeight, TransferError>;

/// Error of the `transfer` call.
#[derive(CandidType, Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
pub enum TransferError {
    BadFee { expected_fee: ICPTs },
    InsufficientFunds { balance: ICPTs },
    TxTooOld { allowed_window_nanos: u64 },
    TxCreatedInFuture,
    TxDuplicate { duplicate_of: BlockHeight },
}

impl fmt::Display for TransferError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::BadFee { expected_fee } => {
                write!(f, "transaction fee should be {}", expected_fee)
            }
            Self::InsufficientFunds { balance } => {
                write!(
                    f,
                    "the debit account doesn't have enough funds to complete the transaction, current balance: {}",
                    balance
                )
            }
            Self::TxTooOld {
                allowed_window_nanos,
            } => write!(
                f,
                "transaction is older than {} seconds",
                allowed_window_nanos / 1_000_000_000
            ),
            Self::TxCreatedInFuture => write!(f, "transaction's created_at_time is in future"),
            Self::TxDuplicate { duplicate_of } => write!(
                f,
                "transaction is a duplicate of another transaction in block {}",
                duplicate_of
            ),
        }
    }
}

#[derive(CandidType, Deserialize)]
pub enum CyclesResponse {
    CanisterCreated(Principal),
    ToppedUp(()),
    Refunded(String, Option<BlockHeight>),
}

#[derive(CandidType, Deserialize)]
pub struct IcpXdrConversionRate {
    pub timestamp_seconds: u64,
    pub xdr_permyriad_per_icp: u64,
}

#[derive(CandidType, Deserialize)]
pub struct IcpXdrConversionRateCertifiedResponse {
    pub data: IcpXdrConversionRate,
    pub hash_tree: Vec<u8>,
    pub certificate: Vec<u8>,
}

/// Position of a block in the chain. The first block has position 0.
pub type BlockHeight = u64;

pub type BlockIndex = u64;

#[derive(
    Serialize,
    Deserialize,
    CandidType,
    Clone,
    Copy,
    Hash,
    Debug,
    PartialEq,
    Eq,
    PartialOrd,
    Ord,
    Default,
)]
pub struct Memo(pub u64);

#[derive(CandidType)]
pub struct AccountBalanceArgs {
    pub account: String,
}

#[derive(CandidType)]
pub struct TimeStamp {
    pub timestamp_nanos: u64,
}

#[derive(CandidType)]
pub struct NotifyCreateCanisterArg {
    pub block_index: BlockIndex,
    pub controller: Principal,
    pub subnet_selection: Option<SubnetSelection>,
    pub settings: Option<CanisterSettings>,
}

#[derive(CandidType)]
pub struct NotifyTopUpArg {
    pub block_index: BlockIndex,
    pub canister_id: Principal,
}

#[derive(CandidType, Deserialize, Debug)]
pub enum NotifyError {
    Refunded {
        reason: String,
        block_index: Option<BlockIndex>,
    },
    Processing,
    TransactionTooOld(BlockIndex),
    InvalidTransaction(String),
    Other {
        error_code: u64,
        error_message: String,
    },
}

#[derive(Serialize, Deserialize, CandidType, Clone, Hash, Debug, PartialEq, Eq)]
pub struct NotifyMintCyclesArg {
    pub block_index: BlockIndex,
    pub to_subaccount: Option<ICRCSubaccount>,
    pub deposit_memo: Option<Vec<u8>>,
}

#[derive(Serialize, Deserialize, CandidType, Clone, Hash, Debug, PartialEq, Eq)]
pub struct NotifyMintCyclesSuccess {
    /// Cycles ledger block index of deposit
    pub block_index: ICRCBlockIndex,
    /// Amount of cycles that were minted and deposited to the cycles ledger
    pub minted: Nat,
    /// New balance of the cycles ledger account
    pub balance: Nat,
}

pub type NotifyMintCyclesResult = Result<NotifyMintCyclesSuccess, NotifyError>;

pub type NotifyCreateCanisterResult = Result<Principal, NotifyError>;

pub type NotifyTopUpResult = Result<u128, NotifyError>;

#[derive(CandidType, Deserialize, Debug)]
pub struct GetSubnetTypesToSubnetsResult {
    pub data: Vec<(String, Vec<Principal>)>,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ledger_canister_id() {
        assert_eq!(
            MAINNET_LEDGER_CANISTER_ID,
            Principal::from_text("ryjl3-tyaaa-aaaaa-aaaba-cai").unwrap()
        );
    }

    #[test]
    fn test_cycle_minter_canister_id() {
        assert_eq!(
            MAINNET_CYCLE_MINTER_CANISTER_ID,
            Principal::from_text("rkp4c-7iaaa-aaaaa-aaaca-cai").unwrap()
        );
    }
}


-----------------------

/src/dfx/src/lib/logger.rs:
-----------------------

use crate::config::dfx_version_str;
use slog::{Drain, Level, Logger};
use std::fs::File;
use std::path::PathBuf;

/// The logging mode to use.
pub enum LoggingMode {
    /// The default mode for logging; output without any decoration, to STDERR.
    Stderr,

    /// Tee logging to a file (in addition to STDERR). This mimics the verbose flag.
    /// So it would be similar to `dfx ... |& tee /some/file.txt
    Tee(PathBuf),

    /// Output Debug logs and up to a file, regardless of verbosity, keep the STDERR output
    /// the same (with verbosity).
    File(PathBuf),
}

/// A Slog formatter that writes to a term decorator.
pub struct DfxFormat<D>
where
    D: slog_term::Decorator,
{
    decorator: D,
}

impl<D: slog_term::Decorator> DfxFormat<D> {
    pub fn new(decorator: D) -> DfxFormat<D> {
        DfxFormat { decorator }
    }
}

impl<D: slog_term::Decorator> slog::Drain for DfxFormat<D> {
    type Ok = ();
    type Err = std::io::Error;

    fn log(
        &self,
        record: &slog::Record<'_>,
        values: &slog::OwnedKVList,
    ) -> Result<Self::Ok, Self::Err> {
        self.decorator.with_record(record, values, |decorator| {
            if record.level() <= slog::Level::Warning {
                decorator.start_level()?;
                write!(decorator, "{}: ", record.level().as_str())?;
                // start_whitespace resets to normal coloring after printing the level
                decorator.start_whitespace()?;
            }

            decorator.start_msg()?;
            write!(decorator, "{}", record.msg())?;

            decorator.start_whitespace()?;
            writeln!(decorator)?;

            decorator.flush()?;
            Ok(())
        })
    }
}

/// Create a log drain.
fn create_drain(mode: LoggingMode) -> Logger {
    match mode {
        LoggingMode::Stderr => {
            let decorator = slog_term::TermDecorator::new().build();
            let drain = DfxFormat::new(decorator).fuse();
            let async_drain = slog_async::Async::new(drain).build().fuse();
            Logger::root(async_drain, slog::o!())
        }
        LoggingMode::File(out) => {
            let file = File::create(out).expect("Couldn't open log file");
            let decorator = slog_term::PlainDecorator::new(file);
            let drain = slog_term::FullFormat::new(decorator).build().fuse();
            Logger::root(slog_async::Async::new(drain).build().fuse(), slog::o!())
        }
        // A Tee mode is basically 2 drains duplicated.
        LoggingMode::Tee(out) => Logger::root(
            slog::Duplicate::new(
                create_drain(LoggingMode::Stderr),
                create_drain(LoggingMode::File(out)),
            )
            .fuse(),
            slog::o!(),
        ),
    }
}

/// Create a root logger.
/// The verbose_level can be negative, in which case it's a quiet mode which removes warnings,
/// then errors entirely.
pub fn create_root_logger(verbose_level: i64, mode: LoggingMode) -> Logger {
    let log_level = match verbose_level {
        -3 => Level::Critical,
        -2 => Level::Error,
        -1 => Level::Warning,
        0 => Level::Info,
        1 => Level::Debug,
        x => {
            if x > 0 {
                Level::Trace
            } else {
                return Logger::root(slog::Discard, slog::o!());
            }
        }
    };

    let drain = slog::LevelFilter::new(create_drain(mode), log_level).fuse();
    let drain = slog_async::Async::new(drain).build().fuse();

    Logger::root(drain, slog::o!("version" => dfx_version_str()))
}


-----------------------

/src/dfx/src/lib/manifest.rs:
-----------------------

use crate::lib::error::{DfxError, DfxResult};
use crate::{error_invalid_argument, error_invalid_data};
use anyhow::Context;
use fn_error_context::context;
use indicatif::{ProgressBar, ProgressDrawTarget};
use semver::Version;
use serde::{Deserialize, Deserializer};
use std::collections::BTreeMap;

fn parse_semver<'de, D>(version: &str) -> Result<Version, D::Error>
where
    D: Deserializer<'de>,
{
    semver::Version::parse(version)
        .map_err(|e| serde::de::Error::custom(format!("invalid SemVer: {}", e)))
}

fn deserialize_tags<'de, D>(deserializer: D) -> Result<BTreeMap<String, Version>, D::Error>
where
    D: Deserializer<'de>,
{
    let tags: BTreeMap<String, String> = Deserialize::deserialize(deserializer)?;
    let mut result = BTreeMap::<String, Version>::new();

    for (tag, version) in tags.into_iter() {
        result.insert(tag, parse_semver::<D>(&version)?);
    }

    Ok(result)
}

fn deserialize_versions<'de, D>(deserializer: D) -> Result<Vec<Version>, D::Error>
where
    D: Deserializer<'de>,
{
    let versions: Vec<String> = Deserialize::deserialize(deserializer)?;
    let mut result = Vec::with_capacity(versions.len());

    for version in versions.iter() {
        result.push(parse_semver::<D>(version)?);
    }

    Ok(result)
}

#[derive(Debug, PartialEq, Eq, Deserialize)]
pub struct Manifest {
    #[serde(deserialize_with = "deserialize_tags")]
    tags: BTreeMap<String, Version>,
    #[serde(deserialize_with = "deserialize_versions")]
    versions: Vec<Version>,
}

pub fn is_upgrade_necessary(latest_version: Option<&Version>, current: &Version) -> bool {
    match latest_version {
        Some(latest) => latest > current && current.pre.is_empty(),
        None => true,
    }
}

#[context("Failed to fetch latest version.")]
pub fn get_latest_version(
    release_root: &str,
    timeout: Option<std::time::Duration>,
) -> DfxResult<Version> {
    let url = reqwest::Url::parse(release_root)
        .map_err(|e| error_invalid_argument!("invalid release root: {}", e))?;
    let manifest_url = url
        .join("manifest.json")
        .map_err(|e| error_invalid_argument!("invalid manifest URL: {}", e))?;
    println!("Fetching manifest {}", manifest_url);

    let b = ProgressBar::new_spinner();
    b.set_draw_target(ProgressDrawTarget::stderr());

    b.set_message("Checking for latest dfx version...");
    b.enable_steady_tick(80);

    let client = match timeout {
        Some(timeout) => reqwest::blocking::Client::builder().timeout(timeout),
        None => reqwest::blocking::Client::builder(),
    };

    let client = client.build().context("Failed to build client.")?;
    let response = client.get(manifest_url).send().map_err(DfxError::new)?;
    let status_code = response.status();
    b.finish_and_clear();

    if !status_code.is_success() {
        return Err(error_invalid_data!(
            "unable to fetch manifest: {}",
            status_code.canonical_reason().unwrap_or("unknown error"),
        ));
    }

    let manifest: Manifest = response
        .json()
        .map_err(|e| error_invalid_data!("invalid manifest: {}", e))?;
    manifest
        .tags
        .get("latest")
        .ok_or_else(|| error_invalid_data!("expected field 'latest' in 'tags'"))
        .map(|v| v.clone())
}

#[cfg(test)]
mod tests {
    use super::*;

    const MANIFEST: &str = r#"{
      "tags": {
        "latest": "0.4.1"
      },
      "versions": [
        "0.3.1",
        "0.4.0",
        "0.4.1"
      ]
}"#;

    #[test]
    fn test_parse_manifest() {
        let manifest: Manifest = serde_json::from_str(MANIFEST).unwrap();
        let mut tags = BTreeMap::new();
        tags.insert(
            "latest".to_string(),
            semver::Version::parse("0.4.1").unwrap(),
        );
        let versions: Vec<Version> = vec!["0.3.1", "0.4.0", "0.4.1"]
            .into_iter()
            .map(|v| semver::Version::parse(v).unwrap())
            .collect();
        assert_eq!(manifest.versions, versions);
    }

    #[test]
    fn test_get_latest_version() {
        let _ = env_logger::try_init();
        let _m = mockito::mock("GET", "/manifest.json")
            .with_status(200)
            .with_header("content-type", "application/json")
            .with_body(MANIFEST)
            .create();
        let latest_version = get_latest_version(&mockito::server_url(), None);
        assert_eq!(latest_version.unwrap(), Version::parse("0.4.1").unwrap());
        let _m = mockito::mock("GET", "/manifest.json")
            .with_status(200)
            .with_header("content-type", "application/json")
            .with_body("Not a valid JSON object")
            .create();
        let latest_version = get_latest_version(&mockito::server_url(), None);
        assert!(latest_version.is_err());
    }
}


-----------------------

/src/dfx/src/lib/metadata/config.rs:
-----------------------

use dfx_core::config::model::dfinity::CanisterMetadataSection;
use std::collections::BTreeMap;

#[derive(Debug)]
pub struct CanisterMetadataConfig {
    pub sections: BTreeMap<String, CanisterMetadataSection>,
}

impl CanisterMetadataConfig {
    pub fn new(sections: &Vec<CanisterMetadataSection>, network: &str) -> Self {
        let mut map = BTreeMap::new();
        for section in sections {
            if section.applies_to_network(network) && !map.contains_key(&section.name) {
                map.insert(section.name.clone(), section.clone());
            }
        }

        CanisterMetadataConfig { sections: map }
    }

    pub fn get(&self, name: &str) -> Option<&CanisterMetadataSection> {
        self.sections.get(name)
    }
}


-----------------------

/src/dfx/src/lib/metadata/dfx.rs:
-----------------------

/src/dfx/src/lib/metadata/mod.rs:
-----------------------

pub mod config;
pub mod dfx;
pub mod names;


-----------------------

/src/dfx/src/lib/metadata/names.rs:
-----------------------

pub const CANDID_ARGS: &str = "candid:args";
pub const CANDID_SERVICE: &str = "candid:service";
pub const DFX: &str = "dfx";


-----------------------

/src/dfx/src/lib/migrate.rs:
-----------------------

use crate::lib::operations::canister::install_wallet;
use crate::lib::{environment::Environment, error::DfxResult, root_key::fetch_root_key_if_needed};
use anyhow::{bail, Context, Error};
use candid::{CandidType, Deserialize, Principal};
use dfx_core::config::model::network_descriptor::NetworkDescriptor;
use dfx_core::identity::wallet::wallet_canister_id;
use dfx_core::identity::Identity;
use ic_agent::{Agent, Identity as _};
use ic_utils::{
    interfaces::{
        management_canister::builders::{CanisterSettings, InstallMode},
        WalletCanister,
    },
    Argument,
};
use itertools::Itertools;

pub async fn migrate(env: &dyn Environment, network: &NetworkDescriptor, fix: bool) -> DfxResult {
    fetch_root_key_if_needed(env).await?;
    let config = env.get_config_or_anyhow()?;
    let config = config.get_config();
    let agent = env.get_agent();
    let mut mgr = env.new_identity_manager()?;
    let ident = mgr.instantiate_selected_identity(env.get_logger())?;
    let mut did_migrate = false;
    let wallet = if let Some(principal) = wallet_canister_id(network, ident.name())? {
        principal
    } else {
        bail!("No wallet found; nothing to do");
    };
    let wallet = if let Ok(wallet) = WalletCanister::create(agent, wallet).await {
        wallet
    } else {
        let cbor = agent
            .read_state_canister_info(wallet, "controllers")
            .await?;
        let controllers: Vec<Principal> = serde_cbor::from_slice(&cbor)?;
        bail!("This identity isn't a controller of the wallet. You need to be one of these principals to upgrade the wallet: {}", controllers.into_iter().join(", "))
    };
    did_migrate |= migrate_wallet(env, agent, &wallet, fix).await?;
    if let Some(canisters) = &config.canisters {
        let store = env.get_canister_id_store()?;
        for name in canisters.keys() {
            if !config.is_remote_canister(name, &network.name)? {
                if let Some(id) = store.find(name) {
                    did_migrate |= migrate_canister(agent, &wallet, id, name, &ident, fix).await?;
                }
            }
        }
    }
    if did_migrate {
        println!("You can also make all of these changes at once with the `dfx fix` command");
    } else {
        println!("No problems found");
    }
    Ok(())
}

async fn migrate_wallet(
    env: &dyn Environment,
    agent: &Agent,
    wallet: &WalletCanister<'_>,
    fix: bool,
) -> DfxResult<bool> {
    if !wallet.version_supports_u128_cycles() {
        if fix {
            println!("Upgrading wallet... ");
            install_wallet(
                env,
                agent,
                *wallet.canister_id_(),
                InstallMode::Upgrade(None),
            )
            .await?
        } else {
            println!("The wallet is outdated; run `dfx wallet upgrade`");
        }
        Ok(true)
    } else {
        Ok(false)
    }
}

async fn migrate_canister(
    agent: &Agent,
    wallet: &WalletCanister<'_>,
    canister_id: Principal,
    canister_name: &str,
    ident: &Identity,
    fix: bool,
) -> DfxResult<bool> {
    let cbor = agent
        .read_state_canister_info(canister_id, "controllers")
        .await?;
    let mut controllers: Vec<Principal> = serde_cbor::from_slice(&cbor)?;
    if controllers.contains(wallet.canister_id_())
        && !controllers.contains(&ident.sender().unwrap())
    {
        if fix {
            println!(
                "Adding the {ident} identity to canister {canister_name}'s controllers...",
                ident = ident.name()
            );
            controllers.push(ident.sender().map_err(Error::msg)?);
            #[derive(CandidType, Deserialize)]
            struct In {
                canister_id: Principal,
                settings: CanisterSettings,
            }
            wallet
                .call(
                    Principal::management_canister(),
                    "update_settings",
                    Argument::from_candid((In {
                        canister_id,
                        settings: CanisterSettings {
                            controllers: Some(controllers),
                            compute_allocation: None,
                            freezing_threshold: None,
                            memory_allocation: None,
                            reserved_cycles_limit: None,
                            wasm_memory_limit: None,
                            log_visibility: None,
                        },
                    },)),
                    0,
                )
                .await
                .context("Could not update canister settings")?;
        } else {
            println!("Canister {canister_name} is outdated; run `dfx canister update-settings` with the --add-controller flag")
        }
        Ok(true)
    } else {
        Ok(false)
    }
}


-----------------------

/src/dfx/src/lib/mod.rs:
-----------------------

pub mod agent;
pub mod builders;
pub mod canister_info;
pub mod canister_logs;
pub mod cycles_ledger_types;
pub mod deps;
pub mod dfxvm;
pub mod diagnosis;
pub mod environment;
pub mod error;
pub mod error_code;
pub mod ic_attributes;
pub mod identity;
pub mod info;
pub mod installers;
pub mod integrations;
pub mod ledger_types;
pub mod logger;
pub mod manifest;
pub mod metadata;
pub mod migrate;
pub mod models;
pub mod named_canister;
pub mod network;
pub mod nns_types;
pub mod operations;
pub mod package_arguments;
pub mod program;
pub mod progress_bar;
pub mod project;
pub mod replica;
pub mod retryable;
pub mod root_key;
pub mod sign;
pub mod state_tree;
pub mod subnet;
pub mod warning;
pub mod wasm;


-----------------------

/src/dfx/src/lib/models.rs:
-----------------------

pub mod canister;


-----------------------

/src/dfx/src/lib/models/canister.rs:
-----------------------

use crate::lib::builders::{
    custom_download, BuildConfig, BuildOutput, BuilderPool, CanisterBuilder, IdlBuildOutput,
    WasmBuildOutput,
};
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::{BuildError, DfxError, DfxResult};
use crate::lib::metadata::dfx::DfxMetadata;
use crate::lib::metadata::names::{CANDID_ARGS, CANDID_SERVICE, DFX};
use crate::lib::wasm::file::{compress_bytes, read_wasm_module};
use crate::util::assets;
use anyhow::{anyhow, bail, Context};
use candid::Principal as CanisterId;
use candid_parser::utils::CandidSource;
use dfx_core::config::model::canister_id_store::CanisterIdStore;
use dfx_core::config::model::dfinity::{
    CanisterMetadataSection, Config, MetadataVisibility, TechStack, WasmOptLevel,
};
use fn_error_context::context;
use ic_wasm::metadata::{add_metadata, remove_metadata, Kind};
use ic_wasm::optimize::OptLevel;
use itertools::Itertools;
use petgraph::graph::{DiGraph, NodeIndex};
use rand::{thread_rng, RngCore};
use slog::{error, info, trace, warn, Logger};
use std::cell::RefCell;
use std::collections::{BTreeMap, HashSet};
use std::convert::TryFrom;
use std::ffi::OsStr;
use std::io::Read;
use std::path::Path;
use std::process::{Command, Stdio};
use std::sync::Arc;

/// Represents a canister from a DFX project. It can be a virtual Canister.
/// Multiple canister instances can have the same info, but would be differentiated
/// by their IDs.
/// Once an instance of a canister is built it is immutable. So for comparing
/// two canisters one can use their ID.
pub struct Canister {
    info: CanisterInfo,
    builder: Arc<dyn CanisterBuilder>,
    output: RefCell<Option<BuildOutput>>,
}
unsafe impl Send for Canister {}
unsafe impl Sync for Canister {}

impl Canister {
    /// Create a new canister.
    /// This can only be done by a CanisterPool.
    pub(super) fn new(info: CanisterInfo, builder: Arc<dyn CanisterBuilder>) -> Self {
        Self {
            info,
            builder,
            output: RefCell::new(None),
        }
    }

    pub fn prebuild(&self, pool: &CanisterPool, build_config: &BuildConfig) -> DfxResult {
        self.builder.prebuild(pool, &self.info, build_config)
    }

    pub fn build(
        &self,
        pool: &CanisterPool,
        build_config: &BuildConfig,
    ) -> DfxResult<&BuildOutput> {
        let output = self.builder.build(pool, &self.info, build_config)?;

        // Ignore the old output, and return a reference.
        let _ = self.output.replace(Some(output));
        Ok(self.get_build_output().unwrap())
    }

    pub fn postbuild(&self, pool: &CanisterPool, build_config: &BuildConfig) -> DfxResult {
        self.builder.postbuild(pool, &self.info, build_config)
    }

    pub fn get_name(&self) -> &str {
        self.info.get_name()
    }

    pub fn get_info(&self) -> &CanisterInfo {
        &self.info
    }

    pub fn canister_id(&self) -> CanisterId {
        self.info.get_canister_id().unwrap()
    }

    // this function is only ever used when build_config.build_mode_check is true
    #[context("Failed to generate random canister id.")]
    pub fn generate_random_canister_id() -> DfxResult<CanisterId> {
        let mut rng = thread_rng();
        let mut v: Vec<u8> = std::iter::repeat(0u8).take(8).collect();
        rng.fill_bytes(v.as_mut_slice());
        CanisterId::try_from(v).context("Failed to convert bytes to canister id.")
    }

    /// Get the build output of a build process. If the output isn't known at this time,
    /// will return [None].
    pub fn get_build_output(&self) -> Option<&BuildOutput> {
        unsafe { (*self.output.as_ptr()).as_ref() }
    }

    #[context("Failed while trying to generate type declarations for '{}'.", self.info.get_name())]
    pub fn generate(&self, pool: &CanisterPool, build_config: &BuildConfig) -> DfxResult {
        self.builder.generate(pool, &self.info, build_config)
    }

    #[context("Failed to post-process wasm of canister '{}'.", self.info.get_name())]
    pub(crate) fn wasm_post_process(
        &self,
        logger: &Logger,
        build_output: &BuildOutput,
    ) -> DfxResult {
        let build_output_wasm_path = match &build_output.wasm {
            WasmBuildOutput::File(p) => p,
            WasmBuildOutput::None => {
                // exclude pull canisters
                return Ok(());
            }
        };
        let wasm_path = self.info.get_build_wasm_path();
        dfx_core::fs::composite::ensure_parent_dir_exists(&wasm_path)?;
        let info = &self.info;
        if info.is_remote() {
            return Ok(());
        }

        let mut m = read_wasm_module(build_output_wasm_path)?;
        let mut modified = false;

        // optimize or shrink
        if let Some(level) = info.get_optimize() {
            trace!(logger, "Optimizing Wasm at level {}", level);
            ic_wasm::optimize::optimize(
                &mut m,
                &wasm_opt_level_convert(level),
                false,
                &None,
                false,
            )
            .context("Failed to optimize the Wasm module.")?;
            modified = true;
        } else if info.get_shrink() == Some(true)
            || (info.get_shrink().is_none() && (info.is_rust() || info.is_motoko()))
        {
            trace!(logger, "Shrinking Wasm");
            ic_wasm::shrink::shrink(&mut m);
            modified = true;
        }

        // metadata
        trace!(logger, "Attaching metadata");
        let mut metadata_sections = info.metadata().sections.clone();
        // Default to write public candid:service unless overwritten
        let mut public_candid = false;
        if (info.is_rust() || info.is_motoko())
            && !metadata_sections.contains_key(CANDID_SERVICE)
            && !metadata_sections.contains_key(CANDID_ARGS)
        {
            public_candid = true;
        }

        // dfx metadata
        let mut set_dfx_metadata = false;
        let mut dfx_metadata = DfxMetadata::default();
        if let Some(pullable) = info.get_pullable() {
            set_dfx_metadata = true;
            dfx_metadata.set_pullable(pullable);
            // pullable canisters must have public candid:service
            public_candid = true;
        }

        if let Some(tech_stack_config) = info.get_tech_stack() {
            set_dfx_metadata = true;
            dfx_metadata.set_tech_stack(tech_stack_config, info.get_workspace_root())?;
        } else if info.is_rust() {
            // TODO: remove this when we have rust extension
            set_dfx_metadata = true;
            let s = r#"{
                "language" : {
                    "rust" : {
                        "version" : "$(rustc --version | cut -d ' ' -f 2)"
                    }
                },
                "cdk" : {
                    "ic-cdk" : {
                        "version" : "$(cargo tree -p ic-cdk --depth 0 | cut -d ' ' -f 2 | cut -c 2-)"
                    }
                }
            }"#;
            let tech_stack_config: TechStack = serde_json::from_str(s)?;
            dfx_metadata.set_tech_stack(&tech_stack_config, info.get_workspace_root())?;
        } else if info.is_motoko() {
            // TODO: remove this when we have motoko extension
            set_dfx_metadata = true;
            let s = r#"{
                "language" : {
                    "motoko" : {}
                }
            }"#;
            let tech_stack_config: TechStack = serde_json::from_str(s)?;
            dfx_metadata.set_tech_stack(&tech_stack_config, info.get_workspace_root())?;
        }

        if set_dfx_metadata {
            let content = serde_json::to_string_pretty(&dfx_metadata)
                .with_context(|| "Failed to serialize `dfx` metadata.".to_string())?;
            metadata_sections.insert(
                DFX.to_string(),
                CanisterMetadataSection {
                    name: DFX.to_string(),
                    visibility: MetadataVisibility::Public,
                    content: Some(content),
                    ..Default::default()
                },
            );
        }

        if public_candid {
            metadata_sections.insert(
                CANDID_SERVICE.to_string(),
                CanisterMetadataSection {
                    name: CANDID_SERVICE.to_string(),
                    visibility: MetadataVisibility::Public,
                    ..Default::default()
                },
            );

            metadata_sections.insert(
                CANDID_ARGS.to_string(),
                CanisterMetadataSection {
                    name: CANDID_ARGS.to_string(),
                    visibility: MetadataVisibility::Public,
                    ..Default::default()
                },
            );
        }

        for (name, section) in &metadata_sections {
            if section.name == CANDID_SERVICE && info.is_motoko() {
                if let Some(specified_path) = &section.path {
                    check_valid_subtype(&info.get_service_idl_path(), specified_path)?
                } else {
                    // Motoko compiler handles this
                    continue;
                }
            }

            let data = match (section.path.as_ref(), section.content.as_ref()) {
                (None, None) if section.name == CANDID_SERVICE => {
                    dfx_core::fs::read(&info.get_service_idl_path())?
                }
                (None, None) if section.name == CANDID_ARGS => {
                    dfx_core::fs::read(&info.get_init_args_txt_path())?
                }
                (Some(path), None) => dfx_core::fs::read(path)?,
                (None, Some(s)) => s.clone().into_bytes(),
                (Some(_), Some(_)) => {
                    bail!(
                    "Metadata section could not specify path and content at the same time. section: {:?}",
                    &section
                )
                }
                (None, None) => {
                    bail!(
                        "Metadata section must specify a path or content. section: {:?}",
                        &section
                    )
                }
            };

            let visibility = match section.visibility {
                MetadataVisibility::Public => Kind::Public,
                MetadataVisibility::Private => Kind::Private,
            };

            // if the metadata already exists in the wasm with a different visibility,
            // then we have to remove it
            remove_metadata(&mut m, name);

            add_metadata(&mut m, visibility, name, data);
            modified = true;
        }

        // If not modified and not set "gzip" explicitly, copy the wasm file directly so that hash match.
        if !modified && !info.get_gzip() {
            dfx_core::fs::copy(build_output_wasm_path, &wasm_path)?;
            return Ok(());
        }

        let new_bytes = if wasm_path.extension() == Some(OsStr::new("gz")) {
            // gzip
            // Unlike using gzip CLI, the compression below only takes the wasm bytes
            // So as long as the wasm bytes are the same, the gzip file will be the same on different platforms.
            trace!(logger, "Compressing Wasm");
            compress_bytes(&m.emit_wasm())?
        } else {
            m.emit_wasm()
        };
        dfx_core::fs::write(&wasm_path, new_bytes)?;

        Ok(())
    }

    pub(crate) fn candid_post_process(
        &self,
        logger: &Logger,
        build_config: &BuildConfig,
        build_output: &BuildOutput,
    ) -> DfxResult {
        trace!(logger, "Post processing candid file");

        let IdlBuildOutput::File(build_idl_path) = &build_output.idl;

        // 1. Separate into constructor.did, service.did and init_args
        let (constructor_did, service_did, init_args) = separate_candid(build_idl_path)?;

        // 2. Copy the constructor IDL file to .dfx/local/canisters/NAME/constructor.did.
        let constructor_idl_path = self.info.get_constructor_idl_path();
        dfx_core::fs::composite::ensure_parent_dir_exists(&constructor_idl_path)?;
        dfx_core::fs::write(&constructor_idl_path, constructor_did)?;
        dfx_core::fs::set_permissions_readwrite(&constructor_idl_path)?;

        // 3. Save service.did into following places in .dfx/local/:
        //   - canisters/NAME/service.did
        //   - IDL_ROOT/CANISTER_ID.did
        //   - LSP_ROOT/CANISTER_ID.did
        let mut targets = vec![];
        targets.push(self.info.get_service_idl_path());
        let canister_id = self.canister_id();
        targets.push(
            build_config
                .idl_root
                .join(canister_id.to_text())
                .with_extension("did"),
        );
        targets.push(
            build_config
                .lsp_root
                .join(canister_id.to_text())
                .with_extension("did"),
        );

        for target in targets {
            if &target == build_idl_path {
                continue;
            }
            dfx_core::fs::composite::ensure_parent_dir_exists(&target)?;
            dfx_core::fs::write(&target, &service_did)?;
            dfx_core::fs::set_permissions_readwrite(&target)?;
        }

        // 4. Save init_args into .dfx/local/canisters/NAME/init_args.txt
        let init_args_txt_path = self.info.get_init_args_txt_path();
        dfx_core::fs::composite::ensure_parent_dir_exists(&init_args_txt_path)?;
        dfx_core::fs::write(&init_args_txt_path, init_args)?;
        dfx_core::fs::set_permissions_readwrite(&init_args_txt_path)?;
        Ok(())
    }
}

fn wasm_opt_level_convert(opt_level: WasmOptLevel) -> OptLevel {
    use WasmOptLevel::*;
    match opt_level {
        O0 => OptLevel::O0,
        O1 => OptLevel::O1,
        O2 => OptLevel::O2,
        O3 => OptLevel::O3,
        O4 => OptLevel::O4,
        Os => OptLevel::Os,
        Oz => OptLevel::Oz,
        Size => OptLevel::Oz,
        Cycles => OptLevel::O3,
    }
}

fn separate_candid(path: &Path) -> DfxResult<(String, String, String)> {
    use candid::pretty::candid::{compile, pp_args};
    use candid::types::internal::TypeInner;
    use candid_parser::{
        pretty_parse,
        types::{Dec, IDLProg},
    };
    let did = dfx_core::fs::read_to_string(path)?;
    let prog = pretty_parse::<IDLProg>(&format!("{}", path.display()), &did)?;
    let has_imports = prog
        .decs
        .iter()
        .any(|dec| matches!(dec, Dec::ImportType(_) | Dec::ImportServ(_)));
    let (env, actor) = CandidSource::File(path).load()?;
    let actor = actor.ok_or_else(|| anyhow!("provided candid file contains no main service"))?;
    let actor = env.trace_type(&actor)?;
    let has_init_args = matches!(actor.as_ref(), TypeInner::Class(_, _));
    if has_imports || has_init_args {
        let (init_args, serv) = match actor.as_ref() {
            TypeInner::Class(args, ty) => (args.clone(), ty.clone()),
            TypeInner::Service(_) => (vec![], actor.clone()),
            _ => unreachable!(),
        };
        let init_args = pp_args(&init_args).pretty(80).to_string();
        let service = compile(&env, &Some(serv));
        let constructor = compile(&env, &Some(actor));
        Ok((constructor, service, init_args))
    } else {
        // Keep the original did file to preserve comments
        Ok((did.clone(), did, "()".to_string()))
    }
}

#[context("{} is not a valid subtype of {}", specified_idl_path.display(), compiled_idl_path.display())]
fn check_valid_subtype(compiled_idl_path: &Path, specified_idl_path: &Path) -> DfxResult {
    use candid::types::subtype::{subtype_with_config, OptReport};
    let (mut env, opt_specified) = CandidSource::File(specified_idl_path)
        .load()
        .context("Checking specified candid file.")?;
    let specified_type =
        opt_specified.expect("Specified did file should contain some service interface");
    let (env2, opt_compiled) = CandidSource::File(compiled_idl_path)
        .load()
        .context("Checking compiled candid file.")?;
    let compiled_type =
        opt_compiled.expect("Compiled did file should contain some service interface");
    let mut gamma = HashSet::new();
    let specified_type = env.merge_type(env2, specified_type);
    subtype_with_config(
        OptReport::Error,
        &mut gamma,
        &env,
        &compiled_type,
        &specified_type,
    )?;
    Ok(())
}

/// A canister pool is a list of canisters.
pub struct CanisterPool {
    canisters: Vec<Arc<Canister>>,
    logger: Logger,
}

struct PoolConstructHelper<'a> {
    config: &'a Config,
    builder_pool: BuilderPool,
    canister_id_store: CanisterIdStore,
    generate_cid: bool,
    canisters_map: &'a mut Vec<Arc<Canister>>,
}

impl CanisterPool {
    #[context("Failed to insert '{}' into canister pool.", canister_name)]
    fn insert(canister_name: &str, pool_helper: &mut PoolConstructHelper<'_>) -> DfxResult<()> {
        let canister_id = match pool_helper.canister_id_store.find(canister_name) {
            Some(canister_id) => Some(canister_id),
            None if pool_helper.generate_cid => Some(Canister::generate_random_canister_id()?),
            _ => None,
        };
        let info = CanisterInfo::load(pool_helper.config, canister_name, canister_id)?;
        let builder = pool_helper.builder_pool.get(&info);
        pool_helper
            .canisters_map
            .insert(0, Arc::new(Canister::new(info, builder)));
        Ok(())
    }

    #[context("Failed to load canister pool.")]
    pub fn load(
        env: &dyn Environment,
        generate_cid: bool,
        canister_names: &[String],
    ) -> DfxResult<Self> {
        let logger = env.get_logger().new(slog::o!());
        let config = env
            .get_config()?
            .ok_or_else(|| anyhow!("Cannot find dfx configuration file in the current working directory. Did you forget to create one?"))?;

        let mut canisters_map = Vec::new();

        let mut pool_helper = PoolConstructHelper {
            config: &config,
            builder_pool: BuilderPool::new(env)?,
            canister_id_store: env.get_canister_id_store()?,
            generate_cid,
            canisters_map: &mut canisters_map,
        };

        for canister_name in canister_names {
            CanisterPool::insert(canister_name, &mut pool_helper)?;
        }

        Ok(CanisterPool {
            canisters: canisters_map,
            logger,
        })
    }

    pub fn get_canister(&self, canister_id: &CanisterId) -> Option<&Canister> {
        for c in &self.canisters {
            let info = &c.info;
            if Some(canister_id) == info.get_canister_id().ok().as_ref() {
                return Some(c);
            }
        }
        None
    }

    pub fn get_canister_list(&self) -> Vec<&Canister> {
        self.canisters.iter().map(|c| c.as_ref()).collect()
    }

    pub fn get_canister_info(&self, canister_id: &CanisterId) -> Option<&CanisterInfo> {
        self.get_canister(canister_id).map(|c| &c.info)
    }

    pub fn get_first_canister_with_name(&self, name: &str) -> Option<Arc<Canister>> {
        for c in &self.canisters {
            if c.info.get_name() == name {
                return Some(Arc::clone(c));
            }
        }
        None
    }

    pub fn get_logger(&self) -> &Logger {
        &self.logger
    }

    /// Builds a dependency graph for the given canisters.
    /// Only canisters in `canisters_to_build` and their dependencies will be
    /// included in the graph.
    #[context("Failed to build dependencies graph for canister pool.")]
    fn build_dependencies_graph(
        &self,
        canisters_to_build: Vec<&Canister>,
    ) -> DfxResult<DiGraph<CanisterId, ()>> {
        let mut graph: DiGraph<CanisterId, ()> = DiGraph::new();

        /// Recursive function that does the actual work of adding the canister
        /// and its dependencies to the graph.
        /// This is separate from the top-level function so that the top-level
        /// function can be called without worrying about the implementation
        /// details.
        ///
        /// Returns the index of the canister's graph node.
        fn add_canister_and_dependencies_to_graph(
            canister_pool: &CanisterPool,
            canister: &Canister,
            graph: &mut DiGraph<CanisterId, ()>,
            canister_id_to_canister: &BTreeMap<CanisterId, &Canister>,
            canister_id_to_index: &mut BTreeMap<CanisterId, NodeIndex<u32>>,
        ) -> DfxResult<NodeIndex> {
            let canister_id = canister.canister_id();

            // If this canister has already been visited, return its index. Its
            // dependencies were already added on a previous visit.
            if let Some(node_ix) = canister_id_to_index.get(&canister_id) {
                return Ok(*node_ix);
            }
            // Otherwise, add it to the graph.
            let node_ix = graph.add_node(canister_id);
            canister_id_to_index.insert(canister_id, node_ix);

            let deps = canister
                .builder
                .get_dependencies(canister_pool, &canister.info)?;

            for dependency_id in deps {
                let dependency = canister_id_to_canister.get(&dependency_id).ok_or_else(|| {
                    DfxError::new(BuildError::DependencyError(format!(
                        "Canister '{}' depends on canister '{}' which does not exist.",
                        canister.info.get_name(),
                        dependency_id.to_text()
                    )))
                })?;
                let dependency_index = add_canister_and_dependencies_to_graph(
                    canister_pool,
                    dependency,
                    graph,
                    canister_id_to_canister,
                    canister_id_to_index,
                )?;
                graph.add_edge(node_ix, dependency_index, ());
            }

            Ok(node_ix)
        }

        let mut canister_id_to_index: BTreeMap<CanisterId, NodeIndex<u32>> = BTreeMap::new();
        let canister_id_to_canister = self
            .canisters
            .iter()
            .map(|c| (c.canister_id(), c.as_ref()))
            .collect::<BTreeMap<CanisterId, &Canister>>();
        for canister in canisters_to_build {
            add_canister_and_dependencies_to_graph(
                self,
                canister,
                &mut graph,
                &canister_id_to_canister,
                &mut canister_id_to_index,
            )?;
        }

        // Verify the graph has no cycles.
        if let Err(err) = petgraph::algo::toposort(&graph, None) {
            let message = match graph.node_weight(err.node_id()) {
                Some(canister_id) => match self.get_canister_info(canister_id) {
                    Some(info) => info.get_name().to_string(),
                    None => format!("<{}>", canister_id.to_text()),
                },
                None => "<Unknown>".to_string(),
            };
            Err(DfxError::new(BuildError::DependencyError(format!(
                "Found circular dependency: {}",
                message
            ))))
        } else {
            Ok(graph)
        }
    }

    #[context("Failed step_prebuild_all.")]
    fn step_prebuild_all(&self, log: &Logger, build_config: &BuildConfig) -> DfxResult<()> {
        // moc expects all .did files of dependencies to be in <output_idl_path> with name <canister id>.did.
        // Because some canisters don't get built these .did files have to be copied over manually.
        for canister in self.canisters.iter().filter(|c| {
            build_config
                .canisters_to_build
                .as_ref()
                .map(|cans| !cans.iter().contains(&c.get_name().to_string()))
                .unwrap_or(false)
        }) {
            let from = canister
                .info
                .get_remote_candid()
                .unwrap_or_else(|| canister.info.get_output_idl_path().to_path_buf());

            if from.exists() {
                let to = build_config.idl_root.join(format!(
                    "{}.did",
                    canister.info.get_canister_id()?.to_text()
                ));
                trace!(
                    log,
                    "Copying .did for canister {} from {} to {}.",
                    canister.info.get_name(),
                    from.to_string_lossy(),
                    to.to_string_lossy()
                );
                dfx_core::fs::composite::ensure_parent_dir_exists(&to)?;
                dfx_core::fs::copy(&from, &to)?;
                dfx_core::fs::set_permissions_readwrite(&to)?;
            } else {
                warn!(
                    log,
                    ".did file for canister '{}' does not exist.",
                    canister.get_name(),
                );
            }
        }

        // cargo audit
        if self
            .canisters_to_build(build_config)
            .iter()
            .any(|can| can.info.is_rust())
        {
            self.run_cargo_audit()?;
        } else {
            trace!(
                self.logger,
                "No canister of type 'rust' found. Not trying to run 'cargo audit'."
            )
        }

        Ok(())
    }

    fn step_prebuild(&self, build_config: &BuildConfig, canister: &Canister) -> DfxResult<()> {
        canister.prebuild(self, build_config)
    }

    fn step_build<'a>(
        &self,
        build_config: &BuildConfig,
        canister: &'a Canister,
    ) -> DfxResult<&'a BuildOutput> {
        canister.build(self, build_config)
    }

    fn step_postbuild(
        &self,
        build_config: &BuildConfig,
        canister: &Canister,
        build_output: &BuildOutput,
    ) -> DfxResult<()> {
        canister.candid_post_process(self.get_logger(), build_config, build_output)?;

        canister.wasm_post_process(self.get_logger(), build_output)?;

        build_canister_js(&canister.canister_id(), &canister.info)?;

        canister.postbuild(self, build_config)
    }

    fn step_postbuild_all(
        &self,
        build_config: &BuildConfig,
        _order: &[CanisterId],
    ) -> DfxResult<()> {
        // We don't want to simply remove the whole directory, as in the future,
        // we may want to keep the IDL files downloaded from network.
        for canister in self.canisters_to_build(build_config) {
            let idl_root = &build_config.idl_root;
            let canister_id = canister.canister_id();
            let idl_file_path = idl_root.join(canister_id.to_text()).with_extension("did");

            // Ignore errors (e.g. File Not Found).
            let _ = std::fs::remove_file(idl_file_path);
        }

        Ok(())
    }

    /// Build all canisters, returning a vector of results of each builds.
    #[context("Failed while trying to build all canisters in the canister pool.")]
    pub fn build(
        &self,
        log: &Logger,
        build_config: &BuildConfig,
    ) -> DfxResult<Vec<Result<&BuildOutput, BuildError>>> {
        self.step_prebuild_all(log, build_config)
            .map_err(|e| DfxError::new(BuildError::PreBuildAllStepFailed(Box::new(e))))?;

        let canisters_to_build = self.canisters_to_build(build_config);
        let graph = self.build_dependencies_graph(canisters_to_build.clone())?;
        let nodes = petgraph::algo::toposort(&graph, None).map_err(|cycle| {
            let message = match graph.node_weight(cycle.node_id()) {
                Some(canister_id) => match self.get_canister_info(canister_id) {
                    Some(info) => info.get_name().to_string(),
                    None => format!("<{}>", canister_id.to_text()),
                },
                None => "<Unknown>".to_string(),
            };
            BuildError::DependencyError(format!("Found circular dependency: {}", message))
        })?;
        let order: Vec<CanisterId> = nodes
            .iter()
            .rev() // Reverse the order, as we have a dependency graph, we want to reverse indices.
            .map(|idx| *graph.node_weight(*idx).unwrap())
            .collect();

        let canisters_to_build = self.canisters_to_build(build_config);
        let mut result = Vec::new();
        for canister_id in &order {
            if let Some(canister) = self.get_canister(canister_id) {
                if canisters_to_build
                    .iter()
                    .map(|c| c.get_name())
                    .contains(&canister.get_name())
                {
                    trace!(log, "Building canister '{}'.", canister.get_name());
                } else {
                    trace!(log, "Not building canister '{}'.", canister.get_name());
                    continue;
                }
                result.push(
                    self.step_prebuild(build_config, canister)
                        .map_err(|e| {
                            BuildError::PreBuildStepFailed(
                                *canister_id,
                                canister.get_name().to_string(),
                                Box::new(e),
                            )
                        })
                        .and_then(|_| {
                            self.step_build(build_config, canister).map_err(|e| {
                                BuildError::BuildStepFailed(
                                    *canister_id,
                                    canister.get_name().to_string(),
                                    Box::new(e),
                                )
                            })
                        })
                        .and_then(|o| {
                            self.step_postbuild(build_config, canister, o)
                                .map_err(|e| {
                                    BuildError::PostBuildStepFailed(
                                        *canister_id,
                                        canister.get_name().to_string(),
                                        Box::new(e),
                                    )
                                })
                                .map(|_| o)
                        }),
                );
            }
        }

        self.step_postbuild_all(build_config, &order)
            .map_err(|e| DfxError::new(BuildError::PostBuildAllStepFailed(Box::new(e))))?;

        Ok(result)
    }

    /// Build all canisters, failing with the first that failed the build. Will return
    /// nothing if all succeeded.
    #[context("Failed while trying to build all canisters.")]
    pub async fn build_or_fail(&self, log: &Logger, build_config: &BuildConfig) -> DfxResult<()> {
        self.download(build_config).await?;
        let outputs = self.build(log, build_config)?;

        for output in outputs {
            output.map_err(DfxError::new)?;
        }

        Ok(())
    }

    async fn download(&self, build_config: &BuildConfig) -> DfxResult {
        for canister in self.canisters_to_build(build_config) {
            let info = canister.get_info();

            if info.is_custom() {
                custom_download(info, self).await?;
            }
        }
        Ok(())
    }

    /// If `cargo-audit` is installed this runs `cargo audit` and displays any vulnerable dependencies.
    fn run_cargo_audit(&self) -> DfxResult {
        let location = Command::new("cargo")
            .args(["locate-project", "--message-format=plain", "--workspace"])
            .output()
            .context("Failed to run 'cargo locate-project'.")?;
        if !location.status.success() {
            bail!(
                "'cargo locate-project' failed: {}",
                String::from_utf8_lossy(&location.stderr)
            );
        }
        let location = Path::new(std::str::from_utf8(&location.stdout)?);
        if !location
            .parent()
            .expect("Cargo.toml with no parent")
            .join("Cargo.lock")
            .exists()
        {
            warn!(
                self.logger,
                "Skipped audit step as there is no Cargo.lock file."
            );
            return Ok(());
        }
        if Command::new("cargo")
            .arg("audit")
            .arg("--version")
            .output()
            .map(|out| out.status.success())
            .unwrap_or(false)
        {
            info!(
                self.logger,
                "Checking for vulnerabilities in rust canisters."
            );
            let out = Command::new("cargo")
                .stdout(Stdio::inherit())
                .stderr(Stdio::inherit())
                .arg("audit")
                .output()
                .context("Failed to run 'cargo audit'.")?;
            if out.status.success() {
                info!(self.logger, "Audit found no vulnerabilities.")
            } else {
                error!(self.logger, "Audit found vulnerabilities in rust canisters. Please address these problems as soon as possible!");
            }
        } else {
            warn!(self.logger, "Cannot check for vulnerabilities in rust canisters because cargo-audit is not installed. Please run 'cargo install cargo-audit' so that vulnerabilities can be detected.");
        }
        Ok(())
    }

    pub fn canisters_to_build(&self, build_config: &BuildConfig) -> Vec<&Canister> {
        if let Some(canister_names) = &build_config.canisters_to_build {
            self.canisters
                .iter()
                .filter(|can| canister_names.contains(&can.info.get_name().to_string()))
                .map(Arc::as_ref)
                .collect()
        } else {
            self.canisters.iter().map(Arc::as_ref).collect()
        }
    }
}

#[context("Failed to decode path to str.")]
fn decode_path_to_str(path: &Path) -> DfxResult<&str> {
    path.to_str().ok_or_else(|| {
        DfxError::new(BuildError::JsBindGenError(format!(
            "Unable to convert output canister js path to a string: {:#?}",
            path
        )))
    })
}

/// Create a canister JavaScript DID and Actor Factory.
#[context("Failed to build canister js for canister '{}'.", canister_info.get_name())]
fn build_canister_js(canister_id: &CanisterId, canister_info: &CanisterInfo) -> DfxResult {
    let output_did_js_path = canister_info
        .get_service_idl_path()
        .with_extension("did.js");
    let output_did_ts_path = canister_info
        .get_service_idl_path()
        .with_extension("did.d.ts");

    let (env, ty) = CandidSource::File(&canister_info.get_constructor_idl_path()).load()?;
    let content = ensure_trailing_newline(candid_parser::bindings::javascript::compile(&env, &ty));
    std::fs::write(&output_did_js_path, content).with_context(|| {
        format!(
            "Failed to write to {}.",
            output_did_js_path.to_string_lossy()
        )
    })?;
    let content = ensure_trailing_newline(candid_parser::bindings::typescript::compile(&env, &ty));
    std::fs::write(&output_did_ts_path, content).with_context(|| {
        format!(
            "Failed to write to {}.",
            output_did_ts_path.to_string_lossy()
        )
    })?;

    let mut language_bindings =
        assets::language_bindings().context("Failed to get language bindings archive.")?;
    let index_js_path = canister_info.get_index_js_path();
    for f in language_bindings
        .entries()
        .context("Failed to read language bindings archive entries.")?
    {
        let mut file = f.context("Failed to read language bindings archive entry.")?;
        let mut file_contents = String::new();
        file.read_to_string(&mut file_contents)
            .context("Failed to read file content.")?;
        let canister_name = canister_info.get_name();
        let canister_name_ident = canister_name.replace('-', "_");

        let new_file_contents = file_contents
            .replace("{canister_id}", &canister_id.to_text())
            .replace("{canister_name}", canister_name)
            .replace("{canister_name_ident}", &canister_name_ident)
            .replace("{canister_name_uppercase}", &canister_name.to_uppercase())
            .replace(
                "{canister_name_ident_uppercase}",
                &canister_name_ident.to_uppercase(),
            );

        match decode_path_to_str(&file.path()?)? {
            "canister.js" => {
                std::fs::write(decode_path_to_str(&index_js_path)?, new_file_contents)
                    .with_context(|| {
                        format!("Failed to write to {}.", index_js_path.to_string_lossy())
                    })?;
            }
            "canisterId.js" => {
                std::fs::write(decode_path_to_str(&index_js_path)?, new_file_contents)
                    .with_context(|| {
                        format!("Failed to write to {}.", index_js_path.to_string_lossy())
                    })?;
            }
            // skip
            "index.js.hbs" => {}
            "index.d.ts.hbs" => {}
            _ => unreachable!(),
        }
    }

    Ok(())
}

fn ensure_trailing_newline(s: String) -> String {
    if s.ends_with('\n') {
        s
    } else {
        let mut s = s;
        s.push('\n');
        s
    }
}


-----------------------

/src/dfx/src/lib/named_canister.rs:
-----------------------

/src/dfx/src/lib/network/id.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::Context;
use dfx_core::config::model::local_server_descriptor::{LocalServerDescriptor, NetworkMetadata};
use fn_error_context::context;
use time::OffsetDateTime;

#[context("Failed write network id to {}.", local_server_descriptor.network_id_path().display())]
pub fn write_network_id(local_server_descriptor: &LocalServerDescriptor) -> DfxResult {
    let settings_digest = local_server_descriptor.settings_digest().to_string();
    let contents = NetworkMetadata {
        created: OffsetDateTime::now_utc(),
        settings_digest,
    };
    let contents =
        serde_json::to_string_pretty(&contents).context("Failed to pretty-format to string")?;
    std::fs::write(local_server_descriptor.network_id_path(), contents)
        .context("Failed to write to file")?;
    Ok(())
}


-----------------------

/src/dfx/src/lib/network/mod.rs:
-----------------------

pub mod id;
pub mod network_opt;


-----------------------

/src/dfx/src/lib/network/network_opt.rs:
-----------------------

use clap::{ArgGroup, Args};

#[derive(Args, Clone, Debug, Default)]
#[clap(
group(ArgGroup::new("network-select").multiple(false)),
)]
pub struct NetworkOpt {
    /// Override the compute network to connect to. By default, the local network is used.
    /// A valid URL (starting with `http:` or `https:`) can be used here, and a special
    /// ephemeral network will be created specifically for this request. E.g.
    /// "http://localhost:12345/" is a valid network name.
    #[arg(long, global(true), group = "network-select")]
    network: Option<String>,

    /// Shorthand for --network=playground.
    /// Borrows short-lived canisters on the real IC network instead of creating normal canisters.
    #[clap(long, global(true), group = "network-select")]
    playground: bool,

    /// Shorthand for --network=ic.
    #[clap(long, global(true), group = "network-select")]
    ic: bool,
}

impl NetworkOpt {
    pub fn to_network_name(&self) -> Option<String> {
        if self.playground {
            Some("playground".to_string())
        } else if self.ic {
            Some("ic".to_string())
        } else {
            self.network.clone()
        }
    }
}


-----------------------

/src/dfx/src/lib/nns_types/account_identifier.rs:
-----------------------

/src/dfx/src/lib/nns_types/icpts.rs:
-----------------------

/src/dfx/src/lib/nns_types/mod.rs:
-----------------------

pub mod account_identifier;
pub mod icpts;


-----------------------

/src/dfx/src/lib/operations/canister/create_canister.rs:
-----------------------

use crate::lib::cycles_ledger_types::create_canister::{
    CmcCreateCanisterArgs, CmcCreateCanisterError,
};
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::ic_attributes::CanisterSettings as DfxCanisterSettings;
use crate::lib::identity::wallet::{get_or_create_wallet_canister, GetOrCreateWalletCanisterError};
use crate::lib::ledger_types::MAINNET_CYCLE_MINTER_CANISTER_ID;
use crate::lib::operations::canister::motoko_playground::reserve_canister_with_playground;
use crate::lib::operations::cycles_ledger::create_with_cycles_ledger;
use crate::util::clap::subnet_selection_opt::SubnetSelectionType;
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use dfx_core::canister::build_wallet_canister;
use dfx_core::identity::CallSender;
use dfx_core::network::provider::get_network_context;
use fn_error_context::context;
use ic_agent::agent::{RejectCode, RejectResponse};
use ic_agent::agent_error::HttpErrorPayload;
use ic_agent::{Agent, AgentError};
use ic_utils::interfaces::management_canister::builders::CanisterSettings;
use ic_utils::interfaces::ManagementCanister;
use ic_utils::Argument;
use icrc_ledger_types::icrc1::account::Subaccount;
use slog::{debug, info, warn};
use std::format;

// The cycle fee for create request is 0.1T cycles.
pub const CANISTER_CREATE_FEE: u128 = 100_000_000_000_u128;
// We do not know the minimum cycle balance a canister should have.
// For now create the canister with 3T cycle balance.
pub const CANISTER_INITIAL_CYCLE_BALANCE: u128 = 3_000_000_000_000_u128;
pub const CMC_CREATE_CANISTER_METHOD: &str = "create_canister";

#[context("Failed to create canister '{}'.", canister_name)]
pub async fn create_canister(
    env: &dyn Environment,
    canister_name: &str,
    with_cycles: Option<u128>,
    specified_id_from_cli: Option<Principal>,
    call_sender: &CallSender,
    no_wallet: bool,
    from_subaccount: Option<Subaccount>,
    settings: DfxCanisterSettings,
    created_at_time: Option<u64>,
    subnet_selection: &mut SubnetSelectionType,
) -> DfxResult {
    let log = env.get_logger();
    info!(log, "Creating canister {}...", canister_name);

    let config = env.get_config_or_anyhow()?;
    let config_interface = config.get_config();

    let mut canister_id_store = env.get_canister_id_store()?;

    let network_name = get_network_context()?;

    if let Some(remote_canister_id) = config_interface
        .get_remote_canister_id(canister_name, &network_name)
        .unwrap_or_default()
    {
        bail!(
            "{} canister is remote on network {} and has canister id: {}",
            canister_name,
            network_name,
            remote_canister_id.to_text()
        );
    }

    let non_default_network = if network_name == "local" {
        String::new()
    } else {
        format!("on network {} ", network_name)
    };

    if let Some(canister_id) = canister_id_store.find(canister_name) {
        info!(
            log,
            "{} canister was already created {}and has canister id: {}",
            canister_name,
            non_default_network,
            canister_id.to_text()
        );
        return Ok(());
    }

    if env.get_network_descriptor().is_playground() {
        return reserve_canister_with_playground(env, canister_name).await;
    }

    // Specified ID from the command line takes precedence over the one in dfx.json.
    let mut specified_id = match (
        config_interface.get_specified_id(canister_name)?,
        specified_id_from_cli,
    ) {
        (Some(specified_id_from_json), Some(specified_id_from_cli)) => {
            if specified_id_from_json != specified_id_from_cli {
                warn!(
                    env.get_logger(),
                    "Canister '{0}' has a specified ID in dfx.json: {1},
which is different from the one specified in the command line: {2}.
The command line value will be used.",
                    canister_name,
                    specified_id_from_json,
                    specified_id_from_cli
                );
            }
            Some(specified_id_from_cli)
        }
        (Some(specified_id_from_json), None) => Some(specified_id_from_json),
        (None, Some(specified_id_from_cli)) => Some(specified_id_from_cli),
        (None, None) => None,
    };

    // If the network is IC mainnet, the specified ID will be overwritten by None.
    if env.get_network_descriptor().is_ic && specified_id.is_some() {
        warn!(
            env.get_logger(),
            "Specified ID is ignored on the IC mainnet."
        );
        specified_id = None;
    }

    // Replace call_sender with wallet canister unless:
    // 1. specified_id is in effect OR
    // 2. --no-wallet is set explicitly OR
    // 3. call_sender is already wallet
    let call_sender =
        if specified_id.is_some() || no_wallet || matches!(call_sender, CallSender::Wallet(_)) {
            *call_sender
        } else {
            match get_or_create_wallet_canister(
                env,
                env.get_network_descriptor(),
                env.get_selected_identity().expect("No selected identity"),
            )
            .await
            {
                Ok(wallet) => CallSender::Wallet(*wallet.canister_id_()),
                Err(err) => {
                    if matches!(
                        err,
                        GetOrCreateWalletCanisterError::NoWalletConfigured { .. }
                    ) {
                        debug!(env.get_logger(), "No wallet configured.");
                        *call_sender
                    } else {
                        bail!(err)
                    }
                }
            }
        };

    let agent = env.get_agent();
    let cid = match call_sender {
        CallSender::SelectedId => {
            let auto_wallet_disabled = std::env::var("DFX_DISABLE_AUTO_WALLET").is_ok();
            let ic_network = env.get_network_descriptor().is_ic;
            if ic_network || auto_wallet_disabled {
                create_with_cycles_ledger(
                    env,
                    agent,
                    canister_name,
                    with_cycles,
                    from_subaccount,
                    settings,
                    created_at_time,
                    subnet_selection,
                )
                .await
            } else {
                create_with_management_canister(env, agent, with_cycles, specified_id, settings)
                    .await
            }
        }
        CallSender::Wallet(wallet_id) => {
            create_with_wallet(agent, &wallet_id, with_cycles, settings, subnet_selection).await
        }
    }?;
    let canister_id = cid.to_text();
    info!(
        log,
        "{} canister created {}with canister id: {}",
        canister_name,
        non_default_network,
        canister_id
    );
    canister_id_store.add(canister_name, &canister_id, None)?;

    Ok(())
}

async fn create_with_management_canister(
    env: &dyn Environment,
    agent: &Agent,
    with_cycles: Option<u128>,
    specified_id: Option<Principal>,
    settings: DfxCanisterSettings,
) -> DfxResult<Principal> {
    let mgr = ManagementCanister::create(agent);
    let mut builder = mgr
        .create_canister()
        .as_provisional_create_with_amount(with_cycles)
        .with_effective_canister_id(env.get_effective_canister_id());
    if let Some(sid) = specified_id {
        builder = builder.as_provisional_create_with_specified_id(sid);
    }
    if let Some(controllers) = settings.controllers {
        for controller in controllers {
            builder = builder.with_controller(controller);
        }
    };
    let res = builder
        .with_optional_compute_allocation(settings.compute_allocation)
        .with_optional_memory_allocation(settings.memory_allocation)
        .with_optional_freezing_threshold(settings.freezing_threshold)
        .with_optional_reserved_cycles_limit(settings.reserved_cycles_limit)
        .with_optional_wasm_memory_limit(settings.wasm_memory_limit)
        .with_optional_log_visibility(settings.log_visibility)
        .await;
    const NEEDS_WALLET: &str = "In order to create a canister on this network, you must use a wallet in order to allocate cycles to the new canister. \
                        To do this, remove the --no-wallet argument and try again. It is also possible to create a canister on this network \
                        using `dfx ledger create-canister`, but doing so will not associate the created canister with any of the canisters in your project.";
    match res {
        Ok((o,)) => Ok(o),
        Err(AgentError::HttpError(HttpErrorPayload {
            status, content, ..
        })) if (400..500).contains(&status) => {
            let message = String::from_utf8_lossy(&content);
            if message.contains(
                "does not belong to an existing subnet and it is not a mainnet canister ID.",
            ) {
                Err(anyhow!("{message}"))
            } else {
                Err(anyhow!(NEEDS_WALLET))
            }
        }
        Err(AgentError::UncertifiedReject(RejectResponse {
            reject_code: RejectCode::CanisterReject,
            reject_message,
            ..
        })) if reject_message.contains("is not allowed to call ic00 method") => {
            Err(anyhow!(NEEDS_WALLET))
        }
        Err(e) => Err(e).context("Canister creation call failed."),
    }
}

async fn create_with_wallet(
    agent: &Agent,
    wallet_id: &Principal,
    with_cycles: Option<u128>,
    settings: DfxCanisterSettings,
    subnet_selection: &SubnetSelectionType,
) -> DfxResult<Principal> {
    let wallet = build_wallet_canister(*wallet_id, agent).await?;
    let cycles = with_cycles.unwrap_or(CANISTER_CREATE_FEE + CANISTER_INITIAL_CYCLE_BALANCE);

    if let Some(subnet_selection) = subnet_selection.get_user_choice() {
        // `wallet_create_canister` only calls the management canister, which means that canisters only get created on the subnet the wallet is on.
        // For any explicit targeting we need to use the CMC.

        let settings = if settings.controllers.is_some() {
            settings
        } else {
            let identity = agent
                .get_principal()
                .map_err(|err| anyhow!("Failed to get selected identity principal: {err}"))?;
            DfxCanisterSettings {
                controllers: Some(vec![*wallet_id, identity]),
                ..settings
            }
        };

        let call_result: Result<
            (Result<Principal, CmcCreateCanisterError>,),
            ic_agent::AgentError,
        > = wallet
            .call128(
                MAINNET_CYCLE_MINTER_CANISTER_ID,
                CMC_CREATE_CANISTER_METHOD,
                Argument::from_candid((CmcCreateCanisterArgs {
                    settings: Some(CanisterSettings::from(settings)),
                    subnet_selection: Some(subnet_selection),
                },)),
                cycles,
            )
            .await;
        match call_result {
            Ok((Ok(canister_id),)) => Ok(canister_id),
            Ok((Err(err),)) => Err(anyhow!(err)),
            Err(AgentError::WalletUpgradeRequired(s)) => Err(anyhow!(
                "{}\nTo upgrade, run dfx wallet upgrade.",
                AgentError::WalletUpgradeRequired(s)
            )),
            Err(other) => Err(anyhow!(other)),
        }
    } else {
        if settings.reserved_cycles_limit.is_some() {
            bail!(
                "Cannot create a canister using a wallet if the reserved_cycles_limit is set. Please create with --no-wallet or use dfx canister update-settings instead.")
        }
        if settings.wasm_memory_limit.is_some() {
            bail!("Cannot create a canister using a wallet if the wasm_memory_limit is set. Please create with --no-wallet or use dfx canister update-settings instead.")
        }
        if settings.log_visibility.is_some() {
            bail!("Cannot create a canister using a wallet if log_visibility is set. Please create with --no-wallet or use dfx canister update-settings instead.")
        }
        match wallet
            .wallet_create_canister(
                cycles,
                settings.controllers,
                settings.compute_allocation,
                settings.memory_allocation,
                settings.freezing_threshold,
            )
            .await
        {
            Ok(result) => Ok(result.canister_id),
            Err(AgentError::WalletUpgradeRequired(s)) => Err(anyhow!(
                "{}\nTo upgrade, run dfx wallet upgrade.",
                AgentError::WalletUpgradeRequired(s)
            )),
            Err(other) => Err(anyhow!(other)),
        }
    }
}


-----------------------

/src/dfx/src/lib/operations/canister/deploy_canisters.rs:
-----------------------

use crate::lib::builders::BuildConfig;
use crate::lib::canister_info::assets::AssetsCanisterInfo;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::ic_attributes::CanisterSettings;
use crate::lib::installers::assets::prepare_assets_for_proposal;
use crate::lib::models::canister::CanisterPool;
use crate::lib::operations::canister::deploy_canisters::DeployMode::{
    ComputeEvidence, ForceReinstallSingleCanister, NormalDeploy, PrepareForProposal,
};
use crate::lib::operations::canister::motoko_playground::reserve_canister_with_playground;
use crate::lib::operations::canister::{
    all_project_canisters_with_ids, create_canister, install_canister::install_canister,
};
use crate::util::clap::install_mode::InstallModeHint;
use crate::util::clap::subnet_selection_opt::SubnetSelectionType;
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use dfx_core::config::model::canister_id_store::CanisterIdStore;
use dfx_core::config::model::dfinity::Config;
use dfx_core::identity::CallSender;
use fn_error_context::context;
use ic_utils::interfaces::management_canister::attributes::{
    ComputeAllocation, FreezingThreshold, MemoryAllocation, ReservedCyclesLimit,
};
use ic_utils::interfaces::management_canister::builders::WasmMemoryLimit;
use icrc_ledger_types::icrc1::account::Subaccount;
use slog::info;
use std::convert::TryFrom;
use std::path::{Path, PathBuf};

#[derive(Eq, PartialEq, Debug, Clone)]
pub enum DeployMode {
    NormalDeploy,
    ForceReinstallSingleCanister(String),
    PrepareForProposal(String),
    ComputeEvidence(String),
}

#[context("Failed while trying to deploy canisters.")]
#[allow(clippy::too_many_arguments)]
pub async fn deploy_canisters(
    env: &dyn Environment,
    some_canister: Option<&str>,
    argument: Option<&str>,
    argument_type: Option<&str>,
    deploy_mode: &DeployMode,
    mode_hint: &InstallModeHint,
    upgrade_unchanged: bool,
    with_cycles: Option<u128>,
    created_at_time: Option<u64>,
    specified_id_from_cli: Option<Principal>,
    call_sender: &CallSender,
    from_subaccount: Option<Subaccount>,
    no_wallet: bool,
    skip_consent: bool,
    env_file: Option<PathBuf>,
    no_asset_upgrade: bool,
    subnet_selection: &mut SubnetSelectionType,
    always_assist: bool,
) -> DfxResult {
    let log = env.get_logger();

    let config = env
        .get_config()?
        .ok_or_else(|| anyhow!("Cannot find dfx configuration file in the current working directory. Did you forget to create one?"))?;
    let initial_canister_id_store = env.get_canister_id_store()?;

    let pull_canisters_in_config = config.get_config().get_pull_canisters()?;
    if let Some(canister_name) = some_canister {
        if pull_canisters_in_config.contains_key(canister_name) {
            bail!(
                "{0} is a pull dependency. Please deploy it using `dfx deps deploy {0}`",
                canister_name
            );
        }
    }

    let canisters_to_deploy = canister_with_dependencies(&config, some_canister)?;

    let canisters_to_build = match deploy_mode {
        PrepareForProposal(canister_name) | ComputeEvidence(canister_name) => {
            vec![canister_name.clone()]
        }
        ForceReinstallSingleCanister(canister_name) => {
            // don't force-reinstall the dependencies too.
            vec![String::from(canister_name)]
        }
        NormalDeploy => canisters_to_deploy
            .clone()
            .into_iter()
            .filter(|canister_name| {
                !config
                    .get_config()
                    .is_remote_canister(canister_name, &env.get_network_descriptor().name)
                    .unwrap_or(false)
            })
            .collect(),
    };

    let canisters_to_install: Vec<String> = canisters_to_build
        .clone()
        .into_iter()
        .filter(|canister_name| !pull_canisters_in_config.contains_key(canister_name))
        .collect();

    if some_canister.is_some() {
        info!(log, "Deploying: {}", canisters_to_install.join(" "));
    } else {
        info!(log, "Deploying all canisters.");
    }
    if canisters_to_deploy
        .iter()
        .any(|canister| initial_canister_id_store.find(canister).is_none())
    {
        register_canisters(
            env,
            &canisters_to_deploy,
            &initial_canister_id_store,
            with_cycles,
            specified_id_from_cli,
            call_sender,
            no_wallet,
            from_subaccount,
            created_at_time,
            &config,
            subnet_selection,
        )
        .await?;
    } else {
        info!(env.get_logger(), "All canisters have already been created.");
    }

    let canisters_to_load = all_project_canisters_with_ids(env, &config);

    let pool = build_canisters(
        env,
        &canisters_to_load,
        &canisters_to_build,
        &config,
        env_file.clone(),
    )
    .await?;

    match deploy_mode {
        NormalDeploy | ForceReinstallSingleCanister(_) => {
            install_canisters(
                env,
                &canisters_to_install,
                &config,
                argument,
                argument_type,
                mode_hint,
                upgrade_unchanged,
                call_sender,
                pool,
                skip_consent,
                env_file.as_deref(),
                no_asset_upgrade,
                always_assist,
            )
            .await?;
            info!(log, "Deployed canisters.");
        }
        PrepareForProposal(canister_name) => {
            prepare_assets_for_commit(env, &initial_canister_id_store, &config, canister_name)
                .await?
        }
        ComputeEvidence(canister_name) => {
            compute_evidence(env, &initial_canister_id_store, &config, canister_name).await?
        }
    }

    Ok(())
}

#[context("Failed to collect canisters and their dependencies.")]
fn canister_with_dependencies(
    config: &Config,
    some_canister: Option<&str>,
) -> DfxResult<Vec<String>> {
    let mut canister_names = config
        .get_config()
        .get_canister_names_with_dependencies(some_canister)?;
    canister_names.sort();
    Ok(canister_names)
}

/// Creates canisters that have not been created yet.
#[context("Failed while trying to register all canisters.")]
async fn register_canisters(
    env: &dyn Environment,
    canister_names: &[String],
    canister_id_store: &CanisterIdStore,
    with_cycles: Option<u128>,
    specified_id_from_cli: Option<Principal>,
    call_sender: &CallSender,
    no_wallet: bool,
    from_subaccount: Option<Subaccount>,
    created_at_time: Option<u64>,
    config: &Config,
    subnet_selection: &mut SubnetSelectionType,
) -> DfxResult {
    let canisters_to_create = canister_names
        .iter()
        .filter(|n| canister_id_store.find(n).is_none())
        .cloned()
        .collect::<Vec<String>>();
    if canisters_to_create.is_empty() {
        info!(env.get_logger(), "All canisters have already been created.");
    } else if env.get_network_descriptor().is_playground() {
        info!(env.get_logger(), "Reserving canisters in playground...");
        for canister_name in &canisters_to_create {
            reserve_canister_with_playground(env, canister_name).await?;
        }
    } else {
        info!(env.get_logger(), "Creating canisters...");
        for canister_name in &canisters_to_create {
            let config_interface = config.get_config();
            let compute_allocation = config_interface
                .get_compute_allocation(canister_name)?
                .map(|arg| {
                    ComputeAllocation::try_from(arg)
                        .context("Compute Allocation must be a percentage.")
                })
                .transpose()?;
            let memory_allocation = config_interface
                .get_memory_allocation(canister_name)?
                .map(|arg| {
                    u64::try_from(arg.get_bytes())
                        .map_err(|e| anyhow!(e))
                        .and_then(|n| Ok(MemoryAllocation::try_from(n)?))
                        .context(
                            "Memory allocation must be between 0 and 2^48 (i.e 256TB), inclusively.",
                        )
                })
                .transpose()?;
            let freezing_threshold =
                config_interface
                    .get_freezing_threshold(canister_name)?
                    .map(|arg| {
                        FreezingThreshold::try_from(arg.as_secs())
                            .expect("Freezing threshold must be between 0 and 2^64-1, inclusively.")
                    });
            let reserved_cycles_limit = config_interface
                .get_reserved_cycles_limit(canister_name)?
                .map(|arg| {
                    ReservedCyclesLimit::try_from(arg)
                        .expect("Reserved cycles limit must be between 0 and 2^128-1, inclusively.")
                });
            let wasm_memory_limit = config_interface.get_wasm_memory_limit(canister_name)?.map(
                |arg| {
                    u64::try_from(arg.get_bytes())
                        .map_err(|e| anyhow!(e))
                        .and_then(|n| Ok(WasmMemoryLimit::try_from(n)?))
                        .context(
                            "Wasm memory limit must be between 0 and 2^48 (i.e 256TB), inclusively.",
                        )
                },
            ).transpose()?;
            let log_visibility = config_interface.get_log_visibility(canister_name)?;

            let controllers = None;
            create_canister(
                env,
                canister_name,
                with_cycles,
                specified_id_from_cli,
                call_sender,
                no_wallet,
                from_subaccount,
                CanisterSettings {
                    controllers,
                    compute_allocation,
                    memory_allocation,
                    freezing_threshold,
                    reserved_cycles_limit,
                    wasm_memory_limit,
                    log_visibility,
                },
                created_at_time,
                subnet_selection,
            )
            .await?;
        }
    }
    Ok(())
}

#[context("Failed to build all canisters.")]
async fn build_canisters(
    env: &dyn Environment,
    canisters_to_load: &[String],
    canisters_to_build: &[String],
    config: &Config,
    env_file: Option<PathBuf>,
) -> DfxResult<CanisterPool> {
    let log = env.get_logger();
    info!(log, "Building canisters...");
    let build_mode_check = false;
    let canister_pool = CanisterPool::load(env, build_mode_check, canisters_to_load)?;

    let build_config =
        BuildConfig::from_config(config, env.get_network_descriptor().is_playground())?
            .with_canisters_to_build(canisters_to_build.into())
            .with_env_file(env_file);
    canister_pool.build_or_fail(log, &build_config).await?;
    Ok(canister_pool)
}

#[context("Failed while trying to install all canisters.")]
async fn install_canisters(
    env: &dyn Environment,
    canister_names: &[String],
    config: &Config,
    argument: Option<&str>,
    argument_type: Option<&str>,
    mode_hint: &InstallModeHint,
    upgrade_unchanged: bool,
    call_sender: &CallSender,
    pool: CanisterPool,
    skip_consent: bool,
    env_file: Option<&Path>,
    no_asset_upgrade: bool,
    always_assist: bool,
) -> DfxResult {
    info!(env.get_logger(), "Installing canisters...");

    let mut canister_id_store = env.get_canister_id_store()?;

    for canister_name in canister_names {
        let canister_id = canister_id_store.get(canister_name)?;
        let canister_info = CanisterInfo::load(config, canister_name, Some(canister_id))?;

        install_canister(
            env,
            &mut canister_id_store,
            canister_id,
            &canister_info,
            None,
            argument,
            argument_type,
            mode_hint,
            call_sender,
            upgrade_unchanged,
            Some(&pool),
            skip_consent,
            env_file,
            no_asset_upgrade,
            always_assist,
        )
        .await?;
    }

    Ok(())
}

#[context("Failed to prepare assets for commit.")]
async fn prepare_assets_for_commit(
    env: &dyn Environment,
    canister_id_store: &CanisterIdStore,
    config: &Config,
    canister_name: &str,
) -> DfxResult {
    let canister_id = canister_id_store.get(canister_name)?;
    let canister_info = CanisterInfo::load(config, canister_name, Some(canister_id))?;

    if !canister_info.is_assets() {
        bail!(
            "Expected canister {} to be an asset canister.",
            canister_name
        );
    }

    let agent = env.get_agent();

    prepare_assets_for_proposal(&canister_info, agent, env.get_logger()).await?;

    Ok(())
}

#[context("Failed to compute evidence.")]
async fn compute_evidence(
    env: &dyn Environment,
    canister_id_store: &CanisterIdStore,
    config: &Config,
    canister_name: &str,
) -> DfxResult {
    let canister_id = canister_id_store.get(canister_name)?;
    let canister_info = CanisterInfo::load(config, canister_name, Some(canister_id))?;

    if !canister_info.is_assets() {
        bail!(
            "Expected canister {} to be an asset canister.",
            canister_name
        );
    }

    let agent = env.get_agent();

    let assets_canister_info = canister_info.as_info::<AssetsCanisterInfo>()?;
    let source_paths = assets_canister_info.get_source_paths();
    let source_paths: Vec<&Path> = source_paths.iter().map(|p| p.as_path()).collect::<_>();

    let canister_id = canister_info
        .get_canister_id()
        .context("Could not find canister ID.")?;

    let canister = ic_utils::Canister::builder()
        .with_agent(agent)
        .with_canister_id(canister_id)
        .build()
        .context("Failed to build asset canister caller.")?;

    let evidence = ic_asset::compute_evidence(&canister, &source_paths, env.get_logger()).await?;
    println!("{}", evidence);

    Ok(())
}


-----------------------

/src/dfx/src/lib/operations/canister/install_canister.rs:
-----------------------

use crate::lib::builders::get_and_write_environment_variables;
use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::installers::assets::post_install_store_assets;
use crate::lib::models::canister::CanisterPool;
use crate::lib::named_canister;
use crate::lib::operations::canister::all_project_canisters_with_ids;
use crate::lib::operations::canister::motoko_playground::authorize_asset_uploader;
use crate::lib::state_tree::canister_info::read_state_tree_canister_module_hash;
use crate::util::assets::wallet_wasm;
use crate::util::clap::install_mode::InstallModeHint;
use crate::util::{blob_from_arguments, get_candid_init_type, read_module_metadata};
use anyhow::{anyhow, bail, Context};
use backoff::backoff::Backoff;
use backoff::ExponentialBackoff;
use candid::Principal;
use dfx_core::canister::{build_wallet_canister, install_canister_wasm, install_mode_to_prompt};
use dfx_core::cli::ask_for_consent;
use dfx_core::config::model::canister_id_store::CanisterIdStore;
use dfx_core::config::model::network_descriptor::NetworkDescriptor;
use dfx_core::identity::CallSender;
use fn_error_context::context;
use ic_agent::Agent;
use ic_utils::interfaces::management_canister::builders::{InstallMode, WasmMemoryPersistence};
use ic_utils::interfaces::ManagementCanister;
use ic_utils::Argument;
use itertools::Itertools;
use sha2::{Digest, Sha256};
use slog::{debug, info, warn};
use std::collections::HashSet;
use std::path::{Path, PathBuf};
use std::process::{Command, Stdio};

use super::motoko_playground::playground_install_code;

#[context("Failed to install wasm module to canister '{}'.", canister_info.get_name())]
pub async fn install_canister(
    env: &dyn Environment,
    canister_id_store: &mut CanisterIdStore,
    canister_id: Principal,
    canister_info: &CanisterInfo,
    wasm_path_override: Option<&Path>,
    argument_from_cli: Option<&str>,
    argument_type_from_cli: Option<&str>,
    mode_hint: &InstallModeHint,
    call_sender: &CallSender,
    upgrade_unchanged: bool,
    pool: Option<&CanisterPool>,
    skip_consent: bool,
    env_file: Option<&Path>,
    no_asset_upgrade: bool,
    always_assist: bool,
) -> DfxResult {
    let log = env.get_logger();
    let agent = env.get_agent();
    let network = env.get_network_descriptor();
    if !network.is_ic && named_canister::get_ui_canister_id(canister_id_store).is_none() {
        named_canister::install_ui_canister(env, canister_id_store, None).await?;
    }
    let installed_module_hash = read_state_tree_canister_module_hash(agent, canister_id).await?;
    debug!(
        log,
        "Previously installed module hash: {:?}",
        installed_module_hash.as_ref().map(hex::encode)
    );
    let wasm_memory_persistence_embeded =
        read_module_metadata(agent, canister_id, "enhanced-orthogonal-persistence")
            .await
            .map(|_| WasmMemoryPersistence::Keep);
    // let mode = if let InstallModeHint::Auto(options) = mode_hint {
    //     if installed_module_hash.is_some() {
    //         InstallMode::Upgrade(Some(CanisterUpgradeOptions {
    //             wasm_memory_persistence,
    //             skip_pre_upgrade: None,
    //         }))
    //     } else {
    //         InstallMode::Install
    //     }
    // } else {
    //     InstallMode::Install
    // };
    let mode = mode_hint.to_install_mode(
        installed_module_hash.is_some(),
        wasm_memory_persistence_embeded,
    );

    // let mode = mode.unwrap_or_else(|| {
    //     if installed_module_hash.is_some() {
    //         InstallMode::Upgrade(Some(CanisterUpgradeOptions {
    //             wasm_memory_persistence,
    //             skip_pre_upgrade: None,
    //         }))
    //     } else {
    //         InstallMode::Install
    //     }
    // });
    let mode_str = install_mode_to_prompt(&mode);
    let canister_name = canister_info.get_name();
    info!(
        log,
        "{mode_str} code for canister {canister_name}, with canister ID {canister_id}",
    );
    if !skip_consent && matches!(mode, InstallMode::Reinstall | InstallMode::Upgrade { .. }) {
        let candid = read_module_metadata(agent, canister_id, "candid:service").await;
        if let Some(candid) = &candid {
            match check_candid_compatibility(canister_info, candid) {
                Ok(None) => (),
                Ok(Some(err)) => {
                    let msg = format!("Candid interface compatibility check failed for canister '{}'.\nYou are making a BREAKING change. Other canisters or frontend clients relying on your canister may stop working.\n\n", canister_info.get_name()) + &err;
                    ask_for_consent(&msg)?;
                }
                Err(e) => {
                    let msg = format!("An error occurred during Candid interface compatibility check for canister '{}'.\n\n", canister_info.get_name()) + &e.to_string();
                    ask_for_consent(&msg)?;
                }
            }
        }
    }
    if !skip_consent && canister_info.is_motoko() && matches!(mode, InstallMode::Upgrade { .. }) {
        let stable_types = read_module_metadata(agent, canister_id, "motoko:stable-types").await;
        if let Some(stable_types) = &stable_types {
            match check_stable_compatibility(canister_info, env, stable_types) {
                Ok(StableCompatibility::Okay) => (),
                Ok(StableCompatibility::Warning(details)) => {
                    let msg = format!("Stable interface compatibility check issued a WARNING for canister '{}'.\n\n", canister_info.get_name()) + &details;
                    ask_for_consent(&msg)?;
                }
                Ok(StableCompatibility::Error(details)) => {
                    let msg = format!("Stable interface compatibility check issued an ERROR for canister '{}'.\nUpgrade will either FAIL or LOSE some stable variable data.\n\n", canister_info.get_name()) + &details;
                    ask_for_consent(&msg)?;
                }
                Err(e) => {
                    let msg = format!("An error occurred during stable interface compatibility check for canister '{}'.\n\n", canister_info.get_name()) + &e.to_string();
                    ask_for_consent(&msg)?;
                }
            }
        }
    }

    let wasm_path: PathBuf = if let Some(wasm_override) = wasm_path_override {
        wasm_override.into()
    } else {
        let build_wasm_path = canister_info.get_build_wasm_path();
        if !build_wasm_path.exists() {
            bail!("The canister must be built before install. Please run `dfx build`.");
        }
        build_wasm_path
    };
    let wasm_module = dfx_core::fs::read(&wasm_path)?;
    let new_hash = Sha256::digest(&wasm_module);
    debug!(log, "New wasm module hash: {}", hex::encode(new_hash));

    if matches!(mode, InstallMode::Upgrade { .. })
        && matches!(&installed_module_hash, Some(old_hash) if old_hash[..] == new_hash[..])
        && !upgrade_unchanged
    {
        println!(
            "Module hash {} is already installed.",
            hex::encode(installed_module_hash.as_ref().unwrap())
        );
    } else if !(canister_info.is_assets() && no_asset_upgrade) {
        let idl_path = canister_info.get_constructor_idl_path();
        let init_type = if wasm_path_override.is_some() {
            None
        } else {
            get_candid_init_type(&idl_path)
        };

        // The argument and argument_type from the CLI take precedence over the dfx.json configuration.
        let argument_from_json = canister_info.get_init_arg()?;
        let (argument, argument_type) = match (argument_from_cli, &argument_from_json) {
            (Some(a_cli), Some(a_json)) => {
                // We want to warn the user when the argument from CLI and json are different.
                // There are two cases to consider:
                // 1. The argument from CLI is in raw format, while the argument from json is always in Candid format.
                // 2. Both arguments are in Candid format, but they are different.
                if argument_type_from_cli == Some("raw") || a_cli != a_json {
                    warn!(
                        log,
                        "Canister '{0}' has init_arg/init_arg_file in dfx.json: {1},
which is different from the one specified in the command line: {2}.
The command line value will be used.",
                        canister_info.get_name(),
                        a_json,
                        a_cli
                    );
                }
                (argument_from_cli, argument_type_from_cli)
            }
            (Some(_), None) => (argument_from_cli, argument_type_from_cli),
            (None, Some(a_json)) => (Some(a_json.as_str()), Some("idl")), // `init_arg` in dfx.json is always in Candid format
            (None, None) => (None, None),
        };
        let install_args = blob_from_arguments(
            Some(env),
            argument,
            None,
            argument_type,
            &init_type,
            true,
            always_assist,
        )?;
        if let Some(timestamp) = canister_id_store.get_timestamp(canister_info.get_name()) {
            let new_timestamp = playground_install_code(
                env,
                canister_id,
                timestamp,
                &install_args,
                &wasm_module,
                mode,
                canister_info.is_assets(),
            )
            .await?;
            canister_id_store.add(
                canister_info.get_name(),
                &canister_id.to_string(),
                Some(new_timestamp),
            )?;
        } else {
            install_canister_wasm(
                agent,
                canister_id,
                Some(canister_info.get_name()),
                &install_args,
                mode,
                call_sender,
                wasm_module,
                skip_consent,
            )
            .await?;
        }
    }

    wait_for_module_hash(
        env,
        agent,
        canister_id,
        installed_module_hash.as_deref(),
        &new_hash,
    )
    .await?;

    if canister_info.is_assets() {
        if let Some(canister_timeout) = canister_id_store.get_timestamp(canister_info.get_name()) {
            // playground installed the code, so playground has to authorize call_sender to upload files
            let uploader_principal = env
                .get_selected_identity_principal()
                .context("Failed to figure out selected identity's principal.")?;
            authorize_asset_uploader(
                env,
                canister_info.get_canister_id()?,
                canister_timeout,
                &uploader_principal,
            )
            .await?;
        }
        if let CallSender::Wallet(wallet_id) = call_sender {
            let wallet = build_wallet_canister(*wallet_id, agent).await?;
            let identity_name = env.get_selected_identity().expect("No selected identity.");
            info!(
                log,
                "Authorizing our identity ({}) to the asset canister...", identity_name
            );
            let self_id = env
                .get_selected_identity_principal()
                .expect("Selected identity not instantiated.");
            // Before storing assets, make sure the DFX principal is in there first.
            wallet
                .call(
                    canister_id,
                    "authorize",
                    Argument::from_candid((self_id,)),
                    0,
                )
                .await
                .context("Failed to authorize your principal with the canister. You can still control the canister by using your wallet with the --wallet flag.")?;
        };

        info!(log, "Uploading assets to asset canister...");
        post_install_store_assets(canister_info, agent, log).await?;
    }
    if !canister_info.get_post_install().is_empty() {
        let config = env.get_config()?;
        run_post_install_tasks(
            env,
            canister_info,
            network,
            pool,
            env_file.or_else(|| config.as_ref()?.get_config().output_env_file.as_deref()),
        )?;
    }

    Ok(())
}

fn check_candid_compatibility(
    canister_info: &CanisterInfo,
    candid: &str,
) -> anyhow::Result<Option<String>> {
    use candid::types::subtype::{subtype_with_config, OptReport};
    use candid_parser::utils::CandidSource;
    let candid_path = canister_info.get_constructor_idl_path();
    let deployed_path = canister_info
        .get_constructor_idl_path()
        .with_extension("old.did");
    std::fs::write(&deployed_path, candid).with_context(|| {
        format!(
            "Failed to write candid to {}.",
            deployed_path.to_string_lossy()
        )
    })?;
    let (mut env, opt_new) = CandidSource::File(&candid_path)
        .load()
        .context("Checking generated did file.")?;
    let new_type = opt_new
        .ok_or_else(|| anyhow!("Generated did file should contain some service interface"))?;
    let (env2, opt_old) = CandidSource::File(&deployed_path)
        .load()
        .context("Checking old candid file.")?;
    let old_type = opt_old
        .ok_or_else(|| anyhow!("Deployed did file should contain some service interface"))?;
    let mut gamma = HashSet::new();
    let old_type = env.merge_type(env2, old_type);
    let result = subtype_with_config(OptReport::Error, &mut gamma, &env, &new_type, &old_type);
    Ok(match result {
        Ok(_) => None,
        Err(e) => Some(e.to_string()),
    })
}

async fn wait_for_module_hash(
    env: &dyn Environment,
    agent: &Agent,
    canister_id: Principal,
    old_hash: Option<&[u8]>,
    new_hash: &[u8],
) -> DfxResult {
    let mut retry_policy = ExponentialBackoff::default();
    let mut times = 0;
    loop {
        match read_state_tree_canister_module_hash(agent, canister_id).await? {
            Some(reported_hash) => {
                if env.get_network_descriptor().is_playground() {
                    // Playground may modify wasm before installing, therefore we cannot predict what the hash is supposed to be.
                    info!(
                        env.get_logger(),
                        "Something is installed in canister {}. Assuming new code is installed.",
                        canister_id
                    );
                    break;
                }
                if reported_hash[..] == new_hash[..] {
                    break;
                } else if old_hash.map_or(true, |old_hash| old_hash == reported_hash) {
                    times += 1;
                    if times > 3 {
                        info!(
                            env.get_logger(),
                            "Waiting for module change to be reflected in system state tree..."
                        )
                    }
                    let interval = retry_policy.next_backoff()
                            .context("Timed out waiting for the module to update to the new hash in the state tree. \
                                Something may have gone wrong with the upload. \
                                No post-installation tasks have been run, including asset uploads.")?;
                    tokio::time::sleep(interval).await;
                } else {
                    bail!("The reported module hash ({reported}) is neither the existing module ({old}) or the new one ({new}). \
                            It has likely been modified while this command is running. \
                            The state of the canister is unknown. \
                            For this reason, no post-installation tasks have been run, including asset uploads.",
                            old = old_hash.map_or_else(|| "none".to_string(), hex::encode),
                            new = hex::encode(new_hash),
                            reported = hex::encode(reported_hash),
                        )
                }
            }
            None => {
                times += 1;
                if times > 3 {
                    info!(
                        env.get_logger(),
                        "Waiting for module change to be reflected in system state tree..."
                    )
                }
                let interval = retry_policy.next_backoff()
                        .context("Timed out waiting for the module to update to the new hash in the state tree. \
                            Something may have gone wrong with the upload. \
                            No post-installation tasks have been run, including asset uploads.")?;
                tokio::time::sleep(interval).await;
            }
        }
    }
    Ok(())
}

enum StableCompatibility {
    Okay,
    Warning(String),
    Error(String),
}

fn check_stable_compatibility(
    canister_info: &CanisterInfo,
    env: &dyn Environment,
    stable_types: &str,
) -> anyhow::Result<StableCompatibility> {
    use crate::lib::canister_info::motoko::MotokoCanisterInfo;
    let info = canister_info.as_info::<MotokoCanisterInfo>()?;
    let stable_path = info.get_output_stable_path();
    let deployed_stable_path = stable_path.with_extension("old.most");
    std::fs::write(&deployed_stable_path, stable_types).with_context(|| {
        format!(
            "Failed to write stable types to {}.",
            deployed_stable_path.to_string_lossy()
        )
    })?;
    let cache = env.get_cache();
    let output = cache
        .get_binary_command("moc")?
        .arg("--stable-compatible")
        .arg(&deployed_stable_path)
        .arg(stable_path)
        .current_dir(canister_info.get_workspace_root())
        .output()
        .context("Failed to run 'moc'.")?;
    let message = String::from_utf8_lossy(&output.stderr).to_string();
    Ok(if !output.status.success() {
        StableCompatibility::Error(message)
    } else if !output.stderr.is_empty() {
        StableCompatibility::Warning(message)
    } else {
        StableCompatibility::Okay
    })
}

#[context("Failed to run post-install tasks")]
fn run_post_install_tasks(
    env: &dyn Environment,
    canister: &CanisterInfo,
    network: &NetworkDescriptor,
    pool: Option<&CanisterPool>,
    env_file: Option<&Path>,
) -> DfxResult {
    let tmp;
    let pool = match pool {
        Some(pool) => pool,
        None => {
            let config = env.get_config_or_anyhow()?;
            let canisters_to_load = all_project_canisters_with_ids(env, &config);

            tmp = CanisterPool::load(env, false, &canisters_to_load)
                .context("Error collecting canisters for post-install task")?;
            &tmp
        }
    };
    let dependencies = pool
        .get_canister_list()
        .iter()
        .map(|can| can.canister_id())
        .collect_vec();
    for task in canister.get_post_install() {
        run_post_install_task(canister, task, network, pool, &dependencies, env_file)?;
    }
    Ok(())
}

#[context("Failed to run post-install task {task}")]
fn run_post_install_task(
    canister: &CanisterInfo,
    task: &str,
    network: &NetworkDescriptor,
    pool: &CanisterPool,
    dependencies: &[Principal],
    env_file: Option<&Path>,
) -> DfxResult {
    let cwd = canister.get_workspace_root();
    let words = shell_words::split(task)
        .with_context(|| format!("Error interpreting post-install task `{task}`"))?;
    let canonicalized = dfx_core::fs::canonicalize(&cwd.join(&words[0]))
        .or_else(|_| which::which(&words[0]))
        .map_err(|_| anyhow!("Cannot find command or file {}", &words[0]))?;
    let mut command = Command::new(canonicalized);
    command.args(&words[1..]);
    let vars =
        get_and_write_environment_variables(canister, &network.name, pool, dependencies, env_file)?;
    for (key, val) in vars {
        command.env(&*key, val);
    }
    command
        .current_dir(cwd)
        .stdout(Stdio::inherit())
        .stderr(Stdio::inherit());
    let status = command.status()?;
    if !status.success() {
        match status.code() {
            Some(code) => {
                bail!("The post-install task `{task}` failed with exit code {code}")
            }
            None => bail!("The post-install task `{task}` was terminated by a signal"),
        }
    }
    Ok(())
}

pub async fn install_wallet(
    env: &dyn Environment,
    agent: &Agent,
    id: Principal,
    mode: InstallMode,
) -> DfxResult {
    if env.get_network_descriptor().is_playground() {
        bail!("Refusing to install wallet. Wallets do not work for playground networks.");
    }
    let mgmt = ManagementCanister::create(agent);
    let wasm = wallet_wasm(env.get_logger())?;
    mgmt.install_code(&id, &wasm)
        .with_mode(mode)
        .await
        .context("Failed to install wallet wasm.")?;
    wait_for_module_hash(env, agent, id, None, &Sha256::digest(&wasm)).await?;
    let wallet = build_wallet_canister(id, agent).await?;
    wallet
        .wallet_store_wallet_wasm(wasm)
        .await
        .context("Failed to store wallet wasm in container.")?;
    Ok(())
}


-----------------------

/src/dfx/src/lib/operations/canister/mod.rs:
-----------------------

pub(crate) mod create_canister;
pub(crate) mod deploy_canisters;
pub(crate) mod install_canister;
pub mod motoko_playground;

pub use create_canister::create_canister;
use ic_utils::interfaces::management_canister::Snapshot;
pub use install_canister::install_wallet;

use crate::lib::canister_info::CanisterInfo;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::ic_attributes::CanisterSettings as DfxCanisterSettings;
use anyhow::{bail, Context};
use candid::utils::ArgumentDecoder;
use candid::CandidType;
use candid::Principal as CanisterId;
use candid::Principal;
use dfx_core::canister::build_wallet_canister;
use dfx_core::config::model::dfinity::Config;
use dfx_core::identity::CallSender;
use fn_error_context::context;
use ic_utils::call::SyncCall;
use ic_utils::interfaces::management_canister::builders::CanisterSettings;
use ic_utils::interfaces::management_canister::{
    FetchCanisterLogsResponse, MgmtMethod, StatusCallResult,
};
use ic_utils::interfaces::ManagementCanister;
use ic_utils::Argument;
use std::collections::HashSet;
use std::path::PathBuf;

#[context(
    "Failed to call update function '{}' regarding canister '{}'.",
    method,
    destination_canister
)]
async fn do_management_call<A, O>(
    env: &dyn Environment,
    destination_canister: Principal,
    method: &str,
    arg: A,
    call_sender: &CallSender,
    cycles: u128,
) -> DfxResult<O>
where
    A: CandidType + Sync + Send,
    O: for<'de> ArgumentDecoder<'de> + Sync + Send,
{
    let agent = env.get_agent();
    let out = match call_sender {
        CallSender::SelectedId => {
            let mgr = ManagementCanister::create(agent);

            mgr.update(method)
                .with_arg(arg)
                .with_effective_canister_id(destination_canister)
                .build()
                .await
                .context("Update call (without wallet) failed.")?
        }
        CallSender::Wallet(wallet_id) => {
            let wallet = build_wallet_canister(*wallet_id, agent).await?;
            let out: O = wallet
                .call(
                    Principal::management_canister(),
                    method,
                    Argument::from_candid((arg,)),
                    cycles,
                )
                .await
                .context("Update call using wallet failed.")?;
            out
        }
    };

    Ok(out)
}

#[context(
    "Failed to query call function '{}' regarding canister '{}'.",
    method,
    destination_canister
)]
async fn do_management_query_call<A, O>(
    env: &dyn Environment,
    destination_canister: Principal,
    method: &str,
    arg: A,
    call_sender: &CallSender,
) -> DfxResult<O>
where
    A: CandidType + Sync + Send,
    O: for<'de> ArgumentDecoder<'de> + Sync + Send,
{
    let agent = env.get_agent();
    let out = match call_sender {
        CallSender::SelectedId => {
            let mgr = ManagementCanister::create(agent);

            mgr.query(method)
                .with_arg(arg)
                .with_effective_canister_id(destination_canister)
                .build()
                .call()
                .await
                .context("Query call (without wallet) failed.")?
        }
        CallSender::Wallet(wallet_id) => {
            let wallet = build_wallet_canister(*wallet_id, agent).await?;
            let out: O = wallet
                .query(method)
                .with_arg(arg)
                .with_effective_canister_id(Principal::management_canister())
                .build()
                .call()
                .await
                .context("Query call using wallet failed.")?;
            out
        }
    };

    Ok(out)
}

#[context("Failed to get canister status of {}.", canister_id)]
pub async fn get_canister_status(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
) -> DfxResult<StatusCallResult> {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }

    let (out,): (StatusCallResult,) = do_management_call(
        env,
        canister_id,
        MgmtMethod::CanisterStatus.as_ref(),
        In { canister_id },
        call_sender,
        0,
    )
    .await?;
    Ok(out)
}

#[context("Failed to get canister logs of {}.", canister_id)]
pub async fn get_canister_logs(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
) -> DfxResult<FetchCanisterLogsResponse> {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }

    let (out,): (FetchCanisterLogsResponse,) = do_management_query_call(
        env,
        canister_id,
        MgmtMethod::FetchCanisterLogs.as_ref(),
        In { canister_id },
        call_sender,
    )
    .await?;
    Ok(out)
}

#[context("Failed to start canister {}.", canister_id)]
pub async fn start_canister(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
) -> DfxResult {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }

    do_management_call(
        env,
        canister_id,
        MgmtMethod::StartCanister.as_ref(),
        In { canister_id },
        call_sender,
        0,
    )
    .await?;
    Ok(())
}

#[context("Failed to stop canister {}.", canister_id)]
pub async fn stop_canister(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
) -> DfxResult {
    if env.get_network_descriptor().is_playground() {
        bail!("Canisters borrowed from a playground cannot be stopped.");
    }

    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }

    do_management_call(
        env,
        canister_id,
        MgmtMethod::StopCanister.as_ref(),
        In { canister_id },
        call_sender,
        0,
    )
    .await?;
    Ok(())
}

#[context("Failed to update settings for {}.", canister_id)]
pub async fn update_settings(
    env: &dyn Environment,
    canister_id: Principal,
    settings: DfxCanisterSettings,
    call_sender: &CallSender,
) -> DfxResult {
    #[derive(candid::CandidType)]
    struct In {
        canister_id: Principal,
        settings: CanisterSettings,
    }
    do_management_call(
        env,
        canister_id,
        MgmtMethod::UpdateSettings.as_ref(),
        In {
            canister_id,
            settings: settings.into(),
        },
        call_sender,
        0,
    )
    .await?;
    Ok(())
}

#[context("Failed to uninstall code for {}.", canister_id)]
pub async fn uninstall_code(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
) -> DfxResult {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }
    do_management_call(
        env,
        canister_id,
        MgmtMethod::UninstallCode.as_ref(),
        In { canister_id },
        call_sender,
        0,
    )
    .await?;

    Ok(())
}

#[context("Failed to delete {}.", canister_id)]
pub async fn delete_canister(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
) -> DfxResult {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }
    do_management_call(
        env,
        canister_id,
        MgmtMethod::DeleteCanister.as_ref(),
        In { canister_id },
        call_sender,
        0,
    )
    .await?;

    Ok(())
}

#[context("Failed to deposit {} cycles into {}.", cycles, canister_id)]
pub async fn deposit_cycles(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
    cycles: u128,
) -> DfxResult {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }
    do_management_call(
        env,
        canister_id,
        MgmtMethod::DepositCycles.as_ref(),
        In { canister_id },
        call_sender,
        cycles,
    )
    .await?;

    Ok(())
}

/// Can only run this locally, not on the real IC.
/// Conjures cycles from nothing and deposits them in the selected canister.
#[context(
    "Failed provisional deposit of {} cycles to canister {}.",
    cycles,
    canister_id
)]
pub async fn provisional_deposit_cycles(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
    cycles: u128,
) -> DfxResult {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
        amount: u128,
    }
    do_management_call(
        env,
        canister_id,
        MgmtMethod::ProvisionalTopUpCanister.as_ref(),
        In {
            canister_id,
            amount: cycles,
        },
        call_sender,
        0,
    )
    .await?;

    Ok(())
}

/// Get the canister id and the path to the candid file for the given canister.
/// The argument `canister` can be either a canister id or a canister name.
pub fn get_canister_id_and_candid_path(
    env: &dyn Environment,
    canister: &str,
) -> DfxResult<(CanisterId, Option<PathBuf>)> {
    let canister_id_store = env.get_canister_id_store()?;
    let (canister_name, canister_id) = if let Ok(id) = Principal::from_text(canister) {
        if let Some(canister_name) = canister_id_store.get_name(canister) {
            (canister_name.to_string(), id)
        } else {
            return Ok((id, None));
        }
    } else {
        (canister.to_string(), canister_id_store.get(canister)?)
    };
    let config = env.get_config_or_anyhow()?;
    let candid_path = match CanisterInfo::load(&config, &canister_name, Some(canister_id)) {
        Ok(info) => Some(info.get_output_idl_path().to_path_buf()),
        // In a rare case that the canister was deployed and then removed from dfx.json,
        // the canister_id_store can still resolve the canister id from the canister name.
        // In such case, technically, we are still able to call the canister.
        Err(_) => None,
    };
    Ok((canister_id, candid_path))
}

pub fn add_canisters_with_ids(
    canister_names: &[String],
    env: &dyn Environment,
    config: &Config,
) -> Vec<String> {
    let mut canister_names: HashSet<_> = canister_names.iter().cloned().collect();

    canister_names.extend(all_project_canisters_with_ids(env, config));

    canister_names.into_iter().collect()
}

pub fn all_project_canisters_with_ids(env: &dyn Environment, config: &Config) -> Vec<String> {
    env.get_canister_id_store()
        .map(|store| {
            config
                .get_config()
                .canisters
                .as_ref()
                .map(|canisters| {
                    canisters
                        .keys()
                        .filter(|canister| store.get(canister).is_ok())
                        .cloned()
                        .collect::<Vec<_>>()
                })
                .unwrap_or_default()
        })
        .unwrap_or_default()
}

#[context("Failed to take snapshot in canister {canister_id}")]
pub async fn take_canister_snapshot(
    env: &dyn Environment,
    canister_id: Principal,
    replace_snapshot: Option<&[u8]>,
    call_sender: &CallSender,
) -> DfxResult<Snapshot> {
    #[derive(CandidType)]
    struct In<'a> {
        canister_id: Principal,
        replace_snapshot: Option<&'a [u8]>,
    }
    let (snapshot,) = do_management_call(
        env,
        canister_id,
        MgmtMethod::TakeCanisterSnapshot.as_ref(),
        &In {
            canister_id,
            replace_snapshot,
        },
        call_sender,
        0,
    )
    .await?;
    Ok(snapshot)
}

#[context(
    "Failed to load snapshot {} in canister {canister_id}",
    hex::encode(snapshot_id)
)]
pub async fn load_canister_snapshot(
    env: &dyn Environment,
    canister_id: Principal,
    snapshot_id: &[u8],
    call_sender: &CallSender,
) -> DfxResult {
    #[derive(CandidType)]
    struct In<'a> {
        canister_id: Principal,
        snapshot_id: &'a [u8],
        sender_canister_version: Option<u64>,
    }
    do_management_call(
        env,
        canister_id,
        MgmtMethod::LoadCanisterSnapshot.as_ref(),
        &In {
            canister_id,
            snapshot_id,
            sender_canister_version: None,
        },
        call_sender,
        0,
    )
    .await?;
    Ok(())
}

#[context("Failed to list snapshots in canister {canister_id}")]
pub async fn list_canister_snapshots(
    env: &dyn Environment,
    canister_id: Principal,
    call_sender: &CallSender,
) -> DfxResult<Vec<Snapshot>> {
    #[derive(CandidType)]
    struct In {
        canister_id: Principal,
    }
    let (snapshots,) = do_management_call(
        env,
        canister_id,
        MgmtMethod::ListCanisterSnapshots.as_ref(),
        &In { canister_id },
        call_sender,
        0,
    )
    .await?;
    Ok(snapshots)
}

#[context(
    "Failed to delete canister snapshot {} in canister {canister_id}",
    hex::encode(snapshot_id)
)]
pub async fn delete_canister_snapshot(
    env: &dyn Environment,
    canister_id: Principal,
    snapshot_id: &[u8],
    call_sender: &CallSender,
) -> DfxResult {
    #[derive(CandidType)]
    struct In<'a> {
        canister_id: Principal,
        snapshot_id: &'a [u8],
    }
    do_management_call(
        env,
        canister_id,
        MgmtMethod::DeleteCanisterSnapshot.as_ref(),
        &In {
            canister_id,
            snapshot_id,
        },
        call_sender,
        0,
    )
    .await?;
    Ok(())
}


-----------------------

/src/dfx/src/lib/operations/canister/motoko_playground.rs:
-----------------------

use crate::lib::{environment::Environment, error::DfxResult};
use anyhow::{bail, Context};
use candid::{encode_args, CandidType, Decode, Deserialize, Encode, Principal};
use dfx_core::config::model::canister_id_store::AcquisitionDateTime;
use dfx_core::config::model::network_descriptor::{
    NetworkTypeDescriptor, MAINNET_MOTOKO_PLAYGROUND_CANISTER_ID,
};
use fn_error_context::context;
use ic_utils::interfaces::management_canister::builders::{CanisterUpgradeOptions, InstallMode};
use num_traits::ToPrimitive;
use rand::Rng;
use slog::{debug, info};
use std::time::SystemTime;

/// Arguments for the `getCanisterId` call.
#[derive(CandidType)]
pub struct GetCanisterIdArgs {
    pub timestamp: candid::Int,
    pub nonce: candid::Nat,
}

/// Used to uniquely identify a canister with the playground
#[derive(CandidType, Deserialize, Debug)]
pub struct CanisterInfo {
    pub id: Principal,
    pub timestamp: candid::Int,
}

impl CanisterInfo {
    pub fn from(id: Principal, timestamp: &AcquisitionDateTime) -> Self {
        let timestamp = candid::Int::from(timestamp.unix_timestamp_nanos());
        Self { id, timestamp }
    }

    #[context("Failed to get timestamp from CanisterInfo")]
    pub fn get_timestamp(&self) -> DfxResult<AcquisitionDateTime> {
        AcquisitionDateTime::from_unix_timestamp_nanos(
            self.timestamp.0.to_i128().context("i128 overflow")?,
        )
        .context("Failed to make unix timestamp from nanos")
    }
}

#[derive(CandidType, Deserialize, Debug)]
pub struct InstallArgs<'a> {
    pub arg: &'a [u8],
    pub wasm_module: &'a [u8],
    pub mode: PlaygroundInstallMode,
    pub canister_id: Principal,
}

#[derive(CandidType, Deserialize, Debug)]
pub enum PlaygroundInstallMode {
    #[serde(rename = "install")]
    Install,
    #[serde(rename = "upgrade")]
    Upgrade,
    #[serde(rename = "reinstall")]
    Reinstall,
}

impl TryFrom<InstallMode> for PlaygroundInstallMode {
    type Error = anyhow::Error;
    fn try_from(m: InstallMode) -> DfxResult<Self> {
        match m {
            InstallMode::Install => Ok(Self::Install),
            InstallMode::Reinstall => Ok(Self::Reinstall),
            InstallMode::Upgrade(
                Some(CanisterUpgradeOptions {
                    skip_pre_upgrade: None | Some(false),
                    ..
                })
                | None,
            ) => Ok(Self::Upgrade),
            InstallMode::Upgrade(Some(CanisterUpgradeOptions {
                skip_pre_upgrade: Some(true),
                ..
            })) => bail!("Cannot skip pre-upgrade on the playground"),
        }
    }
}

#[derive(CandidType)]
struct InstallConfig<'a> {
    profiling: bool,
    is_whitelisted: bool,
    origin: Origin<'a>,
}
#[derive(CandidType)]
struct Origin<'a> {
    origin: &'a str,
    tags: &'a [&'a str],
}
impl<'a> Origin<'a> {
    fn new() -> Self {
        Self {
            origin: "dfx",
            tags: &[],
        }
    }
}

#[context("Failed to reserve canister '{}'.", canister_name)]
pub async fn reserve_canister_with_playground(
    env: &dyn Environment,
    canister_name: &str,
) -> DfxResult {
    let agent = env.get_agent();
    let log = env.get_logger();
    let playground_canister = if let NetworkTypeDescriptor::Playground {
        playground_canister,
        ..
    } = env.get_network_descriptor().r#type
    {
        debug!(log, "playground canister is {}", playground_canister);
        playground_canister
    } else {
        bail!("Trying to reserve canister with playground on non-playground network.")
    };
    if ci_info::is_ci() && playground_canister == MAINNET_MOTOKO_PLAYGROUND_CANISTER_ID {
        bail!("Cannot reserve playground canister in CI, please run `dfx start` to use the local replica.")
    }

    let mut canister_id_store = env.get_canister_id_store()?;
    let (timestamp, nonce) = create_nonce();
    let get_can_arg = Encode!(&GetCanisterIdArgs { timestamp, nonce }, &Origin::new())?;
    let result = agent
        .update(&playground_canister, "getCanisterId")
        .with_arg(get_can_arg)
        .await
        .context("Failed to reserve canister at the playground.")?;
    let reserved_canister = Decode!(&result, CanisterInfo)?;
    canister_id_store.add(
        canister_name,
        &reserved_canister.id.to_string(),
        Some(reserved_canister.get_timestamp()?),
    )?;

    info!(
        env.get_logger(),
        "Reserved canister '{}' with id {} with the playground.",
        canister_name,
        reserved_canister.id
    );

    Ok(())
}

#[context("Failed to authorize asset uploader through playground.")]
pub async fn authorize_asset_uploader(
    env: &dyn Environment,
    canister_id: Principal,
    canister_timestamp: &AcquisitionDateTime,
    principal_to_authorize: &Principal,
) -> DfxResult {
    let agent = env.get_agent();
    let playground_canister = if let NetworkTypeDescriptor::Playground {
        playground_canister,
        ..
    } = env.get_network_descriptor().r#type
    {
        playground_canister
    } else {
        bail!("Trying to authorize asset uploader on non-playground network.")
    };
    let canister_info = CanisterInfo::from(canister_id, canister_timestamp);

    let nested_arg = Encode!(&principal_to_authorize)?;
    let call_arg = Encode!(&canister_info, &"authorize", &nested_arg)?;

    let _ = agent
        .update(&playground_canister, "callForward")
        .with_arg(call_arg)
        .await
        .context("Failed to call playground.")?;
    Ok(())
}

pub async fn playground_install_code(
    env: &dyn Environment,
    canister_id: Principal,
    canister_timestamp: &AcquisitionDateTime,
    arg: &[u8],
    wasm_module: &[u8],
    mode: InstallMode,
    is_asset_canister: bool,
) -> DfxResult<AcquisitionDateTime> {
    let canister_info = CanisterInfo::from(canister_id, canister_timestamp);
    let agent = env.get_agent();
    let playground_canister = match env.get_network_descriptor().r#type {
        NetworkTypeDescriptor::Playground {
            playground_canister,
            ..
        } => playground_canister,
        _ => bail!("Trying to install wasm through playground on non-playground network."),
    };
    let install_arg = InstallArgs {
        arg,
        wasm_module,
        mode: mode.try_into()?,
        canister_id: canister_info.id,
    };
    let install_config = InstallConfig {
        profiling: false,
        is_whitelisted: is_asset_canister,
        origin: Origin::new(),
    };
    let encoded_arg = encode_args((canister_info, install_arg, install_config))?;
    let result = agent
        .update(&playground_canister, "installCode")
        .with_arg(encoded_arg.as_slice())
        .await
        .context("install failed")?;
    let out = Decode!(&result, CanisterInfo)?;
    out.get_timestamp()
}

fn create_nonce() -> (candid::Int, candid::Nat) {
    let now = SystemTime::now()
        .duration_since(SystemTime::UNIX_EPOCH)
        .unwrap()
        .as_nanos();
    let timestamp = candid::Int::from(now);
    let mut rng = rand::thread_rng();
    let mut nonce = candid::Nat::from(rng.gen::<u32>());
    let prefix = format!("{}{}", POW_DOMAIN, timestamp);
    loop {
        let to_hash = format!("{}{}", prefix, nonce).replace('_', "");
        let hash = motoko_hash(&to_hash);
        if (hash & 0xc0000000) == 0 {
            return (timestamp, nonce);
        }
        nonce += 1_u8;
    }
}

const POW_DOMAIN: &str = "motoko-playground";

// djb2 hash function, from http://www.cse.yorku.ca/~oz/hash.html
fn motoko_hash(s: &str) -> i64 {
    fn to_utf16_code_point(c: char) -> u16 {
        let mut b = [0; 2];
        let result = c.encode_utf16(&mut b);

        result[0]
    }

    let mut hash = 5381_u32;
    for c in s.chars() {
        let c_val = to_utf16_code_point(c);
        hash = hash
            .overflowing_mul(33)
            .0
            .overflowing_add(u32::from(c_val))
            .0;
    }
    hash.into()
}


-----------------------

/src/dfx/src/lib/operations/cmc.rs:
-----------------------

use crate::lib::error::{
    DfxResult, NotifyCreateCanisterError, NotifyMintCyclesError, NotifyTopUpError,
};
use crate::lib::ledger_types::{
    BlockHeight, BlockIndex, Memo, NotifyCreateCanisterArg, NotifyCreateCanisterResult,
    NotifyMintCyclesArg, NotifyMintCyclesResult, NotifyMintCyclesSuccess, NotifyTopUpArg,
    NotifyTopUpResult, MAINNET_CYCLE_MINTER_CANISTER_ID, MAINNET_LEDGER_CANISTER_ID,
};
use crate::lib::nns_types::account_identifier::{AccountIdentifier, Subaccount};
use crate::lib::nns_types::icpts::ICPTs;
use crate::lib::operations::ledger::transfer;
use crate::util::clap::subnet_selection_opt::SubnetSelectionType;
use candid::{Decode, Encode, Principal};
use ic_agent::Agent;
use ic_utils::interfaces::management_canister::builders::CanisterSettings;
use icrc_ledger_types::icrc1::account::Subaccount as ICRCSubaccount;
use icrc_ledger_types::icrc1::transfer::Memo as ICRCMemo;
use slog::Logger;

const NOTIFY_CREATE_CANISTER_METHOD: &str = "notify_create_canister";
const NOTIFY_TOP_UP_METHOD: &str = "notify_top_up";
const NOTIFY_MINT_CYCLES_METHOD: &str = "notify_mint_cycles";

pub async fn transfer_cmc(
    agent: &Agent,
    logger: &Logger,
    memo: Memo,
    amount: ICPTs,
    fee: ICPTs,
    from_subaccount: Option<Subaccount>,
    to_principal: Principal,
    created_at_time: Option<u64>,
) -> DfxResult<BlockHeight> {
    let to_subaccount = Subaccount::from(&to_principal);
    let to =
        AccountIdentifier::new(MAINNET_CYCLE_MINTER_CANISTER_ID, Some(to_subaccount)).to_address();
    transfer(
        agent,
        logger,
        &MAINNET_LEDGER_CANISTER_ID,
        memo,
        amount,
        fee,
        from_subaccount,
        to,
        created_at_time,
    )
    .await
}

pub async fn notify_create(
    agent: &Agent,
    controller: Principal,
    block_height: BlockHeight,
    subnet_selection: SubnetSelectionType,
) -> Result<Principal, NotifyCreateCanisterError> {
    let user_subnet_selection = subnet_selection.get_user_choice();
    let caller = agent
        .get_principal()
        .map_err(NotifyCreateCanisterError::GetCallerPrincipal)?;
    let result = agent
        .update(
            &MAINNET_CYCLE_MINTER_CANISTER_ID,
            NOTIFY_CREATE_CANISTER_METHOD,
        )
        .with_arg(
            Encode!(&NotifyCreateCanisterArg {
                block_index: block_height,
                controller: caller, // Must be the caller since https://forum.dfinity.org/t/nns-update-2024-05-15-cycles-minting-canister-hotfix-proposal-129728/30807
                subnet_selection: user_subnet_selection,
                settings: Some(CanisterSettings {
                    controllers: Some(vec![controller]),
                    compute_allocation: None,
                    memory_allocation: None,
                    freezing_threshold: None,
                    reserved_cycles_limit: None,
                    wasm_memory_limit: None,
                    log_visibility: None,
                })
            })
            .map_err(NotifyCreateCanisterError::EncodeArguments)?,
        )
        .await
        .map_err(NotifyCreateCanisterError::Call)?;
    Decode!(&result, NotifyCreateCanisterResult)
        .map_err(NotifyCreateCanisterError::DecodeResponse)?
        .map_err(NotifyCreateCanisterError::Notify)
}

pub async fn notify_top_up(
    agent: &Agent,
    canister: Principal,
    block_height: BlockHeight,
) -> Result<u128, NotifyTopUpError> {
    let result = agent
        .update(&MAINNET_CYCLE_MINTER_CANISTER_ID, NOTIFY_TOP_UP_METHOD)
        .with_arg(
            Encode!(&NotifyTopUpArg {
                block_index: block_height,
                canister_id: canister,
            })
            .map_err(NotifyTopUpError::EncodeArguments)?,
        )
        .await
        .map_err(NotifyTopUpError::Call)?;
    Decode!(&result, NotifyTopUpResult)
        .map_err(NotifyTopUpError::DecodeResponse)?
        .map_err(NotifyTopUpError::Notify)
}

pub async fn notify_mint_cycles(
    agent: &Agent,
    deposit_memo: Option<ICRCMemo>,
    to_subaccount: Option<ICRCSubaccount>,
    block_index: BlockIndex,
) -> Result<NotifyMintCyclesSuccess, NotifyMintCyclesError> {
    let result = agent
        .update(&MAINNET_CYCLE_MINTER_CANISTER_ID, NOTIFY_MINT_CYCLES_METHOD)
        .with_arg(
            Encode!(&NotifyMintCyclesArg {
                block_index,
                to_subaccount,
                deposit_memo: deposit_memo.map(|memo| memo.0.as_ref().into()),
            })
            .map_err(NotifyMintCyclesError::EncodeArguments)?,
        )
        .await
        .map_err(NotifyMintCyclesError::Call)?;
    Decode!(&result, NotifyMintCyclesResult)
        .map_err(NotifyMintCyclesError::DecodeResponse)?
        .map_err(NotifyMintCyclesError::Notify)
}


-----------------------

/src/dfx/src/lib/operations/cycles_ledger.rs:
-----------------------

use std::time::{SystemTime, UNIX_EPOCH};

use crate::lib::cycles_ledger_types;
use crate::lib::cycles_ledger_types::create_canister::{
    CmcCreateCanisterArgs, CreateCanisterArgs, CreateCanisterError, CreateCanisterSuccess,
};
use crate::lib::cycles_ledger_types::deposit::DepositArg;
use crate::lib::cycles_ledger_types::withdraw::WithdrawError;
use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::ic_attributes::CanisterSettings as DfxCanisterSettings;
use crate::lib::operations::canister::create_canister::{
    CANISTER_CREATE_FEE, CANISTER_INITIAL_CYCLE_BALANCE,
};
use crate::lib::retryable::retryable;
use crate::util::clap::subnet_selection_opt::SubnetSelectionType;
use anyhow::{anyhow, bail, Context};
use backoff::future::retry;
use backoff::ExponentialBackoff;
use candid::{Decode, Encode, Nat, Principal};
use dfx_core::canister::build_wallet_canister;
use fn_error_context::context;
use ic_agent::Agent;
use ic_utils::call::SyncCall;
use ic_utils::{Argument, Canister};
use icrc_ledger_types::icrc1;
use icrc_ledger_types::icrc1::account::{Account, Subaccount};
use icrc_ledger_types::icrc1::transfer::{BlockIndex, TransferError};
use icrc_ledger_types::icrc2;
use icrc_ledger_types::icrc2::approve::ApproveError;
use icrc_ledger_types::icrc2::transfer_from::TransferFromError;
use slog::{info, Logger};

const ICRC1_BALANCE_OF_METHOD: &str = "icrc1_balance_of";
const ICRC1_TRANSFER_METHOD: &str = "icrc1_transfer";
const ICRC2_APPROVE_METHOD: &str = "icrc2_approve";
const ICRC2_TRANSFER_FROM_METHOD: &str = "icrc2_transfer_from";
const WITHDRAW_METHOD: &str = "withdraw";
const CREATE_CANISTER_METHOD: &str = "create_canister";
const CYCLES_LEDGER_DEPOSIT_METHOD: &str = "deposit";
const CYCLES_LEDGER_CANISTER_ID: Principal =
    Principal::from_slice(&[0x00, 0x00, 0x00, 0x00, 0x02, 0x10, 0x00, 0x02, 0x01, 0x01]);

pub async fn balance(
    agent: &Agent,
    owner: Principal,
    subaccount: Option<icrc1::account::Subaccount>,
) -> DfxResult<u128> {
    let canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(CYCLES_LEDGER_CANISTER_ID)
        .build()?;
    let arg = icrc1::account::Account { owner, subaccount };

    let retry_policy = ExponentialBackoff::default();

    retry(retry_policy, || async {
        let result = canister
            .query(ICRC1_BALANCE_OF_METHOD)
            .with_arg(arg)
            .build()
            .call()
            .await;
        match result {
            Ok((balance,)) => Ok(balance),
            Err(agent_err) if retryable(&agent_err) => {
                Err(backoff::Error::transient(anyhow!(agent_err)))
            }
            Err(agent_err) => Err(backoff::Error::permanent(anyhow!(agent_err))),
        }
    })
    .await
}

pub async fn transfer(
    agent: &Agent,
    logger: &Logger,
    amount: u128,
    from_subaccount: Option<icrc1::account::Subaccount>,
    owner: Principal,
    to_subaccount: Option<icrc1::account::Subaccount>,
    created_at_time: u64,
    memo: Option<u64>,
) -> DfxResult<BlockIndex> {
    let canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(CYCLES_LEDGER_CANISTER_ID)
        .build()?;

    let retry_policy = ExponentialBackoff::default();

    let block_index = retry(retry_policy, || async {
        let arg = icrc1::transfer::TransferArg {
            from_subaccount,
            to: icrc1::account::Account {
                owner,
                subaccount: to_subaccount,
            },
            fee: None,
            created_at_time: Some(created_at_time),
            memo: memo.map(|v| v.into()),
            amount: Nat::from(amount),
        };
        match canister
            .update(ICRC1_TRANSFER_METHOD)
            .with_arg(arg)
            .build()
            .map(|result: (Result<BlockIndex, TransferError>,)| (result.0,))
            .await
            .map(|(result,)| result)
        {
            Ok(Ok(block_index)) => Ok(block_index),
            Ok(Err(TransferError::Duplicate { duplicate_of })) => {
                info!(
                    logger,
                    "{}",
                    TransferError::Duplicate {
                        duplicate_of: duplicate_of.clone()
                    }
                );
                Ok(duplicate_of)
            }
            Ok(Err(transfer_err)) => Err(backoff::Error::permanent(anyhow!(transfer_err))),
            Err(agent_err) if retryable(&agent_err) => {
                Err(backoff::Error::transient(anyhow!(agent_err)))
            }
            Err(agent_err) => Err(backoff::Error::permanent(anyhow!(agent_err))),
        }
    })
    .await?;

    Ok(block_index)
}

pub async fn transfer_from(
    agent: &Agent,
    logger: &Logger,
    spender_subaccount: Option<icrc1::account::Subaccount>,
    from: icrc1::account::Account,
    to: icrc1::account::Account,
    amount: u128,
    memo: Option<u64>,
    created_at_time: u64,
) -> DfxResult<BlockIndex> {
    let canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(CYCLES_LEDGER_CANISTER_ID)
        .build()?;

    let retry_policy = ExponentialBackoff::default();

    let block_index = retry(retry_policy, || async {
        let arg = icrc2::transfer_from::TransferFromArgs {
            spender_subaccount,
            from,
            to,
            fee: None,
            created_at_time: Some(created_at_time),
            memo: memo.map(|v| v.into()),
            amount: Nat::from(amount),
        };
        match canister
            .update(ICRC2_TRANSFER_FROM_METHOD)
            .with_arg(arg)
            .build()
            .map(|result: (Result<BlockIndex, TransferFromError>,)| (result.0,))
            .await
            .map(|(result,)| result)
        {
            Ok(Ok(block_index)) => Ok(block_index),
            Ok(Err(TransferFromError::Duplicate { duplicate_of })) => {
                info!(
                    logger,
                    "Transfer is a duplicate of block index {}", duplicate_of
                );
                Ok(duplicate_of)
            }
            Ok(Err(transfer_from_err)) => Err(backoff::Error::permanent(anyhow!(
                display_transfer_from_err(transfer_from_err)
            ))),
            Err(agent_err) if retryable(&agent_err) => {
                Err(backoff::Error::transient(anyhow!(agent_err)))
            }
            Err(agent_err) => Err(backoff::Error::permanent(anyhow!(agent_err))),
        }
    })
    .await?;

    Ok(block_index)
}

pub async fn approve(
    agent: &Agent,
    logger: &Logger,
    amount: u128,
    spender: Principal,
    spender_subaccount: Option<icrc1::account::Subaccount>,
    from_subaccount: Option<icrc1::account::Subaccount>,
    expected_allowance: Option<u128>,
    expires_at: Option<u64>,
    created_at_time: u64,
    memo: Option<u64>,
) -> DfxResult<BlockIndex> {
    let canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(CYCLES_LEDGER_CANISTER_ID)
        .build()?;

    let retry_policy = ExponentialBackoff::default();

    let block_index = retry(retry_policy, || async {
        let arg = icrc2::approve::ApproveArgs {
            from_subaccount,
            fee: None,
            created_at_time: Some(created_at_time),
            memo: memo.map(|v| v.into()),
            amount: Nat::from(amount),
            spender: icrc1::account::Account {
                owner: spender,
                subaccount: spender_subaccount,
            },
            expected_allowance: expected_allowance.map(Nat::from),
            expires_at,
        };
        match canister
            .update(ICRC2_APPROVE_METHOD)
            .with_arg(arg)
            .build()
            .map(|result: (Result<BlockIndex, ApproveError>,)| (result.0,))
            .await
            .map(|(result,)| result)
        {
            Ok(Ok(block_index)) => Ok(block_index),
            Ok(Err(ApproveError::Duplicate { duplicate_of })) => {
                info!(logger, "Approval is a duplicate of block {}", duplicate_of);
                Ok(duplicate_of)
            }
            Ok(Err(approve_err)) => Err(backoff::Error::permanent(anyhow!(display_approve_err(
                approve_err
            )))),
            Err(agent_err) if retryable(&agent_err) => {
                Err(backoff::Error::transient(anyhow!(agent_err)))
            }
            Err(agent_err) => Err(backoff::Error::permanent(anyhow!(agent_err))),
        }
    })
    .await?;

    Ok(block_index)
}

pub async fn withdraw(
    agent: &Agent,
    logger: &Logger,
    to: Principal,
    amount: u128,
    created_at_time: u64,
    from_subaccount: Option<icrc1::account::Subaccount>,
) -> DfxResult<BlockIndex> {
    let canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(CYCLES_LEDGER_CANISTER_ID)
        .build()?;

    let retry_policy = ExponentialBackoff::default();
    let block_index: BlockIndex = retry(retry_policy, || async {
        let arg = cycles_ledger_types::withdraw::WithdrawArgs {
            from_subaccount,
            to,
            created_at_time: Some(created_at_time),
            amount: Nat::from(amount),
        };
        match canister
            .update(WITHDRAW_METHOD)
            .with_arg(arg)
            .build()
            .map(|result: (Result<BlockIndex, WithdrawError>,)| (result.0,))
            .await
            .map(|(result,)| result)
        {
            Ok(Ok(block_index)) => Ok(block_index),
            Ok(Err(WithdrawError::Duplicate { duplicate_of })) => {
                info!(
                    logger,
                    "transaction is a duplicate of another transaction in block {}", duplicate_of
                );
                Ok(duplicate_of)
            }
            Ok(Err(WithdrawError::InvalidReceiver { receiver })) => {
                Err(backoff::Error::permanent(anyhow!(
                    "Invalid receiver: {}.  Make sure the receiver is a canister.",
                    receiver
                )))
            }
            Ok(Err(send_err)) => Err(backoff::Error::permanent(anyhow!(
                "send error: {:?}",
                send_err
            ))),
            Err(agent_err) if retryable(&agent_err) => {
                Err(backoff::Error::transient(anyhow!(agent_err)))
            }
            Err(agent_err) => Err(backoff::Error::permanent(anyhow!(agent_err))),
        }
    })
    .await?;

    Ok(block_index)
}

#[context("Failed to create canister via cycles ledger.")]
pub async fn create_with_cycles_ledger(
    env: &dyn Environment,
    agent: &Agent,
    canister_name: &str,
    with_cycles: Option<u128>,
    from_subaccount: Option<Subaccount>,
    settings: DfxCanisterSettings,
    created_at_time: Option<u64>,
    subnet_selection: &mut SubnetSelectionType,
) -> DfxResult<Principal> {
    let cycles = with_cycles.unwrap_or(CANISTER_CREATE_FEE + CANISTER_INITIAL_CYCLE_BALANCE);
    let resolved_subnet_selection = subnet_selection.resolve(env).await?;
    let created_at_time = created_at_time.or_else(|| {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64;
        info!(
            env.get_logger(),
            "created-at-time for canister {canister_name} is {now}."
        );
        Some(now)
    });

    let arg = Encode!(&CreateCanisterArgs {
        from_subaccount,
        created_at_time,
        amount: cycles,
        creation_args: Some(CmcCreateCanisterArgs {
            settings: Some(settings.into()),
            subnet_selection: resolved_subnet_selection,
        }),
    })
    .unwrap();
    let result = loop {
        match agent
            .update(&CYCLES_LEDGER_CANISTER_ID, CREATE_CANISTER_METHOD)
            .with_arg(arg.clone())
            .await
        {
            Ok(result) => break result,
            Err(err) => {
                if retryable(&err) {
                    info!(env.get_logger(), "Request error: {err:?}. Retrying...");
                } else {
                    bail!(err)
                }
            }
        }
    };
    let create_result = Decode!(
        &result,
        Result<CreateCanisterSuccess, CreateCanisterError>
    )
    .map_err(|err| {
        anyhow!(
            "Failed to decode cycles ledger response: {}",
            err.to_string()
        )
    })?;
    match create_result {
        Ok(result) => Ok(result.canister_id),
        Err(CreateCanisterError::Duplicate {
            duplicate_of,
            canister_id,
        }) => {
            if let Some(canister) = canister_id {
                info!(env.get_logger(), "Duplicate of block {duplicate_of}. Canister already created with id {canister}.");
                Ok(canister)
            } else {
                bail!("Duplicate of block {duplicate_of} but no canister id is available.");
            }
        }
        Err(err) => bail!(err),
    }
}

pub async fn wallet_deposit_to_cycles_ledger(
    agent: &Agent,
    wallet_id: Principal,
    cycles_to_withdraw: u128,
    to: Account,
) -> DfxResult {
    build_wallet_canister(wallet_id, agent)
        .await?
        .call128(
            CYCLES_LEDGER_CANISTER_ID,
            CYCLES_LEDGER_DEPOSIT_METHOD,
            Argument::from_candid((DepositArg { to, memo: None },)),
            cycles_to_withdraw,
        )
        .await
        .context("Failed deposit call.")
}

#[test]
fn ledger_canister_id_text_representation() {
    assert_eq!(
        Principal::from_text("um5iw-rqaaa-aaaaq-qaaba-cai").unwrap(),
        CYCLES_LEDGER_CANISTER_ID
    );
}

// TODO once icrc_ledger_types > 0.1.5 is released: function can be removed because ApproveError implements Display
fn display_approve_err(err: ApproveError) -> String {
    match err {
        ApproveError::BadFee { expected_fee } => {
            format!("approve fee should be {}", expected_fee)
        }
        ApproveError::InsufficientFunds { balance } => {
            format!(
                "the debit account doesn't have enough funds to complete the transaction, current balance: {}",
                balance
            )
        }
        ApproveError::AllowanceChanged { current_allowance } =>
            format!(
                "expected_allowance does not match actual allowance, current allowance is {}",
                current_allowance
            ),
        ApproveError::Expired { ledger_time } =>
            format!("the transaction expired before the ledger had a chance to apply it, current time is {}", ledger_time),
        ApproveError::TooOld {} => "transaction's created_at_time is too far in the past".to_string(),
        ApproveError::CreatedInFuture { ledger_time } => format!(
            "transaction's created_at_time is in future, current ledger time is {}",
            ledger_time
        ),
        ApproveError::Duplicate { duplicate_of } => format!(
            "transaction is a duplicate of another transaction in block {}",
            duplicate_of
        ),
        ApproveError::TemporarilyUnavailable {} => "the ledger is temporarily unavailable".to_string(),
        ApproveError::GenericError {
            error_code,
            message,
        } => format!("{} {}", error_code, message)
    }
}

// TODO once icrc_ledger_types > 0.1.5 is released: function can be removed because ApproveError implements Display
fn display_transfer_from_err(err: TransferFromError) -> String {
    match err {
        TransferFromError::BadFee { expected_fee } => {
            format!("transfer_from fee should be {}", expected_fee)
        }
        TransferFromError::BadBurn { min_burn_amount } => format!(
            "the minimum number of tokens to be burned is {}",
            min_burn_amount
        ),
        TransferFromError::InsufficientFunds { balance } =>
            format!(
                "the debit account doesn't have enough funds to complete the transaction, current balance: {}",
                balance
            ),
        TransferFromError::InsufficientAllowance { allowance } => format!("the spender account does not have sufficient allowance, current allowance is {}", allowance),
        TransferFromError::TooOld {} => "transaction's created_at_time is too far in the past".to_string(),
        TransferFromError::CreatedInFuture { ledger_time } => format!(
            "transaction's created_at_time is in future, current ledger time is {}",
            ledger_time
        ),
        TransferFromError::Duplicate { duplicate_of } => format!(
            "transaction is a duplicate of another transaction in block {}",
            duplicate_of
        ),
        TransferFromError::TemporarilyUnavailable {} => "the ledger is temporarily unavailable".to_string(),
        TransferFromError::GenericError {
            error_code,
            message,
        } => format!("{} {}", error_code, message),
    }
}


-----------------------

/src/dfx/src/lib/operations/ledger.rs:
-----------------------

use crate::lib::ledger_types::{AccountIdBlob, BlockHeight, Memo, TransferError};
use crate::lib::nns_types::account_identifier::Subaccount;
use crate::lib::{
    error::DfxResult,
    ledger_types::{
        AccountBalanceArgs, IcpXdrConversionRateCertifiedResponse, TimeStamp, TransferArgs,
        TransferResult, MAINNET_CYCLE_MINTER_CANISTER_ID, MAINNET_LEDGER_CANISTER_ID,
    },
    nns_types::{account_identifier::AccountIdentifier, icpts::ICPTs},
};
use anyhow::{bail, ensure, Context};
use backoff::backoff::Backoff;
use backoff::ExponentialBackoff;
use candid::{Decode, Encode, Principal};
use fn_error_context::context;
use ic_agent::agent::{RejectCode, RejectResponse};
use ic_agent::agent_error::HttpErrorPayload;
use ic_agent::{
    hash_tree::{HashTree, LookupResult},
    lookup_value, Agent, AgentError,
};
use ic_utils::{call::SyncCall, Canister};
use slog::{info, Logger};
use std::time::{SystemTime, UNIX_EPOCH};

const ACCOUNT_BALANCE_METHOD: &str = "account_balance_dfx";
const TRANSFER_METHOD: &str = "transfer";

pub async fn balance(
    agent: &Agent,
    acct: &AccountIdentifier,
    ledger_canister_id: Option<Principal>,
) -> DfxResult<ICPTs> {
    let canister_id = ledger_canister_id.unwrap_or(MAINNET_LEDGER_CANISTER_ID);
    let canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(canister_id)
        .build()?;
    let (result,) = canister
        .query(ACCOUNT_BALANCE_METHOD)
        .with_arg(AccountBalanceArgs {
            account: acct.to_string(),
        })
        .build()
        .call()
        .await?;
    Ok(result)
}

/// Returns XDR-permyriad (i.e. ten-thousandths-of-an-XDR) per ICP.
pub async fn xdr_permyriad_per_icp(agent: &Agent) -> DfxResult<u64> {
    let canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(MAINNET_CYCLE_MINTER_CANISTER_ID)
        .build()?;
    let (certified_rate,): (IcpXdrConversionRateCertifiedResponse,) = canister
        .query("get_icp_xdr_conversion_rate")
        .build()
        .call()
        .await?;
    // check certificate, this is a query call
    let cert = serde_cbor::from_slice(&certified_rate.certificate)?;
    agent
        .verify(&cert, MAINNET_CYCLE_MINTER_CANISTER_ID)
        .context(
            "The origin of the certificate for the XDR <> ICP exchange rate could not be verified",
        )?;
    // we can trust the certificate
    let witness = lookup_value(
        &cert,
        [
            b"canister",
            MAINNET_CYCLE_MINTER_CANISTER_ID.as_slice(),
            b"certified_data",
        ],
    )
    .context("The IC's certificate for the XDR <> ICP exchange rate could not be verified")?;
    let tree = serde_cbor::from_slice::<HashTree<Vec<u8>>>(&certified_rate.hash_tree)?;
    ensure!(
        tree.digest() == witness,
        "The CMC's certificate for the XDR <> ICP exchange rate did not match the IC's certificate"
    );
    // we can trust the hash tree
    let lookup = tree.lookup_path([b"ICP_XDR_CONVERSION_RATE"]);
    let certified_data = if let LookupResult::Found(content) = lookup {
        content
    } else {
        bail!("The CMC's certificate did not contain the XDR <> ICP exchange rate");
    };
    let encoded_data = Encode!(&certified_rate.data)?;
    ensure!(
        certified_data == encoded_data,
        "The CMC's certificate for the XDR <> ICP exchange rate did not match the provided rate"
    );
    // we can trust the exchange rate
    Ok(certified_rate.data.xdr_permyriad_per_icp)
}

#[context("Failed to transfer funds.")]
pub async fn transfer(
    agent: &Agent,
    logger: &Logger,
    canister_id: &Principal,
    memo: Memo,
    amount: ICPTs,
    fee: ICPTs,
    from_subaccount: Option<Subaccount>,
    to: AccountIdBlob,
    created_at_time: Option<u64>,
) -> DfxResult<BlockHeight> {
    let timestamp_nanos = created_at_time.unwrap_or(
        SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64,
    );

    let mut retry_policy = ExponentialBackoff::default();

    let block_height: BlockHeight = loop {
        match agent
            .update(canister_id, TRANSFER_METHOD)
            .with_arg(
                Encode!(&TransferArgs {
                    memo,
                    amount,
                    fee,
                    from_subaccount,
                    to,
                    created_at_time: Some(TimeStamp { timestamp_nanos }),
                })
                .context("Failed to encode arguments.")?,
            )
            .await
        {
            Ok(data) => {
                let result = Decode!(&data, TransferResult)
                    .context("Failed to decode transfer response.")?;
                match result {
                    Ok(block_height) => break block_height,
                    Err(TransferError::TxDuplicate { duplicate_of }) => {
                        info!(logger, "{}", TransferError::TxDuplicate { duplicate_of });
                        break duplicate_of;
                    }
                    Err(transfer_err) => bail!(transfer_err),
                }
            }
            Err(agent_err) if !retryable(&agent_err) => {
                bail!(agent_err);
            }
            Err(agent_err) => match retry_policy.next_backoff() {
                Some(duration) => {
                    eprintln!("Waiting to retry after error: {:?}", &agent_err);
                    tokio::time::sleep(duration).await;
                    println!("Sending duplicate transaction");
                }
                None => bail!(agent_err),
            },
        }
    };

    println!("Transfer sent at block height {block_height}");

    Ok(block_height)
}

fn retryable(agent_error: &AgentError) -> bool {
    match agent_error {
        AgentError::CertifiedReject(RejectResponse {
            reject_code: RejectCode::CanisterError,
            reject_message,
            ..
        }) if reject_message.contains("is out of cycles") => false,
        AgentError::HttpError(HttpErrorPayload {
            status,
            content_type: _,
            content: _,
        }) if *status == 403 => {
            // sometimes out of cycles looks like this
            // assume any 403(unauthorized) is not retryable
            false
        }
        _ => true,
    }
}


-----------------------

/src/dfx/src/lib/operations/mod.rs:
-----------------------

pub mod canister;
pub mod cmc;
pub mod cycles_ledger;
pub mod ledger;


-----------------------

/src/dfx/src/lib/package_arguments.rs:
-----------------------

use crate::lib::error::{BuildError, DfxError, DfxResult};
use anyhow::{anyhow, bail};
use dfx_core::config::cache::Cache;
use fn_error_context::context;
use std::path::Path;
use std::process::Command;

/// Package arguments for moc or mo-ide as returned by
/// a package tool like https://github.com/kritzcreek/vessel
/// or, if there is no package tool, the base library.
pub type PackageArguments = Vec<String>;

#[context("Failed to load package arguments.")]
pub fn load(
    cache: &dyn Cache,
    packtool: &Option<String>,
    project_root: &Path,
) -> DfxResult<PackageArguments> {
    if packtool.is_none() {
        let stdlib_path = cache
            .get_binary_command_path("base")?
            .into_os_string()
            .into_string()
            .map_err(|_| anyhow!("Path contains invalid Unicode data."))?;
        let base = vec![String::from("--package"), String::from("base"), stdlib_path];
        return Ok(base);
    }

    let commandline: Vec<String> = packtool
        .as_ref()
        .unwrap()
        .split_ascii_whitespace()
        .map(String::from)
        .collect();

    let mut cmd = Command::new(commandline[0].clone());
    cmd.current_dir(project_root);

    for arg in commandline.iter().skip(1) {
        cmd.arg(arg);
    }

    let output = match cmd.output() {
        Ok(output) => output,
        Err(err) => bail!(
            "Failed to invoke the package tool {:?}\n the error was: {}",
            cmd,
            err
        ),
    };

    if !output.status.success() {
        return Err(DfxError::new(BuildError::CommandError(
            format!("{:?}", cmd),
            output.status,
            String::from_utf8_lossy(&output.stdout).to_string(),
            String::from_utf8_lossy(&output.stderr).to_string(),
        )));
    }

    let package_arguments = String::from_utf8_lossy(&output.stdout)
        .split_ascii_whitespace()
        .map(String::from)
        .collect();

    Ok(package_arguments)
}


-----------------------

/src/dfx/src/lib/program.rs:
-----------------------

#[cfg(target_os = "windows")]
pub const NPM: &str = "npm.cmd";
#[cfg(unix)]
pub const NPM: &str = "npm";

#[cfg(target_os = "windows")]
pub const NODE: &str = "node.exe";
#[cfg(unix)]
pub const NODE: &str = "node";


-----------------------

/src/dfx/src/lib/progress_bar.rs:
-----------------------

use indicatif::{ProgressBar as IndicatifProgressBar, ProgressDrawTarget};
use std::borrow::Cow;

pub struct ProgressBar {
    bar: Option<IndicatifProgressBar>,
}

macro_rules! forward_fn_impl {
    ($name: ident) => {
        pub fn $name(&self) {
            if let Some(ref progress_bar) = self.bar {
                progress_bar.$name();
            }
        }
    };

    ($name: ident, $( $tname: ident: $t: ty )+) => {
        pub fn $name(&self, $($tname: $t,)+) {
            if let Some(ref progress_bar) = self.bar {
                progress_bar.$name( $($tname,)+ );
            }
        }
    }
}

impl ProgressBar {
    pub fn new_spinner(message: Cow<'static, str>) -> Self {
        let progress_bar = IndicatifProgressBar::new_spinner();
        progress_bar.set_draw_target(ProgressDrawTarget::stderr());

        progress_bar.set_message(message);
        progress_bar.enable_steady_tick(80);

        ProgressBar {
            bar: Some(progress_bar),
        }
    }

    forward_fn_impl!(finish_and_clear);
    forward_fn_impl!(finish_with_message, message: Cow<'static, str>);

    pub fn discard() -> Self {
        ProgressBar { bar: None }
    }
}


-----------------------

/src/dfx/src/lib/project/import.rs:
-----------------------

use crate::lib::error::ProjectError;
use dfx_core::config::model::canister_id_store;
use dfx_core::config::model::canister_id_store::CanisterIds;
use dfx_core::config::model::dfinity::Config;
use dfx_core::error::fs::ReadFileError;
use reqwest::{Client, StatusCode};
use serde::Deserialize;
use serde_json::{Map, Value};
use slog::{info, Logger};
use std::collections::BTreeMap;
use std::path::{Path, PathBuf};
use std::str::FromStr;
use url::Url;

#[derive(Clone, Debug, Deserialize)]
struct DfxJsonCanister {
    pub candid: Option<String>,
}

#[derive(Clone, Debug, Deserialize)]
struct DfxJsonProject {
    pub canisters: BTreeMap<String, DfxJsonCanister>,
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub struct ImportNetworkMapping {
    pub network_name_in_this_project: String,
    pub network_name_in_project_being_imported: String,
}

/// import canister definitions from another project.
/// their_dfx_json_location can either be a URL or a local file path.
pub async fn import_canister_definitions(
    logger: &Logger,
    config: &mut Config,
    their_dfx_json_location: &str,
    prefix: Option<&str>,
    import_only_canister_name: Option<String>,
    network_mappings: &[ImportNetworkMapping],
) -> Result<(), ProjectError> {
    let mut loader = Loader::new();

    let their_dfx_json_url = location_to_url(their_dfx_json_location)?;
    let their_canister_ids_json_url =
        their_dfx_json_url.join("canister_ids.json").map_err(|e| {
            ProjectError::InvalidUrl(
                their_dfx_json_url.clone(),
                "canister_ids.json".to_string(),
                e,
            )
        })?;

    let what = if let Some(ref name) = import_only_canister_name {
        format!("canister '{}'", name)
    } else {
        "all canisters".to_string()
    };
    info!(logger, "Importing {} from {}", what, their_dfx_json_url);

    let their_project = loader.load_project_definition(&their_dfx_json_url).await?;
    let their_canister_ids = loader
        .load_canister_ids(&their_canister_ids_json_url)
        .await?;

    let our_project_root = config.get_project_root().to_path_buf();
    let candid_output_dir = our_project_root.join("candid");
    dfx_core::fs::create_dir_all(&candid_output_dir)?;

    let config_canisters_object = get_canisters_json_object(config)?;

    for (their_canister_name, their_canister) in their_project.canisters {
        if matches!(import_only_canister_name, Some(ref n) if *n != their_canister_name) {
            continue;
        }
        if let Some(ref their_relative_candid) = their_canister.candid {
            let our_canister_name = format!("{}{}", prefix.unwrap_or(""), their_canister_name);
            info!(
                logger,
                "Importing canister '{}' as '{}'", their_canister_name, our_canister_name
            );

            let our_canister_definition =
                ensure_child_object(config_canisters_object, &our_canister_name)?;

            import_candid_definition(
                logger,
                &mut loader,
                &their_dfx_json_url,
                &our_project_root,
                their_relative_candid,
                &our_canister_name,
                our_canister_definition,
            )
            .await?;

            set_remote_canister_ids(
                logger,
                &their_canister_name,
                network_mappings,
                &their_canister_ids,
                our_canister_definition,
            )?;

            set_additional_fields(our_canister_definition);
        }
    }

    config.save()?;

    Ok(())
}

async fn import_candid_definition(
    logger: &Logger,
    loader: &mut Loader,
    their_dfx_json_url: &Url,
    our_project_root: &Path,
    their_relative_candid: &str,
    our_canister_name: &str,
    our_canister: &mut Map<String, Value>,
) -> Result<(), ProjectError> {
    let our_relative_candid_path = format!("candid/{}.did", our_canister_name);
    let their_candid_url = their_dfx_json_url
        .join(their_relative_candid)
        .map_err(|e| {
            ProjectError::InvalidUrl(
                their_dfx_json_url.clone(),
                their_relative_candid.to_string(),
                e,
            )
        })?;
    let our_candid_path_incl_project_root = our_project_root.join(&our_relative_candid_path);
    info!(
        logger,
        "Importing {} from {}",
        our_candid_path_incl_project_root.display(),
        their_candid_url,
    );
    let candid_definition = loader.get_required_url_contents(&their_candid_url).await?;
    dfx_core::fs::write(&our_candid_path_incl_project_root, candid_definition)?;

    our_canister.insert(
        "candid".to_string(),
        Value::String(our_relative_candid_path),
    );
    Ok(())
}

pub fn get_canisters_json_object(
    config: &mut Config,
) -> Result<&mut Map<String, Value>, ProjectError> {
    let config_canisters_object = config
        .get_mut_json()
        .pointer_mut("/canisters")
        .ok_or(ProjectError::DfxJsonMissingCanisters)?
        .as_object_mut()
        .ok_or_else(|| ProjectError::ValueInDfxJsonIsNotJsonObject("/canisters".to_string()))?;
    Ok(config_canisters_object)
}

pub fn set_remote_canister_ids(
    logger: &Logger,
    their_canister_name: &str,
    network_mappings: &[ImportNetworkMapping],
    their_canister_ids: &CanisterIds,
    canister: &mut Map<String, Value>,
) -> Result<(), ProjectError> {
    for network_mapping in network_mappings {
        let remote_canister_id = their_canister_ids
            .get(their_canister_name)
            .and_then(|c| c.get(&network_mapping.network_name_in_project_being_imported));
        if let Some(remote_canister_id) = remote_canister_id {
            let remote = ensure_child_object(canister, "remote")?;
            let id = ensure_child_object(remote, "id")?;
            id.insert(
                network_mapping.network_name_in_this_project.clone(),
                Value::String(remote_canister_id.clone()),
            );
            info!(
                logger,
                "{} canister id on network '{}' is {}",
                their_canister_name,
                network_mapping.network_name_in_this_project,
                remote_canister_id,
            );
        } else {
            info!(
                logger,
                "{} has no canister id for network '{}'",
                their_canister_name,
                network_mapping.network_name_in_this_project
            );
        }
    }
    Ok(())
}

fn set_additional_fields(our_canister: &mut Map<String, Value>) {
    our_canister.insert("type".to_string(), Value::String("custom".to_string()));
    our_canister.insert("build".to_string(), Value::String("".to_string()));
    our_canister.insert("wasm".to_string(), Value::String("".to_string()));
}

fn ensure_child_object<'a>(
    parent: &'a mut Map<String, Value>,
    name: &str,
) -> Result<&'a mut Map<String, Value>, ProjectError> {
    if !parent.contains_key(name) {
        parent.insert(name.to_string(), Value::Object(Map::new()));
    }
    parent
        .get_mut(name)
        .unwrap() // we just added it
        .as_object_mut()
        .ok_or_else(|| ProjectError::ValueInDfxJsonIsNotJsonObject(name.to_string()))
}

fn location_to_url(dfx_json_location: &str) -> Result<Url, ProjectError> {
    Url::parse(dfx_json_location).or_else(|url_error| {
        let path = PathBuf::from_str(dfx_json_location).map_err(|e| {
            ProjectError::ConvertingStringToPathFailed(dfx_json_location.to_string(), e)
        })?;
        let canonical = dfx_core::fs::canonicalize(&path)?;

        Url::from_file_path(canonical)
            .map_err(|_file_error_is_unit| ProjectError::UnableToParseAsUrlOrFile(url_error))
    })
}

struct Loader {
    client: Option<Client>,
}

impl Loader {
    fn new() -> Self {
        Loader { client: None }
    }

    fn client(&mut self) -> Result<&Client, ProjectError> {
        if self.client.is_none() {
            let client = reqwest::Client::builder()
                .use_rustls_tls()
                .build()
                .map_err(ProjectError::CouldNotCreateHttpClient)?;
            self.client = Some(client);
        }
        Ok(self.client.as_ref().unwrap())
    }

    async fn load_project_definition(&mut self, url: &Url) -> Result<DfxJsonProject, ProjectError> {
        let body = self.get_required_url_contents(url).await?;
        let project = serde_json::from_slice(&body)
            .map_err(|e| ProjectError::FailedToLoadProjectDefinition(url.clone(), e))?;
        Ok(project)
    }

    async fn load_canister_ids(
        &mut self,
        url: &Url,
    ) -> Result<canister_id_store::CanisterIds, ProjectError> {
        match self.get_optional_url_contents(url).await? {
            None => Ok(canister_id_store::CanisterIds::new()),
            Some(body) => serde_json::from_slice(&body)
                .map_err(|e| ProjectError::FailedToLoadCanisterIds(url.clone(), e)),
        }
    }

    async fn get_required_url_contents(&mut self, url: &Url) -> Result<Vec<u8>, ProjectError> {
        self.get_optional_url_contents(url)
            .await?
            .ok_or_else(|| ProjectError::NotFound404(url.clone()))
    }

    async fn get_optional_url_contents(
        &mut self,
        url: &Url,
    ) -> Result<Option<Vec<u8>>, ProjectError> {
        if url.scheme() == "file" {
            Ok(Self::read_optional_file_contents(&PathBuf::from(
                url.path(),
            ))?)
        } else {
            self.get_optional_url_body(url).await
        }
    }

    fn read_optional_file_contents(path: &Path) -> Result<Option<Vec<u8>>, ReadFileError> {
        if path.exists() {
            let contents = dfx_core::fs::read(path)?;
            Ok(Some(contents))
        } else {
            Ok(None)
        }
    }

    async fn get_optional_url_body(&mut self, url: &Url) -> Result<Option<Vec<u8>>, ProjectError> {
        let client = self.client()?;
        let response = client
            .get(url.clone())
            .send()
            .await
            .map_err(|e| ProjectError::FailedToGetResource(url.clone(), e))?;
        if response.status() == StatusCode::NOT_FOUND {
            Ok(None)
        } else {
            let body = response
                .error_for_status()
                .map_err(|e| ProjectError::GettingResourceReturnedHTTPError(url.clone(), e))?
                .bytes()
                .await
                .map_err(|e| ProjectError::FailedToGetBodyFromResponse(url.clone(), e))?;
            Ok(Some(body.into()))
        }
    }
}


-----------------------

/src/dfx/src/lib/project/mod.rs:
-----------------------

pub mod import;
pub mod network_mappings;
pub mod templates;


-----------------------

/src/dfx/src/lib/project/network_mappings.rs:
-----------------------

use crate::lib::error::ProjectError;
use crate::lib::project::import::ImportNetworkMapping;

pub fn get_network_mappings(input: &[String]) -> Result<Vec<ImportNetworkMapping>, ProjectError> {
    input
        .iter()
        .map(|v| {
            if let Some(index) = v.find('=') {
                if index == 0 {
                    Err(ProjectError::MalformedNetworkMapping(
                        v.to_string(),
                        "first".to_string(),
                    ))
                } else if index == v.len() - 1 {
                    Err(ProjectError::MalformedNetworkMapping(
                        v.to_string(),
                        "second".to_string(),
                    ))
                } else {
                    Ok(ImportNetworkMapping {
                        network_name_in_this_project: v[..index].to_string(),
                        network_name_in_project_being_imported: v[index + 1..].to_string(),
                    })
                }
            } else {
                Ok(ImportNetworkMapping {
                    network_name_in_this_project: v.clone(),
                    network_name_in_project_being_imported: v.clone(),
                })
            }
        })
        .collect()
}

#[cfg(test)]
mod tests {
    use crate::lib::project::import::ImportNetworkMapping;
    use crate::lib::project::network_mappings::get_network_mappings;

    #[test]
    fn usual() {
        assert_eq!(
            get_network_mappings(&["ic".to_string()]).unwrap(),
            vec![ImportNetworkMapping {
                network_name_in_this_project: "ic".to_string(),
                network_name_in_project_being_imported: "ic".to_string(),
            }],
        );
    }

    #[test]
    fn mapped() {
        assert_eq!(
            get_network_mappings(&["abc=defg".to_string()]).unwrap(),
            vec![ImportNetworkMapping {
                network_name_in_this_project: "abc".to_string(),
                network_name_in_project_being_imported: "defg".to_string(),
            }],
        );
    }

    #[test]
    fn multiple() {
        assert_eq!(
            get_network_mappings(&["abc=defg".to_string(), "ghi=xyz".to_string()]).unwrap(),
            vec![
                ImportNetworkMapping {
                    network_name_in_this_project: "abc".to_string(),
                    network_name_in_project_being_imported: "defg".to_string(),
                },
                ImportNetworkMapping {
                    network_name_in_this_project: "ghi".to_string(),
                    network_name_in_project_being_imported: "xyz".to_string(),
                }
            ],
        );
    }

    #[test]
    #[should_panic(expected = "MalformedNetworkMapping(\"=defg\", \"first\")")]
    fn malformed_missing_first() {
        get_network_mappings(&["=defg".to_string()]).unwrap();
    }

    #[test]
    #[should_panic(expected = "MalformedNetworkMapping(\"abc=\", \"second\")")]
    fn malformed_missing_second() {
        get_network_mappings(&["abc=".to_string()]).unwrap();
    }
}


-----------------------

/src/dfx/src/lib/project/templates.rs:
-----------------------

use crate::util::assets;
use dfx_core::config::project_templates::{
    Category, ProjectTemplate, ProjectTemplateName, ResourceLocation,
};

const NPM_INSTALL: &str = "npm install --quiet --no-progress --workspaces --if-present";
const NPM_INSTALL_SPINNER_MESSAGE: &str = "Installing node dependencies...";
const NPM_INSTALL_FAILURE_WARNING: &str =
    "An error occurred. See the messages above for more details.";
const CARGO_UPDATE_FAILURE_MESSAGE: &str = "You will need to run it yourself (or a similar command like `cargo vendor`), because `dfx build` will use the --locked flag with Cargo.";

pub fn builtin_templates() -> Vec<ProjectTemplate> {
    let motoko = ProjectTemplate {
        name: ProjectTemplateName("motoko".to_string()),
        display: "Motoko".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_motoko_files,
        },
        category: Category::Backend,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 0,
    };

    let rust = ProjectTemplate {
        name: ProjectTemplateName("rust".to_string()),
        display: "Rust".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_rust_files,
        },
        category: Category::Backend,
        post_create: vec!["cargo update".to_string()],
        post_create_failure_warning: Some(CARGO_UPDATE_FAILURE_MESSAGE.to_string()),
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 1,
    };

    let azle = ProjectTemplate {
        name: ProjectTemplateName("azle".to_string()),
        display: "Typescript (Azle)".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_azle_files,
        },
        category: Category::Backend,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![ProjectTemplateName("dfx_js_base".to_string())],
        sort_order: 2,
    };

    let kybra = ProjectTemplate {
        name: ProjectTemplateName("kybra".to_string()),
        display: "Python (Kybra)".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_kybra_files,
        },
        category: Category::Backend,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 2,
    };

    let sveltekit = ProjectTemplate {
        name: ProjectTemplateName("sveltekit".to_string()),
        display: "SvelteKit".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_svelte_files,
        },
        category: Category::Frontend,
        post_create: vec![NPM_INSTALL.to_string()],
        post_create_failure_warning: Some(NPM_INSTALL_FAILURE_WARNING.to_string()),
        post_create_spinner_message: Some(NPM_INSTALL_SPINNER_MESSAGE.to_string()),
        requirements: vec![ProjectTemplateName("dfx_js_base".to_string())],
        sort_order: 0,
    };

    let react = ProjectTemplate {
        name: ProjectTemplateName("react".to_string()),
        display: "React".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_react_files,
        },
        category: Category::Frontend,
        post_create: vec![NPM_INSTALL.to_string()],
        post_create_failure_warning: Some(NPM_INSTALL_FAILURE_WARNING.to_string()),
        post_create_spinner_message: Some(NPM_INSTALL_SPINNER_MESSAGE.to_string()),
        requirements: vec![ProjectTemplateName("dfx_js_base".to_string())],
        sort_order: 1,
    };

    let vue = ProjectTemplate {
        name: ProjectTemplateName("vue".to_string()),
        display: "Vue".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_vue_files,
        },
        category: Category::Frontend,
        post_create: vec![NPM_INSTALL.to_string()],
        post_create_failure_warning: Some(NPM_INSTALL_FAILURE_WARNING.to_string()),
        post_create_spinner_message: Some(NPM_INSTALL_SPINNER_MESSAGE.to_string()),
        requirements: vec![ProjectTemplateName("dfx_js_base".to_string())],
        sort_order: 2,
    };

    let vanilla = ProjectTemplate {
        name: ProjectTemplateName("vanilla".to_string()),
        display: "Vanilla JS".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_vanillajs_files,
        },
        category: Category::Frontend,
        post_create: vec![NPM_INSTALL.to_string()],
        post_create_failure_warning: Some(NPM_INSTALL_FAILURE_WARNING.to_string()),
        post_create_spinner_message: Some(NPM_INSTALL_SPINNER_MESSAGE.to_string()),
        requirements: vec![ProjectTemplateName("dfx_js_base".to_string())],
        sort_order: 3,
    };

    let simple_assets = ProjectTemplate {
        name: ProjectTemplateName("simple-assets".to_string()),
        display: "No JS template".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_assets_files,
        },
        category: Category::Frontend,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 4,
    };

    let sveltekit_tests = ProjectTemplate {
        name: ProjectTemplateName("sveltekit-tests".to_string()),
        display: "SvelteKit Test Files".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_svelte_test_files,
        },
        category: Category::FrontendTest,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 0,
    };

    let react_tests = ProjectTemplate {
        name: ProjectTemplateName("react-tests".to_string()),
        display: "React Test Files".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_react_test_files,
        },
        category: Category::FrontendTest,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 0,
    };

    let vue_tests = ProjectTemplate {
        name: ProjectTemplateName("vue-tests".to_string()),
        display: "Vue Test Files".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_vue_test_files,
        },
        category: Category::FrontendTest,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 0,
    };

    let vanillajs_tests = ProjectTemplate {
        name: ProjectTemplateName("vanilla-tests".to_string()),
        display: "Vanilla JS Test Files".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_vanillajs_test_files,
        },
        category: Category::FrontendTest,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 0,
    };

    let internet_identity = ProjectTemplate {
        name: ProjectTemplateName("internet-identity".to_string()),
        display: "Internet Identity".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_internet_identity_files,
        },
        category: Category::Extra,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 0,
    };

    let bitcoin = ProjectTemplate {
        name: ProjectTemplateName("bitcoin".to_string()),
        display: "Bitcoin (Regtest)".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_bitcoin_files,
        },
        category: Category::Extra,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 1,
    };

    let js_base = ProjectTemplate {
        name: ProjectTemplateName("dfx_js_base".to_string()),
        display: "<never shown>>".to_string(),
        resource_location: ResourceLocation::Bundled {
            get_archive_fn: assets::new_project_js_files,
        },
        category: Category::Support,
        post_create: vec![],
        post_create_failure_warning: None,
        post_create_spinner_message: None,
        requirements: vec![],
        sort_order: 2,
    };

    vec![
        motoko,
        rust,
        azle,
        kybra,
        vanilla,
        sveltekit,
        vue,
        react,
        simple_assets,
        sveltekit_tests,
        react_tests,
        vue_tests,
        vanillajs_tests,
        internet_identity,
        bitcoin,
        js_base,
    ]
}


-----------------------

/src/dfx/src/lib/replica/mod.rs:
-----------------------

pub mod status;


-----------------------

/src/dfx/src/lib/replica/status.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::{bail, Context};
use ic_agent::Agent;
use std::time::Duration;

pub async fn ping_and_wait(url: &str) -> DfxResult {
    let agent = Agent::builder()
        .with_url(url)
        .build()
        .with_context(|| format!("Failed to build agent with url {url}."))?;
    let mut retries = 0;
    loop {
        let status = agent.status().await;
        match status {
            Ok(status) => {
                if matches!(&status.replica_health_status, Some(status) if status == "healthy") {
                    break;
                }
            }
            Err(e) => {
                if retries >= 60 {
                    bail!(e);
                }
                tokio::time::sleep(Duration::from_secs(1)).await;
                retries += 1;
            }
        }
    }
    Ok(())
}


-----------------------

/src/dfx/src/lib/retryable.rs:
-----------------------

use ic_agent::AgentError;

pub fn retryable(agent_error: &AgentError) -> bool {
    matches!(
        agent_error,
        AgentError::TimeoutWaitingForResponse() | AgentError::TransportError(_)
    )
}


-----------------------

/src/dfx/src/lib/root_key.rs:
-----------------------

use crate::{lib::error::DfxResult, Environment};

use dfx_core::network::root_key;

pub async fn fetch_root_key_if_needed(env: &dyn Environment) -> DfxResult {
    let agent = env.get_agent();
    let network = env.get_network_descriptor();
    root_key::fetch_root_key_when_non_mainnet(agent, network).await?;
    Ok(())
}

/// Fetches the root key of the local network.
/// Returns an error if attempted to run on the real IC.
pub async fn fetch_root_key_or_anyhow(env: &dyn Environment) -> DfxResult {
    let agent = env.get_agent();
    let network = env.get_network_descriptor();
    root_key::fetch_root_key_when_non_mainnet_or_error(agent, network).await?;
    Ok(())
}


-----------------------

/src/dfx/src/lib/sign/mod.rs:
-----------------------

pub mod signed_message;


-----------------------

/src/dfx/src/lib/sign/signed_message.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use fn_error_context::context;
use ic_agent::RequestId;
use serde::{Deserialize, Serialize};
use serde_cbor::Value;
use std::convert::TryFrom;
use time::{Duration, OffsetDateTime};

#[derive(Debug, Clone, Deserialize, Serialize)]
pub(crate) struct SignedMessageV1 {
    version: usize,
    #[serde(with = "date_time_utc")]
    pub creation: OffsetDateTime,
    #[serde(with = "date_time_utc")]
    pub expiration: OffsetDateTime,
    pub network: String, // url of the network
    pub is_ic: bool,
    pub call_type: String,
    pub sender: String,
    pub canister_id: String,
    pub method_name: String,
    pub arg: Vec<u8>,
    pub request_id: Option<String>, // only useful for update call
    pub content: String,            // hex::encode the Vec<u8>
    pub signed_request_status: Option<String>, // hex::encode the Vec<u8>, only accompany update call
}

impl SignedMessageV1 {
    pub fn new(
        creation: OffsetDateTime,
        expiration: OffsetDateTime,
        network: String,
        is_ic: bool,
        sender: Principal,
        canister_id: Principal,
        method_name: String,
        arg: Vec<u8>,
    ) -> Self {
        Self {
            version: 1,
            creation,
            expiration,
            network,
            is_ic,
            call_type: String::new(),
            sender: sender.to_string(),
            canister_id: canister_id.to_string(),
            method_name,
            arg,
            request_id: None,
            content: String::new(),
            signed_request_status: None,
        }
    }

    pub fn with_call_type(mut self, request_type: String) -> Self {
        self.call_type = request_type;
        self
    }

    pub fn with_request_id(mut self, request_id: RequestId) -> Self {
        self.request_id = Some(String::from(request_id));
        self
    }

    pub fn with_content(mut self, content: String) -> Self {
        self.content = content;
        self
    }

    pub fn with_signed_request_status(mut self, signed_request_status: String) -> Self {
        self.signed_request_status = Some(signed_request_status);
        self
    }

    #[context("Failed to validate signed message.")]
    pub fn validate(&self) -> DfxResult {
        if self.version != 1 {
            bail!("Invalid message: version must be 1");
        }

        if !["query", "update"].contains(&self.call_type.as_str()) {
            bail!("Invalid message: call_type must be `query` or `update`");
        }

        let content = hex::decode(&self.content).context("Failed to decode content.")?;

        let cbor: Value = serde_cbor::from_slice(&content)
            .map_err(|_| anyhow!("Invalid cbor data in the content of the message."))?;

        if let Value::Map(m) = cbor {
            let cbor_content = m
                .get(&Value::Text("content".to_string()))
                .ok_or_else(|| anyhow!("Invalid cbor content"))?;
            if let Value::Map(m) = cbor_content {
                let ingress_expiry = m
                    .get(&Value::Text("ingress_expiry".to_string()))
                    .ok_or_else(|| anyhow!("Invalid cbor content"))?;
                if let Value::Integer(s) = ingress_expiry {
                    let expiration_from_cbor = OffsetDateTime::from_unix_timestamp_nanos(*s)?;
                    let diff = self.expiration - expiration_from_cbor;
                    if diff > Duration::seconds(5) || diff < Duration::seconds(-5) {
                        bail!(
                            "Invalid message: expiration not match\njson: {}\ncbor: {}",
                            self.expiration,
                            expiration_from_cbor
                        )
                    }
                    if OffsetDateTime::now_utc() > expiration_from_cbor {
                        bail!("The message has been expired at: {}", expiration_from_cbor);
                    }
                }
                let sender = m
                    .get(&Value::Text("sender".to_string()))
                    .ok_or_else(|| anyhow!("Invalid cbor content"))?;
                if let Value::Bytes(s) = sender {
                    let sender_from_cbor =
                        Principal::try_from(s).map_err(|_| anyhow!("Invalid cbor content."))?;
                    let sender_from_json = Principal::from_text(&self.sender)
                        .map_err(|_| anyhow!("Invalid json: sender."))?;
                    if !sender_from_cbor.eq(&sender_from_json) {
                        bail!(
                            "Invalid message: sender principle not match\njson: {}\ncbor: {}",
                            sender_from_json,
                            sender_from_cbor
                        )
                    }
                }

                let canister_id = m
                    .get(&Value::Text("canister_id".to_string()))
                    .ok_or_else(|| anyhow!("Invalid cbor content"))?;
                if let Value::Bytes(s) = canister_id {
                    let canister_id_from_cbor =
                        Principal::try_from(s).map_err(|_| anyhow!("Invalid cbor content."))?;
                    let canister_id_from_json = Principal::from_text(&self.canister_id)
                        .map_err(|_| anyhow!("Invalid json: canister_id."))?;
                    if !canister_id_from_cbor.eq(&canister_id_from_json) {
                        bail!(
                            "Invalid message: canister_id not match\njson: {}\ncbor: {}",
                            canister_id_from_json,
                            canister_id_from_cbor
                        )
                    }
                }

                let method_name = m
                    .get(&Value::Text("method_name".to_string()))
                    .ok_or_else(|| anyhow!("Invalid cbor content"))?;
                if let Value::Text(s) = method_name {
                    if !s.eq(&self.method_name) {
                        bail!(
                            "Invalid message: method_name not match\njson: {}\ncbor: {}",
                            self.method_name,
                            s
                        )
                    }
                }

                let arg = m
                    .get(&Value::Text("arg".to_string()))
                    .ok_or_else(|| anyhow!("Invalid cbor content"))?;
                if let Value::Bytes(s) = arg {
                    if !s.eq(&self.arg) {
                        bail!(
                            "Invalid message: arg not match\njson: {:?}\ncbor: {:?}",
                            self.arg,
                            s
                        )
                    }
                }
            } else {
                bail!("Invalid cbor content");
            }
        } else {
            bail!("Invalid cbor content");
        }
        Ok(())
    }
}

mod date_time_utc {
    time::serde::format_description!(date_time, PrimitiveDateTime, "[year repr:full padding:zero]-[month repr:numerical padding:zero]-[day padding:zero] [hour repr:24 padding:zero]:[minute padding:zero]:[second padding:zero] UTC");

    use serde::{Deserializer, Serializer};
    use time::{OffsetDateTime, PrimitiveDateTime, UtcOffset};

    pub fn serialize<S: Serializer>(datetime: &OffsetDateTime, s: S) -> Result<S::Ok, S::Error> {
        let utc = datetime.to_offset(UtcOffset::UTC);
        let date = utc.date();
        let time = utc.time();
        date_time::serialize(&PrimitiveDateTime::new(date, time), s)
    }
    pub fn deserialize<'de, D: Deserializer<'de>>(d: D) -> Result<OffsetDateTime, D::Error> {
        let primitive = date_time::deserialize(d)?;
        Ok(primitive.assume_utc())
    }
}


-----------------------

/src/dfx/src/lib/state_tree/canister_info.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::{anyhow, bail, Context};
use candid::Principal;
use ic_agent::{Agent, AgentError};
use serde_cbor::Value;

pub async fn read_state_tree_canister_controllers(
    agent: &Agent,
    canister_id: Principal,
) -> DfxResult<Option<Vec<Principal>>> {
    let controller_blob = match agent
        .read_state_canister_info(canister_id, "controllers")
        .await
    {
        Err(AgentError::LookupPathAbsent(_)) => {
            return Ok(None);
        }
        r => r.with_context(|| format!("Failed to read controllers of canister {canister_id}."))?,
    };
    let cbor: Value = serde_cbor::from_slice(&controller_blob)
        .map_err(|_| anyhow!("Invalid cbor data in controllers canister info."))?;
    let controllers = if let Value::Array(vec) = cbor {
        vec.into_iter()
            .map(|elem: Value| {
                if let Value::Bytes(bytes) = elem {
                    Ok(Principal::try_from(&bytes).with_context(|| {
                        format!(
                            "Failed to construct principal of controller from bytes ({}).",
                            hex::encode(&bytes)
                        )
                    })?)
                } else {
                    bail!(
                        "Expected element in controllers to be of type bytes, got {:?}",
                        elem
                    );
                }
            })
            .collect::<DfxResult<Vec<Principal>>>()
    } else {
        bail!("Expected controllers to be an array, but got {:?}", cbor);
    }
    .context("Failed to determine controllers.")?;

    Ok(Some(controllers))
}

/// None can indicate either of these, but we can't tell from here:
/// - the canister doesn't exist
/// - the canister exists but does not have a module installed
pub async fn read_state_tree_canister_module_hash(
    agent: &Agent,
    canister_id: Principal,
) -> DfxResult<Option<Vec<u8>>> {
    let module_hash = match agent
        .read_state_canister_info(canister_id, "module_hash")
        .await
    {
        Ok(blob) => Some(blob),
        Err(AgentError::LookupPathAbsent(_)) => None,
        Err(x) => bail!(x),
    };

    Ok(module_hash)
}


-----------------------

/src/dfx/src/lib/state_tree/mod.rs:
-----------------------

pub mod canister_info;


-----------------------

/src/dfx/src/lib/subnet.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::anyhow;
use backoff::future::retry;
use backoff::ExponentialBackoff;
use candid::{CandidType, Deserialize, Principal};
use ic_agent::{Agent, AgentError};
use ic_utils::call::SyncCall;
use ic_utils::Canister;

use super::retryable::retryable;

pub const MAINNET_REGISTRY_CANISTER_ID: Principal =
    Principal::from_slice(&[0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x01]);

#[derive(CandidType, Debug)]
pub struct GetSubnetForCanisterRequest {
    pub principal: Option<Principal>,
}

#[derive(CandidType, Deserialize, Debug, Clone)]
pub struct GetSubnetForCanisterResponse {
    pub subnet_id: Option<Principal>,
}

pub async fn get_subnet_for_canister(
    agent: &Agent,
    canister_id: Principal,
) -> DfxResult<Principal> {
    let registry_canister = Canister::builder()
        .with_agent(agent)
        .with_canister_id(MAINNET_REGISTRY_CANISTER_ID)
        .build()?;

    let retry_policy = ExponentialBackoff::default();

    retry(retry_policy, || async {
        let arg = GetSubnetForCanisterRequest {
            principal: Some(canister_id),
        };
        let result: Result<Result<GetSubnetForCanisterResponse, String>, AgentError> =
            registry_canister
                .query("get_subnet_for_canister")
                .with_arg(arg)
                .build()
                .call()
                .await
                .map(|(result,)| result);
        match result {
            Ok(Ok(GetSubnetForCanisterResponse {
                subnet_id: Some(subnet_id),
            })) => Ok(subnet_id),
            Ok(Ok(GetSubnetForCanisterResponse { subnet_id: None })) => Err(
                backoff::Error::permanent(anyhow!("no subnet found for canister {}", &canister_id)),
            ),
            Ok(Err(text)) => Err(backoff::Error::permanent(anyhow!(
                "unable to determine subnet: {}",
                text
            ))),
            Err(agent_err) if retryable(&agent_err) => {
                Err(backoff::Error::transient(anyhow!(agent_err)))
            }
            Err(agent_err) => Err(backoff::Error::permanent(anyhow!(agent_err))),
        }
    })
    .await
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_registry_canister_id() {
        assert_eq!(
            MAINNET_REGISTRY_CANISTER_ID,
            Principal::from_text("rwlgt-iiaaa-aaaaa-aaaaa-cai").unwrap()
        );
    }
}


-----------------------

/src/dfx/src/lib/warning.rs:
-----------------------

pub enum DfxWarning {
    MainnetPlainTextIdentity,
}

pub fn is_warning_disabled(warning: DfxWarning) -> bool {
    let warning = match warning {
        DfxWarning::MainnetPlainTextIdentity => "mainnet_plaintext_identity",
    };
    // By default, warnings are all enabled.
    let env_warnings = std::env::var("DFX_WARNING").unwrap_or_else(|_| "".to_string());
    env_warnings
        .split(',')
        .filter(|w| w.starts_with('-'))
        .any(|w| w.chars().skip(1).collect::<String>().eq(warning))
}


-----------------------

/src/dfx/src/lib/wasm/file.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::{bail, Context};
use flate2::read::GzDecoder;
use flate2::write::GzEncoder;
use flate2::Compression;
use fn_error_context::context;
use std::io::{Read, Write};
use std::path::Path;

/// Read wasm module
///
/// Based on the file extension, it may decompress the file before parse the wasm module.
pub fn read_wasm_module(path: &Path) -> DfxResult<walrus::Module> {
    let bytes: Vec<u8> = dfx_core::fs::read(path)?;

    let m = match path.extension() {
        Some(f) if f == "gz" => {
            let unzip_bytes = decompress_bytes(&bytes)
                .with_context(|| format!("Failed to decode gzip file {:?}", path))?;
            bytes_to_module(&unzip_bytes).with_context(|| {
                format!("Failed to parse wasm module from decompressed {:?}", path)
            })?
        }
        Some(f) if f == "wasm" => bytes_to_module(&bytes)
            .with_context(|| format!("Failed to parse wasm module from {:?}", path))?,
        _ => {
            bail!("{:?} is neither a wasm nor a wasm.gz file", path);
        }
    };
    Ok(m)
}

#[context("Failed to parse bytes as wasm")]
pub fn bytes_to_module(bytes: &[u8]) -> DfxResult<walrus::Module> {
    Ok(ic_wasm::utils::parse_wasm(bytes, true)?)
}

#[context("Failed to encode bytes as gzip")]
pub fn compress_bytes(bytes: &[u8]) -> DfxResult<Vec<u8>> {
    let mut e = GzEncoder::new(Vec::new(), Compression::default());
    e.write_all(bytes)?;
    Ok(e.finish()?)
}

#[context("Failed to decode bytes as gzip")]
pub fn decompress_bytes(bytes: &[u8]) -> DfxResult<Vec<u8>> {
    let mut d = GzDecoder::new(bytes);
    let mut unzipped_bytes = vec![];
    d.read_to_end(&mut unzipped_bytes)?;
    Ok(unzipped_bytes)
}


-----------------------

/src/dfx/src/lib/wasm/mod.rs:
-----------------------

pub mod file;


-----------------------

/src/dfx/src/main.rs:
-----------------------

#![allow(special_module_name)]
use crate::config::{dfx_version, dfx_version_str};
use crate::lib::diagnosis::{diagnose, Diagnosis};
use crate::lib::environment::{Environment, EnvironmentImpl};
use crate::lib::error::DfxResult;
use crate::lib::logger::{create_root_logger, LoggingMode};
use crate::lib::project::templates::builtin_templates;
use anyhow::Error;
use clap::{ArgAction, CommandFactory, Parser};
use dfx_core::config::project_templates;
use dfx_core::extension::installed::InstalledExtensionManifests;
use dfx_core::extension::manager::ExtensionManager;
use std::collections::HashMap;
use std::ffi::OsString;
use std::path::PathBuf;

mod actors;
mod commands;
mod config;
mod lib;
mod util;

/// The DFINITY Executor.
#[derive(Parser)]
#[command(name = "dfx", version = dfx_version_str(), styles = util::clap::style(), arg_required_else_help = true)]
pub struct CliOpts {
    /// Displays detailed information about operations. -vv will generate a very large number of messages and can affect performance.
    #[arg(long, short, action = ArgAction::Count, global = true)]
    verbose: u8,

    /// Suppresses informational messages. -qq limits to errors only; -qqqq disables them all.
    #[arg(long, short, action = ArgAction::Count, global = true)]
    quiet: u8,

    /// The logging mode to use. You can log to stderr, a file, or both.
    #[arg(long = "log", default_value = "stderr", value_parser = ["stderr", "tee", "file"], global = true)]
    logmode: String,

    /// The file to log to, if logging to a file (see --logmode).
    #[arg(long, global = true)]
    logfile: Option<String>,

    /// The user identity to run this command as. It contains your principal as well as some things DFX associates with it like the wallet.
    #[arg(long, env = "DFX_IDENTITY", global = true)]
    identity: Option<String>,

    /// The effective canister id for provisional canister creation must be a canister id in the canister ranges of the subnet on which new canisters should be created.
    #[arg(long, global = true, value_name = "PRINCIPAL")]
    provisional_create_canister_effective_canister_id: Option<String>,

    #[command(subcommand)]
    command: commands::DfxCommand,
}

/// Setup a logger with the proper configuration, based on arguments.
/// Returns a topple of whether or not to have a progress bar, and a logger.
fn setup_logging(opts: &CliOpts) -> (i64, slog::Logger) {
    // Create a logger with our argument matches.
    let verbose_level = opts.verbose as i64 - opts.quiet as i64;

    let mode = match opts.logmode.as_str() {
        "tee" => LoggingMode::Tee(PathBuf::from(opts.logfile.as_deref().unwrap_or("log.txt"))),
        "file" => LoggingMode::File(PathBuf::from(opts.logfile.as_deref().unwrap_or("log.txt"))),
        _ => LoggingMode::Stderr,
    };

    (verbose_level, create_root_logger(verbose_level, mode))
}

fn print_error_and_diagnosis(err: Error, error_diagnosis: Diagnosis) {
    let mut stderr = util::stderr_wrapper::stderr_wrapper();

    // print error chain stack
    for (level, cause) in err.chain().enumerate() {
        let (color, prefix) = if level == 0 {
            (term::color::RED, "Error")
        } else {
            (term::color::YELLOW, "Caused by")
        };
        stderr
            .fg(color)
            .expect("Failed to set stderr output color.");
        write!(stderr, "{prefix}: ").expect("Failed to write to stderr.");
        stderr
            .reset()
            .expect("Failed to reset stderr output color.");

        writeln!(stderr, "{cause}").expect("Failed to write to stderr.");
    }

    // print diagnosis
    if let Some(error_explanation) = error_diagnosis.0 {
        stderr
            .fg(term::color::YELLOW)
            .expect("Failed to set stderr output color.");
        writeln!(stderr, "Error explanation:").expect("Failed to write to stderr.");
        stderr
            .reset()
            .expect("Failed to reset stderr output color.");

        writeln!(stderr, "{}", error_explanation).expect("Failed to write to stderr.");
    }
    if let Some(action_suggestion) = error_diagnosis.1 {
        stderr
            .fg(term::color::YELLOW)
            .expect("Failed to set stderr output color.");
        writeln!(stderr, "How to resolve the error:").expect("Failed to write to stderr.");
        stderr
            .reset()
            .expect("Failed to reset stderr output color.");

        writeln!(stderr, "{}", action_suggestion).expect("Failed to write to stderr.");
    }
}

fn get_args_altered_for_extension_run(
    installed: &InstalledExtensionManifests,
) -> DfxResult<Vec<OsString>> {
    let mut args = std::env::args_os().collect::<Vec<OsString>>();

    let installed_extensions = installed.as_clap_commands()?;
    if !installed_extensions.is_empty() {
        let mut app = CliOpts::command_for_update().subcommands(&installed_extensions);
        sort_clap_commands(&mut app);
        // here clap will display the help message if no subcommand was provided...
        let app = app.get_matches();
        // ...therefore we can safely unwrap here because we know a subcommand was provided
        let subcmd = app.subcommand().unwrap().0;
        if installed.contains(subcmd) {
            let idx = args.iter().position(|arg| arg == subcmd).unwrap();
            args.splice(idx..idx, ["extension", "run"].iter().map(OsString::from));
        }
    }
    Ok(args)
}

fn inner_main() -> DfxResult {
    let em = ExtensionManager::new(dfx_version())?;
    let installed_extension_manifests = em.load_installed_extension_manifests()?;
    project_templates::populate(builtin_templates());

    let args = get_args_altered_for_extension_run(&installed_extension_manifests)?;

    let cli_opts = CliOpts::parse_from(args);

    if matches!(cli_opts.command, commands::DfxCommand::Schema(_)) {
        return commands::exec_without_env(cli_opts.command);
    }

    let (verbose_level, log) = setup_logging(&cli_opts);
    let identity = cli_opts.identity;
    let effective_canister_id = cli_opts.provisional_create_canister_effective_canister_id;

    let env = EnvironmentImpl::new(em)?
        .with_logger(log)
        .with_identity_override(identity)
        .with_verbose_level(verbose_level)
        .with_effective_canister_id(effective_canister_id);

    slog::trace!(
        env.get_logger(),
        "Trace mode enabled. Lots of logs coming up."
    );
    commands::exec(&env, cli_opts.command)
}

fn main() {
    let result = inner_main();
    if let Err(err) = result {
        let error_diagnosis = diagnose(&err);
        print_error_and_diagnosis(err, error_diagnosis);
        std::process::exit(255);
    }
}

/// sort subcommands alphabetically (despite this clap prints help as the last one)
pub fn sort_clap_commands(cmd: &mut clap::Command) {
    let mut cli_subcommands: Vec<String> = cmd
        .get_subcommands()
        .map(|v| v.get_display_name().unwrap_or_default().to_string())
        .collect();
    cli_subcommands.sort();
    let cli_subcommands: HashMap<String, usize> = cli_subcommands
        .into_iter()
        .enumerate()
        .map(|(i, v)| (v, i))
        .collect();
    for c in cmd.get_subcommands_mut() {
        let name = c.get_display_name().unwrap_or_default().to_string();
        let ord = *cli_subcommands.get(&name).unwrap_or(&999);
        *c = c.clone().display_order(ord);
    }
}

#[cfg(test)]
mod tests {
    use crate::lib::project::templates::builtin_templates;
    use crate::CliOpts;
    use clap::CommandFactory;
    use dfx_core::config::project_templates;

    #[test]
    fn validate_cli() {
        project_templates::populate(builtin_templates());

        CliOpts::command().debug_assert();
    }
}


-----------------------

/src/dfx/src/util/assets.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::{bail, Context};
use fn_error_context::context;
use slog::info;
use std::io::Read;

include!(concat!(env!("OUT_DIR"), "/load_assets.rs"));

pub fn dfinity_logo() -> String {
    let colors = supports_color::on(supports_color::Stream::Stdout);
    if let Some(colors) = colors {
        //Some terminals, notably MacOS's Terminal.app, do not support Truecolor (RGB-colored characters) properly.
        //Therefore we use xterm256 coloring when the program is running in such a terminal.
        if colors.has_16m {
            return include_str!("../../assets/dfinity-color.aart").to_string();
        } else if colors.has_256 {
            return include_str!("../../assets/dfinity-color-xterm256.aart").to_string();
        }
    }
    include_str!("../../assets/dfinity-nocolor.aart").to_string()
}

#[context("Failed to load wallet wasm.")]
pub fn wallet_wasm(logger: &slog::Logger) -> DfxResult<Vec<u8>> {
    if let Ok(dfx_wallet_wasm) = std::env::var("DFX_WALLET_WASM") {
        info!(logger, "Using wasm at path: {}", dfx_wallet_wasm);
        Ok(dfx_core::fs::read(dfx_wallet_wasm.as_ref())?)
    } else {
        let mut canister_assets =
            wallet_canister().context("Failed to load wallet canister archive.")?;
        for file in canister_assets
            .entries()
            .context("Failed to read wallet canister archive entries.")?
        {
            let mut file = file.context("Failed to read wallet canister archive entry.")?;
            if file
                .header()
                .path()
                .context("Failed to read archive entry path.")?
                .ends_with("wallet.wasm.gz")
            {
                let mut wasm = vec![];
                file.read_to_end(&mut wasm)
                    .context("Failed to read archive entry.")?;
                return Ok(wasm);
            }
        }
        bail!("Failed to find wallet canister archive entry.");
    }
}

#[context("Failed to load assets wasm.")]
pub fn assets_wasm(logger: &slog::Logger) -> DfxResult<Vec<u8>> {
    if let Ok(dfx_assets_wasm) = std::env::var("DFX_ASSETS_WASM") {
        info!(logger, "Using wasm at path: {}", dfx_assets_wasm);
        Ok(dfx_core::fs::read(dfx_assets_wasm.as_ref())?)
    } else {
        let mut canister_assets =
            assetstorage_canister().context("Failed to load asset canister archive.")?;
        for file in canister_assets
            .entries()
            .context("Failed to read asset canister archive entries.")?
        {
            let mut file = file.context("Failed to read asset canister archive entry.")?;
            if file
                .header()
                .path()
                .context("Failed to read archive entry path.")?
                .ends_with("assetstorage.wasm.gz")
            {
                let mut wasm = vec![];
                file.read_to_end(&mut wasm)
                    .context("Failed to read archive entry.")?;
                return Ok(wasm);
            }
        }
        bail!("Failed to find asset canister archive entry.");
    }
}

#[allow(unused)]
#[context("Failed to load bitcoin wasm.")]
pub fn bitcoin_wasm(logger: &slog::Logger) -> DfxResult<Vec<u8>> {
    if let Ok(dfx_assets_wasm) = std::env::var("DFX_BITCOIN_WASM") {
        info!(logger, "Using wasm at path: {}", dfx_assets_wasm);
        Ok(dfx_core::fs::read(dfx_assets_wasm.as_ref())?)
    } else {
        let mut canister_assets =
            btc_canister().context("Failed to load bitcoin canister archive.")?;
        for file in canister_assets
            .entries()
            .context("Failed to read bitcoin canister archive entries.")?
        {
            let mut file = file.context("Failed to read bitcoin canister archive entry.")?;
            if file
                .header()
                .path()
                .context("Failed to read archive entry path.")?
                .ends_with("ic-btc-canister.wasm.gz")
            {
                let mut wasm = vec![];
                file.read_to_end(&mut wasm)
                    .context("Failed to read archive entry.")?;
                return Ok(wasm);
            }
        }
        bail!("Failed to find bitcoin canister archive entry");
    }
}

pub fn management_idl() -> DfxResult<String> {
    // FIXME get idl from replica when it's available
    // Pulled from https://github.com/dfinity/interface-spec/blob/master/spec/_attachments/ic.did
    Ok(r##"
type canister_id = principal;
type wasm_module = blob;

type canister_settings = record {
    controllers : opt vec principal;
    compute_allocation : opt nat;
    memory_allocation : opt nat;
    freezing_threshold : opt nat;
    reserved_cycles_limit : opt nat;
};

type definite_canister_settings = record {
    controllers : vec principal;
    compute_allocation : nat;
    memory_allocation : nat;
    freezing_threshold : nat;
    reserved_cycles_limit : nat;
};

type change_origin = variant {
    from_user : record {
        user_id : principal;
    };
    from_canister : record {
        canister_id : principal;
        canister_version : opt nat64;
    };
};

type change_details = variant {
    creation : record {
        controllers : vec principal;
    };
    code_uninstall;
    code_deployment : record {
        mode : variant { install; reinstall; upgrade };
        module_hash : blob;
    };
    controllers_change : record {
        controllers : vec principal;
    };
};

type change = record {
    timestamp_nanos : nat64;
    canister_version : nat64;
    origin : change_origin;
    details : change_details;
};

type chunk_hash = record {
  hash : blob;
};

type http_header = record {
    name : text;
    value : text;
};

type http_request_result = record {
    status : nat;
    headers : vec http_header;
    body : blob;
};

type ecdsa_curve = variant {
    secp256k1;
};

type satoshi = nat64;

type bitcoin_network = variant {
    mainnet;
    testnet;
};

type bitcoin_address = text;

type block_hash = blob;

type outpoint = record {
    txid : blob;
    vout : nat32;
};

type utxo = record {
    outpoint : outpoint;
    value : satoshi;
    height : nat32;
};

type bitcoin_get_utxos_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    filter : opt variant {
        min_confirmations : nat32;
        page : blob;
    };
};

type bitcoin_get_utxos_query_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    filter : opt variant {
        min_confirmations : nat32;
        page : blob;
    };
};

type bitcoin_get_current_fee_percentiles_args = record {
    network : bitcoin_network;
};

type bitcoin_get_utxos_result = record {
    utxos : vec utxo;
    tip_block_hash : block_hash;
    tip_height : nat32;
    next_page : opt blob;
};

type bitcoin_get_utxos_query_result = record {
    utxos : vec utxo;
    tip_block_hash : block_hash;
    tip_height : nat32;
    next_page : opt blob;
};

type bitcoin_get_balance_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    min_confirmations : opt nat32;
};

type bitcoin_get_balance_query_args = record {
    address : bitcoin_address;
    network : bitcoin_network;
    min_confirmations : opt nat32;
};

type bitcoin_send_transaction_args = record {
    transaction : blob;
    network : bitcoin_network;
};

type millisatoshi_per_byte = nat64;

type node_metrics = record {
    node_id : principal;
    num_blocks_total : nat64;
    num_block_failures_total : nat64;
};

type create_canister_args = record {
    settings : opt canister_settings;
    sender_canister_version : opt nat64;
};

type create_canister_result = record {
    canister_id : canister_id;
};

type update_settings_args = record {
    canister_id : principal;
    settings : canister_settings;
    sender_canister_version : opt nat64;
};

type upload_chunk_args = record {
    canister_id : principal;
    chunk : blob;
};

type clear_chunk_store_args = record {
    canister_id : canister_id;
};

type stored_chunks_args = record {
    canister_id : canister_id;
};

type canister_install_mode = variant {
    install;
    reinstall;
    upgrade : opt record {
        skip_pre_upgrade : opt bool;
    };
};

type install_code_args = record {
    mode : canister_install_mode;
    canister_id : canister_id;
    wasm_module : wasm_module;
    arg : blob;
    sender_canister_version : opt nat64;
};

type install_chunked_code_args = record {
    mode : canister_install_mode;
    target_canister : canister_id;
    store_canister : opt canister_id;
    chunk_hashes_list : vec chunk_hash;
    wasm_module_hash : blob;
    arg : blob;
    sender_canister_version : opt nat64;
};

type uninstall_code_args = record {
    canister_id : canister_id;
    sender_canister_version : opt nat64;
};

type start_canister_args = record {
    canister_id : canister_id;
};

type stop_canister_args = record {
    canister_id : canister_id;
};

type canister_status_args = record {
    canister_id : canister_id;
};

type canister_status_result = record {
    status : variant { running; stopping; stopped };
    settings : definite_canister_settings;
    module_hash : opt blob;
    memory_size : nat;
    cycles : nat;
    reserved_cycles : nat;
    idle_cycles_burned_per_day : nat;
};

type canister_info_args = record {
    canister_id : canister_id;
    num_requested_changes : opt nat64;
};

type canister_info_result = record {
    total_num_changes : nat64;
    recent_changes : vec change;
    module_hash : opt blob;
    controllers : vec principal;
};

type delete_canister_args = record {
    canister_id : canister_id;
};

type deposit_cycles_args = record {
    canister_id : canister_id;
};

type http_request_args = record {
    url : text;
    max_response_bytes : opt nat64;
    method : variant { get; head; post };
    headers : vec http_header;
    body : opt blob;
    transform : opt record {
        function : func(record { response : http_request_result; context : blob }) -> (http_request_result) query;
        context : blob;
    };
};

type ecdsa_public_key_args = record {
    canister_id : opt canister_id;
    derivation_path : vec blob;
    key_id : record { curve : ecdsa_curve; name : text };
};

type ecdsa_public_key_result = record {
    public_key : blob;
    chain_code : blob;
};

type sign_with_ecdsa_args = record {
    message_hash : blob;
    derivation_path : vec blob;
    key_id : record { curve : ecdsa_curve; name : text };
};

type sign_with_ecdsa_result = record {
    signature : blob;
};

type node_metrics_history_args = record {
    subnet_id : principal;
    start_at_timestamp_nanos : nat64;
};

type node_metrics_history_result = vec record {
    timestamp_nanos : nat64;
    node_metrics : vec node_metrics;
};

type provisional_create_canister_with_cycles_args = record {
    amount : opt nat;
    settings : opt canister_settings;
    specified_id : opt canister_id;
    sender_canister_version : opt nat64;
};

type provisional_create_canister_with_cycles_result = record {
    canister_id : canister_id;
};

type provisional_top_up_canister_args = record {
    canister_id : canister_id;
    amount : nat;
};

type raw_rand_result = blob;

type stored_chunks_result = vec chunk_hash;

type upload_chunk_result = chunk_hash;

type bitcoin_get_balance_result = satoshi;

type bitcoin_get_balance_query_result = satoshi;

type bitcoin_get_current_fee_percentiles_result = vec millisatoshi_per_byte;

service ic : {
    create_canister : (create_canister_args) -> (create_canister_result);
    update_settings : (update_settings_args) -> ();
    upload_chunk : (upload_chunk_args) -> (upload_chunk_result);
    clear_chunk_store : (clear_chunk_store_args) -> ();
    stored_chunks : (stored_chunks_args) -> (stored_chunks_result);
    install_code : (install_code_args) -> ();
    install_chunked_code : (install_chunked_code_args) -> ();
    uninstall_code : (uninstall_code_args) -> ();
    start_canister : (start_canister_args) -> ();
    stop_canister : (stop_canister_args) -> ();
    canister_status : (canister_status_args) -> (canister_status_result);
    canister_info : (canister_info_args) -> (canister_info_result);
    delete_canister : (delete_canister_args) -> ();
    deposit_cycles : (deposit_cycles_args) -> ();
    raw_rand : () -> (raw_rand_result);
    http_request : (http_request_args) -> (http_request_result);

    // Threshold ECDSA signature
    ecdsa_public_key : (ecdsa_public_key_args) -> (ecdsa_public_key_result);
    sign_with_ecdsa : (sign_with_ecdsa_args) -> (sign_with_ecdsa_result);

    // bitcoin interface
    bitcoin_get_balance : (bitcoin_get_balance_args) -> (bitcoin_get_balance_result);
    bitcoin_get_balance_query : (bitcoin_get_balance_query_args) -> (bitcoin_get_balance_query_result) query;
    bitcoin_get_utxos : (bitcoin_get_utxos_args) -> (bitcoin_get_utxos_result);
    bitcoin_get_utxos_query : (bitcoin_get_utxos_query_args) -> (bitcoin_get_utxos_query_result) query;
    bitcoin_send_transaction : (bitcoin_send_transaction_args) -> ();
    bitcoin_get_current_fee_percentiles : (bitcoin_get_current_fee_percentiles_args) -> (bitcoin_get_current_fee_percentiles_result);

    // metrics interface
    node_metrics_history : (node_metrics_history_args) -> (node_metrics_history_result);

    // provisional interfaces for the pre-ledger world
    provisional_create_canister_with_cycles : (provisional_create_canister_with_cycles_args) -> (provisional_create_canister_with_cycles_result);
    provisional_top_up_canister : (provisional_top_up_canister_args) -> ();
};
"##.to_string())
}


-----------------------

/src/dfx/src/util/clap/argument_from_cli.rs:
-----------------------

/src/dfx/src/util/clap/install_mode.rs:
-----------------------

use anyhow::bail;
use clap::Args;
use ic_utils::interfaces::management_canister::builders::{
    CanisterUpgradeOptions, InstallMode, WasmMemoryPersistence,
};

use crate::lib::error::DfxResult;

/// CLI options for the mode of canister installation.
///
/// Reused in `dfx canister install` and `dfx deploy`.
#[derive(Args, Clone, Debug, Default)]
pub struct InstallModeOpt {
    /// Specifies the mode of canister installation.
    ///
    /// If set to 'auto', either 'install' or 'upgrade' will be used, depending on whether the canister is already installed.
    #[arg(long, short, value_parser = ["install", "reinstall", "upgrade", "auto"])]
    mode: Option<String>,

    /// Skip the pre_upgrade hook on upgrade.
    ///
    /// This requires the mode to be set to 'upgrade' or 'auto'.
    #[arg(long)]
    skip_pre_upgrade: bool,

    /// Keep or replace the Wasm main memory on upgrade.
    ///
    /// This requires the mode to be set to 'upgrade' or 'auto'.
    #[arg(long, value_parser = ["keep", "replace"])]
    wasm_memory_persistence: Option<String>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum InstallModeHint {
    Install,
    Reinstall,
    Upgrade(Option<CanisterUpgradeOptions>),
    Auto(Option<CanisterUpgradeOptions>),
}

enum HighLevelMode {
    Install,
    Reinstall,
    Upgrade,
    Auto,
}

impl InstallModeOpt {
    /// `dfx canister install` defaults to 'install' mode.
    pub fn mode_for_canister_install(&self) -> DfxResult<InstallModeHint> {
        self.resolve_install_mode(HighLevelMode::Install)
    }

    /// `dfx deploy` defaults to 'auto' mode.
    pub fn mode_for_deploy(&self) -> DfxResult<InstallModeHint> {
        self.resolve_install_mode(HighLevelMode::Auto)
    }

    fn resolve_install_mode(&self, default_mode: HighLevelMode) -> DfxResult<InstallModeHint> {
        let wasm_memory_persistence = match self.wasm_memory_persistence {
            Some(ref s) => match s.as_str() {
                "keep" => Some(WasmMemoryPersistence::Keep),
                "replace" => Some(WasmMemoryPersistence::Replace),
                _ => unreachable!(),
            },
            None => None,
        };
        let canister_upgrade_options = match (self.skip_pre_upgrade, wasm_memory_persistence) {
            (false, None) => None,
            (s, w) => Some(CanisterUpgradeOptions {
                skip_pre_upgrade: Some(s),
                wasm_memory_persistence: w,
            }),
        };

        let high_level_mode = match self.mode.as_deref() {
            Some("install") => HighLevelMode::Install,
            Some("reinstall") => HighLevelMode::Reinstall,
            Some("upgrade") => HighLevelMode::Upgrade,
            Some("auto") => HighLevelMode::Auto,
            None => default_mode,
            _ => unreachable!(),
        };

        if canister_upgrade_options.is_some()
            && matches!(
                high_level_mode,
                HighLevelMode::Install | HighLevelMode::Reinstall
            )
        {
            bail!("--skip-pre-upgrade and --wasm-memory-persistence can only be used with mode 'upgrade' or 'auto'.");
        }
        match high_level_mode {
            HighLevelMode::Install => Ok(InstallModeHint::Install),
            HighLevelMode::Reinstall => Ok(InstallModeHint::Reinstall),
            HighLevelMode::Upgrade => Ok(InstallModeHint::Upgrade(canister_upgrade_options)),
            HighLevelMode::Auto => Ok(InstallModeHint::Auto(canister_upgrade_options)),
        }
    }
}

impl InstallModeHint {
    pub fn to_install_mode_with_wasm_path(&self) -> DfxResult<InstallMode> {
        match self {
            InstallModeHint::Install => Ok(InstallMode::Install),
            InstallModeHint::Reinstall => Ok(InstallMode::Reinstall),
            InstallModeHint::Upgrade(opt) => Ok(InstallMode::Upgrade(*opt)),
            InstallModeHint::Auto(_) => bail!("The install mode cannot be auto when using --wasm"),
        }
    }

    pub fn to_install_mode(
        &self,
        upgrade_in_auto: bool,
        wasm_memory_persistence_embeded: Option<WasmMemoryPersistence>,
    ) -> InstallMode {
        match self {
            InstallModeHint::Install => InstallMode::Install,
            InstallModeHint::Reinstall => InstallMode::Reinstall,
            InstallModeHint::Upgrade(opt) => InstallMode::Upgrade(*opt),
            InstallModeHint::Auto(opt) => {
                let opt = if opt.is_none() && wasm_memory_persistence_embeded.is_some() {
                    Some(CanisterUpgradeOptions {
                        skip_pre_upgrade: None,
                        wasm_memory_persistence: wasm_memory_persistence_embeded,
                    })
                } else {
                    *opt
                };
                match upgrade_in_auto {
                    true => InstallMode::Upgrade(opt),
                    false => InstallMode::Install,
                }
            }
        }
    }
}


-----------------------

/src/dfx/src/util/clap/mod.rs:
-----------------------

use anstyle::{AnsiColor, Style};
use clap::builder::Styles;

pub mod argument_from_cli;
pub mod install_mode;
pub mod parsers;
pub mod subnet_selection_opt;

pub fn style() -> Styles {
    let green = Style::new().fg_color(Some(AnsiColor::Green.into()));
    let yellow = Style::new().fg_color(Some(AnsiColor::Yellow.into()));
    let red = Style::new()
        .fg_color(Some(AnsiColor::BrightRed.into()))
        .bold();
    Styles::styled()
        .literal(green)
        .placeholder(green)
        .error(red)
        .header(yellow)
        .invalid(yellow)
        .valid(green)
}


-----------------------

/src/dfx/src/util/clap/parsers.rs:
-----------------------

use byte_unit::{Byte, ByteUnit};
use candid::Principal;
use ic_utils::interfaces::management_canister::LogVisibility;
use icrc_ledger_types::icrc1::account::Subaccount;
use rust_decimal::Decimal;
use std::{path::PathBuf, str::FromStr};

/// Removes `_`, interprets `k`, `m`, `b`, `t` suffix (case-insensitive)
fn decimal_with_suffix_parser(input: &str) -> Result<Decimal, String> {
    let input = input.replace('_', "").to_lowercase();
    let (number, suffix) = if input
        .chars()
        .last()
        .map(|char| char.is_alphabetic())
        .unwrap_or(false)
    {
        input.split_at(input.len() - 1)
    } else {
        (input.as_str(), "")
    };
    let multiplier: u64 = match suffix {
        "" => Ok(1),
        "k" => Ok(1_000),
        "m" => Ok(1_000_000),
        "b" => Ok(1_000_000_000),
        "t" => Ok(1_000_000_000_000),
        other => Err(format!("Unknown amount specifier: '{}'", other)),
    }?;
    let number = Decimal::from_str(number).map_err(|err| err.to_string())?;
    Decimal::from(multiplier)
        .checked_mul(number)
        .ok_or_else(|| "Amount too large.".to_string())
}

pub fn request_id_parser(v: &str) -> Result<String, String> {
    // A valid Request Id starts with `0x` and is a series of 64 hexadecimals.
    if !v.starts_with("0x") {
        Err(String::from("A Request ID needs to start with 0x."))
    } else if v.len() != 66 {
        Err(String::from(
            "A Request ID is 64 hexadecimal prefixed with 0x.",
        ))
    } else if v[2..].contains(|c: char| !c.is_ascii_hexdigit()) {
        Err(String::from(
            "A Request ID is 64 hexadecimal prefixed with 0x. An invalid character was found.",
        ))
    } else {
        Ok(v.into())
    }
}

pub fn e8s_parser(input: &str) -> Result<u64, String> {
    decimal_with_suffix_parser(input)?
        .try_into()
        .map_err(|_| "Must specify a non-negative whole number.".to_string())
}

pub fn memo_parser(memo: &str) -> Result<u64, String> {
    memo.parse::<u64>()
        .map_err(|_| "Must specify a non negative whole number.".to_string())
}

pub fn cycle_amount_parser(input: &str) -> Result<u128, String> {
    let removed_cycle_suffix = if input.to_lowercase().ends_with('c') {
        &input[..input.len() - 1]
    } else {
        input
    };

    decimal_with_suffix_parser(removed_cycle_suffix)?.try_into().map_err(|_| "Failed to parse amount. Please use digits only or something like 3.5TC, 2t, or 5_000_000.".to_string())
}

pub fn file_parser(path: &str) -> Result<PathBuf, String> {
    let path = PathBuf::from(path);
    if path.exists() {
        Ok(path)
    } else {
        Err("Path does not exist or is not a file.".to_string())
    }
}

pub fn file_or_stdin_parser(path: &str) -> Result<PathBuf, String> {
    if path == "-" {
        // represents stdin
        Ok(PathBuf::from(path))
    } else {
        file_parser(path)
    }
}

pub fn trillion_cycle_amount_parser(input: &str) -> Result<u128, String> {
    if let Ok(cycles) = format!("{}000000000000", input.replace('_', "")).parse::<u128>() {
        Ok(cycles)
    } else {
        decimal_with_suffix_parser(input)?
            .checked_mul(1_000_000_000_000_u64.into())
            .and_then(|total| total.try_into().ok())
            .ok_or_else(|| "Amount too large.".to_string())
    }
}

pub fn compute_allocation_parser(compute_allocation: &str) -> Result<u64, String> {
    if let Ok(num) = compute_allocation.parse::<u64>() {
        if num <= 100 {
            return Ok(num);
        }
    }
    Err("Must be a percent between 0 and 100".to_string())
}

pub fn memory_allocation_parser(memory_allocation: &str) -> Result<Byte, String> {
    // This limit should track MAX_MEMORY_ALLOCATION
    // at https://gitlab.com/dfinity-lab/core/ic/-/blob/master/rs/types/types/src/lib.rs#L492
    let limit = Byte::from_unit(12., ByteUnit::GiB).expect("Parse Overflow.");
    if let Ok(bytes) = memory_allocation.parse::<Byte>() {
        if bytes <= limit {
            return Ok(bytes);
        }
    }
    Err("Must be a value between 0..12 GiB inclusive.".to_string())
}

pub fn wasm_memory_limit_parser(memory_limit: &str) -> Result<Byte, String> {
    let limit = Byte::from_unit(256., ByteUnit::TiB).expect("Parse Overflow.");
    if let Ok(bytes) = memory_limit.parse::<Byte>() {
        if bytes <= limit {
            return Ok(bytes);
        }
    }
    Err("Must be a value between 0..256 TiB inclusive (e.g. `2GiB`).".to_string())
}

pub fn log_visibility_parser(log_visibility: &str) -> Result<LogVisibility, String> {
    match log_visibility {
        "public" => Ok(LogVisibility::Public),
        "controllers" => Ok(LogVisibility::Controllers),
        _ => Err("Must be `controllers` or `public`.".to_string()),
    }
}

pub fn principal_parser(principal_text: &str) -> Result<Principal, String> {
    match Principal::from_text(principal_text) {
        Ok(principal) => Ok(principal),
        _ => Err(("Failed to convert to a principal.").to_string()),
    }
}

pub fn freezing_threshold_parser(freezing_threshold: &str) -> Result<u64, String> {
    freezing_threshold
        .parse::<u64>()
        .map_err(|_| "Must be a value between 0 and 2^64-1 inclusive".to_string())
}

pub fn reserved_cycles_limit_parser(reserved_cycles_limit: &str) -> Result<u128, String> {
    reserved_cycles_limit
        .parse::<u128>()
        .map_err(|_| "Must be a value between 0 and 2^128-1 inclusive".to_string())
}

/// Validate a String can be a valid project name.
/// A project name is valid if it starts with a letter, and is alphanumeric (with hyphens).
/// It cannot end with a dash.
pub fn project_name_parser(name: &str) -> Result<String, String> {
    let mut chars = name.chars();
    // Check first character first. If there's no first character it's empty.
    if let Some(first) = chars.next() {
        if first.is_ascii_alphabetic() {
            // Then check all other characters.
            // Reverses the search here; if there is a character that is not compatible
            // it is found and an error is returned.
            let m: Vec<&str> = name
                .matches(|x: char| !x.is_ascii_alphanumeric() && x != '_' && x != '-')
                .collect();

            if m.is_empty() {
                Ok(name.to_string())
            } else {
                Err(format!(
                    r#"Invalid character(s): "{}""#,
                    m.iter().fold(String::new(), |acc, &num| acc + num)
                ))
            }
        } else {
            Err("Must start with a letter.".to_owned())
        }
    } else {
        Err("Cannot be empty.".to_owned())
    }
}

pub fn icrc_subaccount_parser(subaccount: &str) -> Result<Subaccount, String> {
    if let Ok(Ok(subaccount)) = hex::decode(subaccount).map(|bytes| bytes.try_into()) {
        return Ok(subaccount);
    }

    Err("Failed to parse subaccount. Expected 32 bytes of hex-encoded data.".to_string())
}

pub fn hsm_key_id_parser(key_id: &str) -> Result<String, String> {
    if key_id.len() % 2 != 0 {
        Err("Key id must consist of an even number of hex digits".to_string())
    } else if key_id.contains(|c: char| !c.is_ascii_hexdigit()) {
        Err("Key id must contain only hex digits".to_string())
    } else {
        Ok(key_id.to_string())
    }
}

#[test]
fn test_cycle_amount_parser() {
    assert_eq!(cycle_amount_parser("900c"), Ok(900));
    assert_eq!(cycle_amount_parser("9_887K"), Ok(9_887_000));
    assert_eq!(cycle_amount_parser("0.1M"), Ok(100_000));
    assert_eq!(cycle_amount_parser("0.01b"), Ok(10_000_000));
    assert_eq!(cycle_amount_parser("10T"), Ok(10_000_000_000_000));
    assert_eq!(cycle_amount_parser("10TC"), Ok(10_000_000_000_000));
    assert_eq!(cycle_amount_parser("1.23t"), Ok(1_230_000_000_000));

    assert!(cycle_amount_parser("1ffff").is_err());
    assert!(cycle_amount_parser("1MT").is_err());
    assert!(cycle_amount_parser("-0.1m").is_err());
    assert!(cycle_amount_parser("T100").is_err());
    assert!(cycle_amount_parser("1.1k0").is_err());
    assert!(cycle_amount_parser(&format!("{}0", u128::MAX)).is_err());
}

#[test]
fn test_trillion_cycle_amount_parser() {
    const TRILLION: u128 = 1_000_000_000_000;
    assert_eq!(trillion_cycle_amount_parser("3"), Ok(3 * TRILLION));
    assert_eq!(trillion_cycle_amount_parser("5_555"), Ok(5_555 * TRILLION));
    assert_eq!(trillion_cycle_amount_parser("1k"), Ok(1_000 * TRILLION));
    assert_eq!(trillion_cycle_amount_parser("0.3"), Ok(300_000_000_000));
    assert_eq!(trillion_cycle_amount_parser("0.3k"), Ok(300 * TRILLION));

    assert!(trillion_cycle_amount_parser("-0.1m").is_err());
    assert!(trillion_cycle_amount_parser("1TC").is_err()); // ambiguous in combination with --t
}

#[test]
fn test_e8s_parser() {
    assert_eq!(e8s_parser("1"), Ok(1));
    assert_eq!(e8s_parser("1_000"), Ok(1_000));
    assert_eq!(e8s_parser("1k"), Ok(1_000));
    assert_eq!(e8s_parser("1M"), Ok(1_000_000));
}


-----------------------

/src/dfx/src/util/clap/subnet_selection_opt.rs:
-----------------------

use anyhow::bail;
use candid::Principal;
use clap::{ArgGroup, Args};
use fn_error_context::context;

use crate::lib::{
    cycles_ledger_types::create_canister::{SubnetFilter, SubnetSelection},
    environment::Environment,
    error::DfxResult,
    named_canister::UI_CANISTER,
    subnet::get_subnet_for_canister,
};

#[derive(Args, Clone, Debug, Default)]
#[clap(
group(ArgGroup::new("subnet-selection").multiple(false)),
)]
pub struct SubnetSelectionOpt {
    /// Specify the optional subnet type to create canisters on. If no
    /// subnet type is provided, the canister will be created on a random
    /// default application subnet.
    #[arg(long, group = "subnet-selection")]
    subnet_type: Option<String>,

    /// Specify a specific subnet on which to create canisters on.
    #[arg(long, group = "subnet-selection")]
    subnet: Option<Principal>,

    /// Create canisters on the same subnet as this canister.
    #[arg(long, group = "subnet-selection")]
    next_to: Option<String>,
}

impl SubnetSelectionOpt {
    pub async fn into_subnet_selection_type(
        self,
        env: &dyn Environment,
    ) -> DfxResult<SubnetSelectionType> {
        if let Some(sibling) = self.next_to {
            let next_to = Principal::from_text(&sibling)
                .or_else(|_| env.get_canister_id_store()?.get(&sibling))?;
            let subnet = get_subnet_for_canister(env.get_agent(), next_to).await?;
            Ok(SubnetSelectionType::Explicit {
                user_choice: SubnetSelection::Subnet { subnet },
            })
        } else if let Some(subnet_type) = self.subnet_type {
            Ok(SubnetSelectionType::Explicit {
                user_choice: SubnetSelection::Filter(SubnetFilter {
                    subnet_type: Some(subnet_type),
                }),
            })
        } else if let Some(subnet) = self.subnet {
            Ok(SubnetSelectionType::Explicit {
                user_choice: SubnetSelection::Subnet { subnet },
            })
        } else {
            Ok(SubnetSelectionType::Automatic {
                selected_subnet: None,
            })
        }
    }
}

#[derive(Debug, Clone)]
pub enum SubnetSelectionType {
    Automatic {
        selected_subnet: Option<SubnetSelection>,
    },
    Explicit {
        user_choice: SubnetSelection,
    },
}

impl Default for SubnetSelectionType {
    fn default() -> Self {
        Self::Automatic {
            selected_subnet: None,
        }
    }
}

impl SubnetSelectionType {
    pub fn get_user_choice(&self) -> Option<SubnetSelection> {
        match self {
            SubnetSelectionType::Explicit { user_choice } => Some(user_choice.clone()),
            _ => None,
        }
    }

    #[context("Failed to figure out subnet to create canister on.")]
    pub async fn resolve(&mut self, env: &dyn Environment) -> DfxResult<Option<SubnetSelection>> {
        if matches!(
            self,
            SubnetSelectionType::Automatic {
                selected_subnet: None
            }
        ) {
            self.resolve_automatic(env).await?;
        }

        match self {
            SubnetSelectionType::Explicit { user_choice } => Ok(Some(user_choice.clone())),
            SubnetSelectionType::Automatic { selected_subnet } => Ok(selected_subnet.clone()),
        }
    }

    pub async fn resolve_automatic(&mut self, env: &dyn Environment) -> DfxResult<()> {
        let canisters = env.get_canister_id_store()?.non_remote_user_canisters();
        let subnets: Vec<_> = futures::future::try_join_all(
            canisters
                .into_iter()
                .filter(|(name, _)| name != UI_CANISTER)
                .map(|(_, canister)| get_subnet_for_canister(env.get_agent(), canister)),
        )
        .await?;

        let mut selected_subnet = None;
        for next_subnet in subnets.into_iter() {
            match selected_subnet {
                None => selected_subnet = Some(next_subnet),
                Some(selected_subnet) if selected_subnet == next_subnet => continue,
                Some(_nonmatching_subnet) => {
                    bail!("Cannot automatically decide which subnet to target. Please explicitly specify --subnet or --subnet-type.")
                }
            }
        }

        if let Some(subnet) = selected_subnet {
            *self = Self::Automatic {
                selected_subnet: Some(SubnetSelection::Subnet { subnet }),
            }
        }
        Ok(())
    }
}


-----------------------

/src/dfx/src/util/command.rs:
-----------------------

use crate::lib::error::DfxResult;
use anyhow::Context;
use std::path::Path;
use std::process::Command;

pub fn direct_or_shell_command(s: &str, cwd: &Path) -> DfxResult<Command> {
    let words = shell_words::split(s).with_context(|| format!("Cannot parse command '{}'.", s))?;
    let canonical_result = dfx_core::fs::canonicalize(&cwd.join(&words[0]));
    let mut cmd = if words.len() == 1 && canonical_result.is_ok() {
        // If the command is a file, execute it directly.
        let file = canonical_result.unwrap();
        Command::new(file)
    } else {
        // Execute the command in `sh -c` to allow pipes.
        let mut sh_cmd = Command::new("sh");
        sh_cmd.args(["-c", s]);
        sh_cmd
    };
    cmd.current_dir(cwd);
    Ok(cmd)
}


-----------------------

/src/dfx/src/util/currency_conversion.rs:
-----------------------

use crate::lib::{
    error::DfxResult, nns_types::icpts::ICPTs, operations::ledger::xdr_permyriad_per_icp,
};
use anyhow::Context;
use dfx_core::config::model::dfinity::DEFAULT_IC_GATEWAY;
use fn_error_context::context;
use ic_agent::Agent;
use rust_decimal::Decimal;

/// How many cycles you get per XDR when converting ICP to cycles
const CYCLES_PER_XDR: u128 = 1_000_000_000_000;

/// This returns how many cycles the amount of ICP/e8s is currently worth.
/// Fetches the exchange rate from the (hardcoded) IC network.
#[context("Encountered a problem while fetching the exchange rate between ICP and cycles. If this issue continues to happen, please specify an amount in cycles directly.")]
pub async fn as_cycles_with_current_exchange_rate(icpts: &ICPTs) -> DfxResult<u128> {
    let agent = Agent::builder()
        .with_url(DEFAULT_IC_GATEWAY)
        .build()
        .context("Cannot create mainnet agent.")?;
    let xdr_permyriad_per_icp = xdr_permyriad_per_icp(&agent).await?;
    let xdr_per_icp = Decimal::from_i128_with_scale(xdr_permyriad_per_icp as i128, 4);
    let xdr = xdr_per_icp * icpts.to_decimal();
    let cycles = xdr * Decimal::from(CYCLES_PER_XDR);
    Ok(u128::try_from(cycles)?)
}


-----------------------

/src/dfx/src/util/mod.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::{error_invalid_argument, error_invalid_data, error_unknown};
use anyhow::{anyhow, bail, Context};
use backoff::backoff::Backoff;
use backoff::ExponentialBackoff;
use bytes::Bytes;
use candid::types::{value::IDLValue, Function, Type, TypeEnv, TypeInner};
use candid::{Decode, Encode, IDLArgs, Principal};
use candid_parser::error::pretty_wrap;
use candid_parser::utils::CandidSource;
use dfx_core::fs::create_dir_all;
use fn_error_context::context;
use idl2json::{idl2json, Idl2JsonOptions};
use num_traits::FromPrimitive;
use reqwest::{Client, StatusCode, Url};
use rust_decimal::Decimal;
use socket2::{Domain, Socket};
use std::collections::BTreeMap;
use std::io::{stderr, stdin, stdout, IsTerminal, Read};
use std::net::{IpAddr, SocketAddr, TcpListener};
use std::path::Path;
use std::time::Duration;

pub mod assets;
pub mod clap;
pub mod command;
pub mod currency_conversion;
pub mod stderr_wrapper;
pub mod url;

const DECIMAL_POINT: char = '.';

// The user can pass in port "0" to dfx start i.e. "127.0.0.1:0" or "[::1]:0",
// thus, we need to recreate SocketAddr with the kernel-provided dynamically allocated port here.
#[context("Failed to find available socket address")]
pub fn get_reusable_socket_addr(ip: IpAddr, port: u16) -> DfxResult<SocketAddr> {
    let socket = if ip.is_ipv4() {
        Socket::new(Domain::IPV4, socket2::Type::STREAM, None)
            .context("Failed to create IPv4 socket.")?
    } else {
        Socket::new(Domain::IPV6, socket2::Type::STREAM, None)
            .context("Failed to create IPv6 socket.")?
    };
    socket
        .set_linger(Some(Duration::from_secs(10)))
        .context("Failed to set linger duration of tcp listener.")?;
    socket
        .bind(&SocketAddr::new(ip, port).into())
        .with_context(|| format!("Failed to bind socket to {}:{}.", ip, port))?;
    socket.listen(128).context("Failed to listen on socket.")?;

    let listener: TcpListener = socket.into();
    listener
        .local_addr()
        .context("Failed to fetch local address.")
}

/// Deserialize and print return values from canister method.
#[context("Failed to deserialize idl blob: Invalid data.")]
pub fn print_idl_blob(
    blob: &[u8],
    output_type: Option<&str>,
    method_type: &Option<(TypeEnv, Function)>,
) -> DfxResult<()> {
    let output_type = output_type.unwrap_or("pp");
    match output_type {
        "raw" => {
            let hex_string = hex::encode(blob);
            println!("{}", hex_string);
        }
        "idl" | "pp" | "json" => {
            let result = match method_type {
                None => candid::IDLArgs::from_bytes(blob),
                Some((env, func)) => candid::IDLArgs::from_bytes_with_types(blob, env, &func.rets),
            };
            if result.is_err() {
                let hex_string = hex::encode(blob);
                eprintln!("Error deserializing blob 0x{}", hex_string);
            }
            if output_type == "idl" {
                println!("{:?}", result?);
            } else if output_type == "json" {
                let json = convert_all(&result?)?;
                println!("{}", json);
            } else {
                println!("{}", result?);
            }
        }
        v => return Err(error_unknown!("Invalid output type: {}", v)),
    }
    Ok(())
}

/// Candid typically comes as a tuple of values.  This converts a single value in such a tuple.
fn convert_one(idl_value: &IDLValue) -> DfxResult<String> {
    let json_value = idl2json(idl_value, &Idl2JsonOptions::default());
    serde_json::to_string_pretty(&json_value).with_context(|| anyhow!("Cannot pretty-print json"))
}

fn convert_all(idl_args: &IDLArgs) -> DfxResult<String> {
    let json_structures = idl_args
        .args
        .iter()
        .map(convert_one)
        .collect::<Result<Vec<_>, _>>()?;
    Ok(json_structures.join("\n"))
}

pub async fn read_module_metadata(
    agent: &ic_agent::Agent,
    canister_id: candid::Principal,
    metadata: &str,
) -> Option<String> {
    Some(
        String::from_utf8_lossy(
            &agent
                .read_state_canister_metadata(canister_id, metadata)
                .await
                .ok()?,
        )
        .into(),
    )
}

pub async fn fetch_remote_did_file(
    agent: &ic_agent::Agent,
    canister_id: Principal,
) -> Option<String> {
    Some(
        match read_module_metadata(agent, canister_id, "candid:service").await {
            Some(candid) => candid,
            None => {
                let bytes = agent
                    .query(&canister_id, "__get_candid_interface_tmp_hack")
                    .with_arg(Encode!().ok()?)
                    .call()
                    .await
                    .ok()?;
                Decode!(&bytes, String).ok()?
            }
        },
    )
}

/// Parse IDL file into TypeEnv. This is a best effort function: it will succeed if
/// the IDL file can be parsed and type checked in Rust parser, and has an
/// actor in the IDL file. If anything fails, it returns None.
pub fn get_candid_type(candid: CandidSource, method_name: &str) -> Option<(TypeEnv, Function)> {
    let (env, ty) = candid.load().ok()?;
    let actor = ty?;
    let method = env.get_method(&actor, method_name).ok()?.clone();
    Some((env, method))
}

pub fn get_candid_init_type(idl_path: &std::path::Path) -> Option<(TypeEnv, Function)> {
    let (env, ty) = CandidSource::File(idl_path).load().ok()?;
    let actor = ty?;
    let args = match actor.as_ref() {
        TypeInner::Class(args, _) => args.clone(),
        _ => vec![],
    };
    let res = Function {
        args,
        rets: vec![],
        modes: vec![],
    };
    Some((env, res))
}

pub fn arguments_from_file(file_name: &Path) -> DfxResult<String> {
    if file_name == Path::new("-") {
        let mut content = String::new();
        stdin().read_to_string(&mut content).map_err(|e| {
            error_invalid_argument!("Could not read arguments from stdin to string: {}", e)
        })?;
        Ok(content)
    } else {
        std::fs::read_to_string(file_name)
            .map_err(|e| error_invalid_argument!("Could not read arguments file to string: {}", e))
    }
}

#[context("Failed to create argument blob.")]
pub fn blob_from_arguments(
    dfx_env: Option<&dyn Environment>,
    arguments: Option<&str>,
    random: Option<&str>,
    arg_type: Option<&str>,
    method_type: &Option<(TypeEnv, Function)>,
    is_init_arg: bool,
    always_assist: bool,
) -> DfxResult<Vec<u8>> {
    let arg_type = arg_type.unwrap_or("idl");
    match arg_type {
        "raw" => {
            let bytes = hex::decode(arguments.unwrap_or("")).map_err(|e| {
                error_invalid_argument!("Argument is not a valid hex string: {}", e)
            })?;
            Ok(bytes)
        }
        "idl" => {
            let typed_args = match method_type {
                None => {
                    let arguments = arguments.unwrap_or("()");
                    pretty_wrap("Candid argument", arguments, candid_parser::parse_idl_args)
                        .map_err(|e| error_invalid_argument!("Invalid Candid values: {}", e))?
                        .to_bytes()
                }
                Some((env, func)) => {
                    let is_terminal =
                        stdin().is_terminal() && stdout().is_terminal() && stderr().is_terminal();
                    if let Some(arguments) = arguments {
                        fuzzy_parse_argument(arguments, env, &func.args)
                    } else if func.args.is_empty() {
                        use candid::Encode;
                        Encode!()
                    } else if func
                        .args
                        .iter()
                        .all(|t| matches!(t.as_ref(), TypeInner::Opt(_)))
                        && !always_assist
                    {
                        // If the user provided no arguments, and if all the expected arguments are
                        // optional, then use null values.
                        let nulls = vec![IDLValue::Null; func.args.len()];
                        let args = IDLArgs::new(&nulls);
                        args.to_bytes_with_types(env, &func.args)
                    } else if let Some(random) = random {
                        let random = if random.is_empty() {
                            eprintln!("Random schema is empty, using any random value instead.");
                            "{=}"
                        } else {
                            random
                        };
                        use rand::Rng;
                        let mut rng = rand::thread_rng();
                        let seed: Vec<u8> = (0..2048).map(|_| rng.gen::<u8>()).collect();
                        let config = candid_parser::configs::Configs::from_dhall(random)
                            .context("Failed to create candid parser config.")?;
                        let args = candid_parser::random::any(&seed, &config, env, &func.args)
                            .context("Failed to create idl args.")?;
                        eprintln!("Sending the following random argument:\n{}\n", args);
                        args.to_bytes_with_types(env, &func.args)
                    } else if is_terminal {
                        use candid_parser::assist::{input_args, Context};
                        let mut ctx = Context::new(env.clone());
                        if let Some(env) = dfx_env {
                            let principals = gather_principals_from_env(env);
                            if !principals.is_empty() {
                                let mut map = BTreeMap::new();
                                map.insert("principal".to_string(), principals);
                                ctx.set_completion(map);
                            }
                        }
                        if is_init_arg {
                            eprintln!("This canister requires an initialization argument.");
                        } else {
                            eprintln!("This method requires arguments.");
                        }
                        let args = input_args(&ctx, &func.args)?;
                        eprintln!("Sending the following argument:\n{}\n", args);
                        if is_init_arg {
                            eprintln!(
                                "Do you want to initialize the canister with this argument? [y/N]"
                            );
                        } else {
                            eprintln!("Do you want to send this message? [y/N]");
                        }
                        let mut input = String::new();
                        stdin().read_line(&mut input)?;
                        if !["y", "Y", "yes", "Yes", "YES"].contains(&input.trim()) {
                            return Err(error_invalid_data!("User cancelled."));
                        }
                        args.to_bytes_with_types(env, &func.args)
                    } else {
                        return Err(error_invalid_data!("Expected arguments but found none."));
                    }
                }
            }
            .map_err(|e| error_invalid_data!("Unable to serialize Candid values: {}", e))?;
            Ok(typed_args)
        }
        v => Err(error_unknown!("Invalid type: {}", v)),
    }
}

pub fn gather_principals_from_env(env: &dyn Environment) -> BTreeMap<String, String> {
    let mut res: BTreeMap<String, String> = BTreeMap::new();
    if let Ok(mgr) = env.new_identity_manager() {
        let logger = env.get_logger();
        let mut map = mgr.get_unencrypted_principal_map(logger);
        res.append(&mut map);
    }
    if let Ok(canisters) = env.get_canister_id_store() {
        let mut canisters = canisters.get_name_id_map();
        res.append(&mut canisters);
    }
    res
}

pub fn fuzzy_parse_argument(
    arg_str: &str,
    env: &TypeEnv,
    types: &[Type],
) -> Result<Vec<u8>, candid::Error> {
    let first_char = arg_str.chars().next();
    let is_candid_format = first_char.map_or(false, |c| c == '(');
    // If parsing fails and method expects a single value, try parsing as IDLValue.
    // If it still fails, and method expects a text type, send arguments as text.
    let args = candid_parser::parse_idl_args(arg_str).or_else(|_| {
        if types.len() == 1 && !is_candid_format {
            let is_quote = first_char.map_or(false, |c| c == '"');
            if &TypeInner::Text == types[0].as_ref() && !is_quote {
                Ok(IDLValue::Text(arg_str.to_string()))
            } else {
                pretty_wrap("Candid argument", arg_str, candid_parser::parse_idl_value)
            }
            .map(|v| IDLArgs::new(&[v]))
        } else {
            pretty_wrap("Candid argument", arg_str, candid_parser::parse_idl_args)
        }
    });
    let bytes = args
        .map_err(|e| error_invalid_argument!("Invalid Candid values: {}", e))?
        .to_bytes_with_types(env, types)
        .map_err(|e| error_invalid_data!("Unable to serialize Candid values: {}", e))?;
    Ok(bytes)
}

pub fn format_as_trillions(amount: u128) -> String {
    const SCALE: u32 = 12; // trillion = 10^12
    const FRACTIONAL_PRECISION: u32 = 3;

    // handling edge case when wallet has more than ~10^29 cycles:
    // ::from_u128() returns None if the value is too big to be handled by rust_decimal,
    // in such case, the integer will be simply divided by 10^(SCALE-FRACTIONAL_PRECISION)
    // and returned as int with manually inserted comma character, therefore sacrificing
    // the fractional precision rounding (which is otherwise provided by rust_decimal)
    if let Some(mut dec) = Decimal::from_u128(amount) {
        // safe to .unwrap(), because .set_scale() throws Error only when
        // precision argument is bigger than 28, in our case it's always 12
        dec.set_scale(SCALE).unwrap();
        dec.round_dp(FRACTIONAL_PRECISION).to_string()
    } else {
        let mut v = (amount / 10u128.pow(SCALE - FRACTIONAL_PRECISION)).to_string();
        v.insert(v.len() - FRACTIONAL_PRECISION as usize, DECIMAL_POINT);
        v
    }
}

pub fn pretty_thousand_separators(num: String) -> String {
    /// formats a number provided as string, by dividing digits into groups of 3 using a delimiter
    /// https://en.wikipedia.org/wiki/Decimal_separator#Digit_grouping

    // 1. walk backwards (reverse string) and return characters until decimal point is seen
    // 2. once decimal point is seen, start counting chars and:
    //   - every third character but not at the end of the string: return (char + delimiter)
    //   - otherwise: return char
    // 3. re-reverse the string
    const GROUP_DELIMITER: char = ',';
    let mut count: u32 = 0;
    let mut seen_decimal_point = false;
    num.chars()
        .rev()
        .enumerate()
        .map(|(idx, c)| {
            if c == DECIMAL_POINT {
                seen_decimal_point = true;
                count += 1;
                c.to_string()
            } else if seen_decimal_point
                && count.rem_euclid(3) == 0
                && count > 0
                && num.len() != idx + 1
            {
                count += 1;
                format!("{}{}", c, GROUP_DELIMITER)
            } else if count == 0 {
                c.to_string()
            } else {
                count += 1;
                c.to_string()
            }
        })
        .collect::<String>()
        .chars()
        .rev()
        .collect::<_>()
}

#[context("Failed to download {} to {}.", from, to.display())]
pub async fn download_file_to_path(from: &Url, to: &Path) -> DfxResult {
    let parent_dir = to.parent().unwrap();
    create_dir_all(parent_dir)?;
    let body = download_file(from).await?;
    dfx_core::fs::write(to, body)?;
    Ok(())
}

#[context("Failed to download from url: {}.", from)]
pub async fn download_file(from: &Url) -> DfxResult<Vec<u8>> {
    let client = reqwest::Client::builder()
        .use_rustls_tls()
        .build()
        .context("Could not create HTTP client.")?;

    let mut retry_policy = ExponentialBackoff {
        // Retry at most 60 seconds
        // When the server responds with errors not "404 not found" (e.g. "500 internal server error"),
        // this function retries indefinitely.
        // We arbitrarily set the maximum elapsed time to 60 seconds to avoid infinite retries.
        // See the test case at the bottom of this file.
        max_elapsed_time: Some(Duration::from_secs(60)),
        ..Default::default()
    };

    let body = loop {
        match attempt_download(&client, from).await {
            Ok(Some(body)) => break body,
            Ok(None) => bail!("Not found: {}", from),
            Err(request_error) => match retry_policy.next_backoff() {
                Some(duration) => tokio::time::sleep(duration).await,
                None => bail!(request_error),
            },
        }
    };

    Ok(body.to_vec())
}

async fn attempt_download(client: &Client, url: &Url) -> DfxResult<Option<Bytes>> {
    let response = client.get(url.clone()).send().await?;

    if response.status() == StatusCode::NOT_FOUND {
        Ok(None)
    } else {
        let body = response.error_for_status()?.bytes().await?;
        Ok(Some(body))
    }
}

#[cfg(test)]
mod tests {
    use super::{download_file, format_as_trillions, pretty_thousand_separators};

    #[test]
    fn prettify_balance_amount() {
        // thousands separator
        assert_eq!("3.456", pretty_thousand_separators("3.456".to_string()));
        assert_eq!("33.456", pretty_thousand_separators("33.456".to_string()));
        assert_eq!("333.456", pretty_thousand_separators("333.456".to_string()));
        assert_eq!(
            "3,333.456",
            pretty_thousand_separators("3333.456".to_string())
        );
        assert_eq!(
            "13,333.456",
            pretty_thousand_separators("13333.456".to_string())
        );
        assert_eq!(
            "313,333.456",
            pretty_thousand_separators("313333.456".to_string())
        );
        assert_eq!(
            "3,313,333.456",
            pretty_thousand_separators("3313333.456".to_string())
        );

        // scaling number
        assert_eq!("0.000", format_as_trillions(0));
        assert_eq!("0.000", format_as_trillions(1234));
        assert_eq!("0.000", format_as_trillions(500000000));
        assert_eq!("0.001", format_as_trillions(500000001));
        assert_eq!("0.168", format_as_trillions(167890100000));
        assert_eq!("1.268", format_as_trillions(1267890100000));
        assert_eq!("12.568", format_as_trillions(12567890100000));
        assert_eq!("1234.568", format_as_trillions(1234567890100000));
        assert_eq!(
            "123456123412.348",
            format_as_trillions(123456123412347890100000)
        );
        assert_eq!(
            "10000000000000000.000",
            format_as_trillions(9999999999999999999999999999)
        );
        assert_eq!(
            "99999999999999999.999",
            format_as_trillions(99999999999999999999999999999)
        );
        assert_eq!(
            "340282366920938463463374607.431",
            format_as_trillions(u128::MAX)
        );

        // combined
        assert_eq!("0.000", pretty_thousand_separators(format_as_trillions(0)));
        assert_eq!(
            "100.000",
            pretty_thousand_separators(format_as_trillions(100000000000000))
        );
        assert_eq!(
            "10,000,000,000.000",
            pretty_thousand_separators(format_as_trillions(10000000000000000000000))
        );
        assert_eq!(
            "340,282,366,920,938,463,463,374,607.431",
            pretty_thousand_separators(format_as_trillions(u128::MAX))
        );
    }

    #[test]
    fn download_file_retry_at_most_60s() {
        let url = reqwest::Url::parse("http://httpbin.org/status/500").unwrap();
        let time0 = std::time::Instant::now();
        let _res = tokio::runtime::Runtime::new()
            .unwrap()
            .block_on(download_file(&url));
        let time1 = std::time::Instant::now();
        assert!(time1 - time0 < std::time::Duration::from_secs(61));
    }
}


-----------------------

/src/dfx/src/util/stderr_wrapper.rs:
-----------------------

use term::{Error, StderrTerminal, Terminal};

/// Produces the standard term::StderrTerminal that can write colors.
/// If there is no such terminal available (such as on Github CI), this produces a stderr-wrapper that skips coloring.
pub fn stderr_wrapper() -> Box<StderrTerminal> {
    term::stderr().unwrap_or_else(|| {
        Box::new(BasicStderr {
            stderr: std::io::stderr(),
        })
    })
}

struct BasicStderr<W> {
    stderr: W,
}

impl<W: std::io::Write> Terminal for BasicStderr<W> {
    type Output = W;
    fn fg(&mut self, _color: term::color::Color) -> term::Result<()> {
        Ok(())
    }

    fn bg(&mut self, _color: term::color::Color) -> term::Result<()> {
        Ok(())
    }

    fn attr(&mut self, _attr: term::Attr) -> term::Result<()> {
        Ok(())
    }

    fn supports_attr(&self, _attr: term::Attr) -> bool {
        true
    }

    fn reset(&mut self) -> term::Result<()> {
        Ok(())
    }

    fn supports_reset(&self) -> bool {
        true
    }

    fn supports_color(&self) -> bool {
        true
    }

    fn cursor_up(&mut self) -> term::Result<()> {
        Err(Error::NotSupported)
    }

    fn delete_line(&mut self) -> term::Result<()> {
        Err(Error::NotSupported)
    }

    fn carriage_return(&mut self) -> term::Result<()> {
        Err(Error::NotSupported)
    }

    fn get_ref(&self) -> &Self::Output {
        &self.stderr
    }

    fn get_mut(&mut self) -> &mut Self::Output {
        &mut self.stderr
    }

    fn into_inner(self) -> Self::Output
    where
        Self: Sized,
    {
        self.stderr
    }
}

impl<W: std::io::Write> std::io::Write for BasicStderr<W> {
    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        self.stderr.write(buf)
    }

    fn flush(&mut self) -> std::io::Result<()> {
        self.stderr.flush()
    }
}


-----------------------

/src/dfx/src/util/url.rs:
-----------------------

use crate::lib::environment::Environment;
use crate::lib::error::DfxResult;
use crate::lib::named_canister::get_ui_canister_url;
use anyhow::Context;
use candid::Principal;
use dfx_core::config::model::network_descriptor::NetworkDescriptor;
use fn_error_context::context;
use std::net::{Ipv4Addr, Ipv6Addr};
use url::Host::{Domain, Ipv4, Ipv6};
use url::Url;

#[context("Failed to construct frontend url for canister {} on network '{}'.", canister_id, network.name)]
pub fn construct_frontend_url(
    network: &NetworkDescriptor,
    canister_id: &Principal,
) -> DfxResult<(Url, Option<Url>)> {
    let mut url = Url::parse(&network.providers[0]).with_context(|| {
        format!(
            "Failed to parse url for network provider {}.",
            &network.providers[0]
        )
    })?;
    // For localhost defined by IP address we suggest `<canister_id>.localhost` as an alternate way of accessing the canister because it plays nicer with SPAs.
    // We still display `<IP>?canisterId=<canister_id>` because Safari does not support localhost subdomains
    let url2 = if url.host() == Some(Ipv4(Ipv4Addr::LOCALHOST))
        || url.host() == Some(Ipv6(Ipv6Addr::LOCALHOST))
    {
        let mut subdomain_url = url.clone();
        let localhost_with_subdomain = format!("{}.localhost", canister_id);
        subdomain_url
            .set_host(Some(&localhost_with_subdomain))
            .with_context(|| format!("Failed to set host to {}.", localhost_with_subdomain))?;
        Some(subdomain_url)
    } else {
        None
    };

    if let Some(Domain(domain)) = url.host() {
        let host = format!("{}.{}", canister_id, domain);
        url.set_host(Some(&host))
            .with_context(|| format!("Failed to set host to {}.", host))?;
    } else {
        let query = format!("canisterId={}", canister_id);
        url.set_query(Some(&query));
    };

    Ok((url, url2))
}

#[context("Failed to construct ui canister url for {} on network '{}'.", canister_id, env.get_network_descriptor().name)]
pub fn construct_ui_canister_url(
    env: &dyn Environment,
    canister_id: &Principal,
) -> DfxResult<Option<Url>> {
    let mut url = get_ui_canister_url(env)?;
    if let Some(base_url) = url.as_mut() {
        let query_with_canister_id = if let Some(query) = base_url.query() {
            format!("{query}&id={canister_id}")
        } else {
            format!("id={canister_id}")
        };
        base_url.set_query(Some(&query_with_canister_id));
    };
    Ok(url)
}


-----------------------

/src/distributed/README.md:
-----------------------


# Building the Wallet Canister

The `wallet.wasm` and `wallet.did` files here are built using `dfx build` in the
https://github.com/dfinity/cycles-wallet repo.

To build, clone that repo, run `dfx build`, then copy the `wallet.wasm`.  Or, run
./update-wallet from this directory.  For either, in order for `dfx build` to work,
you will first have to create the canister ids with `dfx start --background` and
`dfx canister create wallet`.

An issue was created to automate this using nix; https://github.com/dfinity-lab/sdk/issues/1078

# Building the Asset Canister

The `assetstorage.wasm.gz` and `assetstorage.did` files here are built using scripts/update-frontend-canister.sh in this repo.

To build, clone that repo, run `dfx build`, then copy the `.dfx/local/canisters/certified_assets/certified_assets.wasm`.

An issue was created to automate this using nix; https://github.com/dfinity-lab/sdk/issues/1078


-----------------------

/src/distributed/assetstorage.did:
-----------------------

type BatchId = nat;
type ChunkId = nat;
type Key = text;
type Time = int;

type CreateAssetArguments = record {
  key: Key;
  content_type: text;
  max_age: opt nat64;
  headers: opt vec HeaderField;
  enable_aliasing: opt bool;
  allow_raw_access: opt bool;
};

// Add or change content for an asset, by content encoding
type SetAssetContentArguments = record {
  key: Key;
  content_encoding: text;
  chunk_ids: vec ChunkId;
  sha256: opt blob;
};

// Remove content for an asset, by content encoding
type UnsetAssetContentArguments = record {
  key: Key;
  content_encoding: text;
};

// Delete an asset
type DeleteAssetArguments = record {
  key: Key;
};

// Reset everything
type ClearArguments = record {};

type BatchOperationKind = variant {
  CreateAsset: CreateAssetArguments;
  SetAssetContent: SetAssetContentArguments;

  SetAssetProperties: SetAssetPropertiesArguments;

  UnsetAssetContent: UnsetAssetContentArguments;
  DeleteAsset: DeleteAssetArguments;

  Clear: ClearArguments;
};

type CommitBatchArguments = record {
  batch_id: BatchId;
  operations: vec BatchOperationKind
};

type CommitProposedBatchArguments = record {
  batch_id: BatchId;
  evidence: blob;
};

type ComputeEvidenceArguments = record {
  batch_id: BatchId;
  max_iterations: opt nat16
};

type DeleteBatchArguments = record {
  batch_id: BatchId;
};

type HeaderField = record { text; text; };

type HttpRequest = record {
  method: text;
  url: text;
  headers: vec HeaderField;
  body: blob;
  certificate_version: opt nat16;
};

type HttpResponse = record {
  status_code: nat16;
  headers: vec HeaderField;
  body: blob;
  streaming_strategy: opt StreamingStrategy;
};

type StreamingCallbackHttpResponse = record {
  body: blob;
  token: opt StreamingCallbackToken;
};

type StreamingCallbackToken = record {
  key: Key;
  content_encoding: text;
  index: nat;
  sha256: opt blob;
};

type StreamingStrategy = variant {
  Callback: record {
    callback: func (StreamingCallbackToken) -> (opt StreamingCallbackHttpResponse) query;
    token: StreamingCallbackToken;
  };
};

type SetAssetPropertiesArguments = record {
  key: Key;
  max_age: opt opt nat64;
  headers: opt opt vec HeaderField;
  allow_raw_access: opt opt bool;
  is_aliased: opt opt bool;
};

type ConfigurationResponse = record {
  max_batches: opt nat64;
  max_chunks: opt nat64;
  max_bytes: opt nat64;
};

type ConfigureArguments = record {
  max_batches: opt opt nat64;
  max_chunks: opt opt nat64;
  max_bytes: opt opt nat64;
};

type Permission = variant {
  Commit;
  ManagePermissions;
  Prepare;
};

type GrantPermission = record {
  to_principal: principal;
  permission: Permission;
};
type RevokePermission = record {
  of_principal: principal;
  permission: Permission;
};
type ListPermitted = record { permission: Permission };

type ValidationResult = variant { Ok : text; Err : text };

type AssetCanisterArgs = variant {
  Init: InitArgs;
  Upgrade: UpgradeArgs;
};

type InitArgs = record {};

type UpgradeArgs = record {
  set_permissions: opt SetPermissions;
};

/// Sets the list of principals granted each permission.
type SetPermissions = record {
  prepare: vec principal;
  commit: vec principal;
  manage_permissions: vec principal;
};

service: (asset_canister_args: opt AssetCanisterArgs) -> {
  api_version: () -> (nat16) query;

  get: (record {
    key: Key;
    accept_encodings: vec text;
  }) -> (record {
    content: blob; // may be the entirety of the content, or just chunk index 0
    content_type: text;
    content_encoding: text;
    sha256: opt blob; // sha256 of entire asset encoding, calculated by dfx and passed in SetAssetContentArguments
    total_length: nat; // all chunks except last have size == content.size()
  }) query;

  // if get() returned chunks > 1, call this to retrieve them.
  // chunks may or may not be split up at the same boundaries as presented to create_chunk().
  get_chunk: (record {
    key: Key;
    content_encoding: text;
    index: nat;
    sha256: opt blob;  // sha256 of entire asset encoding, calculated by dfx and passed in SetAssetContentArguments
  }) -> (record { content: blob }) query;

  list : (record {}) -> (vec record {
    key: Key;
    content_type: text;
    encodings: vec record {
      content_encoding: text;
      sha256: opt blob; // sha256 of entire asset encoding, calculated by dfx and passed in SetAssetContentArguments
      length: nat; // Size of this encoding's blob. Calculated when uploading assets.
      modified: Time;
    };
  }) query;

  certified_tree : (record {}) -> (record {
    certificate: blob;
    tree: blob;
  }) query;

  create_batch : (record {}) -> (record { batch_id: BatchId });

  create_chunk: (record { batch_id: BatchId; content: blob }) -> (record { chunk_id: ChunkId });
  create_chunks: (record { batch_id: BatchId; content: vec blob }) -> (record { chunk_ids: vec ChunkId });

  // Perform all operations successfully, or reject
  commit_batch: (CommitBatchArguments) -> ();

  // Save the batch operations for later commit
  propose_commit_batch: (CommitBatchArguments) -> ();

  // Given a batch already proposed, perform all operations successfully, or reject
  commit_proposed_batch: (CommitProposedBatchArguments) -> ();

  // Compute a hash over the CommitBatchArguments.  Call until it returns Some(evidence).
  compute_evidence: (ComputeEvidenceArguments) -> (opt blob);

  // Delete a batch that has been created, or proposed for commit, but not yet committed
  delete_batch: (DeleteBatchArguments) -> ();

  create_asset: (CreateAssetArguments) -> ();
  set_asset_content: (SetAssetContentArguments) -> ();
  unset_asset_content: (UnsetAssetContentArguments) -> ();

  delete_asset: (DeleteAssetArguments) -> ();

  clear: (ClearArguments) -> ();

  // Single call to create an asset with content for a single content encoding that
  // fits within the message ingress limit.
  store: (record {
    key: Key;
    content_type: text;
    content_encoding: text;
    content: blob;
    sha256: opt blob
  }) -> ();

  http_request: (request: HttpRequest) -> (HttpResponse) query;
  http_request_streaming_callback: (token: StreamingCallbackToken) -> (opt StreamingCallbackHttpResponse) query;

  authorize: (principal) -> ();
  deauthorize: (principal) -> ();
  list_authorized: () -> (vec principal);
  grant_permission: (GrantPermission) -> ();
  revoke_permission: (RevokePermission) -> ();
  list_permitted: (ListPermitted) -> (vec principal);
  take_ownership: () -> ();

  get_asset_properties : (key: Key) -> (record {
    max_age: opt nat64;
    headers: opt vec HeaderField;
    allow_raw_access: opt bool;
    is_aliased: opt bool; } ) query;
  set_asset_properties: (SetAssetPropertiesArguments) -> ();

  get_configuration: () -> (ConfigurationResponse);
  configure: (ConfigureArguments) -> ();

  validate_grant_permission: (GrantPermission) -> (ValidationResult);
  validate_revoke_permission: (RevokePermission) -> (ValidationResult);
  validate_take_ownership: () -> (ValidationResult);
  validate_commit_proposed_batch: (CommitProposedBatchArguments) -> (ValidationResult);
  validate_configure: (ConfigureArguments) -> (ValidationResult);
}


-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/distributed/assetstorage.wasm.gz

-----------------------

/src/distributed/ui.did:
-----------------------

service : {
    did_to_js : (text) -> (opt text) query;
    binding : (text, text) -> (opt text) query;
    subtype : (new: text, old: text) -> (variant { Ok: null; Err: text }) query;
    merge_init_args : (text, text) -> (opt text) query
}


-----------------------

/src/distributed/ui.wasm:
-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/distributed/ui.wasm

-----------------------

/src/distributed/update-wallet:
-----------------------

#!/usr/bin/env bash

# Prerequisites: You must at least run `dfx start` and `dfx canister create wallet`
# in your cycles-wallet workspace

set -e

CYCLES_WALLET_DIR=${1?"Must specify path to a cycles-wallet workspace"}

(
    cd "${CYCLES_WALLET_DIR}"
    dfx build wallet
)

cp "${CYCLES_WALLET_DIR}"/.dfx/local/canisters/wallet/wallet.wasm wallet.wasm
cp "${CYCLES_WALLET_DIR}"/.dfx/local/canisters/wallet/wallet.did wallet.did



-----------------------

/src/distributed/wallet.did:
-----------------------

type EventKind = variant {
  CyclesSent: record {
    to: principal;
    amount: nat64;
    refund: nat64;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat64;
    memo: opt text;
  };
  AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat64;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat64;
  };
  WalletDeployed: record {
    canister: principal;
  }
};

type EventKind128 = variant {
  CyclesSent: record {
    to: principal;
    amount: nat;
    refund: nat;
  };
  CyclesReceived: record {
    from: principal;
    amount: nat;
    memo: opt text;
  };
    AddressAdded: record {
    id: principal;
    name: opt text;
    role: Role;
  };
  AddressRemoved: record {
    id: principal;
  };
  CanisterCreated: record {
    canister: principal;
    cycles: nat;
  };
  CanisterCalled: record {
    canister: principal;
    method_name: text;
    cycles: nat;
  };
  WalletDeployed: record {
    canister: principal;
  };
};

type Event = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind;
};

type Event128 = record {
  id: nat32;
  timestamp: nat64;
  kind: EventKind128;
};

type Role = variant {
  Contact;
  Custodian;
  Controller;
};

type Kind = variant {
  Unknown;
  User;
  Canister;
};

// An entry in the address book. It must have an ID and a role.
type AddressEntry = record {
  id: principal;
  name: opt text;
  kind: Kind;
  role: Role;
};

type ManagedCanisterInfo = record {
  id: principal;
  name: opt text;
  created_at: nat64;
};

type ManagedCanisterEventKind = variant {
  CyclesSent: record {
    amount: nat64;
    refund: nat64;
  };
  Called: record {
    method_name: text;
    cycles: nat64;
  };
  Created: record {
    cycles: nat64;
  };
};

type ManagedCanisterEventKind128 = variant {
  CyclesSent: record {
    amount: nat;
    refund: nat;
  };
  Called: record {
    method_name: text;
    cycles: nat;
  };
  Created: record {
    cycles: nat;
  };
};

type ManagedCanisterEvent = record {
  id: nat32;
  timestamp: nat64;
  kind: ManagedCanisterEventKind;
};

type ManagedCanisterEvent128 = record {
  id: nat32;
  timestamp: nat64;
  kind: ManagedCanisterEventKind128;
};

type ReceiveOptions = record {
  memo: opt text;
};

type WalletResultCreate = variant {
  Ok : record { canister_id: principal };
  Err: text;
};

type WalletResult = variant {
  Ok : null;
  Err : text;
};

type WalletResultCall = variant {
  Ok : record { return: blob };
  Err : text;
};

type WalletResultCallWithMaxCycles = variant {
  Ok : record {
    return: blob;
    attached_cycles: nat;
  };
  Err : text;
};

type CanisterSettings = record {
  controller: opt principal;
  controllers: opt vec principal;
  compute_allocation: opt nat;
  memory_allocation: opt nat;
  freezing_threshold: opt nat;
};

type CreateCanisterArgs = record {
  cycles: nat64;
  settings: CanisterSettings;
};

type CreateCanisterArgs128 = record {
  cycles: nat;
  settings: CanisterSettings;
};

// Assets
type HeaderField = record { text; text; };

type HttpRequest = record {
  method: text;
  url: text;
  headers: vec HeaderField;
  body: blob;
};

type HttpResponse = record {
  status_code: nat16;
  headers: vec HeaderField;
  body: blob;
  streaming_strategy: opt StreamingStrategy;
};

type StreamingCallbackHttpResponse = record {
  body: blob;
  token: opt Token;
};

type Token = record {};

type StreamingStrategy = variant {
  Callback: record {
    callback: func (Token) -> (StreamingCallbackHttpResponse) query;
    token: Token;
  };
};

service : {
  wallet_api_version: () -> (text) query;

  // Wallet Name
  name: () -> (opt text) query;
  set_name: (text) -> ();

  // Controller Management
  get_controllers: () -> (vec principal) query;
  add_controller: (principal) -> ();
  remove_controller: (principal) -> (WalletResult);

  // Custodian Management
  get_custodians: () -> (vec principal) query;
  authorize: (principal) -> ();
  deauthorize: (principal) -> (WalletResult);

  // Cycle Management
  wallet_balance: () -> (record { amount: nat64 }) query;
  wallet_balance128: () -> (record { amount: nat }) query;
  wallet_send: (record { canister: principal; amount: nat64 }) -> (WalletResult);
  wallet_send128: (record { canister: principal; amount: nat }) -> (WalletResult);
  wallet_receive: (opt ReceiveOptions) -> ();  // Endpoint for receiving cycles.

  // Managing canister
  wallet_create_canister: (CreateCanisterArgs) -> (WalletResultCreate);
  wallet_create_canister128: (CreateCanisterArgs128) -> (WalletResultCreate);

  wallet_create_wallet: (CreateCanisterArgs) -> (WalletResultCreate);
  wallet_create_wallet128: (CreateCanisterArgs128) -> (WalletResultCreate);

  wallet_store_wallet_wasm: (record {
    wasm_module: blob;
  }) -> ();

  // Call Forwarding
  wallet_call: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat64;
  }) -> (WalletResultCall);
  wallet_call128: (record {
    canister: principal;
    method_name: text;
    args: blob;
    cycles: nat;
  }) -> (WalletResultCall);
  wallet_call_with_max_cycles: (record{
    canister: principal;
    method_name: text;
    args: blob;
  }) -> (WalletResultCallWithMaxCycles);

  // Address book
  add_address: (address: AddressEntry) -> ();
  list_addresses: () -> (vec AddressEntry) query;
  remove_address: (address: principal) -> (WalletResult);

  // Events
  // If `from` is not specified, it will start 20 from the end; if `to` is not specified, it will stop at the end
  get_events: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event) query;
  get_events128: (opt record { from: opt nat32; to: opt nat32; }) -> (vec Event128) query;
  get_chart: (opt record { count: opt nat32; precision: opt nat64; } ) -> (vec record { nat64; nat64; }) query;

  // Managed canisters
  list_managed_canisters: (record { from: opt nat32; to: opt nat32; }) -> (vec ManagedCanisterInfo, nat32) query;
  // If `from` is not specified, it will start 20 from the end; if `to` is not specified, it will stop at the end
  get_managed_canister_events: (record { canister: principal; from: opt nat32; to: opt nat32; }) -> (opt vec ManagedCanisterEvent) query;
  get_managed_canister_events128: (record { canister: principal; from: opt nat32; to: opt nat32; }) -> (opt vec ManagedCanisterEvent128) query;
  set_short_name: (principal, opt text) -> (opt ManagedCanisterInfo);

  // Assets
  http_request: (request: HttpRequest) -> (HttpResponse) query;
}


-----------------------

https://raw.githubusercontent.com/dfinity/sdk/main/src/distributed/wallet.wasm.gz

-----------------------

/src/lib/apply-patch/Cargo.lock:
-----------------------

# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 3

[[package]]
name = "android-tzdata"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e999941b234f3131b00bc13c22d06e8c5ff726d1b6318ac7eb276997bbb4fef0"

[[package]]
name = "android_system_properties"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "819e7219dbd41043ac279b19830f2efc897156490d7fd6ea916720117ee66311"
dependencies = [
 "libc",
]

[[package]]
name = "apply-patch"
version = "0.1.0"
dependencies = [
 "patch",
]

[[package]]
name = "autocfg"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d468802bab17cbc0cc575e9b053f41e72aa36bfa6b7f55e3529ffa43161b97fa"

[[package]]
name = "bumpalo"
version = "3.13.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a3e2c3daef883ecc1b5d58c15adae93470a91d425f3532ba1695849656af3fc1"

[[package]]
name = "bytecount"
version = "0.6.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2c676a478f63e9fa2dd5368a42f28bba0d6c560b775f38583c8bbaa7fcd67c9c"

[[package]]
name = "cc"
version = "1.0.80"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "51f1226cd9da55587234753d1245dd5b132343ea240f26b6a9003d68706141ba"
dependencies = [
 "libc",
]

[[package]]
name = "cfg-if"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"

[[package]]
name = "chrono"
version = "0.4.26"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ec837a71355b28f6556dbd569b37b3f363091c0bd4b2e735674521b4c5fd9bc5"
dependencies = [
 "android-tzdata",
 "iana-time-zone",
 "js-sys",
 "num-traits",
 "time",
 "wasm-bindgen",
 "winapi",
]

[[package]]
name = "core-foundation-sys"
version = "0.8.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e496a50fda8aacccc86d7529e2c1e0892dbd0f898a6b5645b5561b89c3210efa"

[[package]]
name = "iana-time-zone"
version = "0.1.57"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2fad5b825842d2b38bd206f3e81d6957625fd7f0a361e345c30e01a0ae2dd613"
dependencies = [
 "android_system_properties",
 "core-foundation-sys",
 "iana-time-zone-haiku",
 "js-sys",
 "wasm-bindgen",
 "windows",
]

[[package]]
name = "iana-time-zone-haiku"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f31827a206f56af32e590ba56d5d2d085f558508192593743f16b2306495269f"
dependencies = [
 "cc",
]

[[package]]
name = "js-sys"
version = "0.3.64"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c5f195fe497f702db0f318b07fdd68edb16955aed830df8363d837542f8f935a"
dependencies = [
 "wasm-bindgen",
]

[[package]]
name = "libc"
version = "0.2.147"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b4668fb0ea861c1df094127ac5f1da3409a82116a4ba74fca2e58ef927159bb3"

[[package]]
name = "log"
version = "0.4.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b06a4cde4c0f271a446782e3eff8de789548ce57dbc8eca9292c27f4a42004b4"

[[package]]
name = "memchr"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2dffe52ecf27772e601905b7522cb4ef790d2cc203488bbd0e2fe85fcb74566d"

[[package]]
name = "minimal-lexical"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "68354c5c6bd36d73ff3feceb05efa59b6acb7626617f4962be322a825e61f79a"

[[package]]
name = "nom"
version = "7.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d273983c5a657a70a3e8f2a01329822f3b8c8172b73826411a55751e404a0a4a"
dependencies = [
 "memchr",
 "minimal-lexical",
]

[[package]]
name = "nom_locate"
version = "4.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b1e299bf5ea7b212e811e71174c5d1a5d065c4c0ad0c8691ecb1f97e3e66025e"
dependencies = [
 "bytecount",
 "memchr",
 "nom",
]

[[package]]
name = "num-traits"
version = "0.2.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f30b0abd723be7e2ffca1272140fac1a2f084c77ec3e123c192b66af1ee9e6c2"
dependencies = [
 "autocfg",
]

[[package]]
name = "once_cell"
version = "1.18.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dd8b5dd2ae5ed71462c540258bedcb51965123ad7e7ccf4b9a8cafaa4a63576d"

[[package]]
name = "patch"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "15c07fdcdd8b05bdcf2a25bc195b6c34cbd52762ada9dba88bf81e7686d14e7a"
dependencies = [
 "chrono",
 "nom",
 "nom_locate",
]

[[package]]
name = "proc-macro2"
version = "1.0.66"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "18fb31db3f9bddb2ea821cde30a9f70117e3f119938b5ee630b7403aa6e2ead9"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "quote"
version = "1.0.32"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "50f3b39ccfb720540debaa0164757101c08ecb8d326b15358ce76a62c7e85965"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "syn"
version = "2.0.28"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "04361975b3f5e348b2189d8dc55bc942f278b2d482a6a0365de5bdd62d351567"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "time"
version = "0.1.45"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1b797afad3f312d1c66a56d11d0316f916356d11bd158fbc6ca6389ff6bf805a"
dependencies = [
 "libc",
 "wasi",
 "winapi",
]

[[package]]
name = "unicode-ident"
version = "1.0.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "301abaae475aa91687eb82514b328ab47a211a533026cb25fc3e519b86adfc3c"

[[package]]
name = "wasi"
version = "0.10.0+wasi-snapshot-preview1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1a143597ca7c7793eff794def352d41792a93c481eb1042423ff7ff72ba2c31f"

[[package]]
name = "wasm-bindgen"
version = "0.2.87"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7706a72ab36d8cb1f80ffbf0e071533974a60d0a308d01a5d0375bf60499a342"
dependencies = [
 "cfg-if",
 "wasm-bindgen-macro",
]

[[package]]
name = "wasm-bindgen-backend"
version = "0.2.87"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5ef2b6d3c510e9625e5fe6f509ab07d66a760f0885d858736483c32ed7809abd"
dependencies = [
 "bumpalo",
 "log",
 "once_cell",
 "proc-macro2",
 "quote",
 "syn",
 "wasm-bindgen-shared",
]

[[package]]
name = "wasm-bindgen-macro"
version = "0.2.87"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dee495e55982a3bd48105a7b947fd2a9b4a8ae3010041b9e0faab3f9cd028f1d"
dependencies = [
 "quote",
 "wasm-bindgen-macro-support",
]

[[package]]
name = "wasm-bindgen-macro-support"
version = "0.2.87"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "54681b18a46765f095758388f2d0cf16eb8d4169b639ab575a8f5693af210c7b"
dependencies = [
 "proc-macro2",
 "quote",
 "syn",
 "wasm-bindgen-backend",
 "wasm-bindgen-shared",
]

[[package]]
name = "wasm-bindgen-shared"
version = "0.2.87"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ca6ad05a4870b2bf5fe995117d3728437bd27d7cd5f06f13c17443ef369775a1"

[[package]]
name = "winapi"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
dependencies = [
 "winapi-i686-pc-windows-gnu",
 "winapi-x86_64-pc-windows-gnu",
]

[[package]]
name = "winapi-i686-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"

[[package]]
name = "winapi-x86_64-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"

[[package]]
name = "windows"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e686886bc078bc1b0b600cac0147aadb815089b6e4da64016cbd754b6342700f"
dependencies = [
 "windows-targets",
]

[[package]]
name = "windows-targets"
version = "0.48.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "05d4b17490f70499f20b9e791dcf6a299785ce8af4d709018206dc5b4953e95f"
dependencies = [
 "windows_aarch64_gnullvm",
 "windows_aarch64_msvc",
 "windows_i686_gnu",
 "windows_i686_msvc",
 "windows_x86_64_gnu",
 "windows_x86_64_gnullvm",
 "windows_x86_64_msvc",
]

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "91ae572e1b79dba883e0d315474df7305d12f569b400fcf90581b06062f7e1bc"

[[package]]
name = "windows_aarch64_msvc"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b2ef27e0d7bdfcfc7b868b317c1d32c641a6fe4629c171b8928c7b08d98d7cf3"

[[package]]
name = "windows_i686_gnu"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "622a1962a7db830d6fd0a69683c80a18fda201879f0f447f065a3b7467daa241"

[[package]]
name = "windows_i686_msvc"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4542c6e364ce21bf45d69fdd2a8e455fa38d316158cfd43b3ac1c5b1b19f8e00"

[[package]]
name = "windows_x86_64_gnu"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ca2b8a661f7628cbd23440e50b05d705db3686f894fc9580820623656af974b1"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7896dbc1f41e08872e9d5e8f8baa8fdd2677f29468c4e156210174edc7f7b953"

[[package]]
name = "windows_x86_64_msvc"
version = "0.48.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1a515f5799fe4961cb532f983ce2b23082366b898e52ffbce459c86f67c8378a"


-----------------------

/src/lib/apply-patch/Cargo.toml:
-----------------------

[package]
name = "apply-patch"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
patch = "0.7.0"
thiserror.workspace = true


-----------------------

/src/lib/apply-patch/src/lib.rs:
-----------------------

/src/lib/apply-patch/tests/basic.rs:
-----------------------

use apply_patch::apply_to;
use patch::Patch;

#[test]
fn lao_tzu() {
    let patch = include_str!("example.patch");
    let lao = include_str!("lao.txt");
    let tzu = include_str!("tzu.txt");
    let patch = Patch::from_single(patch).unwrap();
    let patched = apply_to(&patch, lao).unwrap();
    assert_eq!(patched, tzu);
}


-----------------------

/src/lib/apply-patch/tests/example.patch:
-----------------------

--- lao	2002-02-21 23:30:39.942229878 -0800
+++ tzu	2002-02-21 23:30:50.442260588 -0800
@@ -1,7 +1,6 @@
-The Way that can be told of is not the eternal Way;
-The name that can be named is not the eternal name.
 The Nameless is the origin of Heaven and Earth;
-The Named is the mother of all things.
+The named is the mother of all things.
+
 Therefore let there always be non-being,
   so we may see their subtlety,
 And let there always be being,
@@ -9,3 +8,6 @@
 The two are the same,
 But after they are produced,
   they have different names.
+They both may be called deep and profound.
+Deeper and more profound,
+The door of all subtleties!


-----------------------

/src/lib/apply-patch/tests/lao.txt:
-----------------------

The Way that can be told of is not the eternal Way;
The name that can be named is not the eternal name.
The Nameless is the origin of Heaven and Earth;
The Named is the mother of all things.
Therefore let there always be non-being,
  so we may see their subtlety,
And let there always be being,
  so we may see their outcome.
The two are the same,
But after they are produced,
  they have different names.


-----------------------

/src/lib/apply-patch/tests/tzu.txt:
-----------------------

The Nameless is the origin of Heaven and Earth;
The named is the mother of all things.

Therefore let there always be non-being,
  so we may see their subtlety,
And let there always be being,
  so we may see their outcome.
The two are the same,
But after they are produced,
  they have different names.
They both may be called deep and profound.
Deeper and more profound,
The door of all subtleties!